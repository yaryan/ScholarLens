--- Page 1 --- The Cost of Robustness: Tighter Bounds on Parameter Complexity for Robust Memorization in ReLU Nets YujunKim∗ ChaewonMoon∗ ChulheeYun KAIST {kyujun02, chaewon.moon,  Abstract WestudytheparametercomplexityofrobustmemorizationforReLUnetworks: thenumberofparametersrequiredtointerpolateanygivendatasetwithϵ-separation betweendifferentlylabeledpoints,whileensuringpredictionsremainconsistent withinaµ-ballaroundeachtrainingsample. Weestablishupperandlowerbounds ontheparametercountasafunctionoftherobustnessratioρ=µ/ϵ. Unlikeprior work, weprovideafine-grainedanalysisacrosstheentirerangeρ ∈ (0,1)and obtain tighter upper and lower bounds that improve upon existing results. Our findingsrevealthattheparametercomplexityofrobustmemorizationmatchesthat ofnon-robustmemorizationwhenρissmall,butgrowswithincreasingρ. 1 Introduction Thetopicofmemorizationinvestigatestheexpressivepowerofneuralnetworksrequiredtofitany givendatasetexactly. Thislineofinquiryseekstodeterminetheminimalnetworksize—measured in the number of parameters, or equivalently, parameter complexity—needed to interpolate any finitecollectionofN labeledexamples. Anumberofworksstudybothupperandlowerbounds ontheparametercomplexity[Baum,1988,Yunetal.,2019,Bubecketal.,2020,Parketal.,2021]. √ TheVC-dimensionimpliesalowerboundofΩ( N)[Chervonenkis,2015,GoldbergandJerrum, √ 1995,Bartlettetal.,2019],whileVardietal.[2021]showthatΘ˜( N)parameterssufficeforReLU networks. Together, these results establish that memorizing any N distinct samples with ReLU √ networkscanbedonewithΘ˜( N)parameters,tightuptologarithmicfactors. Wenowturntoamorechallengingtaskbeyondmereinterpolationofdata:robustmemorization.We aimtoquantifytheadditionalparametercomplexityrequiredforanetworktoremainrobustagainst adversarialattacks,goingbeyondstandardnon-robustmemorization. Toaddressthesensitivityof neuralnetworkstosmalladversarialperturbations[Szegedyetal.,2014,Goodfellowetal.,2015,Ding etal.,2019,Gowaletal.,2021,Zhangetal.,2021,Bastounisetal.,2025],weconsiderthesetting inwhichnotonlythedatapointsbutallpointswithinadistanceµ—referredtoastherobustness radius—fromeachdatapointmustbemappedtothecorrespondinglabel. Moreconcretely,forany datasetwithϵ-separationbetweendifferentlylabeleddatapoints,thenetworkmustmemorizethe datasetandthepredictionmustremainconsistentwithinaµ-ballcenteredateachtrainingsample. As willbeseenshortly,theparametercomplexityforrobustmemorizationisgovernedbytherobustness ratioρ=µ/ϵ∈(0,1)ratherthantheindividualvaluesofµandϵ. However,apreciseunderstanding ofhowthiscomplexityscaleswithρremainslimited. ∗Authorscontributedequallytothispaper. 39thConferenceonNeuralInformationProcessingSystems(NeurIPS2025). 5202 tcO 82 ]GL.sc[ 1v34642.0152:viXra --- Page 2 --- 1.1 WhatisKnownSoFar? √ ExistingLowerBounds. SinceclassicalmemorizationrequiresΩ( N)parameters,itfollowsthat √ robustmemorizationmustalsosatisfyalowerboundofatleastΩ( N)parametersforanyρ∈(0,1). AlowerboundspecifictorobustmemorizationisestablishedbytheworkofLietal.[2022],which √ showsthatforinputdimensiond,Ω( Nd)parametersarenecessaryforrobustmemorizationunder ℓ -normforsufficientlylargeρ. However,theauthorsdonotcharacterizetherangeofρoverwhich 2 √ thislowerboundremainsvalid. OurProposition3.3presentedlatershowsthattheΩ( Nd)lower (cid:16)(cid:112) (cid:17) boundcanbeextendedtotherangeρ∈ 1−1/d,1 . Combiningtheseobservations,weobtain thefollowingunifiedlowerbound: supposethatforanydatasetDwithinputdimensiondandsizeN, thereexistsaneuralnetworkwithatmostP parametersthatrobustlymemorizesDwithrobustness ratioρunderℓ -norm. Then,thenumberofparametersP mustsatisfy 2 (cid:16)(cid:16) √ (cid:17)√ (cid:17) P =Ω 1+ d·1 √ N +d , (1) ρ≥ 1−1 d wherethedtermaccountsfortheparametersconnectedtotheinputneurons. Inthesettingd = √ √ √ O( N),thelowerboundsincreasediscontinuouslyfrom N to Nd. While our main analysis focuses on the ℓ -norm, there also exist results under the ℓ -norm. In 2 ∞ particular,Yuetal.[2024]showthatundertheℓ -normandcertainassumptions,ρ-robustmemo- ∞ rizationrequiresthefirsthiddenlayertohavewidthatleastd. Ouranalysisnotonlystrengthens butalsogeneralizesthisℓ -normresultbyremovingtheassumptiononthedataset—madeinprior ∞ work—thatthenumberofdatapointsmustbegreaterthand. ExistingUpperBounds. FromtheworkofYuetal.[2024],itisproventhatO(Nd2)parameters sufficeforanyρ ∈ (0,1). SeeAppendixD.2forananalysisoftheparametercomplexityoftheir (cid:16) (cid:17) construction. Furthermore,Egosietal.[2025]showthatforρ∈ 0,√1 ,anetworkofwidthlogN d sufficesforρ-robustmemorization. Althoughtheydonotexplicitlyquantifythetotalnumberof parameters,theirconstructionwithawidthlogN networkrequiresO˜(N)parameters,asweverifyin AppendixD.3. Additionally,westatethattheirconstructionimplicitlyyieldsasmoothinterpolation √ √ betweenO˜(N)andO˜(Nd2)asρvarieswithintheintermediaterange(1/ d,1/6d). Tosumup,theexistingupperboundstatesthatforanydatasetDwithinputdimensiondandsize N,thereexistsaneuralnetworkthatachievesrobustmemorizationonDwiththerobustnessratioρ underℓ -norm,withthenumberofparametersP boundedasfollows: 2 √  O˜(N +d) ifρ∈(0,1/ d].  √ √ P = O˜(Nd3ρ6+d) ifρ∈(1/ d,1/6d]. (2) √ O˜(Nd2) ifρ∈(1/6d,1). Whend=O(N),theupperboundtransitionscontinuouslyfromO˜(N)toO˜(Nd2). 1.2 SummaryofContribution WeinvestigatehowthenumberofparametersrequiredforrobustmemorizationinReLUnetworks varieswiththerobustnessratioρ. Weimprovebothupperandlowerboundsontheminimalnumber ofparametersoverallpossibleρ∈(0,1),whicharetightinsomeregimesandsubstantiallyreduce theexistinggapelsewhere. TheimprovementacrossdifferentregimesofρisvisualizedinFigure1. • NecessaryConditionsforRobustMemorization. Weshowthatthefirsthiddenlayermusthave a width of at least ρ2min{N,d}, by constructing a dataset that cannot be robustly memorized usingasmallerwidth. Consequently,thenetworkmusthaveatleastΩ(ρ2min{N,d}d)parameters. (cid:112) (cid:112) Moreover,weprovethatatleastΩ( N/(1−ρ2))parametersarenecessaryforρ≤ 1−1/d byanalyzingtheVC-dimension. Combiningthesetworesults,weobtainatighterlowerboundon theparametercomplexityofrobustmemorizationoftheform (cid:32) (cid:40) (cid:41) (cid:33) 1 √ √ P =Ω (ρ2min{N,d}+1)d+min , d N . (cid:112) 1−ρ2 2 --- Page 3 --- Nd2 ExistingUpperBound N √ Our Upper Bound Our Lower Bound N ExistingLowerBound N1√ d √1 d √61 d(cid:113) 1− d11 Robustnessratioρ sretemaraP √ Figure1: Summaryofparameterboundsonalog-logscalewhend=Θ( N). Weomitconstant factors in both axes. Solid blue and red curves show the sufficient (Theorem 4.2) and necessary (Theorem3.1)numbersofparameters,respectively;thesolidblackcurvesarethebestpriorbounds. Light-blueshadinghighlightsourimprovementintheupperbound,andlight-redshadinghighlights ourimprovementinthelowerbound. Thecross-hatchedareamarkstheremaininggap. Notably, thisgapdisappearsinthesmallestρregime. Theyellowandgreendashedlinedenotesthefirstterm (Proposition3.2)andthesecondterm(Proposition3.3)inTheorem3.1,respectively. • Sufficient Conditions for Robust Memorization. We establish improved upper bounds on theparametercountbyanalyzingthreedistinctregimesofρ,tighteningtheboundineachcase. (cid:16) (cid:105) √ Forρ ∈ 0, 1√ ,weachieverobustmemorizationusingO˜( N)parameters,matchingthe 5N d (cid:16) (cid:105) existinglowerbound. Forρ∈ 1√ , √1 ,weobtainrobustmemorizationwithO˜(Nd1/4ρ1/2) 5N d 5 d parametersuptoanarbitrarilysmallerror,whichinterpolatesbetweentheexistinglowerbound √ (cid:16) (cid:17) Ω( N)andtheexistingupperboundO˜(N). Finally,forlargervaluesofρ,whereρ∈ √1 ,1 , 5 d robust memorization is achieved with O˜(Nd2ρ4) parameters, which interpolates between the existingupperboundO˜(N)andO˜(Nd2). Alltogether, weprovide, tothebestofourknowledge, thefirsttheoreticalanalysisofparameter complexityforrobustmemorizationthatcharacterizesitsdependenceontherobustnessratioρover theentirerangeρ∈(0,1). Notably,whenρ< 1√ ,thesamenumberofparametersasinclassical 5N d (non-robust)memorizationsufficesforrobustmemorization. Theseresultssuggestthat,interms ofparametercount,achievingrobustnessagainstadversarialattacksisrelativelyinexpensivewhen therobustnessradiusissmall. Astheradiusgrows,however, thenumberofrequiredparameters increases,reflectingtherisingcostofachievingstrongerrobustness. 2 Preliminaries 2.1 Notation Throughoutthepaper,weusedtodenotetheinputdimensionofthedata,N todenotethenumberof datapointsinadataset,andC todenotethenumberofclassesforaclassificationtask. Foranatural numbern∈N,[n]denotestheset{1,2,...,n}. For two sets A,B ⊆ Rd, we denote the ℓ -norm distance between A and B as dist (A,B) := 2 2 inf{∥a−b∥ | a∈A,b∈B},where∥·∥ denotestheEuclideannorm. WheneitherAorBisa 2 2 singletonset,suchas{a}or{b},weidentifythesetwiththeelementandwriteaorbinplaceofA orB,respectively;forexample,dist (a,B). Inthecased=1,weomitthesubscript2andwrite 2 dist(·,·)todenotethestandardabsolutedistanceonR. WeuseB (x,µ)={x′ |∥x′−x∥ <µ} 2 2 todenoteanopenEuclideanballcenteredatxwitharadiusµ. 3 --- Page 4 --- WeuseO˜(·)tohidethepoly-logarithmicdependenciesinproblemparameterssuchasN,d,andρ. 2.2 DatasetandRobustMemorization For d ≥ 1 and N ≥ C ≥ 2, let D be the collection of all datasets of the form D = d,N,C {(x ,y )}N ⊂Rd×[C],suchthatx ̸=x foralli̸=j andhasatleastonedatapointpereach i i i=1 i j class label. Hence, any D ∈ D is apairwise distinctd-dimensionaldataset of sizeN with d,N,C labelsin[C]. Definition2.1. ForD ∈D ,theseparationconstantϵ isdefinedas d,N,C D 1 ϵ := min{∥x −x ∥ |(x ,y ),(x ,y )∈D, y ̸=y }. D 2 i j 2 i i j j i j Sincethedatasetsweconsiderhaveatleastonedatapointforeachclasslabel,thesetweminimize overisnonempty. Moreover,sinceweconsiderDwithx ̸=x foralli̸=j,wehaveϵ >0. Next, i j D wedefinerobustmemorizationofthegivendataset. Definition2.2. ForD ∈D andagivenrobustnessratioρ∈(0,1),definetherobustnessradius d,N,C asµ:=ρϵ . Wesaythatafunctionf :Rd →Rρ-robustlymemorizesDif D f(x′)=y , forall(x ,y )∈Dandx′ ∈B (x ,µ), i i i 2 i andB (x ,µ)isreferredtoastherobustnessballofx . 2 i i Whenρ=0,robustmemorizationreducestoclassicalmemorization,whichrequiresf(x )=y for i i all(x ,y ) ∈ D. Weemphasizethattherangeρ ∈ (0,1)coverstheentireregimeinwhichrobust i i memorizationispossible. Specifically,forρ>1,requiringmemorizationofρϵ -radiusneighbor D of each data point leads to a contradiction as B (x ,ρϵ )∩B (x ,ρϵ ) ̸= ∅ for some y ̸= y . 2 i D 2 j D i j Moreover,ifρ = 1,anycontinuousfunctionf cannotρ-robustlymemorizeD. Iff iscontinuous and1-robustlymemorizesD,wehavef(B (x ,ϵ )) = {y }foralli ∈ [N],whereB (x ,ϵ )is 2 i D i 2 i D theclosedballwithcenterx andradiusϵ . SinceB (x ,ϵ )∩B (x ,ϵ )̸=∅forsomey ̸=y , i D 2 i D 2 j D i j thisleadstoacontradiction. 2.3 ReLUNeuralNetwork Wedefinetheneuralnetworkf recursivelyoverLlayers: a (x)=x, 0 a (x)=σ(W a (x)+b ) forℓ=1,2,...,L−1, ℓ ℓ ℓ−1 ℓ f(x)=W a (x)+b , L L−1 L wheretheactivationσ(u):=max{0,u}istheelement-wiseReLU. Weused ,...,d todenote 1 L−1 thewidthsoftheL−1hiddenlayers. Wedefinethewidthofthenetworktobethemaximumhidden layer width, max ℓ∈[L−1]d ℓ. For ℓ ∈ [L], the symbols W ℓ ∈ Rdℓ×dℓ−1 and b ℓ ∈ Rdℓ denote the weightmatrixandthebiasvectorfortheℓ-thlayer,respectively;here,weusetheconventiond =d 0 andd =1. L WecountthenumberofparametersP off asthecountofallentriesintheweightmatricesand biases{W ,b }L (includingentriessettozero),as ℓ ℓ ℓ=1 L (cid:88) P = (d +1)·d . (3) ℓ−1 ℓ ℓ=1 Thisreflectsthecommonconventionofparametercountinginpractice. Thesetofneuralnetworks withinputdimensiondandatmostP parametersisdenotedas F =(cid:8) f :Rd →R|f isaneuralnetworkwithatmostP parameters(cid:9) . (4) d,P Althoughlessrelevantinpractice,somepriorworkcountsonlynonzeroentrieswhenreportingthe numberofparameters. AppendixEadoptsthisalternativecountingschemeandexplainshowour resultstranslateunderit,enablingcomparisonswithpriorstudiesfromadifferentperspective. Even then,thekeyfindingsofthispaperremaintrue:forsmallρ,robustnessincursnoadditionalparameter cost,whereasasρgrows,thenumberofrequiredparametersincreases. 4 --- Page 5 --- 2.4 WhyOnlyρ=µ/ϵ Matters D Wedescribebothnecessaryandsufficientconditionsforrobustmemorizationintermsoftheratio ρ=µ/ϵ ,ratherthandescribingitintermsofindividualvaluesµandϵ . Thisisbecausetheresults D D remaininvariantunderscalingofthedataset. Specificallyregardingthesufficientcondition,supposef ρ-robustlymemorizesDwithrobustness radius µ = ρϵ . Then for any c > 0, the scaled dataset cD := {(cx ,y )}N , whose separa- D i i i=1 tion ϵ = cϵ , can be ρ-robustly memorized with robustness radius cµ by the scaled function cD D x(cid:55)→ f(1x). Moreover,thescaledfunctioncanbeimplementedthroughanetworkwiththesame c numberofparametersastheneuralnetworkf viascalingthefirsthiddenlayerweightmatrixby1/c. Ontheotherhand,thisimpliesthatthenecessaryconditioncanalsobecharacterizedintermsofρ. SupposewehaveadatasetDwithafixedϵ forwhichρ-robustlymemorizingitrequiresacertain D numberofparametersP. Then,thescaleddatasetcDwithaseparationϵ =cϵ alsorequiresthe cD D samenumberofparametersforρ-robustmemorization. IfcD canbeρ-robustlymemorizedwith lessthan P parameters, thenbyparameter rescalingfromthe previousparagraph, D canalso be ρ-robustlymemorizedwithlessthanP parameters,leadingtoacontradiction. Hence, the robustness ratio ρ = µ/ϵ captures the essential difficulty of robust memorization, D independentofscaling. Wehenceforthstateourupperandlowerboundsintermsofρ. 3 NecessaryNumberofParametersforRobustMemorization In this section, we establish necessity conditions on the number of parameters and the width of neuralnetworksforrobustmemorization,expressedintermsoftherobustnessratioρ∈(0,1). The following theorem presents our main lower bound result on the parameter complexity of robust memorization. Theorem3.1. Letρ∈(0,1). SupposeforanyD ∈D ,thereexistsaneuralnetworkf ∈F d,N,2 d,P thatcanρ-robustlymemorizeD. Then,thenumberofparametersP mustsatisfy (cid:32) (cid:40) (cid:41) (cid:33) 1 √ √ P =Ω (ρ2min{N,d}+1)d+min , d N . (cid:112) 1−ρ2 TheproofofTheorem3.1isprovidedinAppendixA.1. Thetheoremstatesanecessarycondition onthenumberofparametersforbinaryclassification(C =2). ThesameboundappliestoC >2: anyclassifierthatrobustlymemorizesamulticlassdatasetcanbeconvertedintoaone-vs-restbinary classifier by appending a final two-parameter layer (one weight and one bias) that separates a designatedlabelfromtheothers. Therefore,amulticlasstaskrequiresatleasttheparameterscale neededforthebinarycase. Hence,Theorem3.1extendstoC >2. Moreover,whileTheorem3.1 focusesonℓ -norm,weextendthenecessityresultstogeneralℓ -norminTheoremC.5. Thelower 2 p boundonthenumberofparametersconsistsoftwoparts: onederivedfromtherequirementonthe firsthiddenlayerwidthandtheotherfromtheVC-dimension. First Term: Necessary Condition by the First Hidden Layer Width. The first term Ω((ρ2min{N,d}+1)d)comesfromthefollowingpropositiononthefirsthiddenlayerwidth. Proposition3.2. ThereexistsD ∈D suchthat,foranyρ∈(0,1),anyneuralnetworkf :Rd → d,N,2 Rthatρ-robustlymemorizesDmusthavethefirsthiddenlayerwidthatleastρ2min{N −1,d}. ForanyfixedN,d, wecanchooseasingledatasetD thatenforcestheboundsimultaneouslyfor all ρ ∈ (0,1): every ρ-robust memorizer of D must have the first hidden layer width at least ρ2min{N −1,d}. Section5.1treatsthesimplecaseN −1=dtoillustratetheconstructionand provideasketchofproof,whileAppendixA.2providesthefullproofforthegeneralcase. Proposition3.2fortheℓ -normextendstothegeneralℓ -norminPropositionC.6. Foreveryp≥2, 2 p thesamelowerboundonthefirsthiddenlayerwidth,ρ2min{N −1,d},holds. For1 ≤ p < 2,a nontriviallowerboundstillholds. Furthermore,fortheℓ -norm,westrengthentheresultofYuetal. ∞ [2024]—whiletheyshowthatwidthatleastdisnecessarywhenN >dandρ≥0.8,weobtainthe strongerwidthrequirementmin{N −1,d}foranyρ∈(1/2,1),withouttheassumptionN >d,as formalizedinPropositionC.7. 5 --- Page 6 --- WenowdiscusstheimplicationsofProposition3.2ontheparametercomplexityinTheorem3.1. Sincetheinputdimensionisd,anyneuralnetworkf :Rd →Rwiththefirsthiddenlayerwidthm musthaveatleastmdparameters. Moreover,wehaveatriviallowerboundm≥1. Hence,thelower boundofwidthmbecomesmax{ρ2min{N −1,d},1} ≥ 1(ρ2min{N −1,d}+1), yieldinga 2 necessityofΩ((ρ2min{N,d}+1)d)parametersinTheorem3.1. ThewidthfromProposition3.2 (cid:112) dominatesoverthetriviallowerboundof1wheneverρ≥1/ min{N −1,d}. Let us compare the result with Egosi et al. [2025], where they show logarithmic width in N is √ sufficient under the restricted condition of ρ ≤ 1/ d for robust memorization. Our necessary conditiononwidthdoesnotconflictwiththeirlogarithmicsufficiency,astheirsufficiencyholdsonly √ underρ≤1/ d,inwhichourlowerboundbecomestrivial. On the other hand, the necessary condition on width by Egosi et al. [2025] given as 2logN/log(4832ρ−1) exceeds the trivial lower bound 1 only when ρ ≥ 4832/N. Even in the casewheretheirlowerboundbecomesnontrivial,theirboundisstillattheO˜(1)scale,sothatour lowerboundeitherbecomestighterormatchestheirbounduptoapolylogarithmicfactoroverall ρ∈(0,1).Asasidenote,althoughwegenerallyignorepolylogarithmicfactors,wemayalsoconsider logarithmictermsforcompleteness. Underthisconsideration,thelowerboundofEgosietal.[2025] (cid:112) remainslogarithmicallynontrivialwhileoursremainstrivialfor4832/N <ρ<1/ min{N −1,d}, providedthatsuchρexists. SecondTerm: NecessaryConditionbytheVC-Dimension. Now,letuslookatthenecessary numberofparametersgivenbytheVC-dimensionofthefunctionclass. (cid:16) (cid:113) (cid:105) Proposition3.3. Letρ ∈ 0, 1− 1 . SupposeforanyD ∈ D ,thereexistsf ∈ F that d d,N,2 d,P ρ-robustlymemorizesD. Then,thenumberofparametersP mustsatisfy (cid:32)(cid:115) (cid:33) N P =Ω . 1−ρ2 ThedetailedproofofProposition3.3isinAppendixA.3anditsextensiontotheℓ -normappearsin p PropositionC.8.Beforepresentingourapproach,webrieflyreviewhowtheexistingboundisobtained usingVC-dimensionarguments. Gaoetal.[2019],Lietal.[2022]provethatforsufficientlylarge ρ,wheneverF containsρ-robustmemorizerofanyD ∈D ,thenVC-dim(F )=Ω(Nd). d,P d,N,2 d,P CombiningthiswithaknownupperboundVC-dim(F )=O(P2)[GoldbergandJerrum,1995], √ d,P theyobtainP =Ω( Nd). √ However,thepriorlowerboundΩ( Nd)isonlyknowntoapplyforsufficientlylargeρ,without specifyingthepreciserange. Beforeourresult,theonlylowerboundapplicabletoallρ—including √ smallρregime—wastheonethattriviallycomesfromnon-robustmemorization: Ω( N). Awide rangeofρlacksaVC-dimension-basedlowerboundtailoredtorobustmemorization. In Proposition 3.3, we carefully characterize how the VC-dimension scales over the range (cid:112) ρ∈ (0, 1−1/d]. Inthisrangeofρ,weshowwheneverF d,P containsρ-robustmemorizerofany (cid:112) D ∈D d,N,2,thenVC-dim(F d,P)=Ω(N/1−ρ2);thisthusgive √sthetighterboundP =Ω( N/1−ρ2). (cid:112) Attheendpointρ= 1−1/d,Proposition3.3impliesthatΩ( Nd)parametersarerequired. There- (cid:112) fore,thesamelowe √rboundappliesforallρ ≥ 1−1/d,characterizingtheregimeinwhichthe (cid:112) exi √sting bound of Nd ho (cid:112)lds. By combining Proposition 3.3 over ρ ∈ (0 √, 1− √1/d] √and the Ω( Nd)boundoverρ∈ ( 1−1/d,1),weobtainthesecondtermΩ(min{1/ 1−ρ2, d} N)in Theorem3.1. (cid:112) (cid:112) Finally, we clarify why Proposition 3.3 i √s stated for ρ ≤ 1−1/d and why, for ρ > 1−1/d, this approach cannot improve upon the Nd scale. Any such improvement via VC-dimension wouldrequireshowingthatVC-dim(F )strictlyexceedsNd,i.e.,thataρ-robustmemorizerinRd d,P shattersmorethanNdpoints. Ourshatteringargumentshowsthatrobustlymemorizingtwoarbitrary pointsforcesshatteringof(asubsetof)thestandardbasisdirectionsinRd;iteratingoverN/2disjoint pairscanyieldNd/2shatteredpoints. Consequently,ourcurrentconstructionneitherestablishesthat arobustmemorizerofN pointscanshatterbeyondtheNdscale,northatarobustmemorizerof twopointscanshatterbeyondthedscale. Thus,withinthisframework,theVC-dimensioncannot 6 --- Page 7 --- bepushedbeyondNdscale,andtheinducedparameterlowerbounddoesnotimprovebeyondthe √ (cid:112) Ndscaleforρ> 1−1/d. 4 SufficientNumberofParametersforRobustMemorization Inthissection,weestablishsufficientconditionsonthenumberofparametersforrobustmemorization, therebycomplementingthelowerboundspresentedintheprevioussection. Infact,oneofourupper bound results is derived under a relaxed definition of robust memorization. For this, we define ρ-robustmemorizationerrorofaneuralnetwork. Definition 4.1. For any D ∈ D , we define the ρ-robust memorization error of a network d,N,C f :Rd →RonDas L (f,D):= max P [f(x′)̸=y ], ρ (xi,yi)∈D x′∼Unif(B(xi,µ)) i whereµ=ρϵ . WhenL (f,D)<η,wesayf canρ-robustlymemorizeDwitherroratmostη. D ρ Notethatifanetworkf ρ-robustlymemorizesD(asinDefinition2.2),thentheerroriszero;thatis, bydefinitionL (f,D)=0. ρ Wenowstateourmainupperbounds,showingthatanygivendatasetinD canbeρ-robustly d,N,C memorizedbyanetworkwithρ-dependentnumberofparameters. Theorem4.2. ForanydatasetD ∈D andη ∈(0,1),thefollowingstatementshold: d,N,C (cid:16) (cid:105) √ (i) Ifρ∈ 0, 1√ ,thereexistsf ∈F d,P withP =O˜( N)thatρ-robustlymemorizesD. 5N d (cid:16) (cid:105) (ii) Ifρ∈ 1√ , √1 ,thereexistsf ∈F d,P withP =O˜(Nd1 4ρ1 2)thatρ-robustlymemorizes 5N d 5 d Dwitherroratmostη. (cid:16) (cid:17) (iii) Ifρ∈ √1 ,1 ,thereexistsf ∈F d,P withP =O˜(Nd2ρ4)thatρ-robustlymemorizesD. 5 d Wenotethatweomittedthetrivialadditivefactordthataccountsforparametersconnectedtoinput neurons. ThethreeregimesinTheorem4.2—eachreferredtoassmall,moderate,andlargeρregime respectively—collectivelycoverallvaluesofρ∈(0,1)andprovideexplicitupperboundcomplexity for robust memorization. Moreover, the constructions behind Theorem 4.2 use a single network architecturethatdependsonlyontheproblemparametersN,d,C,ρandnotonthedataset: forevery D ∈D andgivenρ,choosingappropriateweightsandbiasesonthissamearchitectureachieves d,N,C thestatedguarantee. We present a proof sketch in Section 5.2 and the detailed proof in Appendix B. The extended version of Theorem 4.2, which additionally states the explicit bounds on depth, width, and bit complexityispresentedasTheoremB.1. Importantly,theupperboundonthenumberofparameters inTheorem4.2doesnotcomeatthecostofimplausiblebitcomplexity. Infact,RemarkB.2shows thattheconstructionsinTheorem4.2(i)and4.2(ii)canbeimplementedwithbitcomplexitiesthat matchthenecessarybitcomplexityrequiredfornetworkswiththestatedparametercounts. The extensionofTheorem4.2totheℓ -normsettingisgiveninTheoremC.11. p In contrast to prior results, Theorems 4.2(i) and 4.2(ii) provide the first upper bounds for robust memorizationthataresublinearinN. Notably,ourconstructionrevealsacontinuousinterpolation— √ driven by the robustness ratio ρ—from the classical memorization complexity of Θ( N) to the existingupperboundofO˜(N)inTheorem4.2(ii),andfurtherfromO˜(N)toO˜(Nd2)asshownin Theorem4.2(iii). Thisdemonstrateshowthesufficientparametercomplexityincreasesgradually withρ,capturingthefullspectrumoftherobustnessratio. TightBoundsforRobustMemorizationwithSmallρ. Theorem4.2(i)establishesatightupper √ boundO˜( N)onthenumberofparametersrequiredforrobustmemorizationwhentherobustness ratiosatisfiesρ < 1√ . SinceVC-dimensiontheory[GoldbergandJerrum,1995]impliesthat 5N d √ anynetworkexactlymemorizinggivenN arbitrarysamplesmustuseatleastΩ( N)parameters, our construction is optimal up to logarithmic factors. This shows that, for sufficiently small ρ, √ robust memorization requires the same parameter complexity Θ˜( N) as classical (non-robust) memorization. 7 --- Page 8 --- PerfectRobustMemorizationwithThresholdActivationFunction. Theorem4.2(ii)buildsupon thetechniquesinTheorem4.2(i),extendingtheapplicabilityfromsmallvaluesofρtomoderate ones. However, theextensionrequirestheallowanceofanarbitrarilysmallrobustmemorization error. AsdiscussedinSection5.2andshownFigure4,theerrorarisesbecauseReLU-onlynetworks canrepresentonlycontinuousfunctions. Neardiscontinuoustransitionregions, theyincursmall errors—thoughthesecanbemadearbitrarilysmall.Incontrast,ifweareallowedtousediscontinuous thresholdactivationincombinationwithReLUnetwork,wecanachieveρ-robustmemorization— andthereforezerorobustmemorizationerror—eveninthemoderateregimeusingO˜(Nd1/4ρ1/2) parameters,thesamerateasTheorem4.2(ii). TightBoundsofWidth. Forsmallandmoderateρ,ourconstructionshowswidthO˜(1)issufficient, recoveringthelogarithmicwidthsufficiencyofEgosietal.[2025]. Forlargeρ, ourconstruction shows width of O˜(ρ2d) is sufficient for ρ-robust memorization. A complementary lower bound (Proposition3.2)requireswidthatleastρ2min{N −1,d}isalsonecessary,whichmatcheswithour upperboundwhenN >d. Asaresult,whenthenumberofdatapointsexceedsthedatadimension, ourresultstightlycharacterizetherequiredwidthuptopolylogarithmicfactorsacrosstheentirerange ρ∈(0,1). 5 KeyProofIdeas Inthissection,weoutlinethesketchofproofforsomeoftheresultsfromSections3and4. 5.1 ProofSketchforProposition3.2 WebrieflyoverviewthesketchoftheproofforProposition3.2. Forsimplicity,wesketchthecase N =d+1,whereProposition3.2reducestoshowingthatthefirsthiddenlayermusthavewidthat leastρ2d. Tothisend,weconstructthedatasetD ={(e ,1)} ∪{(0,2)},assigninglabel1to j j∈[d] thestandardbasispointsandlabel2totheorigin,asshowninFigure2a. Letf beanρ-robustmemorizerofDwiththefirsthiddenlayerwidthm,andletW ∈Rm×ddenote theweightmatrixofthefirsthiddenlayer. Sinceϵ =1/2,therobustnessradiusisµ=ρϵ =ρ/2. D D Foranyj ∈ [d],takeanyx ∈ B (e ,µ)andx′ ∈ B (0,µ). Then,f(x) = 1andf(x′) = 2must 2 j 2 hold,implyingWx ̸= Wx′. Therefore,x−x′ shouldnotlieinthenullspaceofW. Allsuch possibledifferencesx−x′formaballofradius2µaroundeachstandardbasispoint,illustratedas thegrayballinFigure2b. Thus,thedistancebetweeneachstandardbasispointandthenullspaceof W mustbeatleast2µ;otherwise,somegrayballsintersectwiththenullspace. ThenullspaceofW isad−mdimensionalspace,assumingthatW hasfullrowrank. (Thefull proofgeneralizesevenwithoutthisassumption.) ByLemmaA.1,thedistancebetweenthesetof (cid:112) standardbasispointsandanysubspaceofdimensiond−misatmost m/d. Therefore,wehave (cid:0) (cid:1) (cid:112) ρ = 2µ ≤ dist {e } ,Null(W) ≤ m/d and thus the first hidden layer width satisfies 2 j j∈[d] m≥ρ2d. z e 3 z e 3 dist (e ,Null(W)) 2 2 e 2 y e 2µ 1 0 e 2 µy x e 1 x Null(W)={(x,y,z)∈R3 | z =x+y} (a)DatasetforProposition3.2(d=3). (b)Null(W)⊂R3andthestandardbasis Figure2: In(a),blueballshavelabel1;theredballhaslabel2. (b)illustratesthedistancebetween Null(W)⊂R3andthestandardbasisforW =[1 1 −1]withthefirsthiddenlayerwidth1. 8 --- Page 9 --- 5.2 ProofSketchforTheorem4.2 WenowhighlightthekeyconstructiontechniquesusedtoproveTheorem4.2. Separation-Preserving Dimensionality Reduction. Rd Rm All three results in Theorem 4.2 leverage a strength- enedversionoftheJohnson-Lindenstrauss(JL)lemma (LemmaB.18)toprojectdatafromahigh-dimensional µ µ space Rd (left in Figure 3) to a lower-dimensional spaceRm(right),whilepreservingpairwisedistances 2ϵ D 4(cid:112)mϵ uptoamultiplicativefactor. Specifically,anypairof 5 d D pointsthatare2ϵ -separatedinRdcanremainatleast D 4(cid:112)mϵ -separated after the projection. Meanwhile, 5 d D Figure3: Separation-PreservingProjection eachrobustnessballofradiusµispreservedunderthe projectionbecauseourstrengthenedJLlemmausesrandomizedorthonormalprojections[Matousek, 2013]. Sincethegeometryispreserved—specifically,theseparationremainsatleast 4(cid:112)m timesits 5 d originalvalueandtherobustnessradiusisunchangedunderprojection—wecanρ-robustlymemorize data points in Rd by projecting them to Rm and memorizing the projected points, provided that projectedrobustnessballsdonotoverlap,i.e.,aslongasρ≤ 2(cid:112)m. 5 d InTheorems4.2(i)and4.2(ii),weprojecttoRmwithm=O(logN)inthefirsthiddenlayer. The remaininglayershavewidthO(m),sothenetworkwidthisO(m) = O(logN),i.e.,constantup √ topolylogarithmicfactors. Thislogarithmicprojectionisvalidonlyforρ = O(1/ d): projected √ ρ-ballsremaindisjointaslongasρ≤ 2(cid:112) m/d=O˜(1/ d). Ifρexceedsthisscale,theprojected 5 balls overlap. For the larger-ρ regime, Theorem 4.2(iii) increases the projection dimension. As longasρ≤ 2(cid:112) m/d,theprojectedrobustnessballsremaindisjoint;accordingly,takingm∝ρ2d 5 maintainsdisjointness. Consequently,thewidthisproportionaltoρ2d,andtheparametercountis proportionaltoρ4d2. Theideaofseparation-preservingdimensionreductionandderivingconditionsunderwhichrobust- nessballsremaindisjointafterprojectionisconcurrentlyproposedbyEgosietal.[2025]. However, theirapproachtoensuringtheseparabilityofrobustnessballsissubstantiallydifferentfromours. SincetheclassicalJLlemmadoesnotinherentlyguaranteethepreservationofballseparability,the authorsdonotrelyontheJLlemmadirectly. Instead,theyestablishaprobabilisticanaloguethrough a technically involved analysis that bounds the probability that a random projection satisfies the requiredseparationproperty. Incontrast,weemployastrengthenedversionoftheJLlemmaandgive astraightforwardproofthatthereexistsaprojectionpreservingseparability;seeAppendixB.5. √ MappingtoLatticesfromGrid. ForTheorem4.2(i)and4.2(ii),weutilizetheO˜( N)-parameter memorizationdevisedbyVardietal.[2021]. Inordertoadoptthetechnique,itisnecessarytoassign ascalarvalueinRtoeachdatapoint. Thisisbecausetheconstructionmemorizesthedataafter projectingthemontoR. Furthermore,thisscalarassignmentmustmeaningfullyreflectthespatial structureofthedata—preservingrelativedistancesandneighborhoodrelationshipsofrobustnessball. We achieve this using grid-based lattice mapping. Specifically, we first reduce the dimension to m = O(logN). Then we partition Rm into a regular grid, and assign an integer index to each (cid:81) grid cell. Through this grid indexing, we map each unit cube [z ,z + 1) to an index j∈[m] j j z Rm−1+z Rm−2+···+z foreachz =(z ,··· ,z )∈Zmandsomesufficientlylargeinteger 1 2 m 1 m R. Finally,weassociateeachindexwiththelabeloftheprojectedrobustnessballcontainedinthat cell. Thenetworkthenmemorizesthemappingfromeachgridindextoitscorrespondinglabel. UndertheconditiononρinTheorem4.2(i),afteranappropriatetranslationoftheprojecteddata, everyprojectedrobustnessballcanbecontainedinasinglegridcellinawaythatnocellcontains ballsoftwodifferentlabels;seeFigure4a. Hence,thelabelisconstantoneachcellthatcontainsa ball,andallpointsintheballcanbeassociatedwiththecell’sgridindex. WhatremainsisimplementabilitywithReLUnetworks. Thegrid-indexingmapisdiscontinuous, whileReLUnetworksarecontinuousandcanonlyapproximateit. Consequently,approximation errorscanoccuronlyinthinneighborhoodsofcellboundaries(thepurplebandsinFigure4a). 9 --- Page 10 --- Theorem4.2(i)guaranteesatranslationthatplacesevery(projected)robustnessballstrictlyinside acellandsufficientlyfarfromallcellboundariessothattheReLU-basedindexingisaccurateon theentireball. Hence,eachballisdisjointfromthepurpleerror-tolerantregions,everypointinthe √ ballismappedtothesamegridindex,andthisyieldsρ-robustmemorizationusingonlyO˜( N) parameters. However, inTheorem4.2(ii), weconsiderlargerρ, whereprojectedrobustnessballscanoverlap morethanonegridcellandmayintersecttheerror-tolerantregionswheretheReLU-basedindexing isinaccurate. Asρgrows,thenumberofsuchballsincreases. Tocopewiththisregime,weusea sequentialmemorizationstrategy. Werobustlymemorizeonlythesubsetwhoserobustnessballs aredisjointfromtheerror-tolerantregions. Theremainingballsmayintersectthoseregions, but anyresultingerrorisconfinedtothoseerror-tolerantregionsandcanbemadearbitrarilysmallby narrowingtheerror-tolerantregions. Inparticular,wepartitiontheN pointsintomultiplegroupsofapproximatelyequalsizeand,ateach stage,werobustlymemorizeonegroup,whichwecalltheactivegroupofthisstageandwecallthe remaininggroupsofdatapointsasinactivegroups. Weapplyatranslationsothattherobustness ballsoftheactivegroupliestrictlyinsidegridcellsandawayfromtheerror-tolerantregions,while inactiveballsmaycrosscellboundaries,providedtheydonotinterferewiththecellsoccupiedby theactivegroupofthisstage; seeFigure4b. ThegridindexingisthenimplementedbyaReLU approximatorwhoseerror-tolerantregionsarechosensufficientlythin—byincreasingtheslopeasin LemmaB.16—sothatindexingisexactontheactiveballs. Anyerrorfortheinactiveballsisconfined tothosethinerror-tolerantregions. ByLemmaB.11, theportionofarobustnessballcoveredby theerrorregionscaleswiththeregion’swidth,andthiswidthdecreasesastheReLUslopegrows; hence,theerrorcanbedrivenarbitrarilysmall. Theactivegroupisrobustlymemorizedusingthe constructionofTheorem4.2(i),andinactiveballsdonotinterferewiththelabelsassignedinthis stage. IteratingthestagesandcomposingtheresultingsubnetworksyieldsmemorizationofallN pointswitharbitrarilysmallerror. z z 2 2 02 22 11 11 10 10 z z 1 1 (a)ThesettingforTheorem4.2(i),whereeachrobust (b) The relaxed setting in Theorem 4.2(ii) allows ballisentirelycontainedwithinasinglegridcell,and someballstoextendacrossadjacentgridcellbound- notwoballswithdifferentlabelsoccupythesame aries,aslongastheydonotinterferewiththespecific cell.Thisguaranteeswell-definedindexingwithout cellsbeingmemorizedatthatstep. ambiguity. Figure4: Grid-basedLatticeMapping. 6 Conclusion Wepresentatightercharacterizationoftheparametercomplexitynecessaryandsufficientforrobust memorizationacrossthefullrangeofrobustnessratioρ ∈ (0,1). Ourresultsestablishmatching upperandlowerboundsforsmallρ,andshowthatrobustnessdemandssignificantlymoreparameters than classical memorization as ρ grows. These findings highlight how robustness fundamentally increasesmemorizationdifficultyunderadversarialattacks. Weestablishtightcomplexityboundsintheregimewhereρ < 1√ . However,intheremaining 5N d cases,agapbetweentheupperandlowerboundspersists. Aprecisecharacterizationoftheparameter complexityforsomeρremainsopenandisessentialforacompleteunderstandingofthetrade-off betweenrobustnessandnetworkcomplexity. 10 --- Page 11 --- Acknowledgement ThisworkwassupportedbythreeInstituteofInformation&communicationsTechnologyPlanning &Evaluation(IITP)grants(No.RS-2019-II190075,ArtificialIntelligenceGraduateSchoolProgram (KAIST); No. RS-2022-II220184, Development and Study of AI Technologies to Inexpensively ConformtoEvolvingPolicyonEthics;No.RS-2024-00457882,NationalAIResearchLabProject) fundedbytheKoreangovernment(MSIT)andtheInnoCOREprogramoftheMinistryofScience andICT(No.N10250156). References PeterLBartlett,NickHarvey,ChristopherLiaw,andAbbasMehrabian. Nearly-tightvc-dimension andpseudodimensionboundsforpiecewiselinearneuralnetworks. JournalofMachineLearning Research,20(63):1–17,2019. AlexanderBastounis,AndersCHansen,andVernerVlacˇic´. Themathematicsofadversarialattacks inai–whydeeplearningisunstabledespitetheexistenceofstableneuralnetworks,2025. URL  EricBBaum. Onthecapabilitiesofmultilayerperceptrons. Journalofcomplexity,4(3):193–215, 1988. SébastienBubeck,RonenEldan,YinTatLee,andDanMikulincer. Networksizeandweightssize formemorizationwithtwo-layersneuralnetworks,2020. URL 02855. AYaChervonenkis.Ontheuniformconvergenceofrelativefrequenciesofeventstotheirprobabilities. InMeasuresofcomplexity: festschriftforalexeychervonenkis,pages11–30.Springer,2015. GavinWeiguangDing,KryYikChauLui,XiaomengJin,LuyuWang,andRuitongHuang. Onthe sensitivityofadversarialrobustnesstoinputdatadistributions,2019. URL abs/1902.08336. AmitsourEgosi,GiladYehudai,andOhadShamir. Logarithmicwidthsufficesforrobustmemoriza- tion,2025. URL RuiqiGao,TianleCai,HaochuanLi,Cho-JuiHsieh,LiweiWang,andJasonDLee. Convergence of adversarial training in overparametrized neural networks. Advances in Neural Information ProcessingSystems,32,2019. PaulWGoldbergandMarkRJerrum. Boundingthevapnik-chervonenkisdimensionofconcept classesparameterizedbyrealnumbers. MachineLearning,18(2-3):131–148,1995. IanJ.Goodfellow,JonathonShlens,andChristianSzegedy. Explainingandharnessingadversarial examples,2015. URL SvenGowal,ChongliQin,JonathanUesato,TimothyMann,andPushmeetKohli. Uncoveringthe limits of adversarial training against norm-bounded adversarial examples, 2021. URL https: //arxiv.org/abs/2010.03593. BinghuiLi,JikaiJin,HanZhong,JohnHopcroft,andLiweiWang. Whyrobustgeneralizationindeep learningisdifficult: Perspectiveofexpressivepower. AdvancesinNeuralInformationProcessing Systems,35:4370–4384,2022. JiriMatousek. Lecturesondiscretegeometry,volume212. SpringerScience&BusinessMedia, 2013. Sejun Park, Jaeho Lee, Chulhee Yun, and Jinwoo Shin. Provable memorization via deep neural networksusingsub-linearparameters,2021. URL ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,DumitruErhan,IanGoodfellow, andRobFergus. Intriguingpropertiesofneuralnetworks, 2014. URL abs/1312.6199. 11 --- Page 12 --- Matus Telgarsky. Benefits of depth in neural networks. CoRR, abs/1602.04485, 2016. URL  GalVardi,GiladYehudai,andOhadShamir. OntheoptimalmemorizationpowerofReLUneural networks,2021. URL LijiaYu,Xiao-ShanGao,andLijunZhang. OPTIMALROBUSTMEMORIZATIONWITHRELU NEURALNETWORKS. InTheTwelfthInternationalConferenceonLearningRepresentations, 2024. URL ChulheeYun,SuvritSra,andAliJadbabaie. Smallrelunetworksarepowerfulmemorizers: atight analysisofmemorizationcapacity,2019. URL Chongzhi Zhang, Aishan Liu, Xianglong Liu, Yitao Xu, Hang Yu, Yuqing Ma, and Tianlin Li. Interpretingandimprovingadversarialrobustnessofdeepneuralnetworkswithneuronsensitivity. IEEETransactionsonImageProcessing,30:1291–1304,2021. ISSN1941-0042. doi: 10.1109/tip. 2020.3042083. URL 12 --- Page 13 --- Contents 1 Introduction 1 1.1 WhatisKnownSoFar? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 SummaryofContribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2 Preliminaries 3 2.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.2 DatasetandRobustMemorization . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.3 ReLUNeuralNetwork . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.4 WhyOnlyρ=µ/ϵ Matters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 D 3 NecessaryNumberofParametersforRobustMemorization 5 4 SufficientNumberofParametersforRobustMemorization 7 5 KeyProofIdeas 8 5.1 ProofSketchforProposition3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 5.2 ProofSketchforTheorem4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 6 Conclusion 10 A ProofsforSection3 15 A.1 ExplicitProofofTheorem3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 A.2 NecessaryConditiononWidthforRobustMemorization . . . . . . . . . . . . . . 16 A.3 NecessaryConditiononParametersforRobustMemorization. . . . . . . . . . . . 17 A.4 LemmasforAppendixA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 B ProofsforSection4 22 B.1 SufficientConditionforRobustMemorizationwithSmallRobustnessRadius . . . 22 B.2 SufficientConditionforNear-PerfectRobustMemorizationwithModerateRobust- nessRadius . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 B.2.1 MemorizationofIntegerswithSublinearParametersinN . . . . . . . . . 30 B.2.2 PreciseControlofRobustMemorizationError . . . . . . . . . . . . . . . 33 B.3 SufficientConditionforRobustMemorizationwithLargeRobustnessRadius . . . 39 B.4 LemmasforLatticeMapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 B.5 DimensionReductionviaCarefulAnalysisoftheJohnson-LindenstraussLemma . 48 B.6 LemmasforBitComplexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 C Extensionstoℓ -norm 55 p C.1 ExtensionofNecessityConditiontoℓ -norm . . . . . . . . . . . . . . . . . . . . 55 p C.1.1 LemmasforAppendixC.1 . . . . . . . . . . . . . . . . . . . . . . . . . . 59 C.2 ExtensionofSufficiencyConditiontoℓ -norm . . . . . . . . . . . . . . . . . . . 60 p 13 --- Page 14 --- D ComparisontoExistingBounds 64 D.1 SummaryofParameterComplexityacrossℓ -norms. . . . . . . . . . . . . . . . . 64 p D.2 ParameterComplexityoftheConstructionbyYuetal.[2024]. . . . . . . . . . . . 64 D.3 ParameterComplexityoftheConstructionbyEgosietal.[2025] . . . . . . . . . . 65 E NonzeroParameterCounts 66 E.1 NonzeroParameterCounts: Anillustration. . . . . . . . . . . . . . . . . . . . . . 66 E.2 NonzeroParameterCounts: LowerBounds . . . . . . . . . . . . . . . . . . . . . 66 E.3 NonzeroParameterCounts: UpperBounds . . . . . . . . . . . . . . . . . . . . . 67 E.4 LemmasforNonzeroParameterCount . . . . . . . . . . . . . . . . . . . . . . . . 72 14 --- Page 15 --- A ProofsforSection3 A.1 ExplicitProofofTheorem3.1 Theorem3.1. Letρ∈(0,1). SupposeforanyD ∈D ,thereexistsaneuralnetworkf ∈F d,N,2 d,P thatcanρ-robustlymemorizeD. Then,thenumberofparametersP mustsatisfy (cid:32) (cid:40) (cid:41) (cid:33) 1 √ √ P =Ω (ρ2min{N,d}+1)d+min , d N . (cid:112) 1−ρ2 Proof. From Proposition 3.2, we obtain D ∈ D such that any f : Rd → R that ρ-robustly d,N,2 memorizesDmusthavethefirsthiddenlayerwidthatleastρ2min{N −1,d}. Bytheassumption ofTheorem3.1,thereexistsf ∈F thatρ-robustlymemorizesDwiththefirsthiddenlayerwidth d,P m≥ρ2min{N −1,d}. Withthetriviallowerboundthatm≥1,wehave 1 m≥max{ρ2min{N −1,d},1}≥ (ρ2min{N −1,d}+1). 2 SincewecountallparametersaccordingtoEquation(3),thenumberofparametersinthefirstlayer is(d+1)m. Therefore, 1 P ≥(d+1)·m≥(d+1)· (ρ2min{N −1,d}+1)=Ω(d(ρ2min{N,d}+1)). 2 (cid:16) (cid:113) (cid:105) Inaddition,forρ∈ 0, 1− 1 ,usingProposition3.3givesthelowerboundofparameters d (cid:32)(cid:115) (cid:33) N P =Ω . 1−ρ2 (cid:16) (cid:113) (cid:105) √ Forρ∈ 0, 1− 1 ,wehave √1 ≤ dsothatthefollowingrelationholds: d 1−ρ2 (cid:40) (cid:41) (cid:115) 1 √ √ N min , d · N = . (cid:112) 1−ρ2 1−ρ2 (cid:16)(cid:113) (cid:17) √ (cid:113) Forρ∈ 1− 1,1 ,thelowerboundP =Ω( Nd)obtainedbythecaseρ= 1− 1 alsocan d √ d beapplied. Inthiscase, √1 > dsothatthefollowingrelationholds: 1−ρ2 (cid:40) (cid:41) 1 √ √ √ min , d · N = Nd. (cid:112) 1−ρ2 Hence,inbothρregimes, (cid:32) (cid:33) 1 √ √ P =Ω min{ , d} N , (cid:112) 1−ρ2 servesasthelowerboundonthenumberofparameters. BycombiningtheboundsfromProposition3.2andProposition3.3,weconclude: (cid:32) (cid:40) (cid:41)(cid:33) 1 √ √ P =Ω max (ρ2min{N,d}+1)d,min{ , d} N (cid:112) 1−ρ2 (cid:32) (cid:33) 1 √ √ =Ω (ρ2min{N,d}+1)d+min{ , d} N . (cid:112) 1−ρ2 15 --- Page 16 --- A.2 NecessaryConditiononWidthforRobustMemorization Proposition3.2. ThereexistsD ∈D suchthat,foranyρ∈(0,1),anyneuralnetworkf :Rd → d,N,2 Rthatρ-robustlymemorizesDmusthavethefirsthiddenlayerwidthatleastρ2min{N −1,d}. Proof. ToproveProposition3.2,weconsidertwocasesbasedontherelationshipbetweenN −1 andd. Inthefirstcase,whereN −1≤d,establishingthepropositionrequiresthatthefirsthidden layerhaswidthatleastρ2(N −1). Inthesecondcase,whereN −1>d,therequiredwidthisat leastρ2d. Foreachcase,weconstructadatasetD ∈D suchthatanynetworkthatρ-robustly d,N,2 memorizesDmusthaveafirsthiddenlayerofwidthnosmallerthanthecorrespondingbound. Case I : N −1 ≤ d. Let D = {(e ,2)} ∪{(0,1)}. Then, D has separation constant j j∈[N−1] ϵ = 1/2. Letf beaneuralnetworkthatρ-robustmemorizesD,anddenotethewidthofitsfirst D hiddenlayerasm. DenotebyW ∈Rm×dtheweightmatrixofthefirsthiddenlayeroff. Assume forcontradictionthatm<ρ2(N −1). Let µ = ρϵ denote the robustness radius. Then, the network f must distinguish every point in D B (e ,µ)fromeverypointinB (0,µ),forallj ∈[N −1]. Therefore,foranyx∈B (e ,µ)and 2 j 2 2 j x′ ∈B (0,µ),wemusthave 2 Wx̸=Wx′, orequivalently,x−x′ ∈/ Null(W),whereNull(·)denotesthenullspaceofagivenmatrix. Note that B (e ,µ)−B (0,µ):={x−x′ |x∈B (e ,µ)andx′ ∈B (0,µ)}=B (e ,2µ). 2 j 2 2 j 2 2 j Hence,itisnecessarythatB (e ,2µ)∩Null(W)=∅forallj ∈[N −1],orequivalently, 2 j dist (e ,Null(W))≥2µ forallj ∈[N −1]. (5) 2 j Sincedim(Col(W⊤))≤m,whereCol(·)denotesthecolumnspaceofthegivenmatrix,itfollows thatdim(Null(W))≥d−m. UsingLemmaA.2,wecanupperboundthedistancebetweentheset {e } ⊆Rdandanysubspaceofdimensiond−m. j j∈[N−1] LetZ ⊆Null(W)beasubspacesuchthatdim(Z)=d−m,andapplyLemmaA.2withsubstitutions d=d,t=N −1,k =d−mandZ =Z. Theconditionsoflemma,namelyt≤dandk ≥d−t, aresatisfiedsinceN −1≤dandm<ρ2(N −1)≤N −1. Therefore,weobtainthebound (cid:114) m min dist (e ,Z)≤ . j∈[N−1] 2 j N −1 BycombiningtheaboveinequalitywithEquation(5),weobtain (cid:114) (a) m 2µ≤ min dist (e ,Null(W)) ≤ min dist (e ,Z)≤ , (6) j∈[N−1] 2 j j∈[N−1] 2 j N −1 where (a) follows from that Z ⊆ Null(W). Since ϵ = 1/2, we have 2µ = 2ρϵ = ρ, so D D Equation(6)becomes (cid:114) m ρ≤ . N −1 Thisimpliesthatm ≥ ρ2(N −1),contradictingtheassumptionm < ρ2(N −1). Therefore,the widthrequirementm≥ρ2(N−1)isnecessary. ThisconcludesthestatementforthecaseN−1≤d. CaseII:N−1>d. Weconstructthefirstd+1datapointsinthesamemannerasinCaseI,using theconstructionforN =d+1. FortheremainingN −d−1datapoints,wesetthemsufficiently distantfromthefirstd+1datapointstoensurethattheseparationconstantremainsϵ =1/2. D Inparticular,wesetx =2e , x =3e , ··· , x =(N −d)e andassigny =y = d+2 1 d+3 1 N 1 d+2 d+3 ···=y =2. ComparedtothecaseN =d+1,thisconstructionpreservesϵ whileaddingmore N D datapointstomemorize. Sincethefirstd+1datapointsareconstructedasinthecaseN =d+1, the same lower bound applies. Specifically, by the result of Case I, any network that ρ-robustly 16 --- Page 17 --- memorizesthisdatasetmusthaveafirsthiddenlayerofwidthatleastρ2((d+1)−1)=ρ2d. This concludestheargumentforthecaseN −1>d. CombiningtheresultsfromthetwocasesN −1 ≤ dandN −1 > dcompletestheproofofthe proposition. A.3 NecessaryConditiononParametersforRobustMemorization Forsufficientlylargeρ,Gaoetal.[2019]andLietal.[2022]provethat,foranyD ∈ D ,if d,N,C there exists f ∈ F that ρ-robustly memorizes D, the number of parameters P should satisfy √ d,P P =Ω( Nd). However,theauthorsdonotcharacterizetherangeofρoverwhichthislowerbound remainsvalid. MotivatedfromGaoetal.[2019]andLietal.[2022],weestablishalowerboundthatdependson √ (cid:112) (cid:112) ρintheregimeρ≤ 1−1/d,whichbecomes Ndwhenρ= 1−1/d. Thisimpliesthatthe √ (cid:112) existinglowerbound Ndremainsvalidforρ ∈ [ 1−1/d,1). Asaresult,weobtainalower boundthatholdscontinuouslyfromρ ≈ 0uptoρ ≈ 1, andthusinterpolatesbetweenthelower √ √ bound N formemorizationtothelowerbound Ndforrobustmemorization. (cid:16) (cid:113) (cid:105) Proposition3.3. Letρ ∈ 0, 1− 1 . SupposeforanyD ∈ D ,thereexistsf ∈ F that d d,N,2 d,P ρ-robustlymemorizesD. Then,thenumberofparametersP mustsatisfy (cid:32)(cid:115) (cid:33) N P =Ω . 1−ρ2 Proof. Toprovethestatement,weshowthatforanyD ∈D ,ifthereexistsanetworkf ∈F d,N,2 d,P thatρ-robustlymemorizesD,then (cid:18) (cid:19) N VC-dim(F )=Ω .2 (7) d,P 1−ρ2 (cid:112) Iftheaboveboundholds,thenasVC-dim(F )=O(P2),itfollowsthatP =Ω( N/(1−ρ2)). d,P Letk :=⌊ 1 ⌋. ToestablishthedesiredVC-dimensionlowerbound,itsufficestoshowthat 1−ρ2 N VC-dim(F )≥k·⌊ ⌋. d,P 2 ThisimpliesEquation(7),asdesired. Tothisend,itsufficestoconstructk·⌊N⌋pointsinRdthatcan 2 beshatteredbyF . Thesepointsareorganizedasanunionof⌊N⌋groups,eachgroupconsisting d,P 2 ofkpoints. Step1(ConstructingΩ(N/(1−ρ2))pointsX tobeshatteredbyF ). d,P (cid:18) (cid:113) (cid:21) We begin by constructing the first group. Since ρ ∈ 0, d−1 , we have k = ⌊ 1 ⌋ ∈ (1,d]. d 1−ρ2 DefinethefirstgroupX := {e }k ⊆ Rd,consistingofthefirstk standardbasisvectorsinRd. 1 j j=1 Theremaining⌊N⌋−1groupsareconstructedbytranslatingX . Foreachl=1,···⌊N⌋,define 2 1 2 X :=c +X ={c +x | x∈X }, l l 1 l 1 wherec :=2d2(l−1)·e ensuresthatthegroupsaresufficientlydistantfromoneanother. Note l 1 thatc =0,sothatX isconsistentwiththedefinitionabove. Now,defineX :=∪ X asthe 1 1 l∈[⌊N/2⌋] l unionofallgroups,comprisingk×⌊N⌋pointsintotal. 2 Step2(ShowingF shatterX). d,P 2WefollowthedefinitionofVC-dimensionbyBartlettetal.[2019]. NotethattheVC-dimensionofa real-valuedfunctionclassisdefinedastheVC-dimensionofsign(F):={sign◦f |f ∈F}.Sinceweconsider thelabelset[2]={1,2}forrobustmemorizationwhiletheVC-dimensionrequiresthelabelset{+1,−1},we takeanadditionalstepofanaffinetransformationinthelaststepoftheproof. 17 --- Page 18 --- z e 3 × × y e × 2 µ x e 1 Figure5: ReductionofShatteringtoRobustMemorization. Thecrossmarksrefertothepointstobe shattered,andthecirculardotsrefertothepointsforrobustmemorization. Thecentersofrobustness ballschangewithrespecttothelabelsofthepointstobeshattered. WeclaimthatforanyD ∈ D ,ifthereexistsanetworkf ∈ F thatρ-robustlymemorizes d,N,2 d,P D, then the point set X is shattered by F . To prove the claim, consider an arbitrary labeling d,P Y ={y } ofthepointsinX,whereeachlabely ∈{±1}correspondstothepoint l,j l∈[⌊N/2⌋],j∈[k] l,j x :=c +e ∈X. l,j l j GiventhelabelingY,weconstructD ∈D withlabelsin{1,2}suchthatanyfunctionf ∈F d,N,2 d,P thatρ-robustlymemorizesD canbeaffinelytransformedtof′ = 2f −3 ∈ F ,whichsatisfies d,P f′(x )=y ∈{±1}forallx ∈X. Inotherwords,f′exactlymemorizesthegivenlabelingY l,j l,j l,j overX,therebyshowingthatX isshatterdbyF . Theaffinetransformationisnecessarytomatch d,P the{1,2}-valuedoutputsoff withthe{±1}labelingrequiredfortheshatteringargument. Foreachl∈[⌊N/2⌋],definetheindexsets J+ ={j ∈[k] | y =+1}, J− ={j ∈[k] | y =−1}, l l,j l l,j whichpartitionthegroup-wiselabeling{y } ⊂Y intopositiveandnegativeindices. Wethen l,j j∈[k] define (cid:88) (cid:88) x =c + e − e , 2l−1 l j j j∈J+ j∈J− l l (cid:88) (cid:88) x =c + e − e . 2l l j j j∈J− j∈J+ l l Lety = 2,y = 1,anddefinethedatasetD = {(x ,y )} ∈ D . Figure5illustrates 2l−1 2l i i i∈[N] d,N,2 thefirstgroupl =1withk =3wherethelabelsgivestheindexsetsJ+ ={1,3}andJ− ={2}. 1 1 Theblueandreddotsdenotethepointsx andx ,respectively. 1 2 Toanalyzetheseparationconstantϵ ,weconsiderthedistancebetweenpairsofpointswithdifferent D labels. Specifically,foreachl,thetwopointsx andx haveoppositelabelsbyconstruction. 2l−1 2l Considertheirdistance: (cid:13)  (cid:13) (cid:13) (cid:13) √ (cid:13) (cid:88) (cid:88) (cid:13) (a) ∥x 2l−1−x 2l∥ 2 =(cid:13) (cid:13)2 e j − e j(cid:13) (cid:13) = 2 k, (cid:13) j∈J+ j∈J− (cid:13) l l 2 where(a)holdssinceJ+ ∩J− = ∅andJ+ ∪J− = [k]. Now, forl ̸= l′, considerthedistance l l l l betweenx andx ,whichagaincorrespondtodifferentlabels. Wehave: 2l−1 2l′ (a) dist (x ,x ) ≥dist (c ,c )−dist (c ,x )−dist (c ,x ) 2 2l−1 2l′ 2 l l′ 2 l 2l−1 2 l′ 2l′ (b) √ √ ≥2d2− k− k 18 --- Page 19 --- (c) √ ≥2d2−2 d (d) √ ≥2 d (e) √ ≥2 k, √ where(a)followsfromthetriangleinequality,(b)usesdist (c ,x )=dist (c ,x )= k,(c) 2 l 2l−1 √2 l′ 2l′ and(e)usek ≤d,and(d)holdsforalld≥2. Thus,weconcludethatϵ ≥ k. D Letf ∈F beafunctionthatρ-robustlymemorizesD. Webeginbyderivingalowerboundon d,P therobustnessradiusµinordertoverifythatf′ =2f −3correctlymemorizesthegivenlabelingY (cid:113) overX. Defineϕ(t):= t−1. Thefunctionϕisstrictlyincreasingfort≥1,andmaps[1,∞)onto t [0,1). Hence,itadmitsaninverseϕ−1 :[0,1)→[1,∞),definedasϕ−1(ρ)= 1 . Therefore,we 1−ρ2 have (cid:18) (cid:19) (cid:18) (cid:19) (cid:114) 1 1 k−1 ρ=ϕ(ϕ−1(ρ))=ϕ ≥ϕ ⌊ ⌋ =ϕ(k)= . 1−ρ2 1−ρ2 k √ (cid:113) √ Givenϵ ≥ k andρ ≥ k−1, itfollowsthatµ = ρϵ ≥ k−1. Thus, anyfunctionf that D k D √ ρ-robustlymemorizesDmustalsomemorizeallpointswithinanℓ -ballofradius k−1centered 2 ateachpointinD. Next,forx ∈X withpositivelabely =+1,wehave l,j l,j (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) (cid:88) (cid:13) ∥x l,j −x 2l−1∥ 2 =(cid:13) (cid:13)(c l+e j)−(c l+ e j′ − e j′)(cid:13) (cid:13) (cid:13) j′∈J+ j′∈J− (cid:13) l l 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) (cid:88) (cid:13) =(cid:13) e − e (cid:13) (cid:13) j′ j′(cid:13) (cid:13) (cid:13) (cid:13)j′∈J+ j′∈J− (cid:13) √(cid:13) j′̸=jl l (cid:13) 2 = k−1. Nowconsiderasequence{z n} n∈Nsuchthatz n →x l,j asn→∞and √ ∥z −x ∥ < k−1 foralln∈N. n 2l−1 2 Inparticular,wecantake n−1 1 z := x + x , n n l,j n 2l−1 whichsatisfiessuchproperties. Then,z ∈B(x ,µ)foralln,andbyrobustnessoff,f(z )= n 2l−1 n f(x )=2. Bycontinuityoff,wehave 2l−1 f(x )=f( lim z )= lim f(z )= lim 2=2. l,j n n n→∞ n→∞ n→∞ √ Similarly,forx ∈ X withnegativelabely = −1,wehave∥x −x ∥ = k−1,sothat l,j l,j l,j 2l 2 f(x )=1. l,j Since we can adjust the weight and the bias of the last hidden layer, F is closed under affine d,P transformation; thatis, af +b ∈ F wheneverf ∈ F . Inparticular, f′ := 2f −3 ∈ F . d,P d,P d,P Thisf′ satisfiesf′(x ) = 2f(x )−3 = 2·2−3 = +1whenevery = +1andf′(x ) = l,j l,j l,j l,j 2f(x )−3=2·1−3=−1whenevery =−1. Thus,sign◦f′perfectlyclassifiesX according l,j l,j tothegivenlabelingY. Sincesuchf′ ∈F existsforanarbitrarylabelingY,itfollowsthatF d,P d,P shattersX,completingtheproofofthetheorem. 19 --- Page 20 --- A.4 LemmasforAppendixA Thefollowinglemmaupperboundstheℓ -distancebetweenthestandardbasisandanysubspaceofa 2 givendimension. LemmaA.1. Let{e } ⊆ Rd denotethestandardbasisofRd. Then,foranyk-dimensional j j∈[d] subspaceZ ⊆Rd, (cid:114) k max∥Proj (e )∥ ≥ . j∈[d] Z j 2 d Inparticular, (cid:114) d−k mindist (e ,Z)≤ . j∈[d] 2 j d Proof. Let {u ,u ,··· ,u } ⊆ Rd be an orthonormal basis of Z, and denote each u = 1 2 k l (u ,u ,··· ,u )⊤. LetU ∈Rd×k bethematrixwhosecolumnsareu ,··· ,u ,sothat l1 l2 ld 1 k (cid:34) | | | (cid:35) U = u u ··· u . 1 2 k | | | ThentheprojectionmatrixP ontoZ isgivenby k (cid:88) P =U(U⊤U)−1U⊤ =UU⊤ = u u⊤ ∈Rd×d. l l l=1 Now,foreachstandardbasisvectore ,thesquarednormofitsprojectionontoZ is: j (cid:13) (cid:13)2 (cid:13) (cid:13)2 ∥Pe ∥2 =(cid:13) (cid:13)(cid:88)k u u⊤e (cid:13) (cid:13) =(cid:13) (cid:13)(cid:88)k u u (cid:13) (cid:13) =(cid:88)k (u )2, j 2 (cid:13) l l j(cid:13) (cid:13) lj l(cid:13) lj (cid:13) (cid:13) (cid:13) (cid:13) l=1 2 l=1 2 l=1 wherethelastequalityholdsasu areorthonormal. Moreover, l k k k max∥Pe ∥2 ≥ 1 (cid:88) ∥Pe ∥2 = 1 (cid:88) (cid:88) (u )2 = 1(cid:88) (cid:88) (u )2 = 1(cid:88) 1= k . j∈[d] j 2 d j 2 d lj d lj d d j∈[d] j∈[d]l=1 l=1j∈[d] l=1 Thisprovesthefirststatementofthelemma. Toprovethesecondstatement,observethatforany v ∈Rd,wecanwrite v =Proj (v)+Proj (v), Z Z⊥ so that ∥v∥2 = ∥Proj (v)∥2 +∥Proj (v)∥2. Noticing dist (v,Z) = ∥Proj (v)∥ together 2 Z 2 Z⊥ 2 2 Z⊥ 2 withthefirststatement,wehave mindist (e ,Z)= min∥Proj (e )∥ 2 j Z⊥ j 2 j∈[d] j∈[d] (cid:113) = min 1−∥Proj (e )∥2 Z j 2 j∈[d] (cid:114) = 1−max∥Proj (e )∥2 Z j 2 j∈[d] (cid:114) k ≤ 1− d (cid:114) d−k = , d whichconcludesthesecondstatement. 20 --- Page 21 --- ThenextlemmageneralizesLemmaA.1tothecasewhereweconsideronlythedistancetoasubset ofthestandardbasis,insteadofthewholestandardbasis. LemmaA.2. Let1≤t≤d,andlet{e } ⊆Rddenotethefirsttstandardbasisvectors. Then, j j∈[t] foranyk-dimensionalsubspaceZ ⊆Rdwithk ≥d−t,wehave (cid:114) k−(d−t) max∥Proj (e )∥ ≥ . j∈[t] Z j 2 t Inparticular, (cid:114) d−k mindist (e ,Z)≤ . j∈[t] 2 j t Proof. LetQ=[e e ···e ]⊤ ∈Rt×d. Then,wehavetheorthogonaldecomposition: 1 2 t Rd =Col(Q⊤)⊕Null(Q)=(Z∩Col(Q⊤))⊕(Z⊥∩Col(Q⊤))⊕Null(Q). Bytakingdimensions, dim(Z∩Col(Q⊤))=dim(Rd)−dim(Z⊥∩Col(Q⊤))−dim(Null(Q)) ≥dim(Rd)−dim(Z⊥)−dim(Null(Q)) =d−(d−k)−(d−t) =k−(d−t). Now,considertherestrictionofRdtoRtbythelinearmap   (cid:32) t (cid:33) a 1 ϕ:span{e ,...,e }⊂Rd →Rt, ϕ (cid:88) a e = . . . 1 t i i  .  i=1 a t SinceCol(Q⊤)=span{e ,...,e },theprojectionsatisfies: 1 t (cid:13) (cid:13) (cid:13) (cid:13) max(cid:13)Proj (e )(cid:13) =max(cid:13)Proj (ϕ(e ))(cid:13) . (cid:13) Z∩Col(Q⊤) j (cid:13) (cid:13) ϕ(Z∩Col(Q⊤)) j (cid:13) j∈[t] 2 j∈[t] 2 ByapplyingLemmaA.1withtherestrictedspaceRt,weobtain (cid:114) (cid:13) (cid:13) k−(d−t) max(cid:13)Proj (e )(cid:13) ≥ . j∈[t](cid:13) Z∩Col(Q⊤) j (cid:13) 2 t SinceZ ⊇Z∩Col(Q⊤),itfollowsthat (cid:114) (cid:13) (cid:13) k−(d−t) max∥Proj (e )∥ ≥max(cid:13)Proj (e )(cid:13) ≥ . j∈[t] Z j 2 j∈[t](cid:13) Z∩Col(Q⊤) j (cid:13) 2 t Thisprovesthefirststatement. Toprovethesecondstatement,foranyv ∈Rd,decomposevas v =Proj (v)+Proj (v), Z Z⊥ andnotethat∥v∥2 =∥Proj (v)∥2+∥Proj (v)∥2. Usingdist (v,Z)=∥Proj (v)∥ together 2 Z 2 Z⊥ 2 2 Z⊥ 2 withthefirststatement,wehave mindist (e ,Z)=min∥Proj (e )∥ 2 j Z⊥ j 2 j∈[t] j∈[t] (cid:113) =min 1−∥Proj (e )∥2 Z j 2 j∈[t] (cid:114) = 1−max∥Proj (e )∥2 Z j 2 j∈[t] (cid:114) k−(d−t) ≤ 1− t (cid:114) d−k = , t whichconcludesthesecondstatement. 21 --- Page 22 --- B ProofsforSection4 Inthissection,weproveanextendedversionofTheorem4.2,whichadditionallystatestheexplicit boundsondepth,width,andbitcomplexity,inadditiontothesufficientnumberofparameters. We presenttheℓ -normversionofTheorem4.2inTheoremC.11. p TheoremB.1. ForanydatasetD ∈D andη ∈(0,1),thefollowingstatementshold: d,N,C (cid:16) (cid:105) √ √ (i) Ifρ∈ 0, 1√ ,thereexistsf withO˜( N)parameters,depthO˜( N),widthO˜(1)and 5N d √ bitcomplexityO˜( N)thatρ-robustlymemorizesD. (cid:16) (cid:105) (ii) Ifρ∈ 1√ , √1 ,thereexistsf withO˜(Nd1 4ρ21)parameters,depthO˜(Nd1 4ρ1 2),width 5N d 5 d O˜(1)andbitcomplexityO˜(cid:0) 1/d41 ρ21(cid:1) thatρ-robustlymemorizesDwitherroratmostη. (cid:16) (cid:17) (iii) Ifρ∈ √1 ,1 ,thereexistsf withO˜(Nd2ρ4)parameters,depthO˜(N),widthO˜(ρ2d)and 5 d bitcomplexityO˜(N)thatρ-robustlymemorizesD. Here,thebitcomplexityisdefinedasabitneededperparameterunderafixedpointprecision. To prove Theorem B.1, we decompose it into three theorems (Theorems B.3, B.5 and B.14), each correspondingtooneofthecasesinthestatement. TheirproofsareprovidedinAppendicesB.1 toB.3,respectively. Remark B.2 (Tight Bit Complexity). The bit complexities in Theorems B.1(i) and B.1(ii) are essentiallytightwithinourconstructionframework. Vardietal.[2021]providealowerboundon bitcomplexityusingupperandlowerboundsonVC-dimension. Inparticular,foranetworkwithP nonzeroparameters(referAppendixEfordetailedanalysisonnonzeroparameters)andbitcomplexity B,theVC-dimensionisupperboundedas VC-Dim=O(PB+P logP). Since VC-dimension is lower bounded by N by the robust memorization, combining these two boundssuggeststhenecessarybitcomplexityrequiredunderourconstructionsinTheorem4.2. For simplicity,assumethecasewheretheomitteddintheupperboundisnotdominant. InTheoremE.2, √ weshowthatunderourconstructions,thenumberofnonzeroparameterssatisfiesP =O˜( N)for smallρandP =O˜(Nd1/4ρ1/2)formoderateρ. Consequently,thebitcomplexitybecomes √ 1 B =Ω˜( N)andB =Ω˜( ), d1/4ρ1/2 respectively,whichmatchestheupperbounds. B.1 SufficientConditionforRobustMemorizationwithSmallRobustnessRadius (cid:16) (cid:105) √ Theorem B.3. Let ρ ∈ 0, 1√ . For any dataset D ∈ D d,N,C, there exists f with O˜( N) √ 5N d √ parameters,depthO˜( N),widthO˜(1)andbitcomplexityO˜( N)thatρ-robustlymemorizesD. Proof. For given ρ and D = {(x ,y )} ∈ D , we construct a network f ∈ F that √i i i∈[N] d,N,C d,P ρ-robustlymemorizesDwithO˜( N)parameters. Theconstructionproceedsinfourstages. Ineach stage,wedefineafunctionimplementablebyaneuralnetwork,suchthattheircompositionyieldsa ρ-robustmemorizerforD. StageI(Projectionontolog-scaleDimensionandScalingviatheFirstHiddenLayerWeightMatrix). ByLemmaB.20,weobtainanintegerm=O˜(logN)anda1-Lipschitzlinearmapϕ:Rd →Rm suchthattheprojecteddatasetD′ :={(ϕ(x ),y )} ∈D satisfiestheseparationbound i i i∈[N] m,N,C (cid:114) 5 m ϵ′ ≥ ϵ . (8) D 12 d D √ √ Wedefinef :Rd →Rmasf (x)= 11 · dϕ(x),whichis 11 · d-Lipschitz. proj proj 9 ϵD 9 ϵD 22 --- Page 23 --- √ We apply Lemma B.23 with f whose depth is 1, ν = min(cid:8) 109 m, 1 µ, 1 ,1(cid:9) and proj 11880 88 360N R¯ := max{∥x∥ | x ∈ B (x ,µ) forsomei ∈ [N]} to obtain f¯ with the same number 2 2 i proj ofparameters,depthandwidthandO˜(1)bitcomplexitysuchthat max (cid:13) (cid:13)f¯−f(cid:13) (cid:13) ≤ν. (9) ∥x∥2≤R¯ 2 Wesetthefirsthiddenlayerbiasb∈Rmsothat f¯ (x)+b≥0foralli∈[N]andallx∈B (x ,µ), (10) proj 2 i wherethecomparisonbetweentwovectorsiselement-wise. √ We claim that for D′′ := {(σ(f¯ (x )+b),y )} , we have (i) ϵ ≥ m/2 and (ii) for proj i i i∈[N] D′′ ρ′′ := 1 , if g(x) ∈ F can ρ′′-robustly memorize D′′, then g ◦σ ◦(f¯ (x)+b) can 4Nϵ D′′ m,P proj ρ-robustlymemorizeD. Foranyi̸=j withy ̸=y ,wehave i j (cid:13) (cid:13)σ(f¯ proj(x i)+b)−σ(f¯ proj(x j)+b)(cid:13) (cid:13) 2 ( =a)(cid:13) (cid:13)(f¯ proj(x i)+b)−(f¯ proj(x j)+b)(cid:13) (cid:13) 2 =(cid:13) (cid:13)f¯ proj(x i)−f¯ proj(x j)(cid:13) (cid:13) 2 =(cid:13) (cid:13)(f¯ proj(x i)−f proj(x i))−(f¯ proj(x j)−f proj(x j))+(f proj(x i)−f proj(x j))(cid:13) (cid:13) 2. where(a)holdsbytheconstructionofb(Equation(10)). Forsimplicity,wedenote ∆(x ,x ):=(f¯ (x )−f (x ))−(f¯ (x )−f (x )). i j proj i proj i proj j proj j Then,wehave (cid:13) (cid:13)σ(f¯ proj(x i)+b)−σ(f¯ proj(x j)+b)(cid:13) (cid:13)2 2 =∥∆(x ,x )+(f (x )−f (x ))∥2 i j proj i proj j 2 (a) ≥(∥f (x )−f (x )∥ −∥∆(x ,x )∥ )2 proj i proj j 2 i j 2 =∥f (x )−f (x )∥2−2∥f (x )−f (x )∥ ∥∆(x ,x )∥ +∥∆(x ,x )∥2 proj i proj j 2 proj i proj j 2 i j 2 i j 2 ≥∥f (x )−f (x )∥2−2∥f (x )−f (x )∥ ∥∆(x ,x )∥ , proj i proj j 2 proj i proj j 2 i j 2 where(a)holdsfrom∥a+b∥2 ≥(∥a∥ −∥b∥ )2. Bytheconstructionoff¯(Equation(9)), 2 2 2 ∥∆(x ,x )∥ ≤∥f¯ (x )−f (x )∥ +∥f¯ (x )−f (x )∥ ≤2ν, i j 2 proj i proj i 2 proj j proj j 2 sowehave (cid:13) (cid:13)σ(f¯ proj(x i)+b)−σ(f¯ proj(x j)+b)(cid:13) (cid:13)2 2 ≥∥f (x )−f (x )∥2−4ν∥f (x )−f (x )∥ . (11) proj i proj j 2 proj i proj j 2 Nowwederive √ (a) 11 d ∥f (x )−f (x )∥ = · ∥ϕ(x )−ϕ(x )∥ proj i proj j 2 9 ϵ i j 2 √D (b) 11 d ≥ · ·2ϵ 9 ϵ D′ √D (cid:114) (c) 11 d 5 m ≥ · ×2· ϵ 9 ϵ 12 d D D 55√ = m, (12) 54 where(a)isbythedefinitionoff ,(b)isbythedefinitionofD′anditsseparationconstant,and proj (c)followsfromEquation(8). PluggingthisinequalitytoEquation(11)gives (cid:13) (cid:13)σ(f¯ proj(x i)+b)−σ(f¯ proj(x j)+b)(cid:13) (cid:13)2 2 ≥∥f proj(x i)−f proj(x j)∥ 2( 55 45√ m−4ν) 23 --- Page 24 --- (a) 55√ 109 √ ≥ ∥f (x )−f (x )∥ ( m− m) proj i proj j 2 54 2970 54√ =∥f (x )−f (x )∥ · m proj i proj j 2 55 (b) 55√ 54√ ≥ m· m 54 55 =m √ where(a)holdsfromν ≤ 109 m,and(b)holdsfromEquation(12). Thisprovesthefirstclaim √ 11880 ϵ ≥ m/2. Toprovethesecondclaim,letµ:=ρϵ andµ′′ :=ρ′′ϵ . Then, D′′ D D′′ σ(f¯ (B (x ,µ))+b)( =a) f¯ (B (x ,µ))+b proj 2 i proj 2 i (b) ⊆ f (B (x ,µ+ν))+b proj 2 i (c) 89 ⊆ f (B (x , µ))+b proj 2 i 88 √ (d) 89 d ⊆ B (f (x ), · ×µ)+b 2 proj i 72 ϵ D (e) 89 √ = B (f (x ), · dρ)+b 2 proj i 72 (f) 89 ⊆ B (f (x ), )+b 2 proj i 360N (g) 89 ⊆ B (f¯ (x ), +ν)+b 2 proj i 360N (h) 1 ⊆ B (f¯ (x ), )+b 2 proj i 4N ( =i) B (f¯ (x ),ρ′′ϵ )+b 2 proj i D′′ =B (f¯ (x )+b,ρ′′ϵ ) 2 proj i D′′ ( =j) B (σ(f¯ (x )+b),ρ′′ϵ ), 2 proj i D′′ where(a)and(j)arebyEquation(10),(b)and(g)arebytheconstructionoff¯(Equation(9)),(c)is √ becauseν ≤ 81 8µ,(d)isbecausef proj is 1 91 · ϵDd-Lipschitz,(e)usesµ = ρϵ D,(f)usesρ ≤ 5N1√ d, (h)isbecauseν ≤ 1 and(i)isbecauseρ′′ϵ = 1 ϵ = 1 . 360N D′′ 4Nϵ D′′ D′′ 4N Hence,g(x)memorizingtherobustnessballB (σ(f¯ (x )+b),ρ′′ϵ )onprojectedspaceleads 2 proj i D′′ tog◦σ◦(f¯ (x)+b)memorizingtherobustnessballforD. Inotherwords,ifg(x)∈F can proj m,P ρ′′-robustlymemorizeD′′,theng◦σ◦(f¯ (x)+b)canρ-robustlymemorizeD.Withρ′′ = 1 , proj 4Nϵ D′′ StageIItoIVaimstofindaρ′′-robustmemorizergofD′′. StageII(TranslationforDistancingfromLatticeviatheBias)Forsimplicityofthenotation,let z := σ(f¯ (x )+b)foreachi ∈ [N],sothatD′′ = {(z ,y )} . Recallthatρ′′ = 1 i proj i i i i∈[N] 4Nϵ D′′ givestherobustnessradiusisµ′′ =ρ′′ϵ = 1 . D′′ 4N ByapplyingLemmaB.15toz ,··· ,z ,weobtainatranslationvectorb =(b ,··· ,b )∈Rm 1 N 2 21 2m withbitcomplexity⌈log(6N)⌉suchthat 1 dist(z −b ,Z)≥ , ∀i∈[N],j ∈[d], (13) i,j 2j 3N i.e.,thetranslatedpoints{z −b } arecoordinate-wisefarfromtheintegerlattice. Moreover, i 2 i∈[N] byadditionaltranslationto{z −b } (byaddingsomenaturalnumber,coordinate-wise),we i 2 i∈[N] canensureallcoordinatesarepositivewhilekeepingthepropertyEquation(13). Hence,wemay assumewithoutlossofgeneralityb alsohastheproperty 2 z −b ≥0foralli∈[N]. (14) i 2 24 --- Page 25 --- √ Let us denote D′′′ = {(z′,y )} , where z′ := z −b . Then ϵ = ϵ (≥ m/2). For i i i∈[N] i i 2 D′′′ D′′ ρ′′′ := ρ′′ = 1 ,wehavetherobustnessradiusµ′′′ := ρ′′′ϵ = ρ′′ϵ = µ′′ = 1 . Define 4Nϵ D′′ D′′′ D′′ 4N f asf (z) := z−b . Then, f canbeimplementedviaonehiddenlayerinaneural trans trans 2 trans network,withO(m2)parameters. UponthetwolayersconstructedfromstageIandII,itsufficestoconstructanetworkthatρ′′′-robustly memorizesD′′′sincethetranslationpreservesseparationandballcontainmentproperties. Notethat therobustnessballsafterstageIIarenotaffectedwhenpassingtheσ,byEquations(13)and(14). StageIII(GridIndexing)FromEquation(13),eachz′ ∈Rmisatleast 4µ′′′distantawayfromany i 3 latticehyperplaneH :={z ∈Rm |z =z}withanyj ∈[m]andz ∈Z. Thus,eachrobustness z,j j ballofD′′′liescompletelywithinasingleintegerlattice(orunitgrid)oftheform(cid:81)m [n ,n +1), j=1 j j forsome(n ,··· ,n )∈Zm. 1 m √ √ Moreover, as ϵ ≥ m/2, for any i ̸= i′ with y ̸= y , we have ∥z′−z′ ∥ ≥ m . Since D′′′ √ i i′ i i′ 2 sup{∥z−z′∥ | z,z′ ∈(cid:81)m [n ,n +1)}= m,twosuchpointsz′ andz′ thatcorrespondsto 2 j=1 j j i i′ distinctlabelscannotlieinthesamegrid. Sinceeachµ′′′-balllieswithinasinglegrid,weconclude thatnotwoµ′′′-ballwithdifferentlabelsliewithinthesamegrid. We define R := ⌈max ∥z′∥ (= max (z′ ))⌉ ∈ N. Our goal in this stage is to i∈[N] i ∞ i∈[N],j∈[m] i,j constructFlattenmappingdefinedas Flatten(z):=Rm−1⌊z ⌋+Rm−2⌊z ⌋+···+⌊z ⌋. 1 2 m Thismapseachgrid(cid:81)m [n ,n )ontothepoint(cid:80)m Rj−1n . j=1 j j+1 j=1 j However, sinceFlattenisdiscontinuousdue totheuseoffloor functions, weconstruct Flatten, whichisacontinuousapproximationthatexactlymatchesFlattenintheregionofourinterest. By applyingLemmaB.16toγ = 1 andn=⌈log R⌉,weobtainthenetworkFloor:=Floor 4N 2 ⌈log 2R⌉ withO(log R)parameterssuchthat 2 1 Floor(z)=⌊z⌋ ∀z ∈[0,R]withz−⌊z⌋> . (15) 4N Moreover, sinceweapplyγ = 1/4N toLemmaB.16, thelemmaguaranteesthatFloor canbe n exactlyimplementedwithO(n+logN)=O(logR+logN)bitcomplexity. Inparticular,wecan defineournetworkFlattenwithO(logR+logN+logRm−1)=O(logR+logN+logNlogR)= O˜(1)bitcomplexityas Flatten(z)=Rm−1Floor(z )+···+Floor(z ). (16) 1 m This implementation is valid—i.e. Flatten(z) = Flatten(z)—in the region of interest ({z ∈ [0,R]m|dist (z ,Z) ≥ 1 forallj ∈ [m]}) characterized by the margin guaranteed by Equa- 2 j 2N tion(13). AsFloor:R→Rcanbeimplementedwithwidth5anddepthO(log R)network(LemmaB.16), 2 Flattencanbeimplementedwithwidth5manddepthO(log R)network. Thus,wecanconstruct 2 FlattenwithO(m2log R)=O˜(m2)parameters. 2 By Equations (13) and (15), we guarantee that each robustness ball lies in the region where the FlattenisproperlyapproximatedbyFlatten. i.e. Flatten(z)=Flatten(z)forallz ∈B (z′,µ′′′). 2 i SinceFlattenmapseachunitgridintoapointandeachrobustnessballofD′′′liesonasingleunit grid,weconclude Flatten(z)=Flatten(z)=Flatten(z′)forallz ∈B (z′,µ′′′). i 2 i Letm :=Flatten(z′). Theneachrobustnessballaroundx ismappedtom . Wehavem ∈ Z∩ i i i i i [0,Rm+1]foralli∈[N],since m =Flatten(z′) i i =Rm−1⌊z′ ⌋+Rm−2⌊z′ ⌋+···⌊z′ ⌋ i1 i2 im 25 --- Page 26 --- (a) ≤ Rm−1R+Rm−2R+···+R ≤Rm+1, where(a)isby∥z′∥ ≤R. i ∞ Stage IV (Memorization) Finally, it remains to memorize N points {(m ,y )}N ⊂ Z ×[C]. i i i=1 ≥0 SincemultiplerobustnessballsforD′′′withthesamelabelmaycorrespondtothesamegridindexin StageIII,itispossiblethatforsomei̸=j withy =y ,wehavem =m . LetN′ ≤N denotethe i j i j numberofdistinctpairs(m ,y ). ItremainstomemorizetheseN′distinctdatapointsinR. i i Sincem =Flatten(x )≤Rm+1,weapplyTheoremB.4fromVardietal.[2021]withr =Rm+1 i i toconstructf :R→Rwithwidth12anddepth mem √ √ √ √ √ √ O˜( N′·log(5RmN2ϵ−1 πm))=O˜(m N′)=O˜(logN N′)=O˜( N′)=O˜( N) suchthatf (m )=y . mem i i Thefinalnetworkf :Rd →Risdefinedas f(x)=f ◦σ◦Flatten◦σ◦f ◦σ◦(f¯ (x)+b). mem trans proj The depth 1 network f¯ (x)+b has width m, and also the depth 1 network f has width proj trans√ m. Flattenhaswidth5manddepthO(log R)andf haswidth12anddepthO˜( N). The 2 √ mem √ totalconstructionrequiresO˜(md+m2+m2+ N)=O˜(d+ N)parameters,whereeachterm √ md,m2,m2,and N comesfromf ,f ,Flatten,andf respectively. Thewidthofthe proj√trans mem finalnetworkisO˜(1)andthedepthisO˜( N). Thebitcomplexityoff¯ isO˜(1)andthatofbis proj O(⌈log(6N)⌉,log(max{∥f¯ (x)∥ |x∈B (x ,µ) forsomei∈[N]}))=O˜(1). proj ∞ 2 i Thenetworkf hasthebitcomplexitylog(max{∥z ∥ |i∈[N]})=O˜(1). Flattenhasthebit trans √ i ∞ complexityO˜(1),andf needsatmostO˜( N). Hence,thebitcomplexityofthefinalnetworkis √ mem O˜( N). ThefollowingistheclassicalmemorizationupperboundofparametersusedintheproofofTheo- remB.3 TheoremB.4(ClassicalMemorization,Theorem3.1fromVardietal.[2021]). LetN,d,C ∈N,and r,ϵ> 0,andlet(x ,y ),...,(x ,y ) ∈ Rd×[C]beasetofN labeledsampleswith∥x ∥ ≤ r 1 1 N N √ i foreveryiand∥x −x ∥≥2ϵforeveryi̸=j. DenoteR :=5rN2ϵ−1 πd. Then,thereexistsa i j neuralnetworkF :Rd →Rwithwidth12anddepth (cid:32) (cid:115) (cid:33) (cid:112) N O NlogN + ·max{logR,logC} , logN (cid:113) andbitcomplexityboundedbyO(logd+ N ·max{logR,logC})suchthatF(x ) = y for logN i i everyi∈[N]. B.2 SufficientConditionforNear-PerfectRobustMemorizationwithModerateRobustness Radius (cid:16) (cid:105) Theorem B.5. Let ρ ∈ 0, √1 , and η ∈ (0,1). For any dataset D ∈ D d,N,C, there exists f 5 d with O˜(Nd41ρ1 2) parameters, depth O˜(Nd1 4ρ1 2), width O˜(1) and bit complexity O˜(cid:0) 1/d1 4ρ1 2(cid:1) that ρ-robustlymemorizesDwitherroratmostη. Proof. Forgivenρ,anydesirederrorη,andD ={(x ,y )} ∈D ,weconstructanetwork i i i∈[N] d,N,C f thatρ-robustlymemorizesDwithO˜(Nd1 4ρ1 2)parameters. StageI(Projectionontolog-scaleDimensionandScalingviatheFirstHiddenLayerWeightMatrix). 26 --- Page 27 --- ThefirststagecloselyfollowsthatofTheoremB.3. ByLemmaB.20,weobtainanintegerm = O˜(logN) and a 1-Lipschitz linear map ϕ : Rd → Rm such that the projected dataset D′ := {(ϕ(x ),y )} ∈D satisfiestheseparationbound i i i∈[N] m,N,C (cid:114) 5 m ϵ′ ≥ ϵ . (17) D 12 d D √ √ Wedefinef :Rd →Rmasf (x)= 11 · dϕ(x),whichis 11 · d-Lipschitz. proj proj 9 ϵD 9 ϵD √ We apply Lemma B.23 with f whose depth is 1, ν = min(cid:8) 109 m, 1 µ, 1 ,1(cid:9) and proj 11880 88 360N R¯ := max{∥x∥ | x ∈ B (x ,µ) forsomei ∈ [N]} to obtain f¯ with the same number 2 2 i proj ofparameters,depthandwidthandO˜(1)bitcomplexitysuchthat max (cid:13) (cid:13)f¯−f(cid:13) (cid:13) ≤ν. (18) ∥x∥2≤R¯ 2 Wesetthefirsthiddenlayerbiasb∈Rmsothat f¯ (x)+b≥0foralli∈[N]andallx∈B (x ,µ), (19) proj 2 i wherethecomparisonbetweentwovectorsiselement-wise. We obtain the grouping scale α ∈ [0,1] here for Stage II—we call α the grouping scale, as we groupthepointsbyapproximatelyNαpointspergroupinStageII.Fromtheρcondition,wehave 1√ ≥ 1. Thus,thereexistsα ∈ [0,1]suchthatsatisfies⌈Nα⌉ = ⌊ 1√ ⌋. Letusboundtheρin 5ρ d 5ρ d termsofα. Since⌈Nα⌉=⌊ 1√ ⌋≤ 1√ ,wehave 5ρ d 5ρ d 1 1 ρ≤ √ ≤ √ . (20) 5⌈Nα⌉ d 5⌊Nα⌋ d √ WeclaimthatforD′′ :={(σ(f¯ (x )+b),y )} ∈D ,wehave(i)ϵ ≥ m/2and proj i i i∈[N] m,N,C D′′ (ii)forρ′′ := 1 ,ifg(x)∈F canρ′′-robustlymemorizeD′′,theng◦σ◦(f¯ (x )+b) 4⌊Nα⌋ϵ D′′ m,P proj i canρ-robustlymemorizeD. Foranyi̸=j withy ̸=y ,wehave i j (cid:13) (cid:13)σ(f¯ proj(x i)+b)−σ(f¯ proj(x j)+b)(cid:13) (cid:13) 2 ( =a)(cid:13) (cid:13)(f¯ proj(x i)+b)−(f¯ proj(x j)+b)(cid:13) (cid:13) 2 =(cid:13) (cid:13)f¯ proj(x i)−f¯ proj(x j)(cid:13) (cid:13) 2 =(cid:13) (cid:13)(f¯ proj(x i)−f proj(x i))−(f¯ proj(x j)−f proj(x j))+(f proj(x i)−f proj(x j))(cid:13) (cid:13) 2. where(a)holdsbytheconstructionofb(Equation(19)). Forsimplicity,wedenote ∆(x ,x ):=(f¯ (x )−f (x ))−(f¯ (x )−f (x )). i j proj i proj i proj j proj j Then,wehave (cid:13) (cid:13)σ(f¯ proj(x i)+b)−σ(f¯ proj(x j)+b)(cid:13) (cid:13)2 2 =∥∆(x ,x )+(f (x )−f (x ))∥2 i j proj i proj j 2 (a) ≥(∥f (x )−f (x )∥ −∥∆(x ,x )∥ )2 proj i proj j 2 i j 2 =∥f (x )−f (x )∥2−2∥f (x )−f (x )∥ ∥∆(x ,x )∥ +∥∆(x ,x )∥2 proj i proj j 2 proj i proj j 2 i j 2 i j 2 ≥∥f (x )−f (x )∥2−2∥f (x )−f (x )∥ ∥∆(x ,x )∥ , proj i proj j 2 proj i proj j 2 i j 2 where(a)holdsfrom∥a+b∥2 ≥(∥a∥ −∥b∥ )2. Bytheconstructionoff¯(Equation(18)), 2 2 2 ∥∆(x ,x )∥ ≤∥f¯ (x )−f (x )∥ +∥f¯ (x )−f (x )∥ ≤2ν, i j 2 proj i proj i 2 proj j proj j 2 sowehave (cid:13) (cid:13)σ(f¯ proj(x i)+b)−σ(f¯ proj(x j)+b)(cid:13) (cid:13)2 2 27 --- Page 28 --- ≥∥f (x )−f (x )∥2−4ν∥f (x )−f (x )∥ . (21) proj i proj j 2 proj i proj j 2 Nowwederive √ (a) 11 d ∥f (x )−f (x )∥ = · ∥ϕ(x )−ϕ(x )∥ proj i proj j 2 9 ϵ i j 2 √D (b) 11 d ≥ · ·2ϵ 9 ϵ D′ √D (cid:114) (c) 11 d 5 m ≥ · ×2· ϵ 9 ϵ 12 d D D 55√ = m, (22) 54 where(a)isbythedefinitionoff ,(b)isbythedefinitionofD′anditsseparationconstant,and proj (c)followsfromEquation(17). PluggingthisinequalitytoEquation(21)gives (cid:13) (cid:13)σ(f¯ proj(x i)+b)−σ(f¯ proj(x j)+b)(cid:13) (cid:13)2 2 ≥∥f proj(x i)−f proj(x j)∥ 2( 55 45√ m−4ν) (a) 55√ 109 √ ≥ ∥f (x )−f (x )∥ ( m− m) proj i proj j 2 54 2970 54√ =∥f (x )−f (x )∥ · m proj i proj j 2 55 (b) 55√ 54√ ≥ m· m 54 55 =m √ where(a)holdsfromν ≤ 109 m,and(b)holdsfromEquation(22). Thisprovesthefirstclaim √ 11880 ϵ ≥ m/2. Toprovethesecondclaim,letµ:=ρϵ andµ′′ :=ρ′′ϵ . Then, D′′ D D′′ σ(f¯ (B (x ,µ))+b)( =a) f¯ (B (x ,µ))+b proj 2 i proj 2 i (b) ⊆ f (B (x ,µ+ν))+b proj 2 i (c) 89 ⊆ f (B (x , µ))+b proj 2 i 88 √ (d) 89 d ⊆ B (f (x ), · ×µ)+b 2 proj i 72 ϵ D (e) 89 √ = B (f (x ), · dρ)+b 2 proj i 72 (f) 89 ⊆ B (f (x ), )+b 2 proj i 360N (g) 89 ⊆ B (f¯ (x ), +ν)+b 2 proj i 360N (h) 1 ⊆ B (f¯ (x ), )+b 2 proj i 4N ( =i) B (f¯ (x ),ρ′′ϵ )+b 2 proj i D′′ =B (f¯ (x )+b,ρ′′ϵ ) 2 proj i D′′ ( =j) B (σ(f¯ (x )+b),ρ′′ϵ ), 2 proj i D′′ where(a)and(j)arebyEquation(19),(b)and(g)arebytheconstructionoff¯(Equation(18)),(c)is √ becauseν ≤ 81 8µ,(d)isbecausef proj is 1 91 · ϵDd-Lipschitz,(e)usesµ = ρϵ D,(f)usesρ ≤ 5N1√ d, (h)isbecauseν ≤ 1 and(i)isbecauseρ′′ϵ = 1 ϵ = 1 . 360N D′′ 4Nϵ D′′ D′′ 4N 28 --- Page 29 --- Hence,g(x)memorizingtherobustnessballB (σ(f¯ (x )+b),ρ′′ϵ )onprojectedspaceleads 2 proj i D′′ tog◦σ◦(f¯ (x)+b)memorizingtherobustnessballforD. Inotherwords,ifg(x) ∈ F proj m,P can ρ′′-robustly memorize D′′, then g ◦ σ ◦ (f¯ (x) + b) can ρ-robustly memorize D. With proj ρ′′ = 1 ,StageIIaimstofindaρ′′-robustmemorizergofD′′. Forsimplicityofthenotation, letz :4 =⌊N σα (⌋ fϵ ¯D′′ (x )+b)foreachi∈[N],sothatD′′ ={(z ,y )} . i proj i i i i∈[N] StageII(MemorizingNαPointsatEachLayer): UsingthegroupingscaleαobtaininStageI,we groupN datapointsto⌈N1−α⌉groupswithindex{I }N1−α,eachwith|I |≤⌊Nα⌋+1. Then,we j j=1 j constructf˜ thatmemorizesdatapointsandtheirrobustnessballswithindexI ,andtheerrorrate j j remainssmallforotherdatapointsandtheirrobustnessballs. For each j ∈ [⌈N1−α⌉], we apply Lemma B.13 with error rate η ← η , α ← α, D ← √N1−α D′′ ∈ D , ρ ← ρ′′ and I ← I . Then it satisfies that ϵ ≥ m/2, ρ′′ = 1 , m,N,C j D′′ 4⌊Nα⌋ϵ D′′ and|I|≤⌊Nα⌋+1. Thus,weobtainaneuralnetworkf˜ j withwidthO(m)=O˜(1),depthO˜(Nα 2) andO˜(cid:0) Nα 2 +m2(cid:1) =O˜(cid:0) Nα 2(cid:1) parametersandbitcomplexityO˜(cid:0) Nα 2 +m(cid:1) =O˜(cid:0) Nα 2(cid:1) suchthat: f˜(z)=y ∀i∈I ,z ∈B(z ,ρ′′ϵ ), j i j i D′′ (cid:104) (cid:105) η P f˜(z)∈{0,y } ≥1− ∀i∈[N]\I . z∈Unif(B(zi,ρ′′ϵ D′′)) j i N1−α j Thus,wehave (cid:104) (cid:105) η P f˜(z)∈{0,y } ≥1− ∀i∈[N],j ∈[N1−α]. (23) z∈Unif(B(zi,ρ′′ϵ D′′)) j i N1−α Wedefineforeachj: (cid:18)(cid:18) (cid:19)(cid:19) (cid:32) z (cid:33) z f j y = y+σ(cid:16) f˜(z)−y(cid:17) , j (cid:16) (cid:17) sothatthelastcoordinateisgivenasy+σ f˜(z)−y =max{f˜(z),y}. Finally,wedefinethe j j fullrobustmemorizingnetworkas (cid:18) 0(cid:19)⊤ (cid:18) f¯ (x)+b(cid:19) f(x):= f ◦···◦f ◦f proj . 1 ⌈N1−α⌉ 2 1 0 Wenowverifythecorrectnessoftheconstruction. Foranyx ∈ B(x ,ρϵ )andz = f (x) ∈ i D proj B(z ,ρ′′ϵ ),sincewepartition[N]intodisjointgroups{I } ,thereexistsauniqueindex i D′′ j j∈[N1−α] j suchthati∈I andthusf˜ (z)=y holds. Forallj ̸=j ,thenetworkssatisfyf˜(z)∈{0,y } i ji ji i i j i withhighprobability,sononeofthemcanexceedy . Sincethefinalnetworkoutputsthemaximum i amongyandallf˜(z),wehavef(x)=y aslongaseachf˜(z)∈{0,y }. Therefore,itsufficesto j i j i showthatf˜(z)∈{0,y }holdsfor∀j,namely, j i (cid:104) (cid:105) f˜(z)∈{0,y } ∀j ∈[N1−α] ⇒f(x)=y . j i i Consideringthecontrapositive,wehave (cid:104) (cid:105) f(x)̸=y ⇒ f˜(z)∈/ {0,y } forsomej ∈[N1−α] i j i Sinceeachf˜ satisfiesP [f˜(z) ∈ {0,y }] ≥ 1− η forallj ∈ [N1−α],we j z∼Unif(B(zi,ρ′′ϵ D′′)) j i N1−α upperboundtheerrorprobabilityusingtheunionbound: (cid:104) (cid:105) P [f(x)̸=y ]≤P f˜(z)∈/ {0,y } forsomej ∈[N1−α] x∈Unif(B(xi,ρϵD)) i z∈Unif(B(zi,ρ′′ϵ D′′)) j i ≤ (cid:88) P (cid:104) f˜(z)∈/ {0,y }(cid:105) z∈Unif(B(zi,ρ′′ϵ D′′)) j i j∈[N1−α] (a) (cid:16) η (cid:17) ≤N1−α× N1−α 29 --- Page 30 --- ≤η, where(a)holdsbyEquation(23). Hence, P [f(x)=y ]=1−P [f(x)̸=y ] x∈Unif(B(xi,ρϵD)) i x∈Unif(B(xi,ρϵD)) i ≥1−η. Weverifywidth,depthandthenumberofparametersoff. Recallthatthefinalnetworkis (cid:18) 0(cid:19)⊤ (cid:18) f¯ (x)+b(cid:19) f(x):= f ◦···◦f ◦f proj . 1 ⌈N1−α⌉ 2 1 0 Thedepth1networkf¯ (x)+bhaswidthm=O˜(1). Forj ∈[⌈N1−α⌉],eachf˜ haswidthO˜(1), proj j depthO˜(Nα 2)andO˜(cid:0) Nα 2(cid:1) parameters. Thenetworkf¯ needsdm=O˜(d)parameters. Hence,thenumberofparametersoff is proj O˜(cid:0) N1−α×Nα 2(cid:1) =O˜(cid:0) N1−α 2(cid:1)( =a) O˜(cid:16) Nd1 4ρ1 2(cid:17) , where (a) holds by ⌈Nα⌉ = ⌊ 1√ ⌋. The width of f is O˜(1) and the depth of f is 5ρ d O˜(cid:0) N1−α×Nα 2(cid:1) =O˜(cid:16) Nd1 4ρ1 2(cid:17) . The bit complexity of f¯ is O˜(1) and that of b is log(max{∥f¯ (x)∥ | x ∈ proj proj ∞ B (x ,µ) forsomei ∈ [N]}) = O˜(1). The network f has the same bit complexity as f˜, 2 i j j whichisO˜(cid:0) Nα 2(cid:1) =O˜(cid:0) 1/d41 ρ21(cid:1) . Hence,thebitcomplexityofthefinalnetworkisO˜(cid:0) 1/d1 4ρ21(cid:1) . The above construction is motivated by the need to handle overlapped robustness balls with the samelabel. WetransformtheconstructionofclassicalmemorizationinVardietal.[2021]intwo key directions: first, from memorizing isolated data points x to memorizing entire robustness i neighborhoodsB (x ,µ);andsecond,toensuringcorrectclassificationevenwithinregionswhere p i multiplerobustnessballswiththesamelabeloverlap. Toaccomplishthis, weintroducedisjoint, integer-alignedintervalencodingsandcarefullycontroltheerrorpropagationcausedbydimension reduction,asaddressedinLemmaB.11. B.2.1 MemorizationofIntegerswithSublinearParametersinN Lemmas in this subsection are a slight extension of those in Vardi et al. [2021], adapted to our integer-basedencodingscheme. Fromhere,BIN (n)denotesthebitstringfrompositionitoj(inclusive)inthebinaryrepresentation i:j of n. For example, BIN (37) = 4, since (37) = (100101) so that BIN (37) = (100) = 1:3 10 2 1:3 2 (4) . 10 LemmaB.6. Letη >0andm,n∈Nwithm<n. Then,thereexistsaneuralnetworkF :R→R withwidth2,depth2andbitcomplexityO˜(1)suchthat (cid:26) 1 forx∈[m,n−η], F(x)= 0 forx∈(−∞,m−η]∪[n,∞). Proof. WeconstructanetworkF: (cid:18) (cid:18) (cid:19)(cid:19) (cid:18) (cid:18) (cid:19)(cid:19) 1 1 F(x)=σ 1−σ − (x−m) +σ 1−σ (x−(n−η)) −1. η η It satisfies the requirements with depth 2 and width 2. The bit complexity is O(logm+logn+ log(1/η))=O˜(1). 30 --- Page 31 --- LemmaB.7. Letη ∈(0,1),andletm <···<m benaturalnumbers. LetN ,N ∈Nsatisfy 1 N 1 2 N ·N ≥ N, and let w ,...,w ∈ N. Then, there exists a neural network F : R → R with 1 2 1 N1 width4,depth3N +2andbitcomplexityO˜(1)suchthat, 1 (cid:40) w ifx∈[m ,m +1−η]forsomei∈[N], ⌈ i ⌉ i i F(x)= N2 (cid:83) 0 ifx∈/ (m −η, m +1). j∈[N1] (j−1)N2+1 jN2 wherewedefinem =···=m =m . N+1 N1N2 N Proof. Letj ∈ [N ]. WedefinenetworkblocksF˜ : R → RandF : R2 → R2 asfollows. By 1 j j applyingLemmaB.6,weconstructF˜ suchthat: j (cid:26) (cid:2) (cid:3) 1 ifx∈ m , m +1−η , F˜ (x)= (j−1)N2+1 jN2 j 0 ifx≤m −ηorx≥m +1. (j−1)N2+1 jN2 Asaresult,foranyi∈[N],anyx∈[m ,m +1−η]satisfies i i (cid:26) 1 ifi∈[(j−1)·N +1,j·N ], F˜(x)= 2 2 i 0 otherwise. Next,wedefine: (cid:18)(cid:18) (cid:19)(cid:19) (cid:18) (cid:19) x x F = . j y y+w ·F˜ (x) j j (cid:18) (cid:19)⊤ (cid:18)(cid:18) (cid:19)(cid:19) 0 x Finally,wedefinethenetworkF(x)= F ◦···◦F . 1 N1 1 0 We now verify the correctness of the construction. For i ∈ [N], let x ∈ [m ,m +1−η]. For i i j =⌈ i ⌉,wehaveF˜ (x)=1,andforallj′ ̸=j,F˜ (x)=0. Therefore,theoutputofF satisfies N2 j j′ F(x)=w =w . j ⌈ i ⌉ N2 ThewidthofeachF isatmostthewidthrequiredtoimplementF˜ ,plustwoadditionalunitsto j j carrythevaluesofxandy. SincethewidthofF˜ is2,thewidthofF isatmost4. EachblockF j j hasdepth3,andF isacompositionofN blocks. Additionally,onelayerisusedfortheinputtoget 1 (cid:18) (cid:19) x x(cid:55)→ ,andanothertoextractthelastcoordinateofthefinalinput. Thus,thetotaldepthofF is 0 3N +2. ThebitcomplexityisO˜(1). 1 LemmaB.8(LemmaA.7,Vardietal.[2021]). Letn∈Nandleti,j ∈Nwithi<j ≤n. Denote Telgarsky’strianglefunctionbyφ(z):=σ(σ(2z)−σ(4z−2)). Then,thereexistsaneuralnetwork F :R2 →R3withwidth5anddepth3(j−i+1),andbitcomplexityn+2,suchthatforanyx∈N withx≤2n,iftheinputofF is(cid:18) φφ (( ii −− 11 )) (cid:0)(cid:0) 2 xx n + + 2n1 1+1(cid:1) (cid:1)(cid:19) ,thenitoutputs:  φ φ( (j j) )(cid:0) (cid:0)2 2x xn n + + 2 2n n1 1+ +1 2(cid:1) (cid:1) . 2n 2n+2 BIN (x) i:j Inthefollowinglemma,notethatρdoesnotrefertotherobustnessratio. Lemma B.9 (Extension of Lemma A.5, Vardi et al. [2021]). Let η > 0, and let n,ρ,c ∈ N and u,w ∈N. Assumethatforallℓ,k ∈{0,1,...,n−1}withℓ̸=k,thebitsegmentsofusatisfy BIN (u)̸=BIN (u). ρ·ℓ+1:ρ·(ℓ+1) ρ·k+1:ρ·(k+1) Then,thereexistsaneuralnetworkF :R3 →Rwithwidth12,depth3n·max{ρ,c}+2n+2and bitcomplexitynmax{ρ,c}+2,suchthatthefollowingholds: Foreveryx>0,ifthereexistj ∈{0,1,...,n−1}suchthat x∈[BIN (u),BIN (u)+1−η], ρ·j+1:ρ·(j+1) ρ·j+1:ρ·(j+1) thenthenetworksatisfies (cid:32)(cid:32)x(cid:33)(cid:33) F w =BIN (w). c·j+1:c·(j+1) u 31 --- Page 32 --- (cid:32)(cid:32)x(cid:33)(cid:33) Moreover,F w =0for u (cid:91) x∈R\ (BIN (u)−η,BIN (u)+1). ρ·j+1:ρ·(j+1) ρ·j+1:ρ·(j+1) j∈{0,···,n−1} Proof. Wedefinethetrianglefunctionφ(z):=σ(σ(2z)−σ(4z−2))asintroducedbyTelgarsky [2016]. Fori∈{0,1,...,n−1},weconstructanetworkblockF : i  x   x  F i :      φ φ φφ( ( ((i i ii· · ··ρ ρ cc) ) ))(cid:0) (cid:0) (cid:0)(cid:0)2 2 2 2n n n nwwu u· · · ·ρ ρ c c y+ ++ + 2 22 2 n nn n · ·· · 1 11 1 c cρ ρ + ++ + 1 21 2(cid:1) (cid:1)(cid:1) (cid:1)      (cid:55)→       φφ φ φ(( ( ((( ( (ii + i i+ + +11 1 1)) ) )·· ρ · ·ρ c c)) ) )(cid:16) y(cid:0) (cid:0) (cid:0)22 2 2 +nn n nu w wu ·· · ·ρ yρ c c + + ++ 2 2 22 n n nn · · ·· 1 11 1 ρ c cρ + +++ 1 21 2 (cid:1) (cid:1)(cid:1) (cid:17)       , i where (cid:26) BIN (w) ifx∈[BIN (u), BIN (u)+1−η], y = i·c+1:(i+1)·c i·ρ+1:(i+1)·ρ i·ρ+1:(i+1)·ρ i 0 ifx≤BIN (u)−ηorx≥BIN (u)+1. i·ρ+1:(i+1)·ρ i·ρ+1:(i+1)·ρ Tocomputey ,wefirstextracttherelevantbitsegmentsfromuandwusingLemmaB.8. Wedefine i twosubnetworksFw,Fu: i i F iu :(cid:18) φφ (( ii ·· ρρ )) (cid:0)(cid:0) 2n uu ·ρ + + 2n·1 1ρ+1(cid:1) (cid:1)(cid:19) (cid:55)→ φ φ( (( (i i+ +1 1) )· ·ρ ρ) )(cid:0) (cid:0)2 2n nu u· ·ρ ρ + + 2 2n n· ·1 1ρ ρ+ +1 2(cid:1) (cid:1)  2n·ρ 2n·ρ+2 BIN (u) i·ρ+1:(i+1)·ρ F iw :(cid:18) φφ ((i i·· cc )) (cid:0)(cid:0) 2n ww ·c + + 2n·1 1c+1(cid:1) (cid:1)(cid:19) (cid:55)→ φ φ( (( (i i+ +1 1) )· ·c c) )(cid:0)(cid:0) 2 2n nw w· ·c c + + 2 2n n· ·1 1c c+ +1 2(cid:1) (cid:1)  . 2n·c 2n·c+2 BIN (w) i·c+1:(i+1)·c A subnetwork Fu maps the pair of triangle encodings of u to the updated encodings for i+1, i alongwiththeextractedbitsBIN (u). AsubnetworkFw doesthesameforw,yielding i·ρ+1:(i+1)·ρ i BIN (w). i·c+1:(i+1)·c Wethenconstructanetworkwithwidth2anddepth2toobtainy frominputsBIN (u) i i·ρ+1:(i+1)·ρ andx. Firstly,weuseLemmaB.6toconstructanetworkthatoutputy˜: i (cid:26) 1 ifx∈[BIN (u), BIN (u)+1−η], y˜ = i·ρ+1:(i+1)·ρ i·ρ+1:(i+1)·ρ i 0 ifx≤BIN (u)−ηorx≥BIN (u)+1. i·ρ+1:(i+1)·ρ i·ρ+1:(i+1)·ρ Secondly,weconstructthefollowing1-layernetworkthatusey˜ asinput: i (cid:18) (cid:19) y˜ i (cid:55)→σ(cid:0) y˜ ·2c+1−2c+1+BIN (w)(cid:1) . BIN (w) i i·c+1:(i+1)·c i·c+1:(i+1)·c This ensures that the output is BIN (w) if y˜ = 1, and the output is 0 if y˜ = 0 since i·c+1:(i+1)·c i i BIN (w)≤2c. i·c+1:(i+1)·c Finally,thefullnetworkF isconstructedasacomposition: F :=G◦F ◦···◦F ◦H , n−1 0 whereforx,w,u>0: (1)H : R3 → R6 isa1-layernetworkthatmaps(x,w,u)totherequiredinitialencodinginputs, namely:  x  u + 1 (cid:32)x(cid:33) 2n·ρ 2n·ρ+1  u + 1  H : w (cid:55)→2n·ρ 2n·ρ+2,  w + 1  u 2n·c 2n·c+1  w + 1  2n·c 2n·c+2 0 32 --- Page 33 --- (2)G:R6 →Risa1-layernetworkthatoutputsthelastcoordinate. Weverifythecorrectnessoftheconstruction. Theoutputofthefullnetworkisgivenby: (cid:32)(cid:32)x(cid:33)(cid:33) n−1 (cid:88) F w = y . i u i=0 Ifthereexistsj ∈{0,1...,n−1}suchthatx∈[BIN (u),BIN (u)+1−η], ρ·j+1:ρ·(j+1) ρ·j+1:ρ·(j+1) thenbytheconstructionweobtainy = BIN (w),whiley = 0forallℓ ̸= j. Thisis j c·j+1:c·(j+1) ℓ becausethebit-encodedintervalsaredisjointasBIN (u)̸=BIN (u). Hence, ρ·ℓ+1:ρ·(ℓ+1) ρ·k+1:ρ·(k+1) thefinaloutputofF is: n−1 (cid:88) y =y =BIN (w). i j c·j+1:c·(j+1) i=0 WenowanalyzethewidthanddepthoftheconstructednetworkF. EachblockF comprisesFw i i andFu,eachofwidth5. Inaddition,twoneuronsareusedtoprocessxandy,resultinginatotal i widthof12. Theoutputsy˜ andy areproducedbyadditionallayerswithwidth2and1,respectively, i i bothofwhicharesmallerthan12. WealsocomposethenetworksH andG,withwidth6and1, respectively,againremainingwithin12. EachofthenetworksFu andFw hasdepthatmost3max{ρ,c}. Thelayersobtainingy˜ andy i i i i contribute an additional 2 layers, resulting in a total depth of 3max{ρ,c}+2 for each block F . i Composingallnsuchblocks,andincludingoneadditionallayereachforH andG,thetotaldepthof thenetworkF is3n·max{ρ,c}+2n+2. ThebitcomplexityofFu,FwandHisboundedbynmax{ρ,c}+2,andallotherpartsofthenetwork i i requirelessbitcomplexity. Hence,thebitcomplexityofF isboundedbynmax{ρ,c}+2. B.2.2 PreciseControlofRobustMemorizationError LemmaB.13constructsthenetworkforStageIIinTheoremB.5,whiletherobustmemorization erroriscontrolledinLemmaB.11. LemmaB.10. LetN,C ∈N,andlet(m ,y ),...,(m ,y )∈D ⊂N×[C]beasetofN 1 1 N N 1,N,C labeledsampleswithm ̸=m foreveryi̸=j. Then,thereexistsaneuralnetworkF :R→Rwith √ i √j √ width12,depthO˜( N),O˜( N)parametersandbitcomplexityO˜( N)suchthat (cid:26) y foreverym=m withi∈[N], F(m)= i i 0 foreverym∈N\{m } . i i∈[N] √ Proof. LetM={m } . WegrouptheelementsinMto⌈ N⌉groups,eachcontainingatmost √ i i∈[N] √ ⌊ N⌋+1naturalnumbersinside. Foreachintervalindexedbyj ∈{1,...,⌈ N⌉},wedefinetwo integersw ,u ∈Ntoencodetheintegerm ∈Mandthecorrespondinglabelsy asfollows. j j i i (cid:108) (cid:109) √ Foreachi ∈ [N],lettingj := ⌊√ Ni ⌋+1 ,k := i mod (⌊ N⌋+1)andR := max i∈[N]m i,we define: BIN (u )=m k·log R+1:(k+1)·log R j i 2 2 BIN (w )=y . k·log C+1:(k+1)·log C j i 2 2 Thus,ineachgroupj,theintegeru containslog Rbitsperinteger,whichrepresentthek-thinteger j 2 inthisgroup. Inthesamemanner,w containslog C bitsperinteger,whichrepresentthelabelof j 2 thek-thintegerinthisgroup. ByapplyingLemmaB.7toη = 1,weconstructaneuralnetworkF thatmapsm ∈ Mtotheir corresponding groups, and maps2 m ∈ N\(cid:83) √ [m √ 1 , m √ +1) to 0. j∈[⌈ N⌉] (j−1)(⌊ N⌋+1)+1 j(⌊ N⌋+1) Thus,allnaturalnumbersareassignedtotheircorrespondinggroupor0. Foreachi∈[N],wedefinethegroupindex (cid:24) (cid:25) i j := √ . i ⌊ N⌋+1 33 --- Page 34 --- Then,thenetworkF mapsanyinputm∈Mtotherepresentation 1 (cid:32)m(cid:33) F (m)= w , 1 ji u ji (cid:32)m(cid:33) andF (m) = 0 form ∈ N\(cid:83) √ [m √ , m √ +1). Thenetwork 1 j∈[⌈ N⌉] (j−1)(⌊ N⌋+1)+1 j(⌊ N⌋+1) 0 √ F haswidth9,depthO˜( N)andbitcomplexityO˜(1). 1 Now,weapplyLemmaB.9toconstructanetworkF : R3 → Rwiththefollowingproperty. For (cid:104) √ (cid:105) (cid:110) √ (cid:111)2 eachi∈[N],j ∈ ⌈ N⌉ ,andk ∈ 0,...,⌊ N⌋ ,supposethatm isthek-thintegerinthej-th i group. Then,thenetworksatisfies: (cid:32)(cid:32)m (cid:33)(cid:33) i F w =BIN (w )=y . 2 j k·log C+1:(k+1)·log C j i u 2 2 j (cid:32)(cid:32)m(cid:33)(cid:33) (cid:32)(cid:32)m(cid:33)(cid:33) Moreover,form∈N\M,F w =0orF 0 =0. Thus,thenetworkF extracts 2 j 2 2 u 0 j thelabelcorrespondingtoeachdatapointfromtheencodedlabelsetofthegrouptowhichtheinterval √ √ belongsoroutputs0. ThenetworkF haswidth12,depthO˜( N)andbitcomplexityO˜( N). 2 Finally,wedefinetheclassifiernetworkF :Rd →Ras F(x)=F ◦F (x). 2 1 √ TheoverallnetworkF haswidth12anddepthO˜( N),whichcorrespondstothemaximumwidth √ andtotaldepthofitscomponentnetworks. ThebitcomplexityofF isO˜( N). LemmaB.11. LetB (x ,µ)beaEuclideanballwithcenterx ∈Rdandradiusµ>0.Letu∈Rd 2 0 0 beaunitvector,anddefinetheaffinefunctionf(x):= 1 (u⊤x+b)forsomeb∈R. Thenforany 2µ intervalI ⊂Roflengthη,thevolumefractionoftheballmappedintoI satisfies: Vol({x∈B (x ,µ)|f(x)∈I}) V 2 0 ≤2η d−1, Vol(B (x ,µ)) V 2 0 d whereV = πd/2 denotesthevolumeofthed-dimensionalunitball. d Γ(d+1) 2 Proof. Letx=x +µy,sothaty ∈B (0,1). Underthischangeofvariables, 0 d 1 1 1 f(x)= (u⊤(x +µy)+b)= (u⊤y)+ (u⊤x +b). 2µ 0 2 2µ 0 Thus,f(x)∈I ifandonlyifu⊤y ∈J,where 1 J :=2I− (u⊤x +b)⊂R µ 0 isanintervaloflength2η. WedefinethepreimageofI underf withtheintersectionofB (x ,µ)as 2 0 A:={x∈B (x ,µ)|f(x)∈I}. 2 0 Then, Vol(A)=µd·Vol(cid:0)(cid:8) y ∈B d(0,1)(cid:12) (cid:12)u⊤y ∈J(cid:9)(cid:1) . Thedistributionofu⊤y,wherey ∼Unif(B (0,1)),hasdensity 2 V πd/2 p(t)= Vd−1(1−t2)d− 21 fort∈[−1,1], whereV d = Γ(d +1). d 2 34 --- Page 35 --- Thus, (cid:90) (cid:90) Vol(A)=µd·V p(t)dt≤µd V dt=2η·µdV , d d−1 d−1 J J Vol(B (x ,µ))=µdV . 2 0 d Hence, Vol(x∈B (x ,µ):f(x)∈I) Vol(A) V 2 0 = ≤2η d−1. Vol(B (x ,µ)) Vol(B (x ,µ)) V 2 0 2 0 d LemmaB.12. Forallintegersd≥1, V d ≤2, V d−1 πd/2 whereV = isthevolumeofthed-dimensionalunitball. d Γ(cid:0)d +1(cid:1) 2 Proof. Set V √ Γ(cid:0)d+1(cid:1) R := d = π 2 . d V Γ(cid:0)d +1(cid:1) d−1 2 Letx= d anddefine 2 g(x):=logR = 1logπ+logΓ(cid:0) x+ 1(cid:1) −logΓ(x+1). 2x 2 2 Differentiatingandusingthedigammafunctionψ =Γ′/Γ,weget g′(x)=ψ(cid:0) x+ 1(cid:1) −ψ(x+1)<0, 2 sinceψisstrictlyincreasing. HenceR isstrictlydecreasingind. Thereforemax R =R = d d≥1 d 1 V /V =2,whichprovesR ≤2withequalityonlyatd=1. 1 0 d Lemma B.13. Let η ∈ (0,1), α ∈ [0,1] and D = {(x ,y )} ∈ D be a dataset with √ i i i∈[N] d,N,C separation ϵ ≥ d/2, and let the robustness ratio be ρ = 1 . Then, for any index set D 4⌊Nα⌋ϵD I ⊆ [N] with |I| ≤ ⌊Nα⌋+1, there exists a neural network f with width O(d), depth O˜(Nα 2), O˜(cid:0) Nα 2 +d2(cid:1) parametersandO˜(Nα 2 +d)bitcomplexitysuchthat: f(x)=y ∀i∈I,x∈B(x ,ρϵ ), i i D P [f(x)∈{0,y }]≥1−η ∀i∈[N]\I. x∈Unif(B(xi,ρϵD)) i Anetworkf,obtainedfromthislemma,memorizeseachdatapointanditsrobustnessballforall indicesi∈I. f mapseveryotherdatapointanditsrobustnessballtoeitheritscorrectlabelor0with highprobability1−η. Proof. Weconstructanetworkproceedinginthreestages. Ineachstage,wedefinesubnetworkssuch thattheircompositionsatisfiestherequirements. StageI(TranslationforDistancingfromLatticeviatheBias)Wefirsttranslatethedatapointssothat fori∈I,therobustnessballcenteredatx liesfarfromintegerlatticeboundaries. Thisensuresthat i eachballliesentirelywithinasingleunitgridcell. ByapplyingLemmaB.15tothepoints{x } , i i∈I weobtainatranslationvectorb=(b ,··· ,b )∈Rdwithbitcomplexity⌈log(6|I|)⌉suchthat 1 d 1 dist(x −b ,Z)≥ , ∀i∈I,j ∈[d], (24) i,j j 3⌊Nα⌋ i.e.,thetranslatedpoints{x −b} arecoordinate-wisefarfromtheintegerlattice.Additionally,we i i∈I applyaninteger-valuedtranslation(coordinate-wise)sothatallcoordinatesofthepoints{x −b} i i∈[N] becomepositive,whilepreservingthedistancepropertyinEquation(24). Hence,withoutlossof generality,wecanassumebalsohastheproperty x −b≥0foralli∈[N]. (25) i 35 --- Page 36 --- LetD′ :={(x′,y )} ,wherex′ :=x −b. Thenϵ =ϵ . Forρ′ :=ρ= 1 ,wehave i i i∈[N] i i D′ D 4⌊Nα⌋ϵD the robustness radius µ′ := ρ′ϵ = ρϵ = 1 . Define f as f (x) := x−b. Then, D′ D 4⌊Nα⌋ trans trans f canbeimplementedviaonehiddenlayerwithO(d2)parametersinaneuralnetwork. trans Sincethetranslationpreservesseparation(ϵ =ϵ )andballcontainmentproperties(robustness D D′ ballofDismappedtotherobustnessballofD′ throughthetranslation),itsufficestoconstructa networkthatsatisfiestherequirementswithρ←ρ′andD ←D′. Observethattherobustnessballs afterStageIarenotaffectedwhenpassingtheσ,byEquations(24)and(25). StageII(GridIndexing)FromEquation(24),eachx′ ∈Rd(fori∈I)isatleast 4µ′distantfrom i 3 any lattice hyperplane H := {x ∈ Rd | x = z} for each j ∈ [d] and z ∈ Z. Hence, each z,j j robustnessballcenteredatx′ (fori∈I)liescompletelywithinasingleintegerlattice(orunitgrid) i (cid:81)d [n ,n +1), forsome(n ,··· ,n ) ∈ Zd. Moreover, foranyx ∈ B (x′,µ′), thedistance j=1 j j 1 d 2 i fromtheintegerlatticeremainsatleastµ′. √ Furthermore,bytheseparationconditionϵ =ϵ ≥ d/2,foranyi̸=i′withy ̸=y ,wehave √ D′ D √ i i′ ∥x′ −x′ ∥ ≥ d. Sincesup{∥x−x′∥ | x,x′ ∈ (cid:81)d [n ,n +1)} = d,twosuchpoints i i′ 2 2 j=1 j j cannotlieinthesamegrid. Recalltheseparationconditionholdsforalldatapointsx′ fori∈[N] i andeachballB (x′,µ′)(fori∈I)lieswithinasinglegrid. Weconcludethatforeachi ∈ I,the 2 i robustnessballB (x′,µ′)isnotintersectedbyanyotherrobustnessballB (x′,µ′)withadifferent 2 i 2 j label,foranyj ∈[N],i.e.,noballwithadifferentlabeloverlapsthegridcellcontainingB (x′,µ′). 2 i WedefineR:=⌈max ∥x′∥ (=max (x′ ))⌉∈N. Ourgoalinthisstageistoconstruct i∈I i ∞ i∈I,j∈[d] i,j Flattenmappingdefinedas Flatten(x):=Rd−1⌊x ⌋+Rd−2⌊x ⌋+···+⌊x ⌋. 1 2 d Thismapseachgrid(cid:81)d [n ,n )ontothepoint(cid:80)d Rj−1n . j=1 j j+1 j=1 j However, since Flatten is discontinuous due to the use of floor functions, we construct Flatten whichisacontinuousapproximationthatexactlymatchesFlattenontheregion(cid:83) B (x′,µ′), i∈I 2 i andincursonlyasmallerrorontheremainingregion(cid:83) B (x′,µ′). Wechooselargeenough i∈[N]\I 2 i t d-∈ diN mes no st ih oa nt af lo ur nη it′ b:= all1 ./ Mt, ow ree ovh eav r,e wη e′ c≤ an2d taVV kdd e−1 sµ uc′η hw t∈he Nre wV d hi= chΓ a( tπ d 2d th+/2 e1) sad men eo tt ie ms eth se atv iso filu em s eofthe 2dV 2dΓ(d +1) t≤ d−1 +1= 2 +1=O(d2/(µ′η))=O(d2⌊Nα⌋/η). V µ′η π1/2Γ(d−1 +1)µ′η d 2 By Lemma B.16, for γ := 1/t = η′ and n := ⌈log R⌉, we obtain the network Floor := 2 Floor withO(log R)parameterssuchthat ⌈log 2R⌉ 2 Floor(x)=⌊x⌋ ∀x∈[0,R]withx−⌊x⌋>η′. (26) Sinceweapplyγ =1/ttoLemmaB.16,FloorcanbeimplementedwithO(n+logt)=O(logR+ log(d2⌊Nα⌋/η)) = O(log(dRN/η)) bit complexity. In particular, we can define our network FlattenwithO(log(dRN/η)+logRd−1)=O(log(dRN/η)+dlogR)=O˜(d)bitcomplexityas Flatten(x)=Rd−1Floor(x )+···+Floor(x ). (27) 1 d AsFloor:R→Rcanbeimplementedwithwidth5anddepthO(log R)network(LemmaB.16), 2 Flattencanbeimplementedwithwidth5danddepthO(log R)network. Thus,wecanconstruct 2 FlattenwithO(d2log R)=O˜(d2)parameters. 2 We first observe that this implementation is valid on the region (cid:83) B (x′,µ′). For i ∈ I and i∈I 2 i x∈B (x′,µ′),wehave 2 i x −⌊x ⌋( >a) µ′ ( >b) µ′η ( ≥c) V d µ′η > V d µ′η ( ≥d) η′, j j 2V 2dV d−1 d−1 where(a)holdsbyEquation(24),(b)holdssinceη <1,(c)holdssince Vd ≤2byLemmaB.12, Vd−1 and(d)holdsfromthechoiceofη′. Thus,foranyi ∈ I andanyx ∈ B (x′,µ′),x satisfiesthe 2 i j 36 --- Page 37 --- requirementinEquation(26). Therefore,weguaranteethateachrobustnessballcenteredatx′ for i i∈I liesintheregionwheretheFlattenisproperlyapproximatedbyFlatten. i.e. Flatten(x)=Flatten(x)foralli∈I andx∈B (x′,µ′). 2 i SinceFlattenmapseachunitgridintoapointandeachrobustnessballcenteredatx′ fori∈I lies i onasingleunitgrid,weconclude Flatten(x)=Flatten(x)=Flatten(x′)foralli∈I andx∈B (x′,µ′). i 2 i Letm :=Flatten(x′)fori∈I. Thenfori∈I,eachrobustnessballcenteredatx′ ismappedto i i i m . Wehavem ∈ Z∩[0,Rd+1]foralli∈I,since i i m =Flatten(x′) i i =Rd−1⌊x′ ⌋+Rd−2⌊x′ ⌋+···⌊x′ ⌋ i1 i2 id (a) ≤ Rd−1R+Rd−2R+···+R ≤Rd+1, where(a)isby∥x′∥ ≤R. i ∞ Next,weconsiderthecasei∈[N]\I.NotethatthelatticedistanceconditioninEquation(24)applies onlytothesubset{(x ,y )} ,ratherthantheentiredataset. Asaresult,forindicesi∈[N]\I,the i i i∈I distancefromthelatticeisnotguaranteed. Thus,itcanlieacrossthelattice. Fori∈[N]\I,weanalyzetheerrorofFlattenontheremainingregion(cid:83) B (x′,µ′). For i∈[N]\I 2 i i∈[N]\I,wehave P (cid:2) Flatten(x)̸=Flatten(x)(cid:3) x∈Unif(B2(x′ i,µ′)) (a) ≤P [∃j ∈[d], x −⌊x ⌋≤η′] x∈Unif(B2(x′ i,µ′)) j j (cid:88) ≤ P [x −⌊x ⌋≤η′] x∈Unif(B(x′,µ′)) j j i j∈[d] (cid:88) ≤ max P [x ∈I] x∈Unif(B(x′,µ′)) j I⊆R i j∈[d]s.t. Len(I)=η′ ( ≤b) (cid:88) 2η′V d−1 µ′V d j∈[d] 2η′dV = d−1 µ′V d (c) ≤η, where(a)followsfromEquation(26)andthefactthatFlatten(x) = Flatten(x)wheneverx − j ⌊x ⌋>η′forallj ∈[d],(b)followsfromLemmaB.11appliedtoaunitvectoru=e ,b=0,and j j aninterval Ij,and(c)holdsbythechoiceofη′. Hence,wehave µ′ P (cid:2) Flatten(x)̸=Flatten(x)(cid:3) ≤η. (28) x∈Unif(B2(x′ i,µ′)) WeobserveatwhathappensifFlatten(x) = Flatten(x)fori ∈ [N]\I andx ∈ B (x′,µ′). To 2 i ensure that no robustness ball centered at x for i ∈ [N]\I is mapped to grid index m with a i j differentlabel,namely,satisfyingj ∈I withy ̸=y ,wedefinelabel-specificgridindexsets. For i j eachclassc∈[C],definetheset (cid:91) (cid:91) G := {m }, andG:= G ={m } , c i c i i∈[N] i∈I c∈[C] s.t.yi=c whereG isthecollectionofallgridindicesm assignedtodatapointsinI thathavelabely =c. c i i Inotherwords,G containsallgridcellsthatareclaimedbyclassc. ThesetGrepresentsallvalid c gridindices. 37 --- Page 38 --- Recallthatforeachi∈I,therobustnessballB (x′,µ′)isnotintersectedbyanyotherrobustness 2 i ballB (x′,µ′)withadifferentlabel. Specifically,consideri ∈ [N]\I. Forj ∈ I withy = y , 2 j i j therobustnessballB (x′,µ′)canhaveaportionthatintersectsthegridcontainingB (x′,µ′),then 2 i 2 j theportionismappedtothecorrespondinggridindexm . However,forj ∈ I withy ̸= y ,the j i j robustnessballneverintersectsthegrid,andisnevermappedtom . Formally,ifFlatten(x)∈G,it j mustbeFlatten(x)∈G . Otherwise,Flatten(x)∈/ G,i.e.,therobustnessballdoesnotintersect yi anyselectedgrid. Thus,combiningtheprobabilities, P (cid:2) Flatten(x)∈G orFlatten(x)∈/ G(cid:3) x∈Unif(B2(x′ i,µ′)) yi ( ≥a) P (cid:2) Flatten(x)=Flatten(x)(cid:3) x∈Unif(B2(x′ i,µ′)) (b) ≥1−η, (29) where (a) holds since P [Flatten(x)∈G orFlatten(x)∈/ G] = 1, and (b) fol- lowsbyEquation(28). Hx e∈ nU cn ei ,f( iB f2 w(x e′ i m,µ e′) m) orize{(m ,y )} yi andmapotherintegerN\{m } to i i i∈I i i∈I zero,B (x′,µ′)fori ∈ I isexactlymappedtoy ,andwithhighprobability1−η,B (x′,µ′)for 2 i i 2 i i∈[N]\I ismappedtoeithery or0. i Stage III (Memorization) Finally, we construct the network to memorize ⌊Nα⌋ points {(m ,y )}I ⊂ Z × [C]. Since multiple robustness balls for D′ with the same label may i i i=1 ≥0 correspondtothesamegridindexinStageII,itispossiblethatforsomei̸=jwithy =y ,wehave i j m =m . LetN′ ≤|I|denotethenumberofdistinctpairs(m ,y ). Itremainstomemorizethese i j i i N′distinctdatapointsinR. Applying Lemma B.10, we obtain a neural network f mem with width 12, depth O˜(N), O˜(Nα 2) parametersandbitcomplexityO˜(Nα 2)satisfying: (cid:26) y foreverym=m withi∈I, f (m)= i i mem 0 foreverym∈N\G. Form∈N,f (m)=cforsomec∈[C]ifandonlyifm∈G . mem c Thefinalnetworkf :Rd →Risdefinedas f =f ◦σ◦Flatten◦σ◦f . mem trans Letusverifythecorrectnessoftheconstruction. Fori∈I andanyx∈B(x ,ρϵ ),wehave i D (a) (b) f(x)=f ◦σ◦Flatten◦σ◦f (x) = f ◦σ(m ) = f (m )=y , mem trans mem i mem i i where(a)holdssinceFlatten◦σ◦f (x)=m ,(b)holdssincem ∈N,and(c)followsthatf is trans i i constructedtomemorize{(m ,y )}I . i i i=1 Next,consideri∈[N]\I. Weobserve P [f(x)∈{0,y }] x∈Unif(B(xi,ρϵD)) i ( =a)P (cid:2) σ◦Flatten◦σ◦f (x)∈G orσ◦Flatten◦σ◦f (x)∈/ G(cid:3) x∈Unif(B2(xi,µ)) trans yi trans ( =b)P (cid:2) σ◦Flatten(x′)∈G orσ◦Flatten(x′)∈/ G(cid:3) x′∈Unif(B2(x′ i,µ′)) yi ( =c)P (cid:2) Flatten(x′)∈G orFlatten(x′)∈/ G(cid:3) x′∈Unif(B2(x′ i,µ′)) yi (d) ≥1−η, where(a)holdsfromtheconstructionoff , (b)holdsusingx′ := σ◦f (x), (c)holdsby mem trans Equation(25)and(d)holdsbyEquation(29). Thisconcludestheproof. Thedepth1networkf haswidthd. Flattenhaswidth5danddepthO(log R)andf has trans 2 mem width12anddepthO˜(Nα 2). ThetotalconstructionrequiresO˜(d2 +d2 +Nα 2) = O˜(d2 +Nα 2) 38 --- Page 39 --- parameters,whereeachtermd2,d2,andNα 2 comesfromf trans,Flatten,andf mem respectively. ThewidthofthefinalnetworkisO(d)andthedepthisO˜(Nα 2). Thebitcomplexityoff isO(logNα,log(max{∥x ∥ | i ∈ [N]})) = O˜(1). Flattenhasthe trans i ∞ bitcomplexityO˜(d),andf memneedsatmostO˜(Nα 2). Hence,thebitcomplexityofthefinalnetwork isO˜(Nα 2 +d). B.3 SufficientConditionforRobustMemorizationwithLargeRobustnessRadius (cid:16) (cid:17) TheoremB.14. Letρ ∈ √1 ,1 . ForanydatasetD ∈ D d,N,C, thereexistsf withO˜(Nd2ρ4) 5 d parameters,depthO˜(N),widthO˜(ρ2d)andbitcomplexityO˜(N)thatρ-robustlymemorizesD. Proof. LetD = {(x ,y )} ∈ D begiven. Wedividetheproofintofivecases,thefirst i i i∈[N] d,N,C √ case under ρ ∈ [1/3,1), the second case under ρ ∈ (1/5 d,1/3) and d < 600logN, the third √ √ caseunderρ ∈ (1/5 d,1/3)andN < 600logN ≤ d,thefourthcaseunderρ ∈ (1/5 d,1/3), √ N ≥d≥600logN,andfinallythefifthcaseunderρ∈(1/5 d,1/3)andd>N ≥600logN. To checkthatthesecasescoverallthecases,refertoFigure6. ρ∈(cid:2)1,1(cid:1) ? 3 CaseI d<600logN? CaseII N <600logN? CaseIII d≤N? CaseIV CaseV Figure6: DifferentcasesforTheoremB.14. Theleftchildisfortheanswer“Yes”,andtherightchild isfortheanswer“No” Thefirsttwocasesfolloweasilyfrompriorworks,whiletheremainingcasesrequirecarefulanalysis usingdimensionreductiontechniques. ThemostinterestingcasesarecasesIVandV.Whilewetrack thewidth,depth,andparametercomplexityforeachcase,weinitiallyimplementthemusinginfinite precision. Weaddressthebitcomplexitybyapproximatingtheinfiniteprecisionnetworkusinga finiteprecisionnetworkattheverylastpartoftheproof. Asaspoiler,thebitcomplexityofallcases ishandledwithinaunifiedframeworkusingLemmaB.23. Letusdealwitheachcaseonebyone. CaseI:ρ∈[1/3,1). Inthefirstcase,whereρ∈[1/3,1),theresultdirectlyfollowsfromtheprior resultbyYuetal.[2024]. Inparticular,weapplyLemmaD.2. LetusdenoteR:=max ∥x ∥ i∈[N] i 2 andγ := (1−ρ)ϵ . NotethatR ≥ ∥x ∥ foralli ∈ [N]as∥x∥ ≥ ∥x∥ forallx ∈ Rd. By D i ∞ 2 ∞ applyingLemmaD.2,thereexistsf ∈F withP =O(Nd2(log( d )+logR))parametersthat d,P γ2 ρ-robustlymemorizeD. Thenumberofparameterscanbefurtherboundedasfollows: O(Nd2(log( d )+logR))( =a) O(Nd2ρ4·(log( d )+logR))( =b) O˜(Nd2ρ4), γ2 γ2 where (a) is due to ρ = Ω(1), (b) hides the logarithmic factors. Moreover, by Lemma D.2, the networkhaswidthO(d)=O(ρ2d)anddepthO˜(N). √ Case II: ρ ∈ (1/5 d,1/3) and d < 600logN. In the second case, where d < 600logN and √ (1/5 d,1/3),theresultalsodirectlyfollowsfromthepriorresultbyYuetal.[2024].Inparticular,we applyLemmaD.2. LetusdenoteR:=max ∥x ∥ andγ :=(1−ρ)ϵ . NotethatR≥∥x ∥ i∈[N] i 2 D i ∞ for all i ∈ [N] as ∥x∥ ≥ ∥x∥ for all x ∈ Rd. By Lemma D.2, there exists f ∈ F with 2 ∞ d,P 39 --- Page 40 --- P =O(Nd2(log( d )+logR))parametersthatρ-robustlymemorizeD. Thenumberofparameters γ2 canbefurtherboundedasfollows: O(Nd2(log( d )+logR))( =a) O(N(logN)2·(log( d )+logR))( =b) O˜(N)( =c) O˜(Nd2ρ4), γ2 γ2 where (a) is due to d ≤ 600logN, (b) hides the logarithmic factors, and (c) is because (cid:16) (cid:17) N ≤ 625Nd2ρ4 for all ρ ∈ √1 ,1 . Moreover, by Lemma D.2, the network has width 5 d 3 O(d)=O(logN)=O˜(1)=O˜(ρ2d)anddepthO˜(N). √ CaseIII:ρ∈(1/5 d,1/3)andN <600logN ≤d. Inthethirdcase,whereN <600logN ≤ √ dand(1/5 d,1/3),wefirstapplyPropositionB.21toDtoobtain1-Lipschitzlinearφ:Rd →RN suchthatD′ :={(φ(x ),y )} hasϵ =ϵ . Thisispossibleasd≥N. i i i∈[N] D′ D We apply Lemma D.2 by Yu et al. [2024] to D′. Let us denote R := max ∥φ(z )∥ and i∈[N] i 2 γ := (1−ρ)ϵ . Note that R ≥ ∥φ(z )∥ for all i ∈ [N] as ∥z∥ ≥ ∥z∥ for all z ∈ RN. D′ i ∞ 2 ∞ ByLemmaD.2,thereexistsf ∈ F withP = O(N ·N2(log(N)+logR))parametersthat 1 N,P γ2 ρ-robustlymemorizeD′. f haswidthO(N)anddepthO˜(N). 1 Letf =f ◦φ.Thiscanbeimplementedbychangingthefirsthiddenlayermatrixoff bycomposing 1 1 φ. Thisispossiblebecauseφislinear. f hasatmostdN additionalparameterscomparedtof ,and 1 hassamewidthanddepthasf . Sincef is1-Lipschitzandϵ =ϵ ,everyrobustnessballofDis 1 1 D′ D mappedtotherobustnessballofD′viaf . Asf ρ-robustlymemorizesD′,thecomposedf satisfies 1 1 thedesiredproperty. Thenumberofparameterscanbefurtherboundedasfollows: O(Nd+N ·N2(log( d )+logR))( =a) O(dlogN +(logN)3·(log( d )+logR))( =b) O˜(d), γ2 γ2 where (a) is due to N ≤ 600logN, and (b) hides the logarithmic factors. The width of f is O(N)=O(log(N))=O˜(1)=O˜(ρ2d). Thedepthoff isO˜(N). √ CaseIV:ρ∈(1/5 d,1/3),andN ≥d≥600logN. Inthefourthcase,whered≥600logN, weutilizethedimensionreductiontechniquebyPropositionB.19. WeapplyPropositionB.19toD withm=max{⌈9dρ2⌉,⌈600logN⌉,⌈10logd⌉}andα=1/5. Letusfirstcheckthatthespecified msatisfiesthecondition24α−2logN ≤ m ≤ dforthepropositiontobeapplied. α = 1/5and m ≥ 600logN ensure the first inequality 24α−2logN ≤ m. The second inequality m ≤ d is decomposedintothreeparts. Sinceρ≤ 1,wehave9dρ2 ≤dsothat 3 ⌈9dρ2⌉≤d. (30) Moreover,600logN ≤dimplies ⌈600logN⌉≤d. (31) Additionally,asN ≥2,wehaved≥600logN ≥600log2≥400. ByLemmaB.22,thisimplies 10logd≤dandtherefore ⌈10logd⌉≤d. (32) GatheringEquations(30)to(32)provesm≤d. BythePropositionB.19,thereexists1-Lipschitzlinearmappingϕ:Rd →Rmandβ >0suchthat D′ :={(ϕ(x ),y )} ∈D satisfies i i i∈[N] m,N,C 4 ϵ ≥ βϵ . (33) D′ 5 D Asm≥10logd,theinequalityβ ≥ 1(cid:112)m isalsosatisfiedbyPropositionB.19. Therefore,wehave 2 d (cid:114) (cid:114) (cid:114) 1 m (a) 1 ⌈9dρ2⌉ 1 9dρ2 3 β ≥ ≥ ≥ = ρ, (34) 2 d 2 d 2 d 2 40 --- Page 41 --- where(a)isbythedefinitionofm. Moreover,sinceϕis1-Lipschitzlinear, ∥ϕ(x )∥ =∥ϕ(x −0)∥ =∥ϕ(x )−ϕ(0)∥ ≤∥x −0∥ =∥x ∥ , (35) i 2 i 2 i 2 i 2 i 2 foralli∈[N]. Hence,bylettingR:=max {∥x ∥ },wehave∥ϕ(x )∥ ≤Rforalli∈[N]. i∈[N] i 2 i 2 Now,wesetthefirstlayerhiddenmatrixasthematrixW ∈Rm×dcorrespondingtoϕunderthestan- dardbasisofRdandRm. Moreover,setthefirsthiddenlayerbiasasb:=2R1=2R(1,1,··· ,1)∈ Rm. Then,wehave Wx+b≥0, (36) forallx∈B (x ,ϵ )foralli∈[N],wherethecomparisonbetweentwovectorsareelement-wise. 2 i D Thisisbecauseforalli∈[N],j ∈[m]andx∈B (x,ϵ ),wehave 2 D (a) (b) (c) (Wx+b) =(Wx) +2R≥2R−∥Wx∥ ≥ 2R−∥x∥ ≥ 2R−(R+ϵ ) ≥ 0, j j 2 2 D where(a)isbyEquation(35),(b)isbythetriangleinequality,and(c)isduetoR>ϵ . D We construct the first layer of the neural network as f (x) := σ(Wx+b) which includes the 1 activationσ. Then,byaboveproperties,D′′ :={(f (x ),y )} satisfies 1 i i i∈[N] 6 ϵ ≥ ρϵ . (37) D′′ 5 D Thisisbecausefori̸=j withy ̸=y wehave i j ∥f (x )−f (x )∥ =∥σ(Wx +b)−σ(Wx +b)∥ 1 i 1 j 2 i j 2 (a) = ∥(Wx +b)−(Wx +b)∥ i j 2 =∥ϕ(x )−ϕ(x )∥ i k 2 (b) ≥ 2ϵ D′ (c) 4 ≥ 2× βϵ 5 D (d) 4 3 ≥ 2× × ρϵ 5 2 D 12 = ρϵ , 5 D where(a)isbyEquation(36), (b)isbythedefinitionoftheϵ , (c)isbyEquation(33), and(d) D′ is by Equation (34). By Lemma D.2 applied to D′′ ∈ D , there exists f ∈ F with m,N,C 2 m,P P =O(Nm2(log( m )+logR′′))numberofparametersthat 5-robustlymemorizeD′′,where (γ′′)2 6 5 (a) 1 12 2 γ′′ :=(1− )ϵ ≥ × ρϵ = ρϵ , 6 D′′ 6 5 D 5 D R′′ := max∥f (x )∥ = max∥σ(Wx +b)∥ = max∥Wx +b∥ 1 i 2 i 2 i 2 i∈[N] i∈[N] i∈[N] ≤ max∥Wx ∥ +∥b∥ ≤3R. i 2 2 i∈[N] Here(a)isbyEquation(37). Moreoverf haswidthO(m)anddepthO˜(N)byLemmaD.2. 2 Now,weclaimthatf := f ◦f ρ-robustlymemorizeD. Foranyi ∈ [N],takex ∈ B (x ,ρϵ ). 2 1 2 i D Then,byEquation(36),wehavef (x)=Wx+bandf (x )=Wx +bsothat 1 1 i i ∥f (x)−f (x )∥ =∥Wx−Wx ∥ ≤∥x−x ∥ ≤ρϵ . (38) 1 1 i 2 i 2 i 2 D Moreover, combining Equations (37) and (38) results ∥f (x)−f (x )∥ ≤ 5ϵ . Since f 5- 1 1 i 2 6 D′′ 2 6 robustlymemorizeD′′,wehave f(x)=f (f (x))=f (f (x ))=y . 2 1 2 1 i i In particular, f(x) = y for any x ∈ B (x ,ρϵ ), concluding that f is a ρ-robust memorizer D. i 2 i D Regardingthenumberofparameterstoconstructf,noticethatf consistsof(d+1)m=O˜(d2ρ2) 1 41 --- Page 42 --- parametersasm = O˜(dρ2). f consistsofO˜(Nm2) = O˜(Nd2ρ4)parameters. SincethecaseIV 2 assumesN ≥dandlargeρregimedealswithρ≥ √1 ,wehave 5 d d2ρ2 ≤Ndρ2 ≤25Nd2ρ4 Therefore,f intotalconsistsofO˜(d2ρ2+Nd2ρ4)=O˜(Nd2ρ4)numberofparameters. Moreover, sincef hasthesamewidthasf anddepthonelargerthanthedepthoff ,itfollowsthatf haswidth 2 2 O(m)=O˜(ρ2d)anddepthO˜(N). Thisprovesthetheoremforthefourthcase. √ CaseV:ρ∈(1/5 d,1/3),andd>N ≥600logN. Thelastcasecombinesthetwotechniques usedinCasesIIIandIV.WefirstapplyPropositionB.21toDtoobtain1-Lipschitzlinearφ:Rd → RN such that D′ := {(φ(x ),y )} ∈ D has ϵ = ϵ . Note that we can apply the i i i∈[N] N,N,C D′ D propositionsinced≥N. Next, we apply Proposition B.19 to D′ ∈ D with m = max{⌈9Nρ2⌉,⌈600logN⌉} and N,N,C α=1/5.Letusfirstcheckthatthespecifiedmsatisfiesthecondition24α−2logN ≤m≤N forthe propositiontobeapplied. α=1/5andm≥600logN ensurethefirstinequality24α−2logN ≤m. Thesecondinequalitym≤N isdecomposedintotwoparts. Sinceρ≤ 1,wehave9Nρ2 ≤N so 3 that ⌈9Nρ2⌉≤N. (39) Moreover,600logN ≤N implies ⌈600logN⌉≤N. (40) Gathering Equations (30) and (31) proves m ≤ N. Additionally, as N ≥ 2, we have N ≥ 600logN ≥600log2≥400. ByLemmaB.22,thisimplies10logN ≤N. BythePropositionB.19,thereexists1-Lipschitzlinearmappingϕ:RN →Rmandβ >0suchthat D′′ :={(ϕ(φ(x )),y )} ∈D satisfies i i i∈[N] m,N,C 4 ϵ ≥ βϵ′ . (41) D′′ 5 D As m ≥ 600logN ≥ 10logN, the inequality β ≥ 1(cid:112)m is also satisfied by Proposition B.19. 2 N Therefore,wehave (cid:114) (cid:114) (cid:114) 1 m (a) 1 ⌈9Nρ2⌉ 1 9Nρ2 3 β ≥ ≥ ≥ = ρ, (42) 2 N 2 N 2 N 2 where(a)isbythedefinitionofm. Moreover, sinceφandϕareboth1-Lipschitzlinear, ϕ◦φ : Rd →Rmisalso1-Lipschitzlinear. Therefore, ∥ϕ(φ(x ))∥ =∥ϕ(φ(x −0))∥ =∥ϕ(φ(x ))−ϕ(φ(0))∥ ≤∥x −0∥ =∥x ∥ , (43) i 2 i 2 i 2 i 2 i 2 for all i ∈ [N]. Hence, by letting R := max {∥x ∥ }, we have ∥ϕ(φ(x ))∥ ≤ R for all i∈[N] i 2 i 2 i∈[N]. Now, we set the first layer hidden matrix as the matrix W ∈ Rm×d corresponding to ϕ ◦ φ underthestandardbasisofRd andRm. Moreover,setthefirsthiddenlayerbiasasb := 2R1 = 2R(1,1,··· ,1)∈Rm. Then,wehave Wx+b≥0, (44) forallx∈B (x ,ϵ )foralli∈[N],wherethecomparisonbetweentwovectorsareelement-wise. 2 i D Thisisbecauseforalli∈[N],j ∈[m]andx∈B (x,ϵ ),wehave 2 D (a) (b) (c) (Wx+b) =(Wx) +2R≥2R−∥Wx∥ ≥ 2R−∥x∥ ≥ 2R−(R+ϵ ) ≥ 0, j j 2 2 D where(a)isbyEquation(43),(b)isbythetriangleinequality,and(c)isduetoR≥ϵ . D We construct the first layer of the neural network as f (x) := σ(Wx+b) which includes the 1 activationσ. Next,weshowthat,D′′ :={(f (x ),y )} satisfies 1 i i i∈[N] 6 ϵ ≥ ρϵ , (45) D′′ 5 D 42 --- Page 43 --- bytheaboveproperties. Thisisbecausefori̸=j withy ̸=y wehave i j ∥f (x )−f (x )∥ =∥σ(Wx +b)−σ(Wx +b)∥ 1 i 1 j 2 i j 2 (a) = ∥(Wx +b)−(Wx +b)∥ i j 2 =∥ϕ(φ(x ))−ϕ(φ(x ))∥ i k 2 (b) ≥ 2ϵ D′′ (c) 4 ≥ 2× βϵ′ 5 D (d) 4 3 ≥ 2× × ρϵ′ 5 2 D 12 = ρϵ′ 5 D (e) 12 = ρϵ , 5 D where(a)isbyEquation(44),(b)isbythedefinitionoftheϵ ,(c)isbyEquation(41),(d)isby D′′ Equation(42),and(e)isbecauseϵ =ϵ . D′ D ByLemmaD.2appliedtoD′′ ∈D ,thereexistsf ∈F withP =O(Nm2(log( m )+ m,N,C 2 m,P (γ′′)2 logR′′))numberofparametersthat 5-robustlymemorizeD′′,where 6 5 (a) 1 12 2 γ′′ :=(1− )ϵ ≥ × ρϵ = ρϵ , 6 D′′ 6 5 D 5 D R′′ := max∥f (x )∥ = max∥σ(Wx +b)∥ = max∥Wx +b∥ 1 i 2 i 2 i 2 i∈[N] i∈[N] i∈[N] ≤ max∥Wx ∥ +∥b∥ ≤3R. i 2 2 i∈[N] Here,(a)isbyEquation(45). Moreover,f haswidthO(m)anddepthO˜(N)byLemmaD.2. 2 Now,weclaimthatf := f ◦f ρ-robustlymemorizeD. Foranyi ∈ [N],takex ∈ B (x ,ρϵ ). 2 1 2 i D Then,byEquation(44),wehavef (x)=Wx+bandf (x )=Wx +bsothat 1 1 i i ∥f (x)−f (x )∥ =∥Wx−Wx ∥ ≤∥x−x ∥ ≤ρϵ . (46) 1 1 i 2 i 2 i 2 D Moreover, putting Equation (45) to Equation (46) results ∥f (x)−f (x )∥ ≤ 5ϵ . Since f 1 1 i 2 6 D′′ 2 5-robustlymemorizeD′′,wehave 6 f(x)=f (f (x))=f (f (x ))=y . 2 1 2 1 i i Inparticular,f(x)=y foranyx∈B (x ,ρϵ ),concludingthatf isaρ-robustmemorizerD. i 2 i D Regardingthenumberofparameterstoconstructf,noticethatf consistsof(d+1)m=O˜(Ndρ2) 1 parametersasm = O˜(Nρ2). f consistsofO˜(Nm2) = O˜(N3ρ4)parameters. SincethecaseV 2 assumesN <dandlargeρregimedealswithρ≥ √1 ,wehave 5 d Ndρ2 ≤25Nd2ρ4, N3ρ4 ≤Nd2ρ4. Therefore,f intotalconsistsofO˜(N3ρ4+Ndρ2)=O˜(Nd2ρ4)numberofparameters. Moreover, sincef hasthesamewidthasf anddepthonelargerthanthedepthoff ,itfollowsthatwidthof 2 2 f isO(m)=O˜(ρ2N)=O˜(ρ2d)andthedepthoff isO˜(N). Thisprovesthetheoremforthelast case. BoundingtheBitComplexity. Now,letusanalyzehowwecanimplementtheabovenetwork underafiniteprecision. Wehavedemonstratedthatforeveryfivecases,thedepthO˜(N)andwidth ρ2dsufficeforconstructingf thatrobustlymemorizesD. 43 --- Page 44 --- LetR := max ∥x ∥ +µ. LetD = O˜(ρ2d)andL = O˜(N)denotethewidthandthedepth i∈[N] i 2 of the constructed network. Let M be the maximum absolute value of the parameter used for constructingf. Finallyletν =0.1. ByLemmaB.23,thereexistsf¯withO˜(N)bitcomplexity,that approximatesf uniformlyoverB (0,R)witherroratmostν,whereO˜(·)herehidespolylogarithmic 2 termsinD,M,LandR. i.e. max (cid:12) (cid:12)f¯−f(cid:12) (cid:12)≤ν. ∥x∥ ≤R 2 Finally, to handle the error ν, we use the floor function approximation from Lemma B.16. By LemmaB.16withγ =1/10,thereexistsFloor:R→Rwithdepthn:=⌈log (C+1)⌉andwidth 2 5suchthatFloor(x)=⌊x⌋forallx∈[0,C +1)withx−⌊x⌋>γ =0.1. Moreover,thelemma guarantees that Floor can be exactly implemented with O(n+log10) = O(logC) = O˜(1) bit complexity. Thus,ify′ ∈Rsatisfies|y′−y|≤ν =0.1forsomey ∈[C],then y′+0.5∈[y−0.1+0.5,y+0.1+0.5]⊆(y+0.1,y+1). Inparticular,⌊y′+0.5⌋ = y and⌊y′+0.5⌋−(y′+0.5) ∈ (0.1,1)sothatFloor(y′) = ⌊y′⌋ = y. Forx∈B (x ,µ),wehavef(x)=y sothattheapproximationf¯outputsy′ =f¯(x)suchthat 2 i i (cid:12) (cid:12)f¯(x)−f(x)(cid:12) (cid:12)=|y′−y i|≤ν. ThisshowsFloor(f¯(x))=Floor(y′)=y . Moreover,Floor◦f¯canbeimplementedwithparame- i ters,width,anddepthofthesamescaleasf¯,andbitcomplexityO˜(N). Thisfinishestheprooffor thebitcomplexity. B.4 LemmasforLatticeMapping LemmaB.15(AvoidingBeingNearGrid). LetN,d∈Nandx ,··· ,x ∈Rd. Then,thereexists 1 N atranslationvectorb∈Rdsuchthat: 1 dist(x −b ,Z)≥ , ∀i∈[N],j ∈[d], i,j j 2N i.e.,thetranslatedpoints{x −b} arecoordinate-wise 1 -farfromtheintegerlattice. i i∈[N] 2N Moreover,thereexistsb¯∈Rdwhichhasbitcomplexity⌈log(6N)⌉andsatisfies 1 dist(x −b ,Z)≥ , ∀i∈[N],j ∈[d]. i,j j 3N Proof. Foreachcoordinatej ∈ [d],considertheset{x } ofallj-thcoordinatevalues. For i,j i∈[N] x∈R,let{x}:=x−⌊x⌋denotethefractionalpartofx. Weconsiderthecollectionoffractional parts {{x }} . Without loss of generality, we may assume 0 ≤ {x } < {x } < ··· < i,j i∈[N] 1,j 2,j {x }<1. N,j Foreachj ∈[d],definethemaximumfractionalgapg ∈[0,1)as j (cid:18) (cid:19) g :=max max ({x }−{x }), 1−{x }+{x } . j i+1,j i,j N,j 1,j i∈[N−1] Weclaim: 1 g ≥ , forallj ∈[d]. j N Supposeforacontradictionthatg < 1 forsomej ∈[d]. Then,wehavebythedefinitionofg , j N j N−1 (cid:88) N −1 {x }−{x }= ({x }−{x })≤(N −1)g < , (47) N,j 1,j i+1,j i,j j N i=1 1 N −1 1−{x }+{x }≤g < ⇐⇒ {x }−{x }> . (48) N,j 1,j j N N,j 1,j N 44 --- Page 45 --- Equations(47)and(48)leadtoacontradiction,provingtheclaim. Now, we define the translation of the j-th coordinate, b ∈ R, based on the location where the j maximumg isattained. Ifthemaximuminthedefinitionofg occursbythedifferenceofsome j j consecutivepair({x },{x })satisfying{x }−{x }=g ,weset i′,j i′+1,j i′+1,j i′,j j {x }+{x } b = i′,j i′+1,j . j 2 Inthiscase,dist(x −b ,Z)=dist(x −b ,Z)= 1g ≥ 1 . Forotheri,dist(x −b ,Z) i′,j j i′+1,j j 2 j 2N i,j j isevenlargerbytheorderrelationwithin{{x }} . i,j i∈[N] Otherwise,ifthemaximuminthedefinitionofg isattainedas1−{x }+{x }=g ,wedefine j N,j 1,j j ({x }−1)+{x } b = N,j 1,j . j 2 Inthiscase,dist(x −b ,Z)=dist(x −b ,Z)= 1g ≥ 1 . Forotheri,dist(x −b ,Z)is 1,j j N,j j 2 j 2N i,j j evenlargerbytheorderrelationwithin{{x }} . i,j i∈[N] Wedefinethefulltranslationvectorb=(b ,...,b )∈Rd. Thenthetranslatedpoints{x −b} 1 d i i∈[N] satisfy: 1 dist(x −b ,Z)≥ , foralli∈[N],j ∈[d]. i,j j 2N Intuitively,b ischosenasthemidpointofthewidestgapbetweenfractionalvalues,ensuringthat j allfractionalpartsafterthetranslationareatleast gj awayfromthenearestinteger. Therefore,the 2 translatedpointsarecoordinate-wise 1 -farfromlatticepoints. 2N Wedefineb¯suchthateachofitscoordinatesisequaltothefirst⌈log(6N)⌉bitsofthecorresponding coordinateofb.Then,forallj ∈[d],wehave|b −¯b |≤ 1 ≤ 1 .Usingb¯withbitcomplexity j j 2log(6N) 6N ⌈log(6N)⌉,wecanstillensurethedistance 1 fromthelatticepoints. 3N 1 1 1 dist(x −¯b ,Z)≥dist(x −b ,Z)−|b −¯b |≥ − = , foralli∈[N],j ∈[d]. i,j j i,j j j j 2N 6N 3N Thefollowinglemmashowsthatwecanapproximatethefloorfunctionusingalogarithmicnumber ofReLUunitswithrespecttothelengthoftheintervalofinterest. LemmaB.16(FloorFunctionApproximation). Foranyn∈Nandanyγ ∈(0,1),thereexistsan n-layernetworkFloor withwidth5and5nReLUunitssuchthat n Floor (x)=⌊x⌋forallx∈[0,2n)suchthatx−⌊x⌋>γ. n Moreover,ifγ = 1 forsomet ∈ N,thenFloor canbeexactlyimplementedwith2n+log tbit t n 2 complexityunderafixedpointprecision. Proof. ToreconcilethediscontinuityofthefloorfunctionwiththecontinuityofReLUnetworks,we firstdefineadiscontinuousidealbuildingblockthatexactlyreplicatesthefloorfunctiononthetarget interval[0,2n). Wethenapproximatethisbuildingblockusingacontinuousneuralnetworkwith ReLUactivations. Theidealbuildingblock∆isdefinedas:  2x ifx∈(0,1],  2 ∆(x):= 2x−1 ifx∈(1,1], 2 0 otherwise. Forn∈N,definethefunctionFloor by: n x Floor (x)=∆n(− +1)+x−1. n 2n 45 --- Page 46 --- WewillshowbyinductionthatFloor =⌊x⌋forallx∈[0,2n). n Forthebasecasen=1,  2(−x +1)−1+x−1=0 ifx∈[0,1), x  2 Floor (x)=∆(− +1)+x−1= 2(−x +1)+x−1=1 ifx∈[1,2), 1 2 2 0+x−1=x−1 otherwise. Thisprovesthebasecase: forallx∈[0,2),wehaveFloor (x)=⌊x⌋. 1 Fortheinductivestep,assumethatFloor (x)=⌊x⌋holdsforallx∈[0, 2n). Weaimtoprovethat n Floor (x)=⌊x⌋forallx∈[0, 2n+1). n+1 Recallthat:  x x x  − 2n +1 ifx∈[0,2n) (⇔− 2n+1 +1∈(1 2,1]), ∆(− 2n+1 +1)= − x +2=−x−2n +1 ifx∈[2n,2n+1) (⇔− x +1∈(0,1]),  0 2n 2n otherwise. 2n+1 2 Thus,wehave x Floor (x)=∆n+1(− +1)+x−1 n+1 2n+1 x =∆n(∆(− +1))+x−1 2n+1  ∆n(− x +1)+x−1=Floor (x)=⌊x⌋ ifx∈[0,2n),  2n n = ∆n(−x−2n +1)+x−1=Floor (x−2n)+2n =⌊x⌋ ifx∈[2n,2n+1), 2n n ∆n(0)+x−1=x−1 otherwise. Therefore,byinduction, Floor =⌊x⌋forallx∈[0,2n). n Next,wedefinetheσapproximation∆ ofthediscontinuousblock∆as: γ,n (cid:18) (cid:19) (cid:18) (cid:19) 1 1 1 1 ∆ (x):=2σ(x)− σ x− +γ + σ x− γ,n γ 2 n γ 2 n n (cid:18) (cid:19) 1 1 − σ(x−1+γ )+ −2 σ(x−1), (49) γ n γ n n whereγ = γ . CheckFigure7foranillustrationofhow∆ lookslikeon[0,1]. Itisstraightfor- n 2n γ,n wardtocheckthat 1 1 ∆ (x)=∆(x)forallx∈[0, −γ ]∪[ ,1−γ ]. γ,n 2 n 2 n y 1 y=∆ (x) γ,n x 1 2 −γ n1 2 1−γ n1 Figure7: PlotoftheReLU-basedapproximation∆ (x)oftheidealdiscontinuousbuildingblock γ,n ∆(x)on[0,1]. 46 --- Page 47 --- Wenowexplainwhythisapproximationremainsvalidunderrecursivecompositionuptodepthn. Letusdefinethevariablex′ :=− x +1,sothatx=2n(1−x′)andx′ ∈(0,1]. Ourtargetfunction 2n is: x Floor (x)=∆n(− +1)+x−1=∆n(x′)+x−1. n 2n We are given the assumption x−⌊x⌋ > γ, and we aim to express this in terms of x′ to ensure ∆ n (x′)=∆n(x′). Weproceedstep-by-step: γ,n x−⌊x⌋>γ ⇐⇒2n(1−x′)−⌊2n(1−x′)⌋>γ ⇐⇒ −2nx′−⌊−2nx′⌋>γ ⇐⇒ −2nx′+⌈2nx′⌉>γ ⇐⇒2nx′ <⌈2nx′⌉−γ ⇐⇒2nx′ ∈(⌈2nx′⌉−1,⌈2nx′⌉−γ) (cid:91) ⇐⇒2nx′ ∈ (k−1,k−γ) k∈Z (cid:18) (cid:19) (cid:91) k−1 k−γ ⇐⇒x′ ∈ , . 2n 2n k∈Z Sincex′ ∈(0,1],weonlyneedtoconsiderk ∈[2n],i.e., (cid:18) (cid:19) (cid:91) k−1 k−γ x′ ∈ , . 2n 2n k∈[2n] Wewillnowprovebyinductiononnthefollowingstatement: (cid:18) (cid:19) Foranyγ ∈(0,1), ∆ n (x)=∆n(x)forx∈ (cid:91) k−1 ,k−γ . γ,n 2n 2n k∈[2n] For the base case n = 1, by construction of ∆ (x), we know ∆ (x) = ∆(x) for all x ∈ γ,1 γ,1 (cid:16) (cid:17) [0,1 − γ]∪[1,1− γ],whichcontainstheunion(cid:83) k−1,k−γ .Hencethebasecaseholds. 2 2 2 2 k∈[2] 2 2 Fortheinductivestep,assumetheclaimholdsforn. Weshowitholdsforn+1. Byusingγ/2in placeofγ fortheinductivehypothesis,wehave (cid:18) (cid:19) ∆ n (x)=∆ n (x)=∆n(x)forallx∈ (cid:91) k−1 ,k−γ/2 . (50) γ/2,n γ,n+1 2n 2n k∈[2n] (cid:16) (cid:17) Letx∈(cid:83) k−1, k−γ . Weanalyzetwocasesbasedonx∈[0,1)orx∈[1,1). k∈[2n+1] 2n+1 2n+1 2 2 (cid:16) (cid:17) First, consider the case x ∈ (cid:83) k−1, k−γ ⊂ [0,1). Then x < k−γ ≤ 1 −γ , so k∈[2n] 2n+1 2n+1 2 2n+1 2 n+1 ∆ (x)=2x. Lety :=2x. Then: γ,n+1 (cid:18) (cid:19) (cid:18) (cid:19) (cid:91) k−1 k−γ (cid:91) k−1 k−γ/2 y ∈ , ⊆ , . 2n 2n 2n 2n k∈[2n] k∈[2n] Thus,wehave n+1 n ∆ (x)=∆ (∆ (x)) γ,n+1 γ,n+1 γ,n+1 n =∆ (2x) γ,n+1 n =∆ (y) γ,n+1 ( =a) ∆n(y) 47 --- Page 48 --- =∆n(2x) =∆n+1(x), wheretheequation(a)followsbyEquation(50). (cid:16) (cid:17) Second, consider the case x ∈ (cid:83) k−1, k−γ ⊂ [1,1). Then 1 ≤ x < k−γ ≤ k∈[2n+1]\[2n] 2n+1 2n+1 2 2 2n+1 1−γ ,so∆ (x)=2x−1. Lety :=2x−1. Then: n+1 γ,n+1 (cid:91) (cid:18) k−2n−1 k−2n−γ(cid:19) (cid:91) (cid:18) k−1 k−γ(cid:19) (cid:91) (cid:18) k−1 k−γ/2(cid:19) y ∈ , = , ⊆ , . 2n 2n 2n 2n 2n 2n k∈[2n+1]\[2n] k∈[2n] k∈[2n] Thus,wehave n+1 n ∆ (x)=∆ (∆ (x)) γ,n+1 γ,n+1 γ,n+1 n =∆ (2x−1) γ,n+1 n =∆ (y) γ,n+1 ( =a) ∆n(y) =∆n(2x−1) =∆n+1(x), wheretheequation(a)followsbyEquation(50). Therefore,byinduction,wehaveshownthatforanyγ ∈(0,1)andanyn∈N, (cid:18) (cid:19) ∆ n (x′)=∆n(x′) forallx′ ∈ (cid:91) k−1 , k−γ . γ,n 2n 2n k∈[2n] WenowdefinetheReLU-basedfloorapproximationby n(cid:16) x (cid:17) Floor (x):=∆ − +1 +x−1. (51) n γ,n 2n Recallthattheidealtargetfunctionisgivenby (cid:16) x (cid:17) Floor (x)=∆n − +1 +x−1. n 2n Letusdenotex′ :=− x +1. Whenx−⌊x⌋>γ,thevaluex′satisfies 2n (cid:18) (cid:19) (cid:91) k−1 k−γ x′ ∈ , , 2n 2n k∈[2n] sothat∆ n (x′)=∆n(x′)bytheresultabove. Therefore,weconclude: γ,n Floor (x)=Floor (x)=⌊x⌋ forallx∈[0,2n)suchthatx−⌊x⌋>γ. n n Finallytoprovetheadditionalstatementregardingthebitcomplexity,considerthecaseγ = 1 for t somet∈N. ByEquation(51),thebitcomplexitytoimplementFloor isupperboundedbynplus n thebitcomplexitytoimplement∆ . Now,itsufficestoconsiderthebitcomplexityrequiredto γ,n implement ∆ . From Equation (49), observe that 1/γ = 2n/γ = 2n ×t for γ = 1/t. Since γ,n n t∈N,thiscanbeexactlyimplementedwithlog(2n×t)=n+log tbitcomplexity. Thus,Floor 2 n canbeimplementedexactlywith2n+log tbitcomplexity. 2 B.5 DimensionReductionviaCarefulAnalysisoftheJohnson-LindenstraussLemma Webeginwithalemmathatstatesaconcentrationofthelengthoftheprojection. 48 --- Page 49 --- LemmaB.17(Lemma15.2.2,Matousek[2013]). Foraunitvectorx∈Sd−1,let ϕ(x)=(x ,x ,··· ,x ) 1 2 m be the mapping of x onto the subspace spanned by the first m coordinates. Consider x ∈ Sd−1 chosenuniformlyatrandom. Then,thereexistsβ suchthat∥ϕ(x)∥ issharplyconcentratedaround 2 β, P[∥ϕ(x)∥ ≥β+t]≤2e−t2d/2andP[∥ϕ(x)∥ ≤β−t]≤2e−t2d/2, 2 2 whereform≥10logd,wehaveβ ≥ 1(cid:112)m. 2 d Basedontheaboveconcentrationinequality,westatetheJohnson-Lindenstrausslemma,inaversion whichreflectsthebenefitontheratioofthenormpreservedwhentheprojectingdimensionincreases. TheprooffollowsthatofTheorem15.2.1inMatousek[2013]withaslightmodification. LemmaB.18(StrengthenedVersionoftheJohnson-LindenstraussLemma). ForN ≥2,letX ⊆Rd beanN pointset. Then,foranyα ∈(0,1)and24α−2logN ≤m≤d,thereexistsa1-Lipschitz linearmappingϕ:Rd →Rmandβ >0suchthat (1−α)β∥x−x′∥ ≤∥ϕ(x)−ϕ(x′)∥ ≤(1+α)β∥x−x′∥ , (52) 2 2 2 forallx,x′ ∈X. Moreover,β ≥ 1(cid:112)m wheneverm≥10logd. 2 d Proof. Ifx = x′,theinequalitytriviallyholdsforanyϕ. Hence,itsufficestofindϕthatsatisfies Equation(52)forallx,x′ ∈X withx̸=x′. Considerarandomm-dimensionalsubspaceL,and (cid:13) (cid:13) ϕbeaprojectionontoL. Foranyfixedx̸=x′ ∈X,LemmaB.17impliesthat(cid:13)ϕ( x−x′ )(cid:13) is (cid:13) ∥x−x′∥ (cid:13) 2 2 concentratedaroundsomeconstantβ. i.e. P(cid:20)(cid:13) (cid:13) (cid:13)ϕ(cid:18) x−x′ (cid:19)(cid:13) (cid:13) (cid:13) ≥(1+α)β(cid:21) ≤2e−α2β2d/2 ( ≤a) 2e−α2m/8 ( ≤b) 2e−3logN = 2 ( ≤c) 1 , (cid:13) ∥x−x′∥ (cid:13) N3 N2 2 2 whereweuseβ ≥ 1(cid:112)m at(a),m≥24α−2logN at(b),andN ≥2at(c). Similarly, 2 d P(cid:20)(cid:13) (cid:13) (cid:13)ϕ(cid:18) x−x′ (cid:19)(cid:13) (cid:13) (cid:13) ≤(1−α)β(cid:21) ≤ 1 . (cid:13) ∥x−x′∥ (cid:13) N2 2 2 By linearity of ϕ, we have ϕ(x−x′) = ϕ(x)−ϕ(x′). Taking the union bound over the two probabilityboundsabove,thefollowingeventhappenswithprobabilityatmost2/N2: ∥ϕ(x)−ϕ(x′)∥ ≥(1+α)β∥x−x′∥ or ∥ϕ(x)−ϕ(x′)∥ ≤(1−α)β∥x−x′∥ . (53) 2 2 2 2 Next,wetakeaunionboundoverall N(N−1) pairsx,x′ ∈X withx̸=x′. Then,theprobability 2 thatEquation(53)happensforanyx,x′ ∈X withx̸=x′isatmost 2 × N(N−1) =1− 1 <1. N2 2 N Hence,thereexistsam-dimensionalsubspaceLsuchthatEquation(53)doesnotholdforanypair ofx,x′ ∈X. Inotherwords,thereexistsam-dimensionalsubspaceLsuchthat (1−α)β∥x−x′∥ ≤∥ϕ(x)−ϕ(x′)∥ ≤(1+α)β∥x−x′∥ , 2 2 2 forallx̸=x′.ByLemmaB.17,β ≥ 1(cid:112)m wheneverm≥10logd.Thisconcludesthelemma. 2 d Proposition B.19 (Lipschitz Projection with Separation). For N ≥ 2, let D = {(x ,y )}N ∈ i i i=1 D . Foranyα ∈ (0,1)and24α−2logN ≤ m ≤ d,thereexists1-Lipschitzlinearmapping d,N,C ϕ:Rd →Rmandβ >0suchthatD′ :={(ϕ(x ),y )}N ∈D satisfies i i i=1 m,N,C ϵ′ ≥(1−α)βϵ . D D Inparticular,D′ ∈D wheneverD ∈D . Moreover,β ≥ 1(cid:112)m wheneverm≥10logd. m,N,C d,N,C 2 d Proof. LetX ={x }N . ByLemmaB.18,thereexists1-Lipschitzlinearmappingϕ:Rd →Rm i i=1 andβ >0suchthat (1−α)β∥x −x ∥ ≤∥ϕ(x )−ϕ(x )∥ ≤(1+α)β∥x −x ∥ (54) i j 2 i j 2 i j 2 49 --- Page 50 --- foralli,j ∈[N]. Theinequalityϵ ≥(1−α)βϵ followsfromtheinequalityfromLemmaB.18. Inparticular, D′ D 1 ϵ = min{∥ϕ(x )−ϕ(x )∥ | i,j ∈[N]andy ̸=y } D′ 2 i j 2 i j (a) 1 ≥ min{(1−α)β∥x −x ∥ | i,j ∈[N]andy ̸=y } 2 i j 2 i j 1 =(1−α)β× min{∥x −x ∥ | i,j ∈[N]andy ̸=y } 2 i j 2 i j =(1−α)βϵ , D whereweuseEquation(54)at(a). WenextshowD′ ∈D wheneverD ∈D . Toshowthis,weneedtoproveϕ(x )̸=ϕ(x ) m,N,C d,N,C i j foralli̸=j. Since1−α>0andβ >0,wehave∥ϕ(x )−ϕ(x )∥ ≥(1−α)β∥x −x ∥ >0 i j 2 i j 2 wheneverx ̸=x . Moreover,D ∈D indicatesthatx ̸=x wheneveri̸=j. Alltogether,we i j d,N,C i j haveϕ(x )̸=ϕ(x )foralli̸=j sothatD′ ∈D . i j m,N,C LemmaB.20(Projectionontolog-scaleDimension). LetD ∈D . Then,thereexistaninteger d,N,C m = O˜(logN) and a 1-Lipschitz linear map ϕ : Rd → Rm such that the projected dataset D′ ={(ϕ(x ),y )} ∈D satisfiestheseparationbound i i i∈[N] m,N,C (cid:114) 5 m ϵ′ ≥ ϵ . D 12 d D Proof. Let α = 1/6 and m := min{d,max{⌈24α−2logN⌉,⌈10logd⌉}}, then m = O˜(logN). Weconstructthelinearmappingintomdimensionbydividingthecasesintod<24α−2logN or d≥24α−2logN. For the case d < 24α−2logN, we have d < max{⌈24α−2logN⌉,⌈10logd⌉}, and therefore m = d. We consider the identity map ϕ : Rd → Rd(= Rm), which is 1-Lipschitz. We have D′ :={(ϕ(x ),y )} ={(x ,y )} =D,sothatϵ =ϵ > 5 ϵ = 5 (cid:112)mϵ . i i i∈[N] i i i∈[N] D′ D 12 D 12 d D Otherwise,forthecased≥24α−2logN,wefirstobservethatm≤d. Since24α−2logN ≤d,we have ⌈24α−2logN⌉≤d. (55) Additionally,asN ≥2,wehaved≥24α−2logN ≥864log2≥e4. ByLemmaB.22,thisimplies 10logd≤dandtherefore ⌈10logd⌉≤d. (56) By Equations (55) and (56), we have max{⌈24α−2logN⌉,⌈10logd⌉} ≤ d. Thus, it follows m = max{⌈24α−2logN⌉,⌈10logd⌉} ≤ d. By Proposition B.19 with α = 1, there exists 1- 6 Lipschitz linear mapping ϕ : Rd → Rm and β > 0 such that D′ = {(ϕ(x ),y )} satisfies i i i∈[N] ϵ ≥ 5βϵ . Sincem=max{⌈24α−2logN⌉,⌈10logd⌉}≥10logd,theinequalityβ ≥ 1(cid:112)m is D′ 6 D 2 d alsosatisfiedbyPropositionB.19. Therefore,ϵ ≥ 5βϵ ≥ 5 (cid:112)mϵ . D′ 6 D 12 d D Inbothcases,wehave1-LipschitzlinearmapϕsuchthatD′ ={(ϕ(x ),y )} hasseparation i i i∈[N] (cid:114) 5 m ϵ ≥ ϵ . D′ 12 d D Proposition B.21 (Natural Projection of High Dimensional Data). For d ≥ N, let D = {(x ,y )} ∈ D . Then, there exists 1-Lipschitz linear mapping φ : Rd → RN such i i i∈[N] d,N,C thatD′ ={(φ(x ),y )} ∈D satisfies i i i∈[N] N,N,C ϵ =ϵ D′ D 50 --- Page 51 --- Proof. ConsiderthetallmatrixX ∈Rd×N definedas (cid:34) | | ··· | (cid:35) X = x x ··· x . (57) 1 2 N | | ··· | ThendimCol(X)≤N ≤d. TakeanysubspaceV suchthatCol(X)⊆V ⊆RdanddimV =N, and let B = {v ,··· ,v } be an orthonormal basis of V. Let V ∈ Rd×N be the matrix whose 1 N columnsconsistofvectorsinB: (cid:34) | | ··· | (cid:35) V = v v ··· v . 1 2 N | | ··· | Define φ : Rd → RN as φ(x) = V⊤x. We first verify that φ is 1-Lipschitz. For any x ∈ Rd, let x = x +x where x ∈ V and x ∈ V⊥. Then, x = Vz for some z ∈ RN, as V V⊥ V V⊥ V x ∈Col(V). Moreover, V V⊤x=V⊤(x +x ) V V⊥ =V⊤x +0 V =V⊤Vz =I z N =z. (58) Therefore,wehave (cid:13) (cid:13)V⊤x(cid:13) (cid:13) 2 ( =a) ∥z∥ 2 ( =b) ∥x V∥ 2 ( ≤c) ∥x∥ 2, (59) (cid:13) (cid:13)2 where(a)isbyEquation(58),(b)isbecause∥x ∥2 =(cid:13)(cid:80) z v (cid:13) =(cid:80) z2 =∥z∥2,and V 2 (cid:13) i∈[N] i i(cid:13) i∈[N] i 2 2 (c)isbecause∥x∥2 =∥x ∥2+∥x ∥2. Moreover,wheneverx∈V,thentheequalityholdsfor 2 V 2 (cid:13) V⊥ (cid:13)2 (c)ofEquation(59). Therefore,(cid:13)V⊤x(cid:13) =∥x∥ forallx∈V. 2 2 Sinceφislinear ∥φ(x)−φ(x′)∥ =∥φ(x−x′)∥ 2 2 =(cid:13) (cid:13)V⊤(x−x′)(cid:13) (cid:13) 2 (a) ≤ ∥x−x′∥ , 2 where(a)isbyEquation(59). Thisshowsthatφis1-Lipschitz. Next,fori,j ∈[N],wehave ∥φ(x )−φ(x )∥ =∥φ(x −x )∥ i j 2 i j 2 =(cid:13) (cid:13)V⊤(x i−x j)(cid:13) (cid:13) 2 (a) = ∥x −x ∥ , i j 2 wherethelastequalityholdsbecausex −x ∈Col(X)⊆V. i j Thisshowsthat 1 ϵ = min{∥φ(x )−φ(x )∥ | y ̸=y } D′ 2 i j 2 i j 1 = min{∥x −x ∥ | y ̸=y } 2 i j 2 i j =ϵ . D ThisshowsthatD′alsohasthedesiredproperty. LemmaB.22. Fort≥e4,wehavet≥10logt. 51 --- Page 52 --- Proof. Defineu(t):=t−10logtonthedomain(0,∞). Then,forallt>10, du 10 =1− >0, dt t sothatuisanincreasingfunctionon(10,∞). Inparticular, u(e4)=e4−10log(e4)=e4−40≥0 Thisconcludesthatu(t)≥0forallt≥e4,orequivalently,t≥10logtforallt≥e4. B.6 LemmasforBitComplexity Thefollowinglemmaboundshowmuchbitcomplexityissufficientforimplementingtheparameters oftheneuralnetworkinordertoobtaintherequiredprecisionoftheoutput. Notethatwedonot requirethenetworktooutputscalavalues. i.e. thefollowinglemmaalsoappliestoneuralnetworks thatoutputvectors. Lemma B.23. Let f be a neural network of P parameters, depth L and width D in which the parameters have infinite precision. Let R ≥ 1 be the radius of the domain in which we want to approximatef. Ifalltheparametersoff areboundedbysomeM ≥ 1,thenforany0 < ν < 1, thereexistsf¯,whichisimplementedwithP parameters,depthL,widthD,andO˜(L)bitcomplexity suchthat max (cid:13) (cid:13)f¯−f(cid:13) (cid:13) ≤ν, ∥x∥ ≤R 2 2 whereO˜(·)hidesapolylogarithmicdependencyonD,M,L,Rand1/ν. Proof. Letf :Rd →Rbetheneuralnetworkdefinedas a =x 0 a =σ(W a (x)+b )forℓ=1,2,··· ,L−1 ℓ ℓ ℓ−1 ℓ f(x)=W (a )+b , L L−1 L whereW ℓ ∈ Rdℓ×dℓ−1,b ℓ ∈ Rdℓ withd ℓ ≤ D forallℓ ∈ [L]. Althougha ℓ dependsonxforall ℓ = 0,··· ,L−1,weomitxinthenotation. Notethatd = d. GiventhateveryelementsofW 0 ℓ andb areboundedbyM,forany0<ζ ≤M,thereexistsW¯ andb¯ thatcanbeimplementedwith ℓ ℓ ℓ ⌈log (M/ζ)⌉bitcomplexityinwhich 2 (cid:13) (cid:13)W¯ ℓ−W ℓ(cid:13) (cid:13) ∞ ≤ζ (cid:13) (cid:13)b¯ ℓ−b ℓ(cid:13) (cid:13) ∞ ≤ζ. UsingtheapproximatedparametersW¯ andb¯ ,werecursivelydefinef¯:Rd →R,thefinite-precision ℓ ℓ approximationoff. a¯ =x 0 a¯ =σ(W¯ a¯ (x)+b¯ )forℓ=1,2,··· ,L−1 ℓ ℓ ℓ−1 ℓ f¯(x)=W¯ (a¯ )+b¯ . L L−1 L Similarly,althougha¯ dependsonxforallℓ=0,··· ,L−1,weomitxinthenotation. ℓ Letusdenotethedifferenceofparametersas∆W :=W¯ −W ,∆b :=b¯ −b forℓ∈[L]and ℓ ℓ ℓ ℓ ℓ ℓ thedifferenceoflayeroutputs∆a :=a¯ −a forℓ∈[L−1]. Itisstraightforwardtocheck ℓ ℓ ℓ (cid:112) ∥W ∥≤∥W ∥ ≤ d d M ≤DM ℓ ℓ F √ l l−1 (cid:112) ∥b ∥≤ d M ≤ DM ℓ l (cid:112) ∥∆W ∥≤∥∆W ∥ ≤ d d ζ ≤Dζ ℓ ℓ F√ l l−1 (cid:112) ∥∆b ∥≤ d ζ ≤ Dζ, ℓ l where the norm ∥·∥ and ∥·∥ for the matrix denote the spectral norm and the Frobenius norm, F respectively. 52 --- Page 53 --- Wefirstclaimthatthereexistsadegree2L+1polynomialSonD,M,LandRsuchthat∥a ∥ ≤S ℓ 2 forallℓ∈[L−1]. ∥a ∥ =∥σ(W a +b )∥ ℓ 2 ℓ ℓ−1 ℓ 2 ≤∥W a +b ∥ ℓ ℓ−1 ℓ 2 ≤∥W ∥∥a ∥ +∥b ∥ ℓ ℓ−1 2 √ ℓ 2 ≤DM∥a ∥ + DM. ℓ−1 2 Thusforallℓ∈[L−1],   √ ∥a ℓ∥ 2 ≤ DM (cid:88) (DM)ℓ′−1 +(DM)l∥a 0∥ 2 ℓ′∈[ℓ] √ ≤ DML(DM)L−1+(DM)L−1R ≤(DM)L−1(DML+R)=:S Thisprovesthefirstclaim. Moreover,S iscomposedoftwomonomialswhosecoefficientsareall1. Wenextclaimthattheerror∥∆a ∥≤Qζ forsomedegree4L+1polynomialQonD,M,Land L−1 R. Considerthefollowingrecurrence ∥∆a ∥ =∥a¯ −a ∥ ℓ 2 ℓ ℓ 2 =(cid:13) (cid:13)σ(W¯ ℓa¯ ℓ−1+b¯ ℓ)−σ(W ℓa ℓ−1+b ℓ)(cid:13) (cid:13) 2 ≤(cid:13) (cid:13)(W¯ ℓa¯ ℓ−1+b¯ ℓ)−(W ℓa ℓ−1+b ℓ)(cid:13) (cid:13) 2 =∥((W +∆W )(a +∆a )+(b +∆b ))−(W a +b )∥ ℓ ℓ ℓ−1 ℓ ℓ ℓ ℓ ℓ−1 ℓ 2 =∥∆W a +W ∆a +∆W ∆a +∆b ∥ ℓ ℓ−1 ℓ ℓ−1 ℓ ℓ−1 ℓ 2 ≤∥∆W ∥∥a ∥ +∥W ∥∥∆a ∥ +∥∆W ∥∥∆a ∥ +∥∆b ∥ ℓ ℓ−1 2 ℓ ℓ−1 2 ℓ √ℓ−1 2 ℓ 2 ≤Dζ∥a ∥ +DM∥∆a ∥ +Dζ∥∆a ∥ + Dζ ℓ−1 2 ℓ−1 2 √ℓ−1 2 =(DM +Dζ)∥∆a ∥ +(D∥a ∥ + D)ζ ℓ−1 2 ℓ− √1 2 ≤(DM +Dζ)∥∆a ∥ +(DS+ D)ζ. ℓ−1 2 Thusnotingthat∆a =x−x=0wehave, 0 (cid:16) √ (cid:17) (cid:88) ∥∆a ∥ ≤ DS+ D ζ (DM +Dζ)ℓ−1 L−1 2 ℓ∈[L−1] (cid:16) √ (cid:17) ≤ DS+ D ζ×L(DM +Dζ)L−1 ≤DL(S+1)(DM +Dζ)L−1ζ ≤DL(S+1)(2DM)L−1ζ, where the last inequality follows from ζ ≤ M. Let Q := DL(S +1)(2DM)L−1. Since S is a degree2L+1polynomialonD,M,LandR,itfollowsthatQisadegree4L+1polynomialon D,M,LandR. Thisprovesthesecondclaim. Moreover,Qiscomposedofthreemonomialswhose coefficientsareatmost2L. Thus, (cid:13) (cid:13)f¯(x)−f(x)(cid:13) (cid:13) 2 =(cid:13) (cid:13)(W¯ La¯ L−1+b¯ L)−W La L−1+b L(cid:13) (cid:13) 2 ≤∥((W +∆W )(a +∆a )+(b +∆b ))−(W a +b )∥ L L L−1 L−1 L L L L−1 L 2 =∥∆W a +W ∆a +∆W ∆a +∆b ∥ L L−1 L L−1 L L−1 L 2 ≤∥∆W ∥∥a ∥ +∥W ∥∥∆a ∥ +∥∆W ∥∥∆a ∥ +∥∆b ∥ L L−1 2 L L−1 2 L √ L−1 2 L 2 ≤Dζ∥a ∥ +DM∥∆a ∥ +Dζ∥∆a ∥ + Dζ L−1 2 L−1 2 L−1 2 ≤DζS+DMQζ+DζQζ+Dζ ≤(DS+DMQ+DMQ+D)ζ, 53 --- Page 54 --- whereweuseζ ≤M inthelastinequality. Now,bylettingζ := ν ,itfollowsthat DS+2DMQ+D (cid:13) (cid:13)f¯(x)−f(x)(cid:13) (cid:13) ≤ν, 2 forallxwith∥x∥ ≤R. Thus,itsufficestohavelog (M/ζ)=log ((DS+2DMQ+D)M/ν) 2 2 2 bitcomplexitytoattainanapproximationofaccuracyν uniformlyovertheboundeddomainwith radiusR. (DS+2DMQ+D)M isadegree4L+4polynomialonD,M,LandR. Moreover,itis composedof2+3+1=6monomials,whosecoefficientsareatmost2L+1. Hence,itfollowsthat log (M/ζ)=log ((DS+2DMQ+D)M/ν) 2 2 ≤log (6×2L+1×(DMLR)4L+4)+log(1/ν) 2 =O(Llog (2DMLR)+log(1/ν)) 2 =O˜(L) bitcomplexitysuffices. 54 --- Page 55 --- C Extensionstoℓ -norm p Inthissection,weextendthepreviousresultsonℓ -normtoarbitraryp-norm,wherep∈[1,∞]. 2 Inthefollowing,weusedist (·,·)todenotetheℓ -normdistancebetweentwopoints,apointand p p aset,ortwosets. Forthecased = 1,weomitthenotationpsinceeveryℓ -normin1-dimension p denotestheabsolutevalue. WedenoteB p(x,µ)=(cid:8) x′ ∈Rd(cid:12) (cid:12)∥x′−x∥ p <µ(cid:9) anopenℓ p-ballcenteredatxwitharadiusµ. DefinitionC.1. ForD ∈D ,theseparationconstantϵ underℓ -normisdefinedas d,N,C D,p p 1 ϵ := min{∥x −x ∥ |(x ,y ),(x ,y )∈D, y ̸=y }. D,p 2 i j p i i j j i j AsweconsiderDwithx ̸=x foralli̸=j,wehaveϵ >0.Next,wedefinerobustmemorization i j D,p underℓ -norm. p DefinitionC.2. ForD ∈ D ,p ∈ [1,∞],andagivenrobustnessratioρ ∈ (0,1),definethe d,N,C robustnessradiusasµ=ρϵ . Wesaythatafunctionf :Rd →Rρ-robustlymemorizesDunder D,p theℓ -normif p f(x′)=y , forall(x ,y )∈Dandx′ ∈B (x ,µ), i i i p i andB (x ,µ)isreferredastherobustnessballofx . p i i Similarly,weextendthenotionofρ-robustmemorizationerrortoℓ -norm. p Definition C.3. Let D ∈ D be a class(or point)-separated dataset. The ρ-robust error of a d,N,C networkf :Rd →RonDundertheℓ -normisdefinedas p L (f,D)= max P [f(x′)̸=y ], whereµ=ρϵ (orµ=ρϵ′ ). ρ,p (xi,yi)∈D x′∼Unif(Bp(xi,µ)) i D,p D,p Thefollowinginclusionbetweenp-normballswithdifferentp-valuesiswellknown. LemmaC.4(InclusionBetweenBalls). Let0<p<q ≤∞. Then,foranyx∈Rdandµ>0, B p(x,µ)⊆B q(x,µ)⊆B p(x,dp1− q1 µ), orequivalently, B q(x,dq1− p1 µ)⊆B p(x,µ)⊆B q(x,µ). Foranyp∈[1,∞],letusdenote γ p(d):=d| 21− p1| throughoutthissection. For0<p<q ≤∞,wehave ϵ D,q ≤ϵ D,p ≤dp1− q1 ϵ D,q, (60) since∥x∥ ≤∥x∥ ≤dp1− q1 ∥x∥ . Inparticular,wehave q p q ϵ ≤ϵ whenp≥2, (61) D,p D,2 ϵ ≤γ (d)ϵ whenp<2. (62) D,p p D,2 C.1 ExtensionofNecessityConditiontoℓ -norm p TheoremC.5. Letρ∈(0,1). SupposeforanyD ∈D ,thereexistsaneuralnetworkf ∈F d,N,2 d,P thatcanρ-robustlymemorizeDunderℓ -norm. Then,thenumberofparametersP mustsatisfy p • P =Ω(cid:16)(cid:0) ρ2min{N,d}+1(cid:1) d+min(cid:110) √ 1 ,√ d(cid:111)√ N(cid:17) ifp≥2. 1−ρp (cid:18)(cid:18)(cid:16) (cid:17)2 (cid:19) (cid:110) √ (cid:111)√ (cid:19) • P =Ω ρ min{N,d}+1 d+min √ 1 , d N if1≤p<2. γp(d) 1−ρp 55 --- Page 56 --- Proof. ThisfollowsbycombiningPropositionC.6andPropositionC.8. PropositionC.6. ThereexistsD ∈D suchthatanyneuralnetworkf :Rd →Rthatρ-robustly d,N,2 memorizesDunderℓ -normmusthavethefirsthiddenlayerwidthatleast p • ρ2min{N −1,d}ifp≥2. (cid:16) (cid:17)2 • ρ min{N −1,d}if1≤p<2. γp(d) Proof. WetakeDthesamedatasetasinProposition3.2. RecallthatintheproofofProposition3.2, we take the dataset D = {e ,2} ∪{0,1} when N ≤ d+1, with additional data points j j∈[N−1] (2e ,2),(3e ,2),··· ,((N −d)e ,2) when N > d+1. This has a separation ϵ = 1 under 1 1 1 D,p 2 ℓ -normforallp≥1,onthebothcaseN ≤d+1andN >d+1. Letf beaneuralnetworkthat p robustlymemorizesD underℓ -norm. Sinceϵ = ϵ ,therobustnessradiusµunderℓ -norm p D,p D,2 2 satisfiesµ=ρϵ =ρϵ . Withthisinmind,wenowprovetheproposition. Thestatementofthe D,p D,2 propositionconsistsoftwoparts,p≥2and1≤p<2. PartI:p≥2. First,weprovetheresultunderp≥2Robustmemorizationunderℓ -normimplies p f(x)=y forall(x ,y )∈Dandx∈B (x ,µ), i i i p i whereµ=ρϵ =ρϵ . Forp≥2,wehaveB (x ,µ)⊆B (x ,µ)byLemmaC.4. Thus, D,p D,2 2 i p i f(x)=y forall(x ,y )∈Dandx∈B (x ,µ). i i i 2 i Sinceµ=ρϵ thisimpliesthatf ρ-robustlymemorizeDunderℓ -norm. ByProposition3.2,f D,2 2 shouldhavethefirsthiddenlayerwidthatleastρ2min{N −1,d}. Part II: 1 ≤ p < 2. Next, we prove the result under 1 ≤ p < 2. Robust memorization under ℓ -normimplies p f(x)=y forall(x ,y )∈Dandx∈B (x ,µ), i i i p i where µ = ρϵ D,p = ρϵ D,2. For 1 ≤ p < 2, we have B 2(x,d21− p1 µ) ⊆ B p(x i,µ) by applying p = pandq = 2toLemmaC.4. Sinceγ p(d) = dp1−1 2,wehaveB 2(x i,µ/γ p(d)) ⊆ B p(x i,µ). In particular,f memorizeeveryµ/γ (d)neighboraroundthedatapointunderℓ -norm. Let p 2 µ/γ (d) ρϵ /γ (d) ρ ρ′ := p = D,2 p = ϵ ϵ γ (d) D,2 D,2 p Then,f memorizeeveryµ/γ (d)=ρ′ϵ radiusneighboraroundeachdatapointunderℓ -norm. p D,2 2 Inotherwords,f ρ′-robustlymemorizeD underℓ -norm. ByProposition3.2,f shouldhavethe 2 firsthiddenlayerwidthatleast(ρ′)2min{N −1,d}. Puttingbackρ′ = ρ concludesthedesired γp(d) statement. Proposition C.7. There exists a point separated D ∈ D such that any neural network that d,N,2 ρ-robustlymemorizesDunderℓ -normmusthavethefirsthiddenlayerwidthatleast ∞ • ρ2min{d,N −1}ifρ∈(0,1]. 2 • min{d,N −1}ifρ∈(1,1). 2 Proof. ThefirstbulletisanimmediatecorollaryofPropositionC.6,sowefocusonthesecondbullet forρ∈(1/2,1). Toprovethesecondbullet,weconsidertwocasesbasedontherelationshipbetween N −1andd. Inthefirstcase,whereN −1≤d,establishingthepropositionrequiresthatthefirst hiddenlayerhaswidthatleastN −1. Inthesecondcase,whereN −1>d,therequiredwidthis atleastd. Foreachcase,weconstructadatasetD ∈D suchthatanynetworkthatρ-robustly d,N,2 memorizesDmusthaveafirsthiddenlayerofwidthnosmallerthanthecorrespondingbound. 56 --- Page 57 --- CaseI:N −1 ≤ d. LetD = {(e ,2)} ∪{(0,1)}. Then, D hasaseparationconstant j j∈[N−1] ϵ =1/2underℓ -norm. Letf beaρ-robustmemorizerofDunderℓ -normwhosefirsthidden D,∞ ∞ ∞ layerwidthism. LetW ∈Rm×ddenotethefirsthiddenweightmatrix. Supposeforacontradiction, m<N −1. Letµ=ρϵ denotetherobustnessradius. Then,f hastodistinguisheverypointineachB (e ) D,∞ µ j fromeverypointinB (0)forallj ∈[N −1]. Therefore,forx∈B (e ,µ)andx′ ∈B (0,µ), µ ∞ j ∞ wehave Wx̸=Wx′, orequivalently,x−x′ ∈/ Null(W). Moreover B (e ,µ)−B (0,µ):={x−x′ :x∈B (e ,µ)andx′ ∈B (0,µ)}=B (e ,2µ). ∞ j ∞ ∞ j ∞ ∞ j Hence,itisnecessarytohaveB (e ,2µ)∩Null(W)=∅forallj ∈[N −1],orequivalently, ∞ j dist (e ,Null(W))≥2µ (63) ∞ j forallj ∈[N −1]. SincedimCol(W⊤) ≤ dimRm = m,wehavedimNull(W) ≥ d−m. UsingLemmaC.10,we canupperboundsthemaximumpossibledistancebetween{e } ⊆Rdandarbitrarysubspace j j∈[N−1] ofafixeddimension. TakeZ ⊆ Null(W)suchthatdimZ = d−mandsubstituted = d,t = N −1,k = d−mand Z =Z intoLemmaC.10. Theassumptionst≤dforthelemmaaresatisfiedsinceN −1≤d. The additionalassumptionk ≥d−t+1isequivalenttod−m≥d−(N −1)+1andissatisfiedsince m<N −1. Therefore,wehave 1 min dist (e ,Z)≤ . j∈[N−1] ∞ j 2 BycombiningtheaboveinequalitywithEquation(63), (a) 1 2µ≤ min dist (e ,Null(W)) ≤ min dist (e ,Z)≤ , (64) j∈[N−1] ∞ j j∈[N−1] ∞ j 2 where (a) is due to Z ⊆ Null(W). Since ϵ = 1/2, we have 2µ = 2ρϵ = ρ so that D,∞ D,∞ Equation(64)becomesρ ≤ 1/2. Thiscontradictsourassumptionρ ∈ (1/2,1),andthereforethe widthrequirementm≥N −1isnecessary. ThisconcludestheproofforthecaseN −1≤d. Case II : N −1 > d. We construct the first d+1 data points in the same manner as in Case I,usingtheconstructionforN = d+1. FortheremainingN −d−1datapoints, wesetthem sufficientlydistantfromthefirstd+1datapointstokeepϵ = 1/2. Inparticular,wecanset D,∞ x = 2e ,x = 3e ,··· ,x = (N −d)e andy = y = ··· = y = 2. Compared d+2 1 d+3 1 N 1 d+2 d+3 N tothecaseN =d+1,wehaveϵ unchangedwhilehavingmoredatapointstomemorize. By D,∞ thenecessityforthecaseN =d+1,thisdatasetalsorequiresthefirsthiddenlayerwidthatleast (d+1)−1=d. ThisconcludesthestatementforthecaseN −1>d. CombiningtheresultofthetwocasesN−1≤dandN−1>dconcludestheproofofthetheorem. PropositionC.8. Forp∈[1,∞),letρ∈(cid:16) 0,(cid:0) 1− 1(cid:1)1/p(cid:105) . SupposeforanyD ∈D thereexists d d,N,2 f ∈ F thatρ-robustlymemorizesD underℓ -norm. Then, thenumberofparametersP must d,P p (cid:113) satisfyP =Ω( N ). 1−ρp Proof. The main idea of the proof is the same as Proposition 3.3. We construct ⌊N⌋×⌊ 1 ⌋ 2 1−ρp numberofdatapointsthatcanbeshatteredbyF .ThisprovesVC-dim(F )≥⌊N⌋×⌊ 1 ⌋= d,P d,P 2 1−ρp (cid:112) Ω(N/(1−ρp)). SinceVC-dim(F )=O(P2),thisprovesP =Ω( N/(1−ρp)). d,P 57 --- Page 58 --- For simplicity of the notation, let us denote k := ⌊ 1 ⌋. To prove the lower bound on the VC- 1−ρp dimension,weconstructk×⌊N⌋pointsinRd thatcanbeshatteredbyF . Asintheproofof 2 d,P Proposition3.3,wedefine⌊N⌋×knumberofpointsas⌊N⌋groups,whereeachgroupconsistsofk 2 2 points. Westartbyconstructingthefirstgroup. Sinceρ∈(0,(cid:0)d−1(cid:1)1/p ],wehavek =⌊ 1 ⌋∈[1,d]. The d 1−ρp firstgroupX := {e }k ⊆ Rd isdefinedasthesetofthefirstkvectorsinthestandardbasisof 1 j j=1 Rd. Theremaining⌊N⌋−1groupsaresimplyconstructedasatranslationofX . Inparticular,for 2 1 l∈[⌊N⌋],wedefine 2 X :=c +X ={c +x | x∈X } l l 1 l 1 wherec :=2d2(l−1)×e ensuresthateachgroupissufficientlyfarfromoneanother. Notethat l 1 c =0ensuresX alsosatisfiestheconsistencyofthenotation. Now,defineX =∪ X ,the 1 1 l∈[⌊N/2⌋] l unionofall⌊N⌋groupswhichconsistsofk×⌊N⌋points. 2 2 We claim that if for any D ∈ D , there exists f ∈ F that ρ-robustly memorizes D under d,N,2 d,P ℓ -norm, then X is shattered by F . To prove the claim, suppose we are given arbitrary label p d,P Y ={y } ofX,wherey ∈{±1}denotesthelabelforx :=c +e ∈X. Given l,j l∈[⌊N/2⌋],j∈[d] l,j l,j l j thelabelY,weconstructD ∈D suchthatwheneverf ∈F ρ-robustlymemorizeDunder d,N,2 d,P ℓ -norm,thenitsaffinetranslationf′ =2f −3∈F satisfiesf′(x )=y forallx ∈X. p d,P l,j l,j l,j Foreachl∈[⌊N/2⌋],letJ+ ={j ∈[k] | y =+1}andJ− ={j ∈[k] | y =−1}. Define l l,j l l,j (cid:88) (cid:88) x =c + e − e 2l−1 l j j j∈J+ j∈J− l l (cid:88) (cid:88) x =c + e − e 2l l j j j∈J− j∈J+ l l Furthermore, define y = 2,y = 1 and let D = {(x ,y )} ∈ D . To consider the 2l−1 2l i i i∈[N] d,N,2 separationϵ ,noticethat D,2 (cid:13)  (cid:13) (cid:13) (cid:13) ∥x 2l−1−x 2l∥ p =(cid:13) (cid:13) (cid:13)2(cid:88) e j − (cid:88) e j(cid:13) (cid:13) (cid:13) ( =a) 2k1/p, (cid:13) j∈J+ j∈J− (cid:13) l l p where(a)isduetoJ+∩J− =∅andJ+∪J− =[k]. Forl̸=l′, l l l l (a) d (x ,x ) ≥ d (c ,c )−d (c ,x )−d (c ,x ) p 2l−1 2l′ p l l′ p l 2l−1 p l′ 2l′ (b) ≥ 2d2−k1/p−k1/p (c) ≥ 2d2−2d1/p (d) ≥ 2d1/p (e) ≥ 2k1/p, where(a)isbythetriangleinequalityunderℓ -norm(namely,theMinkowskiinequality),(b)uses p d (c ,x ) = d (c ,x ) = k1/p, (c),(e)isbyk ≤ d, and(d)holdsforalld ≥ 2andp ≥ 1. p l 2l−1 p l′ 2l′ Thus,wehaveϵ ≥k1/p. D,p Takef ∈ F thatρ-robustlymemorizeD. Wefirstlowerboundtherobustnessradiusµ. Since d,P (cid:113) t(cid:55)→ϕ p t−1 isanstrictlyincreasingfunctionfromt≥1onto[0,1)3 ,ithasawelldefinedinverse t mappingϕ−1 :[0,1)→[1,∞)definedasϕ−1(ρ)= 1 . Therefore, 1−ρp (cid:18) (cid:19) (cid:18) (cid:19) (cid:114) 1 1 k−1 ρ=ϕ(ϕ−1(ρ))=ϕ ≥ϕ ⌊ ⌋ =ϕ(k)= p . 1−ρp 1−ρp k 3ϕisacompositionoftwostrictlyincreasingone-to-onecorrespondingfunctionst(cid:55)→ t−1 from[1,∞)onto √ t [0,1)andu(cid:55)→ pufrom[0,1)onto[0,1) 58 --- Page 59 --- Sinceϵ ≥ k1/p andρ ≥ (k−1)1/p,wehaveµ = ρϵ ≥ ρk1/p ≥ (k−1)1/p. Thus,everyf D,p k D,p thatρ-robustlymemorizesDmustalsomemorize(k−1)1/pradiusopenℓ -ballaroundeachpoint p inDasthesamelabelasthedatapoint. Moreover,forx ∈X withpositivelabely =+1,wehave l,j l,j (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) (cid:88) (cid:13) ∥x l,j −x 2l−1∥ p =(cid:13) (cid:13)(c l+e j)−(c l+ e j′ − e j′)(cid:13) (cid:13) (cid:13) j′∈J+ j′∈J− (cid:13) l l p (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) (cid:88) (cid:13) =(cid:13) e − e (cid:13) (cid:13) j′ j′(cid:13) (cid:13) (cid:13) (cid:13)j′∈J+ j′∈J− (cid:13) (cid:13) j′̸=jl l (cid:13) p =(k−1)1/p. Takeasequenceofpoints{z n} n∈Nsuchthatz n →x l,j asn→∞4and ∥z −x ∥ <(k−1)1/p, n 2l−1 p foralln∈N. Inparticular, n−1 1 z := x + x n n l,j n 2l−1 satisfiessuchproperties. Then,wehavef(z ) = f(x ) = 2foralln ∈ N. Moreover,bythe n 2l−1 continuityoff (undertheusualtopology), f(x )=f( lim z )= lim f(z )= lim 2=2. l,j n n n→∞ n→∞ n→∞ Similarly, for x with negative label y = −1, we have ∥x −x ∥ = (k −1)1/p, so that l,j l,j l,j 2l p f(x )=1. l,j Since we can adjust the weight and the bias of the last hidden layer, F is closed under affine d,P transformation; thatis, af +b ∈ F wheneverf ∈ F . Inparticular, f′ := 2f −3 ∈ F . d,P d,P d,P Thisf′ satisfiesf′(x ) = 2f(x )−3 = 2·2−3 = +1whenevery = +1andf′(x ) = l,j l,j l,j l,j 2f(x )−3 = 2·1−3 = −1whenevery = −1. Thus,sign◦f′ perfectlyclassifyX withthe l,j l,j labelY. Sincewecantakesuchf′ ∈ F givenanarbitrarylabelY ofX, itfollowsthatF d,P d,P shattersX,concludingtheproofofthetheorem. C.1.1 LemmasforAppendixC.1 LemmaC.9. Let{e } ⊆ Rd denotethestandardbasisinRd. Then,foranyk-dimensional j j∈[d] subspaceZ ofRdwithk ≥1wehave, 1 mindist (e ,Z)≤ . j∈[d] ∞ j 2 Proof. ForanysubspaceZ′ofZ,wehave mindist (e ,Z)≤ mindist (e ,Z′). ∞ j ∞ j j∈[d] j∈[d] Aseveryk-dimensionalsubspaceofRd withk ≥1hasaone-dimensionalsubspace,itsufficesto provethesecondstatementfork =1. i.e.,foranyone-dimensionalsubspaceZ ofRd, 1 mindist (e ,Z)≤ . j∈[d] ∞ j 2 4Weconsidertheconvergenceofthesequenceontheusualtopologyinducedbyℓ -norm. 2 59 --- Page 60 --- LetZ =Span(z),wherez =(z ,··· ,z )̸=0. Withoutlossofgenerality,let∥z∥ =1andtake 1 d ∞ j ∈[d]suchthat|z |=1. Letz′ = zjz ∈Z. Then, j 2 (cid:13) z z z z z z z z z z (cid:13) ∥z′−e ∥ =(cid:13)( j 1,··· , j j−1, j j −1, j j+1,··· , j d)(cid:13) j ∞ (cid:13) 2 2 2 2 2 (cid:13) ∞ (cid:13) (cid:13) ( =a)(cid:13) (cid:13)(z jz 1,··· ,z jz j−1,−1 ,z jz j+1,··· ,z jz d)(cid:13) (cid:13) (cid:13) 2 2 2 2 2 (cid:13) ∞ (b) 1 ≤ , 2 where(a)isby|z |=1,and(b)isby∥z∥ =1. Therefore, j ∞ 1 min dist (e ,Z)≤dist (e ,Z)≤∥z′−e ∥ ≤ , j′∈[d] ∞ j′ ∞ j j ∞ 2 concludingthestatement. ThefollowinglemmageneralizesLemmaC.9tothecasewhereweconsideronlythedistancetoa subsetofthestandardbasis,insteadofthewholestandardbasis. LemmaC.10. For1≤t≤d,let{e } ⊆Rddenotethefirsttvectorsfromthestandardbasis j j∈[t] inRd. Then,foranyk-dimensionalsubspaceZ ofRdwithk ≥d−t+1, 1 mindist (e ,Z)≤ . j∈[t] ∞ j 2 Proof. SimilartoLemmaA.2,westartbyconsideringthedimensionoftheintersectionbetweenZ andRt,bothasasubspaceofRd. LetQ=[e e ···e ]⊤ ∈Rt×d. Then, 1 2 t Rd =Col(Q⊤)⊕Null(Q)=(Z∩Col(Q⊤))⊕(Z⊥∩Col(Q⊤))⊕Null(Q). Byconsideringthedimension, dim(Z∩Col(Q⊤))=dimRd−dim(Z⊥∩Col(Q⊤))−dimNull(Q) ≥dimRd−dimZ⊥−dimNull(Q) =d−(d−k)−(d−t) =k−(d−t) Undertheassumptionk ≥d−t+1,wehave dimϕ(Z∩Col(Q⊤)=dim(Z∩Col(Q⊤)≥k−(d−t)≥1. Then, mindist (e ,Z)≤mindist (e ,Z∩Col(Q⊤)) ∞ j ∞ j j∈[t] j∈[t] =mindist (ϕ(e ),ϕ(Z∩Col(Q⊤)) ∞ j j∈[t] (b) 1 ≤ , 2 where(b)isbyLemmaC.9. C.2 ExtensionofSufficiencyConditiontoℓ -norm p Theorem C.11. Let p ∈ [1,∞]. For any dataset D ∈ D and η ∈ (0,1), the following d,N,C statementshold: (cid:16) (cid:105) √ (i) Ifρ∈ 0, √1 ,thereexistsf ∈F d,P withP =O˜( N)thatρ-robustlymemorizes 5N dγp(d) Dunderℓ -norm. p 60 --- Page 61 --- (cid:16) (cid:105) (ii) If ρ ∈ √1 , √ 1 , there exists f ∈ F d,P with P = O˜(Nd1 4ρ1 2γ p(d)1 2) that 5N dγp(d) 5 dγp(d) ρ-robustlymemorizesDunderℓ -normwitherroratmostη. p (cid:16) (cid:17) (iii) Ifρ∈ 5√ dγ1 p(d), γp1 (d) ,thereexistsf ∈F d,P withP =O˜(Nd2ρ4γ p(d)4)thatρ-robustly memorizesDunderℓ -norm. p To prove Theorem C.11, we decompose it into three theorems (Theorems C.12 to C.14), each correspondingtooneofthecasesinthestatement. Theyarefollowing. (cid:16) (cid:105) TheoremC.12. Letρ∈ 0, √1 andp∈[1,∞]. ForanydatasetD ∈D d,N,C,thereexists √ 5N dγp(d) f ∈F withP =O˜( N)thatρ-robustlymemorizesDunderℓ -norm. d,P p (cid:16) (cid:105) Proof. Letρ′ =γ p(d)ρ. Then,wehaveρ′ ∈ 0, 1√ fromtheconditionofρ. ByTheorem4.2(i), √ 5N d there exists f ∈ F with P = O˜( N) that ρ′-robustly memorizes D under ℓ -norm. In other d,P 2 words,itholdsf(x′)=y ,forall(x ,y )∈Dandx′ ∈B (x ,ρ′ϵ ). i i i 2 i D,2 Weconsidertwocasesdependingonwhetherp≥2orp<2,whichaffectthedirectionofinclusion betweenℓ andℓ balls. p 2 CaseI:p≥2. Inthiscase,wehave (a) (b) B (x ,ρϵ ) ⊆ B (x ,ρϵ ) ⊆ B (x ,γ (d)ρϵ )=B (x ,ρ′ϵ ), p i D,p p i D,2 2 i p D,2 2 i D,2 where(a)holdsbyEquation(61)and(b)holdsbyLemmaC.4applyingp=2andq =p. Thus, for all (x ,y ) ∈ D and x′ ∈ B (x ,ρϵ ), it also holds f(x′) = y . In other words, f i i p i D√,p i ρ-robustlymemorizesDunderℓ -normwithO˜( N)parameters. p CaseII:p<2. Inthiscase,wehave (a) (b) B (x ,ρϵ ) ⊆ B (x ,γ (d)ρϵ ) ⊆ B (x ,γ (d)ρϵ )=B (x ,ρ′ϵ ), p i D,p p i p D,2 2 i p D,2 2 i D,2 where(a)holdsbyEquation(62)and(b)holdsbyLemmaC.4applyingp=pandq =2. Thus, for all (x ,y ) ∈ D and x′ ∈ B (x ,ρϵ ), it also holds f(x′) = y . In other words, f i i p i D√,p i ρ-robustlymemorizesDunderℓ -normwithO˜( N)parameters. p (cid:16) (cid:105) Theorem C.13. Let ρ ∈ √1 , √ 1 and p ∈ [1,∞]. For any dataset D ∈ D d,N,C, 5N dγp(d) 5 dγp(d) thereexists f ∈ F d,P with P = O˜(Nd1 4ρ1 2γ p(d)21)that ρ-robustlymemorizes D underℓ p-norm witherroratmostη. (cid:16) (cid:17) Proof. Letρ′ =γ p(d)ρ. Then,wehaveρ′ ∈ 1√ , √1 fromtheconditionofρ. 5N d 5 d Weconsidertwocasesdependingonwhetherp≥2orp<2,whichaffectthedirectionofinclusion betweenℓ andℓ balls. p 2 CaseI:p≥2. Inthiscase,wehave: (a) (b) B (x ,ρϵ ) ⊆ B (x ,ρϵ ) ⊆ B (x ,γ (d)ρϵ )=B (x ,ρ′ϵ ), p i D,p p i D,2 2 i p D,2 2 i D,2 where(a)holdsbyEquation(61)and(b)holdsbyLemmaC.4applyingp=2andq =p. 61 --- Page 62 --- CaseII:p<2. Inthiscase,wehave: (a) (b) B (x ,ρϵ ) ⊆ B (x ,γ (d)ρϵ ) ⊆ B (x ,γ (d)ρϵ )=B (x ,ρ′ϵ ), p i D,p p i p D,2 2 i p D,2 2 i D,2 where(a)holdsbyEquation(62)and(b)holdsbyLemmaC.4applyingp=pandq =2. Thus,inbothcases,itholds: B (x ,ρϵ )⊆B (x ,ρ′ϵ ). (65) p i D,p 2 i D,2 Wedefineη′ = ηVol(Bp(xi,ρϵD,p)). WeapplyTheorem4.2(ii)withtherobustnessratioρ′ andthe Vol(B2(xi,ρ′ϵD,2) errorrateη′,thenweobtainf ∈F d,P withP =O˜(Nd1 4ρ′1 2)=O˜(Nd1 4ρ1 2γ p(d)1 2)thatρ′-robustly memorizesDwitherroratmostη′underℓ -norm. Inotherwords,forall(x ,y )∈D,itholdsthat 2 i i P [f(x′)̸=y ]<η′. (66) x′∼Unif(B2(xi,ρ′ϵD,2)) i Forsimplicity,wedenoteE ={x∈Rd |f(x′)̸=y }. Then,wehave i P [f(x′)̸=y ] x′∼Unif(Bp(xi,ρϵD,p)) i =P [x∈E] x′∼Unif(Bp(xi,ρϵD,p)) Vol(E∩B (x ,ρϵ )) = p i D,p Vol(B (x ,ρϵ )) p i D,p (a)Vol(E∩B (x ,ρ′ϵ )) ≤ 2 i D,2 Vol(B (x ,ρϵ )) p i D,p Vol(E∩B (x ,ρ′ϵ ))Vol(B (x ,ρ′ϵ )) = 2 i D,2 2 i D,2 Vol(B (x ,ρ′ϵ )) Vol(B (x ,ρϵ )) 2 i D,2 p i D,p Vol(B (x ,ρ′ϵ )) =P [x′ ∈E]· 2 i D,2 x′∼Unif(B2(xi,ρ′ϵD,2)) Vol(B (x ,ρϵ )) p i D,p Vol(B (x ,ρ′ϵ )) =P [f(x′)̸=y ]· 2 i D,2 x′∼Unif(B2(xi,ρ′ϵD,2)) i Vol(B (x ,ρϵ )) p i D,p ( <b) η′Vol(B 2(x i,ρ′ϵ D,2)) Vol(B (x ,ρϵ )) p i D,p (c) =η, where(a)holdsbyEquation(65),(b)holdsbyEquation(66),and(c)holdsbythedefinitionofη′. Thus,forall(x ,y )∈D,itholds: i i P [f(x′)̸=y ]<η. x′∼Unif(Bp(xi,ρϵD,p)) i Inotherwords,f ρ-robustlymemorizesDunderℓ p-normwitherroratmostηandO˜(Nd1 4ρ1 2γ p(d)21) parameters. (cid:16) (cid:17) TheoremC.14. Letρ ∈ 5√ dγ1 p(d), γp1 (d) andp ∈ [1,∞]. ForanydatasetD ∈ D d,N,C, there existsf ∈F withP =O˜(Nd2ρ4γ (d)4)thatρ-robustlymemorizesDunderℓ -norm. d,P p p (cid:16) (cid:17) Proof. Letρ′ =γ p(d)ρ. Then,wehaveρ′ ∈ √1 ,1 fromtheconditionofρ. ByTheorem4.2(iii), 5 d there exists f ∈ F with P = O˜(Nd2ρ′4) = O˜(Nd2ρ4γ (d)4) that ρ′-robustly memorizes D d,P p underℓ -norm. Inotherwords,itholdsf(x′)=y ,forall(x ,y )∈Dandx′ ∈B (x ,ρ′ϵ ). 2 i i i 2 i D,2 Weconsidertwocasesdependingonwhetherp≥2orp<2,whichaffectthedirectionofinclusion betweenℓ andℓ balls. p 2 62 --- Page 63 --- CaseI:p≥2. Inthiscase,wehave: (a) (b) B (x ,ρϵ ) ⊆ B (x ,ρϵ ) ⊆ B (x ,γ (d)ρϵ )=B (x ,ρ′ϵ ), p i D,p p i D,2 2 i p D,2 2 i D,2 where(a)holdsbyEquation(61)and(b)holdsbyLemmaC.4applyingp=2andq =p. Thus, for all (x ,y ) ∈ D and x′ ∈ B (x ,ρϵ ), it also holds f(x′) = y . In other words, f i i p i D,p i ρ-robustlymemorizesDunderℓ -normwithO˜(Nd2ρ4γ (d)4)parameters. p p CaseII:p<2. Inthiscase,wehave: (a) (b) B (x ,ρϵ ) ⊆ B (x ,γ (d)ρϵ ) ⊆ B (x ,γ (d)ρϵ )=B (x ,ρ′ϵ ), p i D,p p i p D,2 2 i p D,2 2 i D,2 where(a)holdsbyEquation(62)and(b)holdsbyLemmaC.4applyingp=pandq =2. Thus, for all (x ,y ) ∈ D and x′ ∈ B (x ,ρϵ ), it also holds f(x′) = y . In other words, f i i p i D,p i ρ-robustlymemorizesDunderℓ -normwithO˜(Nd2ρ4γ (d)4)parameters. p p 63 --- Page 64 --- D ComparisontoExistingBounds D.1 SummaryofParameterComplexityacrossℓ -norms p Table1: Summaryofourresultsandacomparisonwithpriorworks. Weomittheconstantsfor therangeofρ. γ (d)=1underp=2reducestotheresultsinSections3and4. p ℓ -norm RobustnessRatioρ BoundonParameters p p>2 (0,1) Ω(cid:0) min{N,d}dρ2(cid:1) ,PropositionC.6 (cid:16) (cid:17) p≤2 (0,1) Ω min{N,d}d(ρ/γ (d))2 ,PropositionC.6 p p=∞ (1/2,1) Ω(min{N,d}d),PropositionC.7 LB p=∞ 0.8 Ω(cid:0) d2(cid:1) ,Yuetal.[2024]1 p<∞ (cid:0) 0,(1− 1)1/p(cid:3) Ω(cid:16)(cid:113) N (cid:17) ,PropositionC.8 d 1−ρp (cid:16)√ (cid:17) p=2 ρ→1 Ω Nd ,Lietal.[2022]2 (cid:16)√ (cid:17) (0, 1 √ ) O˜ N ,TheoremC.12 (cid:16) 1γ √p(d ,)N d 1√ (cid:17) O˜(cid:0) Nd1/4(ργ p(d))1/2(cid:1) ,TheoremC.13 γp( (cid:16)d)N d γp(d (cid:17)) d 0, 1√ O˜(N),Egosietal.[2025] UB p=p γp(d) d (cid:16) (cid:17) O˜(Nd2(ργ (d))2),TheoremC.14 1√ , 1 p γp(d) d γp(d) O˜(cid:0) Nd3(ργ (d))6(cid:1) ,Egosietal.[2025] p (0,1) O˜(cid:0) Nd2p2(cid:1) ,Yuetal.[2024]3 1 RequiresN >d. 2 Theresultonlyholdsforρsufficientlycloseto1. 3 Requiresp∈N. D.2 ParameterComplexityoftheConstructionbyYuetal.[2024] WenowanalyzethenumberofparametersofthenetworkconstructionproposedbyYuetal.[2024], whichprovidestheupperboundnotdependingonρ,butstillappliestoallρ∈(0,1). LemmaD.1(TheoremB.6,Yuetal.[2024]). Letp ∈ N. ForanydatasetD = {(x ,y )} ∈ i i i∈[N] D , let R > 1 by any real value with ∥x ∥ ≤ R for all i ∈ [N]. For ρ ∈ (0,1), define d,N,C i ∞ γ := (1−ρ)ϵ > 0. Then,thereexistsanetworkwithwidthO(d),anddepthO(Np(log( d )+ D,p γp plogR+logp))thatρ-robustlymemorizeDunderℓ -norm. p WenotethatintheYuetal.[2024]usesthenotationλp/2forϵ , andtheradiusλp/2−γ in D D,p D theoriginalstatementcorrespondstothevalueµ := ρϵ inournotation. Wecountparameters D,p intheirconstructioninthefollowinglemma,specificallyinthecasep=2. Althoughtheoriginal statementofYuetal.[2024]includesaparametercount,theyconsideradifferentparametercounting strategy–bycountingonlythenumberofnonzeroparameters. Wethereforecountthenumberofall parametersfollowingEquation(3)inthesubsequentlemma. Notethatresultsandcomparisonunder nonzeroparametercountsareprovidedinAppendixE. LemmaD.2. ForanyD ∈ D andρ ∈ (0,1), defineγ := (1−ρ)ϵ > 0andR > 1with d,N,C D ∥x ∥ ≤Rforalli∈[N]. Then,thereexistsaneuralnetworkf suchthatρ-robustlymemorizesD i ∞ usingatmostO(Nd2(log( d )+logR))parameters. Moreover,thenetworkhaswidthO(d)and γ2 depthO˜(N). Proof. ByapplyingLemmaD.1withp=2,weobtainaneuralnetworkf thatρ-robustlymemorizes D with width O(d), and depth L = O(N(log( d +logR))). In their construction, d = Θ(d) γ2 l throughalll,astheinputxpropagatesoverthelayersusingawidthd. Wecountallparametersas definedinEquation(3),sowecanupperboundthenumberofparametersusedfortheconstruction 64 --- Page 65 --- off asfollows: L L (cid:88) (cid:88) (d +1)·d = Θ(d)·Θ(d) l−1 l l=1 l=1 d =Θ(N(log( )+logR))·Θ(d2) γ2 d =Θ(Nd2(log( )+logR)). γ2 D.3 ParameterComplexityoftheConstructionbyEgosietal.[2025] WeobservethatalthoughEgosietal.[2025]donotexplicitlyquantifythetotalnumberofparameters intheirconstruction,itimplicitlyyieldsanetworkwithO(Nd3ρ6)parameters. Specifically,wecan establishthefollowing: For any D ∈ D d,N,C and ρ ∈ (√1 ,1), there exists a neural network f that d ρ-robustlymemorizesDusingO˜(Nd3ρ6)parameters. ThisresultfollowsfromthenetworkconstructedinTheorem4.4ofEgosietal.[2025]. Theproof (cid:113) ofTheorem4.4proceedsundertheassumptionthatfor7≤k ≤d+5,andρ≤ √1 k−6N− k−2 6. 4 e d Giventhisrange,Theorem4.2ofEgosietal.[2025]isappliedtoconstructarobustmemorizerof theprojecteddatafromRdtoRk. Figures4and5intheirpaperillustratethisconstruction. Inthis construction,theprojectedpointpropagatesthroughthenetworkΘ(Nk)times. Thewidthofthe networkscaleswithk,whiletheothercomponent,thatisnotpropagatingthepointremainsconstant inwidth. Thus,thenumberofparametersusedfortheconstructionisgivenby: L Θ(Nk) (cid:88) (cid:88) (d +1)·d = Θ(k2)=Θ(Nk·k2)=Θ(Nk3). l−1 l l=1 l=1 To translate this to a bound in terms of ρ, we analyze the relationship between ρ and k. For k ≥4logN +6,weverifythefollowinginequality: (cid:114) (cid:114) (cid:114) √1 k−6 N− k−2 6 ≥ √1 k−6 N− 2lo1 gN ( =a) 1 k−6 4 e d 4 e d 4e d (cid:113) where(a)holdsbyN =elogN. Therefore,forρ= 1 k−6,thenetworkρ-robustlymemorizesD 4e d withΘ(Nk3)parameters. Fromtherelationshipbetweenρandk,solvingforkintermsofρyields k =Θ(dρ2). Sincetheminimumvalueofkundertheassumptionis7,theminimumachievableρis 1 √1 . 4e d Thus,forρ> √1 ,theconstructionyieldsanetworkthatρ-robustlymemorizesDwithΘ(Nk3)= d Θ(Nd3ρ6)parameters,asdesired. 65 --- Page 66 --- E NonzeroParameterCounts Whileourmainparametercountingmethodfollowstheapproachofcountingallparameters,including zeros,asdefinedinEquation(3),somepriorworksonmemorizationandrobustmemorizationadopt adifferentparametercountingstrategy—countingonlythenonzeroparameters. Weemphasizethat countingallparameters,includingzeros,betteralignswithhowthematricesarestoredinpractice. Nevertheless,wealsopresenthowourresultsextendtothecaseofcountingonlynonzeroparameters, offeringanalternativeperspectiveforinterpretingourfindingsandcomparingthemwithpriorwork. IncontrasttoEquation(4),letusdefinethesetofneuralnetworkswithinputdimensiondandat mostP nonzeroparametersby F¯ =(cid:8) f :Rd →R|f isaneuralnetworkwithatmostP nonzeroparameters(cid:9) . (67) d,P E.1 NonzeroParameterCounts: Anillustration. We provide the corresponding illustration of Figure 1 under only nonzero parameter counting in Figure8,combiningTheoremE.1andTheoremE.2. Nd ExistingUpperBound N B o u n d √ O ur U p p er Our Lower Bound N ExistingLowerBound N1√ d √1 d √41 d(cid:113) 1− d11 Robustnessratioρ sretemaraP Figure8: Summaryofparameterbounds,countingonlynonzeroparametersonalog-logscalewhen √ d=Θ( N). Weomitconstantfactorsinbothaxes. Solidblueandredcurvesshowthesufficient (TheoremE.2)andnecessary(TheoremE.1)numbersofparameters,respectively;thesolidblack curveisthebestpriorbound. Light-blueshadinghighlightsourimprovementintheupperbound, andlight-redshadinghighlightsourimprovementinthelowerbound. Thecross-hatchedareamarks theremaininggap. E.2 NonzeroParameterCounts: LowerBounds ThelowerboundinTheorem3.1thatcountsallparametersconsistsoftwoterms: onebasedonthe networkwidthandanotherbasedontheVC-dimension. AlthoughthelowerboundbyVC-dimension remains valid even when counting only nonzero parameters, the lower bound on the first hidden layerwidthcanbetranslatedintoalowerboundonparametersonlyifwealsoincludezero-valued parametersintheparametercountingconvention. Asaresult,weobtainthefollowinglowerbound consistingofonlythelowerboundfromtheVC-dimension. TheoremE.1. Letρ∈(0,1). SupposeforanyD ∈D ,thereexistsaneuralnetworkf ∈F¯ d,N,2 d,P thatcanρ-robustlymemorizeD. Then,thenumberofparametersP mustsatisfy (cid:32) (cid:40) (cid:41) (cid:33) 1 √ √ P =Ω min , d N . (cid:112) 1−ρ2 ThemainreasonwhytheVC-dimensionlowerboundremainsvalidevenforthenonzeroparameter countisbecausethekeyrelationVC-dim(F¯ )=O(P2)[GoldbergandJerrum,1995]holdseven d,P fortheF¯ insteadofF . Below,weprovideanexplicitproofoftheTheoremE.1. d,P d,P 66 --- Page 67 --- Proof. Since F ⊆ F¯ , we have VC-dim(F ) ≤ VC-dim(F¯ ). In particular, by Equa- d,P d,P d,P d,P (cid:16) (cid:113) (cid:105) tion(7),wehaveforρ∈ 0, 1− 1 that d (cid:18) (cid:19) N VC-dim(F¯ )≥VC-dim(F )=Ω . d,P d,P 1−ρ2 By[GoldbergandJerrum,1995],wehaveVC-dim(F¯ )=O(P2). Combiningthetworelations d,P (cid:16) (cid:113) (cid:105) provesthatforρ∈ 0, 1− 1 , d (cid:32)(cid:115) (cid:33) N P =Ω . 1−ρ2 √ (cid:16) (cid:113) (cid:105) Since √1 ≤ dforρ∈ 0, 1− 1 ,thefollowingrelationholds: 1−ρ2 d (cid:115) 1 √ √ N min{ , d} N = . (cid:112) 1−ρ2 1−ρ2 (cid:16)(cid:113) (cid:17) √ (cid:113) Forρ∈ 1− 1,1 ,thelowerboundP =Ω( Nd)obtainedbythecaseρ= 1− 1 alsocan d d √ (cid:16)(cid:113) (cid:17) beapplied. Since √1 > dforρ∈ 1− 1,1 ,thefollowingrelationholds: 1−ρ2 d 1 √ √ √ min{ , d} N = Nd. (cid:112) 1−ρ2 (cid:16)(cid:113) (cid:17) (cid:16) (cid:113) (cid:105) √ (cid:16)(cid:113) (cid:17) Asaresult,applyingP = Ω N forρ ∈ 0, 1− 1 andΩ( Nd)forρ ∈ 1− 1,1 1−ρ2 d d resultsin (cid:32) (cid:40) (cid:41) (cid:33) 1 √ √ P =Ω min , d N (cid:112) 1−ρ2 E.3 NonzeroParameterCounts: UpperBounds WhileupperboundsonparametercountsofallparametersinTheorem4.2arenaturallyanupper boundforparametercountsofnonzeroparameters,weprovideatighterupperboundregardingthe nonzeroparameters. TheoremE.2. ForanydatasetD ∈D andη ∈(0,1),thefollowingstatementshold: d,N,C (cid:16) (cid:105) √ (i) Ifρ∈ 0, 1√ ,thereexistsf ∈F¯ d,P withP =O˜( N +d)thatρ-robustlymemorizes 5N d D. (cid:16) (cid:105) (ii) If ρ ∈ 1√ , √1 , there exists f ∈ F¯ d,P with P = O˜(Nd1 4ρ1 2 +d) that ρ-robustly 5N d 5 d memorizesDwitherroratmostη. (cid:16) (cid:17) (iii) Ifρ∈ √1 ,1 ,thereexistsf ∈F¯ d,P withP =O˜(Ndρ2+d)thatρ-robustlymemorizes 5 d D. IncomparisontothetotalparametercountasinTheorem4.2,onlyTheoremE.2(iii)haveamodified ratefromP =O˜(Nd2ρ4)toP =O˜(Ndρ2). Below,weprovideanexplicitproofofTheoremE.2. Thedtermintheparameterboundsofallthreecasescomesfromtheupperboundontheparameters ofthefirsthiddenlayer. Proof. Upperboundsonallparametercountsarenaturalupperboundsonthenonzeroparameter counts. Since Theorem E.2(i) and Theorem E.2(ii) claims the same rate as Theorem 4.2(i) and 67 --- Page 68 --- Theorem4.2(ii)respectively, theytriviallyfollowsfromTheorem4.2. Anotherwayofspeaking, F ⊆F¯ andthefirsttwocasesdirectlyfollowfromTheorem4.2. d,P d,P Now let us prove Theorem E.2(iii). Here, we mainly follow the proof of Theorem B.14, where insteadofcountingeveryparameterusingLemmaD.2,wecountonlythenonzeroparametersusing LemmaE.3. Wedividethecasesintofive,followingTheoremB.14asinFigure6. Let D = {(x ,y )} ∈ D be given. We divide the proof into five cases, the first case i i i∈[N] d,N,C √ under ρ ∈ [1/3,1), the second case under ρ ∈ (1/5 d,1/3) and d < 600logN, the third case √ √ under ρ ∈ (1/5 d,1/3) and N < 600logN ≤ d, the fourth case under ρ ∈ (1/5 d,1/3), √ N ≥d≥600logN,andfinallythefifthcaseunderρ∈(1/5 d,1/3)andd>N ≥600logN. To checkthatthesecasescoverallthecases,refertoFigure6. Case I: ρ ∈ [1/3,1). Let us denote R := max ∥x ∥ and γ := (1 − ρ)ϵ . Note that i∈[N] i 2 D R≥∥x ∥ foralli∈[N]as∥x∥ ≥∥x∥ forallx∈Rd. ByapplyingLemmaE.3,thereexists i ∞ 2 ∞ f ∈F¯ withP =O(Nd(log( d )+logR))nonzeroparametersthatρ-robustlymemorizeD. The d,P γ2 numberofnonzeroparameterscanbefurtherboundedasfollows: O(Nd(log( d )+logR))( =a) O(Ndρ2·(log( d )+logR))( =b) O˜(Ndρ2), γ2 γ2 where(a)isduetoρ=Ω(1),(b)hidesthelogarithmicfactors. √ CaseII:ρ ∈ (1/5 d,1/3)andd < 600logN. LetusdenoteR := max ∥x ∥ andγ := i∈[N] i 2 (1−ρ)ϵ . NotethatR≥∥x ∥ foralli∈[N]as∥x∥ ≥∥x∥ forallx∈Rd. ByLemmaE.3, D i ∞ 2 ∞ there exists f ∈ F¯ with P = O(Nd(log( d ) + logR)) nonzero parameters that ρ-robustly d,P γ2 memorizeD. Thenumberofnonzeroparameterscanbefurtherboundedasfollows: O(Nd(log( d )+logR))( =a) O(N(logN)·(log( d )+logR))( =b) O˜(N)( =c) O˜(Ndρ2), γ2 γ2 where(a)isduetod≤600logN,(b)hidesthelogarithmicfactors,and(c)isbecauseN ≤ 25Ndρ2 (cid:16) (cid:17) forallρ∈ √1 ,1 . 5 d 3 √ CaseIII:ρ ∈ (1/5 d,1/3)andN < 600logN ≤ d. WefirstapplyPropositionB.21toD to obtain1-Lipschitzlinearφ : Rd → RN suchthatD′ := {(φ(x ),y )} hasϵ = ϵ . Thisis i i i∈[N] D′ D possibleasd≥N. Takeb∈RN suchthatφ(x)−b≥0forallx∈B (φ(x ),ρϵ ),ensuringthatσdoesnotaffect 2 i D′ theoutputofthefirsthiddenlayer. LetD′′ ={(φ(x )−b,y )} . Then,ϵ =ϵ =ϵ . For i i i∈[N] D D′ D′′ simplicityofthenotation,letusdenotez :=φ(x )−b. Moreover,thefirsthiddenlayerisdefined i i asf (x)=φ(x)−b. 1 WeapplyLemmaE.3toD′′. LetusdenoteR := max ∥φ(z )∥ andγ := (1−ρ)ϵ . Note i∈[N] i 2 D′′ that R ≥ ∥z ∥ for all i ∈ [N] as ∥z∥ ≥ ∥z∥ for all z ∈ RN. By Lemma E.3, there exists i ∞ 2 ∞ f ∈F¯ withP =O(N·N(log(N)+logR))nonzeroparametersthatρ-robustlymemorizeD′′. 2 N,P γ2 Letf = f ◦σ◦f . Sincef is1-Lipschitzandϵ = ϵ ,everyrobustnessballofDismapped 2 1 1 D′′ D totherobustnessballofD′′ viaf . Sincetheσdoesnotaffectthefirsthiddenlayeroutputofthe 1 robustnessball,andf ρ-robustlymemorizesD′′,thecomposedf satisfiesthedesiredproperty 2 Thenumberofnonzeroparameterscanbefurtherboundedasfollows: O(Nd+N ·N(log( d )+logR))( =a) O(dlogN +(logN)2·(log( d )+logR))( =b) O˜(d), γ2 γ2 where(a)isduetoN ≤600logN,and(b)hidesthelogarithmicfactors. √ Case IV: ρ ∈ (1/5 d,1/3), and N ≥ d ≥ 600logN. We utilize the dimension reduction technique by Proposition B.19. We apply Proposition B.19 to D with m = 68 --- Page 69 --- max{⌈9dρ2⌉,⌈600logN⌉,⌈10logd⌉}andα=1/5. Letusfirstcheckthatthespecifiedmsatisfies thecondition24α−2logN ≤m≤dforthepropositiontobeapplied. α=1/5andm≥600logN ensurethefirstinequality24α−2logN ≤m. Thesecondinequalitym≤disdecomposedintothree parts. Sinceρ≤ 1,wehave9dρ2 ≤dsothat 3 ⌈9dρ2⌉≤d. (68) Moreover,600logN ≤dimplies ⌈600logN⌉≤d. (69) Additionally,asN ≥2,wehaved≥600logN ≥600log2≥400. ByLemmaB.22,thisimplies 10logd≤dandtherefore ⌈10logd⌉≤d. (70) GatheringEquations(68)to(70)provesm≤d. BythePropositionB.19,thereexists1-Lipchitzlinearmappingϕ:Rd →Rmandβ >0suchthat D′ :={(ϕ(x ),y )} ∈D satisfies i i i∈[N] m,N,C 4 ϵ ≥ βϵ . (71) D′ 5 D Asm≥10logd,theinequalityβ ≥ 1(cid:112)m isalsosatisfiedbyPropositionB.19. Therefore,wehave 2 d (cid:114) (cid:114) (cid:114) 1 m (a) 1 ⌈9dρ2⌉ 1 9dρ2 3 β ≥ ≥ ≥ = ρ, (72) 2 d 2 d 2 d 2 where(a)isbythedefinitionofm. Moreover,sinceϕis1-Lipchitzlinear, ∥ϕ(x )∥ =∥ϕ(x −0)∥ =∥ϕ(x )−ϕ(0)∥ ≤∥x −0∥ =∥x ∥ , (73) i 2 i 2 i 2 i 2 i 2 foralli∈[N]. Hence,bylettingR:=max {∥x ∥ },wehave∥ϕ(x )∥ ≤Rforalli∈[N]. i∈[N] i 2 i 2 Now,wesetthefirstlayerhiddenmatrixasthematrixW ∈Rm×dcorrespondingtoϕunderthestan- dardbasisofRdandRm. Moreover,setthefirsthiddenlayerbiasasb:=2R1=2R(1,1,··· ,1)∈ Rm. Then,wehave Wx+b≥0, (74) forallx∈B (x ,ϵ )foralli∈[N],wherethecomparisonbetweentwovectorsareelement-wise. 2 i D Thisisbecauseforalli∈[N],j ∈[m]andx∈B (x,ϵ ),wehave 2 D (a) (b) (c) (Wx+b) =(Wx) +2R≥2R−∥Wx∥ ≥ 2R−∥x∥ ≥ 2R−(R+ϵ ) ≥ 0, j j 2 2 D where(a)isbyEquation(73),(b)isbythetriangleinequality,and(c)isduetoR>ϵ . D We construct the first layer of the neural network as f (x) := σ(Wx+b) which includes the 1 activationσ. Then,byaboveproperties,D′′ :={(f (x ),y )} satisfies 1 i i i∈[N] 6 ϵ ≥ ρϵ . (75) D′′ 5 D Thisisbecausefori̸=j withy ̸=y wehave i j ∥f (x )−f (x )∥ =∥σ(Wx +b)−σ(Wx +b)∥ 1 i 1 j 2 i j 2 (a) = ∥(Wx +b)−(Wx +b)∥ i j 2 =∥ϕ(x )−ϕ(x )∥ i j 2 (b) ≥ 2ϵ D′ (c) 4 ≥ 2× βϵ 5 D (d) 4 3 ≥ 2× × ρϵ 5 2 D 69 --- Page 70 --- 12 = ρϵ , 5 D where(a)isbyEquation(74), (b)isbythedefinitionoftheϵ , (c)isbyEquation(71), and(d) D′ is by Equation (72). By Lemma E.3 applied to D′′ ∈ D , there exists f ∈ F with m,N,C 2 m,P P = O(Nm(log( m )+logR′′))nonzeronumberofparametersthat 5-robustlymemorizeD′′, (γ′′)2 6 where 5 (a) 1 12 2 γ′′ :=(1− )ϵ ≥ × ρϵ = ρϵ , 6 D′′ 6 5 D 5 D R′′ := max∥f (x )∥ = max∥σ(Wx +b)∥ = max∥Wx +b∥ 1 i 2 i 2 i 2 i∈[N] i∈[N] i∈[N] ≤ max∥Wx ∥ +∥b∥ ≤3R. i 2 2 i∈[N] Here(a)isbyEquation(75). Now,weclaimthatf := f ◦f ρ-robustlymemorizeD. Foranyi ∈ [N],takex ∈ B (x ,ρϵ ). 2 1 2 i D Then,byEquation(74),wehavef (x)=Wx+bandf (x )=Wx +bsothat 1 1 i i ∥f (x)−f (x )∥ =∥Wx−Wx ∥ ≤∥x−x ∥ ≤ρϵ . (76) 1 1 i 2 i 2 i 2 D Moreover, combining Equations (75) and (76) results ∥f (x)−f (x )∥ ≤ 5ϵ . Since f 5- 1 1 i 2 6 D′′ 2 6 robustlymemorizeD′′,wehave f(x)=f (f (x))=f (f (x ))=y . 2 1 2 1 i i In particular, f(x) = y for any x ∈ B (x ,ρϵ ), concluding that f is a ρ-robust memorizer D. i 2 i D Regardingthenumberofparameterstoconstructf,noticethatf consistsof(d+1)m=O˜(d2ρ2) 1 parameters (and thus O˜(d2ρ2) nonzero parameters) as m = O˜(dρ2). f consists of O˜(Nm) = 2 O˜(Ndρ2)nonzeroparameters. SincethecaseIVassumesN ≥d,wehave d2ρ2 ≤Ndρ2 Therefore,f intotalconsistsofO˜(d2ρ2+Ndρ2)=O˜(Ndρ2)numberofnonzeroparameters. This provesthetheoremforthefourthcase. √ CaseV:ρ∈(1/5 d,1/3),andd>N ≥600logN. Thelastcasecombinesthetwotechniques usedinCasesIIIandIV.WefirstapplyPropositionB.21toDtoobtain1-Lipschitzlinearφ:Rd → RN such that D′ := {(φ(x ),y )} ∈ D has ϵ = ϵ . Note that we can apply the i i i∈[N] N,N,C D′ D propositionsinced≥N. Next, we apply Proposition B.19 to D′ ∈ D with m = max{⌈9Nρ2⌉,⌈600logN⌉} and N,N,C α=1/5.Letusfirstcheckthatthespecifiedmsatisfiesthecondition24α−2logN ≤m≤N forthe propositiontobeapplied. α=1/5andm≥600logN ensurethefirstinequality24α−2logN ≤m. Thesecondinequalitym≤N isdecomposedintotwoparts. Sinceρ≤ 1,wehave9Nρ2 ≤N so 3 that ⌈9Nρ2⌉≤N. (77) Moreover,600logN ≤N implies ⌈600logN⌉≤N. (78) Gathering Equations (68) and (69) proves m ≤ N. Additionally, as N ≥ 2, we have N ≥ 600logN ≥600log2≥400. ByLemmaB.22,thisimplies10logN ≤N. BythePropositionB.19,thereexists1-Lipchitzlinearmappingϕ:RN →Rmandβ >0suchthat D′′ :={(ϕ(φ(x )),y )} ∈D satisfies i i i∈[N] m,N,C 4 ϵ ≥ βϵ′ . (79) D′′ 5 D As m ≥ 600logN ≥ 10logN, the inequality β ≥ 1(cid:112)m is also satisfied by Proposition B.19. 2 N Therefore,wehave (cid:114) (cid:114) (cid:114) 1 m (a) 1 ⌈9Nρ2⌉ 1 9Nρ2 3 β ≥ ≥ ≥ = ρ, (80) 2 N 2 N 2 N 2 70 --- Page 71 --- where(a)isbythedefinitionofm. Moreover,sinceφandϕareboth1-Lipchitzlinear,ϕ◦φ:Rd → Rmisalso1-Lipschitzlinear. Therefore, ∥ϕ(φ(x ))∥ =∥ϕ(φ(x −0))∥ =∥ϕ(φ(x ))−ϕ(φ(0))∥ ≤∥x −0∥ =∥x ∥ , (81) i 2 i 2 i 2 i 2 i 2 for all i ∈ [N]. Hence, by letting R := max {∥x ∥ }, we have ∥ϕ(φ(x ))∥ ≤ R for all i∈[N] i 2 i 2 i∈[N]. Now, we set the first layer hidden matrix as the matrix W ∈ Rm×d corresponding to ϕ ◦ φ underthestandardbasisofRd andRm. Moreover,setthefirsthiddenlayerbiasasb := 2R1 = 2R(1,1,··· ,1)∈Rm. Then,wehave Wx+b≥0, (82) forallx∈B (x ,ϵ )foralli∈[N],wherethecomparisonbetweentwovectorsareelement-wise. 2 i D Thisisbecauseforalli∈[N],j ∈[m]andx∈B (x,ϵ ),wehave 2 D (a) (b) (c) (Wx+b) =(Wx) +2R≥2R−∥Wx∥ ≥ 2R−∥x∥ ≥ 2R−(R+ϵ ) ≥ 0, j j 2 2 D where(a)isbyEquation(81),(b)isbythetriangleinequality,and(c)isduetoR≥ϵ . D We construct the first layer of the neural network as f (x) := σ(Wx+b) which includes the 1 activationσ. Next,weshowthat,D′′ :={(f (x ),y )} satisfies 1 i i i∈[N] 6 ϵ ≥ ρϵ , (83) D′′ 5 D bytheaboveproperties. Thisisbecausefori̸=j withy ̸=y wehave i j ∥f (x )−f (x )∥ =∥σ(Wx +b)−σ(Wx +b)∥ 1 i 1 j 2 i j 2 (a) = ∥(Wx +b)−(Wx +b)∥ i j 2 =∥ϕ(φ(x ))−ϕ(φ(x ))∥ i j 2 (b) ≥ 2ϵ D′′ (c) 4 ≥ 2× βϵ′ 5 D (d) 4 3 ≥ 2× × ρϵ′ 5 2 D 12 = ρϵ′ 5 D (e) 12 = ρϵ , 5 D where(a)isbyEquation(82),(b)isbythedefinitionoftheϵ ,(c)isbyEquation(79),(d)isby D′′ Equation(80),and(e)isbecauseϵ =ϵ . D′ D ByLemmaE.3appliedtoD′′ ∈ D ,thereexistsf ∈ F withP = O(Nm(log( m )+ m,N,C 2 m,P (γ′′)2 logR′′))nonzeronumberofparametersthat 5-robustlymemorizeD′′,where 6 5 (a) 1 12 2 γ′′ :=(1− )ϵ ≥ × ρϵ = ρϵ , 6 D′′ 6 5 D 5 D R′′ := max∥f (x )∥ = max∥σ(Wx +b)∥ = max∥Wx +b∥ 1 i 2 i 2 i 2 i∈[N] i∈[N] i∈[N] ≤ max∥Wx ∥ +∥b∥ ≤3R. i 2 2 i∈[N] Here,(a)isbyEquation(83). Now,weclaimthatf := f ◦f ρ-robustlymemorizeD. Foranyi ∈ [N],takex ∈ B (x ,ρϵ ). 2 1 2 i D Then,byEquation(82),wehavef (x)=Wx+bandf (x )=Wx +bsothat 1 1 i i ∥f (x)−f (x )∥ =∥Wx−Wx ∥ ≤∥x−x ∥ ≤ρϵ . (84) 1 1 i 2 i 2 i 2 D 71 --- Page 72 --- Moreover, putting Equation (83) to Equation (84) results ∥f (x)−f (x )∥ ≤ 5ϵ . Since f 1 1 i 2 6 D′′ 2 5-robustlymemorizeD′′,wehave 6 f(x)=f (f (x))=f (f (x ))=y . 2 1 2 1 i i Inparticular,f(x)=y foranyx∈B (x ,ρϵ ),concludingthatf isaρ-robustmemorizerD. i 2 i D Regardingthenumberofnonzeroparameterstoconstructf,noticethatf consistsof(d+1)m= 1 O˜(Ndρ2) nonzero parameters as m = O˜(Nρ2). f consists of O˜(Nm) = O˜(N2ρ2) nonzero 2 parameters. SincethecaseVassumesN <d,wehave N2ρ2 ≤Ndρ2. Therefore,f intotalconsistsofO˜(Ndρ2+N2ρ2)=O˜(Ndρ2)numberofnonzeroparameters. This provesthetheoremforthelastcase. NonzeroParameterCounts: ExistingUpperBounds. InSection1.1,theexistingupperboundis statedbycountingallparameters. Whencountingonlythenonzeroparameters,thecorresponding existingupperboundtakesadifferentform. Specifically,foranydatasetDwithinputdimensiond andsizeN,thereexistaneuralnetworkthatachievesrobustmemorizationonDwiththerobustness ratioρunderℓ -norm,withthenumberofparametersP boundedasfollows: 2 √  O˜(N +d) ifρ∈(0,1/ d].  √ √ P = O˜(Nd2ρ4+d) ifρ∈(1/ d,1/4d]. (85) √ O˜(Nd) ifρ∈(1/4d,1). ThisisthecounterparttoEquation(2)thatconsidersallparametercounts. Asinthecaseoffull parametercount,thefirstandthethirdcaseinEquation(85)directlyfollowfromYuetal.[2024] andEgosietal.[2025]respectively. TheworkbyEgosietal.[2025]canbeimplicitlyimprovedto thesecondcaseunderthemoderateρcondition,usingthesametranslationtechniqueprovidedin AppendixD.3. E.4 LemmasforNonzeroParameterCount Here,westateLemmasD.1andD.2—thatcorrespondstoTheoremB.6ofYuetal.[2024]—toits originalversionthatcontainsthenonzeroparametercountwithℓ -normintotheconsideration. 2 LemmaE.3(TheoremB.6, Yuetal.[2024]). ForanyD ∈ D andρ ∈ (0,1), defineγ := d,N,C (1−ρ)ϵ > 0andR > 1with∥x ∥ ≤ Rforalli ∈ [N]. Then,thereexistsaneuralnetwork D i ∞ f with width O(d), depth O(N(log( d ) + logR)) that ρ-robustly memorizes D using at most γ2 O(Nd(log( d )+logR))nonzeroparameters. γ2 72