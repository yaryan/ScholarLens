--- Page 1 --- ALL IN ONE TIMESTEP: ENHANCING SPARSITY AND ENERGY EFFICIENCY IN MULTI-LEVEL SPIKING NEURAL NETWORKS AndreaCastagnetti AlainPegatoquet UniversitéCôted’Azur UniversitéCôted’Azur LEAT LEAT SophiaAntipolis,France SophiaAntipolis,France   BenoîtMiramond UniversitéCôted’Azur LEAT SophiaAntipolis,France  ABSTRACT SpikingNeuralNetworks(SNNs)areoneofthemostpromisingbio-inspiredneuralnetworksmodels andhavedrawnincreasingattentioninrecentyears. Theevent-drivencommunicationmechanismof SNNsallowsforsparseandtheoreticallylow-poweroperationsondedicatedneuromorphichardware. However,thebinarynatureofinstantaneousspikesalsoleadstoconsiderableinformationlossin SNNs,resultinginaccuracydegradation. Toaddressthisissue,weproposeamulti-levelspiking neuron model able to provide both low-quantization error and minimal inference latency while approachingtheperformanceoffullprecisionArtificialNeuralNetworks(ANNs). Experimental resultswithpopularnetworkarchitecturesanddatasets,showthatmulti-levelspikingneuronsprovide betterinformationcompression,allowingthereforeareductioninlatencywithoutperformanceloss. WhencomparedtobinarySNNsonimageclassificationscenarios,multi-levelSNNsindeedallow reducingby2to3timestheenergyconsumptiondependingonthenumberofquantizationintervals. Onneuromorphicdata,ourapproachallowsustodrasticallyreducetheinferencelatencyto1timestep, whichcorrespondstoacompressionfactorof10comparedtopreviouslypublishedresults. Atthe architecturallevel,weproposeanewresidualarchitecturethatwecallSparse-ResNet. Througha carefulanalysisofthespikespropagationinresidualconnectionswehighlightaspikeavalanche effect,thataffectsmostspikingresidualarchitectures. UsingourSparse-ResNetarchitecture,wecan providestate-of-the-artaccuracyresultsinimageclassificationwhilereducingbymorethan20%the networkactivitycomparedtothepreviousspikingResNets. Keywords SpikingNeuralNetworks·multi-levelspikes·low-powerartificialintelligence 1 Introduction SpikingNeuralNetworks(SNNs)areconsideredasthethethirdgenerationofArtificialNeuralNetworks(ANNs). SNNs focus on the encoding and the processing of the information using binary and asynchronous signals known asspikes. Thiscomputingparadigm,inspiredbythemessagingmechanismusedbybiologicalneurons,isthought tobeamongstthesourcesoftheenergyefficiencyofthebrain. However,thebinarynatureofspikesalsoleadsto considerableinformationloss,i.e. quantizationerrors,causingperformancedegradationcomparedtoANNsusing floating-pointoperations. SNNsquantizationerrorcanbereducedbyincreasingthenumberoftimesteps,thereforethe latencyoverthenetwork. However,withalongerconversiontime,morespikesaregenerated,thusincreasingtheenergy consumptionaswell. Severaltechniqueshavebeenproposedtominimizeboththequantizationerrorandthelatencyof SNNs.TheseapproachescanbeeitherappliedtotheANN-to-SNNconversionordirectlyduringtheSNNtrainingusing 5202 tcO 82 ]EN.sc[ 1v73642.0152:viXra --- Page 2 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks thesurrogategradient(SG)method. InLietal.[2022]andRathiandRoy[2021]theauthorsadoptanANN-to-SNN conversionschemewherethefiringthresholdofthespikingneuronsisoptimizedafterconversiontobettermatch thedistributionofthemembranepotential. InCastagnettietal.[2023a]aSNNistrainedusingSGandtheAdaptive Integrate-and-Fire(ATIF)neuron. ThisATIFneuronhasbeenproposedasanalternativetotheoriginalIntegrateand Fire(IF)neuron,thefiringthreshold(V )beingalearnableparameterratherthananempiricalhyper-parameter. InGuo th etal.[2022a],theauthorsalsouseSGtotraintheSNN,butintroduceadistributionlosstoshiftthemembranepotential distributionintotheconversionrangeofthespikingneurons. Withtheseapproaches,requiringonlyfewtimesteps, itispossibletogetSNNswithalmostnoaccuracylosswhencomparedtotheequivalentANNs. Asanexample,a SNNtrainedon4timestepscanachieveanaccuracyonly1%belowtheequivalentnon-quantized,i.e. FloatingPoint (FP),ANNsonCIFAR-10Lietal.[2022]. Tofurtherdecreasethelatency,recentapproachesproposetogobeyond binaryspikesandintroducemulti-levelspikingneurons. Thismechanismexpandstheoutputofspikingneuronsfroma singlebittomultiplebits,thusincreasingtheinformationthatcanbetransmittedateachtimestep. InXiaoetal.[2024] forinstance,ithasbeenshownthatusinga4-levelspikingneuronandonetimestep,thesamelevelofaccuracyofa welloptimizedbinarySNNstrainedon4timestepsLietal.[2022]canbeachieved. Mostofthepreviousworksonly focusontheSNNlatency. However,ithasbeenshownLemaireetal.[2023],Dampfhofferetal.[2023]thatbesidesthe latency,anotherimportantparameterthathastobeoptimizedtoimprovetheenergyefficiencyisthesparsityofthe network,inotherwordsthenumberofspikes,eitherbinaryormulti-level,generatedduringtheinference. Inthiswork weproposetoenhancethesparsityofSNNfromtwopointsofview. Firstattheneuronallevel,whereweintroducea multi-levelspikingneuronmodelthatcanseamlesslydealwithbothtimeandthespikevaluetoreducethequantization error. Thenatthearchitecturallevel,whereanovelspikingResNetarchitectureisproposed. Throughacarefulanalysis ofthespikepropagationintheresiduallayersofthearchitecturewehighlightaneffectthatwecallspikeavalanche. Eventscomingfromthedirectandresidualpathssumupandcreatepeaksofactivitythatcanpotentiallydecrease thesparsityoftheSNN.Finallywecomparebinaryandmulti-levelSNNsfromtheenergy-efficiencypointofview. Our analysis based on the metric proposed in Lemaire et al. [2023], is intended to be independent from low-level implementationchoices. Moreover,ourenergyestimationtakesintoaccountnotonlythesynapticoperationsbutalso allthememoryaccessesthattakeplaceinanevent-drivenexecutionscenarioonaneuromorphichardwareaccelerator. Wethereforeprovideenergyestimationresultsthatareclosertowhatcouldbereasonablyobtainedonarealhardware implementation. Thisapproachisessentialtoavoidoverestimatingtheenergygains. Themaincontributionsofourworkarelistedbelow: • Weproposeamulti-levelmodelofanIFspikingneuroncompatiblewithSNNdirecttrainingusingSG. • WetrainSNNsondifferentimageclassificationproblemsandcharacterizethecorrespondingspikingactivity. Weprovidestate-of-the-artaccuracyresultsonCIFAR-10/100imagedatasets,usingonly1timestepwhile reducingtheenergyconsumptionbyafactorof3comparedtoanequivalentANN. • On neuromorphic data we provide a new state of the art latency/accuracy results for the CIFAR-10-DVS dataset. Here we approach the previously published best accuracies, which are obtained with 10 or more timesteps,whilecompressingthelatencyto1timestep. • WeintroduceSparse-ResNet,anewspikingresidualarchitecture. OurexperimentalresultsshowthatSparse- ResNetcanachieveaccuracyequivalenttothestateoftheartSEW-ResNetFangetal.[2021a]whilereducing thespikingactivitybymorethan20%ontheCIFAR-10dataset. 2 RelatedWorks 2.1 SNNtraining TraininglargeSNNsmodelsonmoderncomplexdatasetshasbeenthesubjectofanincreasingnumberofstudiesinthe recentyears. TwomethodsaretypicallyusedtoobtainanefficientSNN:ANN-SNNconversionordirecttrainingwith surrogategradients. TheANN-SNNconversionmethodisbasedontheideathatfiringratesofspikingneuronsshould matchtheactivationsofanalog,i.e. FloatingPoints,neurons. EarlierworksproposedtodirectlyconvertfromFPANN modelstoSNNs. Diehletal.[2015]usesweightnormalizationandspikingneuronthresholdbalancingtominimizethe conversionloss. InRueckaueretal.[2017]andHanetal.[2020]theauthorsproposetoadoptreset-by-subtraction,i.e. soft-reset,spikingneuronstolowertheconversionloss. MoreovertheyshowthattheSNNaccuracycanbeimproved byusingrealinputsinsteadofPoissonspiketrains. InStanojevicetal.[2023]theANNisconvertedinanequivalent SNNthatusesaTime-To-First-Spike(TTFS)codingmethodtopropagatetheinformationbetweenthespikingneurons. However,directlyconvertingaFPANNtoanSNNtypicallyleadstoaveryhighlatencysolution. Nevertheless,ithas beenrecentlyshowninWangetal.[2024]thatitispossibletoachievehighaccuracyatlowlatencyusingaspecific neuron initialization, called data-based neuronal initialization (DNI) and by taking into account the asynchronous transmissioncharacteristicsofSNNs. 2 --- Page 3 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks Anotherlineofworkshasshownthattoachieveverylowlatencyandhighaccuracyatthesametime,thequantization process has to be taken into account directly when training the ANN model. In Li et al. [2022] the ANN model isfirsttrainedusingaQuantization-Aware-Training(QAT)procedure, andthenconvertedintoanequivalentSNN. SimilarlyDingetal.[2021]replacestheReLUactivationfunctionswiththeproposedquantizationclip-floor-shift (QCFS)activation. TheANNistrainedusingstochasticgradientdescentandastraight-throughestimatorBengioetal. [2013]isusedtoapproximatethederivativeofthequantizedactivationfunction. InYangetal.[2025]theconversion methodproposedinDingetal.[2021]isextendedtoChannel-wisequantization. Insteadofconvertingfromapre-trainedANN,thedirecttrainingmethodNeftcietal.[2019]proposestodirectlytrain theSNNusingstandardback-propagation. Asurrogatefunctionisused,duringgradientback-propagation,toreplace thebinarynon-linearityofspikingneurons. Thisallowsgradientflowingthusmakingback-propagationpossibleinthe spikingdomain. Intuitively,directtrainingofSNNswithsurrrogategradientsharesomesimilaritieswiththequantized ANNtoSNNconversionmethodsdiscussedabove. ThespikeinformationcompressionisanalyzedinCastagnetti etal.[2023a,b],Lietal.[2022],wherethequantizationschemeisdescribedasafunctionoftheneuronparameters, i.e. V andthenumberoftimesteps,i.e. T. Aspikingneuroncanactuallybeconsideredasauniformquantizerthat th discretizesitsinputintoT +1quantizationintervals. Thedirecttrainingmethodleveragesthisfactandthusoptimizes thequantizationmappingwhileseekingtominimizethetaskloss. Severalworksimprovetheaccuracy/latencytrade-off byusingtechniquestorelievethegradientexploding/vanishingproblemincurredbytheback-propagation-through-time (BPTT)algorithm.Guoetal.[2022a]introducesamembranepotentialdistributionlosstopenalizeundesiredmembrane potentialshiftthusalleviatingthegradientvanishingorexplosion. Lietal.[2024]proposesMaskedSurrogateGradient (MSG)tokeepthebalancebetweenthegradientmismatch,causedbyawidesurrogatefunction,andthevanishing gradientrisksincurredbyanarrowsurrogatefunction. 2.2 Quantizationnoiseinspikingneurons Several other works try to improve the accuracy/latency trade-off of SNNs by minimizing the quantization noise introducedbythespikingneurons. Guoetal.[2022b]introducesanInformation-Maximizationlosstominimizethe information-losscausedbythequantizationfromthefull-precisiontensor,i.e. theinputofthespikingneurons,tothe spiketensor. Castagnettietal.[2023a]proposestominimizethequantizationerrorbylearningthefiringthreshold(V ) th ofthespikingneuronduringtraining. InChenetal.[2024]atime-basedcodingschemecalledAt-most-two-spikes ExponentialCoding(AEC)isintroduced. AECneuronsemployquantization-compensatingspikestoimprovecoding accuracyandcapacity. Finally,Chenetal.[2024]proposesaneuronmodelthatexploitstheoptionoftemporalcoding withspikepatterns,wherethetimingofaspiketransmitsextrainformation. 2.3 Multi-levelspikingneurons Tofurtherdecreasethelatency,recentapproachesproposetogobeyondbinaryspikesandintroducemulti-levelspiking neurons. Thismechanismexpandstheoutputofspikingneuronsfromasinglebittomultiplebits,thusincreasingthe informationthatcanbetransmittedateachtimestep. InGuoetal.[2023]theauthorsproposeaternaryspikingneuron thattransmitsinformationwith{-1,0,1}spikes. AsimilarternarycodingisproposedinSunetal.[2022]butonlywith positivevalues. Moreover,inmulti-levelspikingneuronsthespikeisextendedtoafixed-pointunsignedbinarynumber Fengetal.[2022],Xiaoetal.[2024]. InXiaoetal.[2024],ithasbeenshownthatusinga4-levelspikingneuronand onetimestep,thesamelevelofaccuracyofawelloptimizedbinarySNNstrainedon4timestepsLietal.[2022]canbe achieved. WangandZhang[2024]introducesaspikingneuronmodelwithmultiplethresholds,calledMT-SNNthat cangeneratemultiplespikesequences. MT-SNNcanachieveahigheraccuracywithfewertimestepssincetheprecision losscausedbytheshorterlatency,i.e. T,isrestoredbytheinformationencodingobtainedusingmultiplethresholds. 2.4 Event-basedprocessing,sparsityandenergyconsumptionestimation SNNsholdthepromiseoflowerenergyconsumptioninembeddedhardwareduetotheirsparseevent-drivenspike-based computationscomparedtotraditionalANNs. InSNNstheneuroncomputationconsistsinaccumulatingthespikes, weightedbythesynapticconnectionsintothemembranepotentialandfiringanoutputspikeswhenthemembrane potentialexceedsV . Initssimplestformthespikingneuronoperationsonlyrequireaccumulate(AC)operations th whileartificialneuronsinANNsperformmultiply-and-accumulate(MAC)operationsbetweeninputactivationsand weights. Mostresearchworks,consideringeitherbinaryWangetal.[2024],Kimetal.[2020],RathiandRoy[2021]or multi-levelSNNGuoetal.[2023],Xiaoetal.[2024],WangandZhang[2024],positsthattheenergyefficiencyof SNNsisassociatedwiththeenergysavingsobtainedbyreplacingMACswithACCsforsynapticoperationscoupled withtheinherentsparsityoftheSNNmodels. However,thispointofviewhasbeenquestionedinrecentworksLemaire etal.[2023],Dampfhofferetal.[2023]whereithasbeenshownthattheenergycostofsynapticoperationsisnegligible 3 --- Page 4 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks comparedtotheoneoftransferringdata,i.e. synapticweightsandneuronpotentials,fromortothememory. Asan example,Dampfhofferetal.[2023]estimatesthatforanSNNwithaVGG16topologytheenergyconsumedbythe synapticoperationsrepresentsonly1%ofthetotalenergyconsumption. Moreover,theseworkspointoutthatthemain advantageofSNNscomparedtoANNs(ondigitalhardware)comesprimarilyfromexploitingthesparsityofspikes andnotfromthereplacementofMACbyACCoperationsDampfhofferetal.[2023]. Basedontheseresultswebelieve thattoimprovetheSNNsenergy/accuracytrade-off,theclassicallatency/accuracytrade-offhastoberevisited. We thenproposetoanalyzetheSNNsbehaviorbyfocusingonthesparsityaswellasitsnativeabilitytoprocessdatainthe timedomain. Inthefollowingwefirstanalyzehowinformationiscodedinbinaryspikingneuron. Thenweintroducea multi-levelspikingneuronmodelcompatiblewithdirecttrainingusingSG.Wecharacterizethequantizationnoiseof theproposedneuronmodel. Wethenmovetonetworklevelandanalyzethepropagationofspikesinresiduallayers. WeshowthatwithoutacarefulhandlingofresidualconnectionsinSNNs,thesparsityofresidualnetworkscanbe compromised,thusincreasingtheenergyconsumption. 3 Preliminaries Inthissection,weintroducetheIntegrate-and-Fire(IF)binaryspikingmodelanddescribetheencodinganddecoding processusedinSNNs. 3.1 Informationcodingwithspikingneurons AnIFspikingneuronimplementstheReLUnon-linearity(max(0,x))anddiscretizesitsinputsignalintospikes. The followingequationsdescribetheIntegrate-and-Fire(IF)neuronmodelwithsoft-reset: Hl(t)=Vl(t−1)+il(t) (1) il(t)=zl−1(t)Wl+bl (2) zl(t)=Θ(Hl(t)−V ) (3) th Vl(t)=Hl(t)(1−zl(t))+(Hl(t)−V )zl(t) (4) th ThepreviousequationsdescribethebehavioroftheIFneuronattheoutputofthelayerloftheSNN.Themembrane potential after the input integration and after the reset operation are represented by Hl(t) and Vl(t) respectively. Thebinaryspikingoutputattimetisrepresentedbyzl(t). Theinputofthespikingneuron,il(t),isthesumofthe weightedspikescomingfromthelayerl−1plusaconstantbias. AsshowninEq. 1and2,thesynapticoperations are implemented with ACC in a spiking neuron. Eq. 3 and 4 describe the generation of a spike and the soft-reset operationrespectively. ThefunctionΘ(·)representstheHeavisidestepfunctionusedintheforwardpass. Thesurrogate ofΘ(·),thatisΘ′(x)=σ′(x),isusedduringthebackwardpass. Inthisworkweusethesigmoidσ(x,α)= 1 1−e−αx assurrogatefunction. Wherexrepresentsthemembranevoltageofthespikingneuronandαisanhyper-parameterthat modulatesthewidthofthesurrogatederivative. Theintegrate-and-fireoperationsdescribedbythepreviousequations,arerepeatedthroughT timesteps. Theoutputyl s isthenrate-decodedasfollows: T 1 (cid:88) yl = zl(t) (5) s T t=1 TheEq. 5representsthefiring-rateoftheneuron. Inthisschemetheinformationiscodedbythenumberofspikes generatedbyaspikingneuronoverafixeddurationoftimeT. Moreover,sincezl(t)isabinaryvariable,thedecoded outputyl isquantizedintoT +1differentvalues. Thenumberofquantizationintervalsdeterminesthedistortion s incurred during the quantization process. For a binary spiking neuron the quantization noise can be reduced by increasingT,i.e. thelatencyoftheSNN.However,augmentingthelatencyleadstoanincreaseoftheSNNenergy consumptionasmorespikesaregenerated. Thereisthereforeatrade-offbetweentheamountofquantizationnoiseand theenergyefficiencyoftheSNN.OurobjectiveisthereforetoreducethequantizationnoisewithoutincreasingT. The nextsectionproposesasolutionforthatproblembyintroducingthemulti-levelspikingneuronmodel. 4 Methods 4.1 Multi-levelspikingneuronmodel The neuron model and the associated decoding scheme are shown in Fig. 1. This model is characterized by two 4 --- Page 5 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks Figure1: Multi-levelIFneuronmodelandtheassociateddecodingscheme. parameters: the firing threshold V and the number of levels of a spike, that we note N. The multi-level spiking th neuroniscomposedbyastandardIFneuron,describedbytheEq. 1-4giveninSec. 3. Thespikegenerationprocessis dividedintotwophases. AteachtimesteptheIFneuronisiterativelycharged,N times,withthecurrentinputx(t). Throughthisoperation,thecurrenttimestepisindeedsubdividedintoN intervals,thatwecallmicro-timesteps. Then, theinternalmembranepotentialoftheIFneuronisdischargedthroughtheN micro-timestepstogenerateg(n)binary spikes. Thevalueofthespikeemittedattheendofthetimestepiscomputedbysummingupallthebinaryspikesg(n) generatedthroughthedischargeprocess: N (cid:88) z(t)= g(n) (6) n=1 It is worth noticing that only the valued spike z(t) is communicated to the next layer. The binary spikes g(n) are internaltothespikingneuronanddonotcontributetotheoverallactivityofthenetwork. 4.2 Quantizationerroranalysis ThespikingneuronoperationdescribedinSec. 4.1canberepeatedthroughanarbitrarilynumberofTtimesteps,where theoutputateachtimestepsisavaluedspikez(t)∈[0,N]. Moreover,themodelcanberevertedtoanIFbinaryspiking neuronbysettingN =1. Inthatcase,thedischargeprocessisexecutedonlyonceandtheoutputateachtimestepsisa binaryspikez(t)∈[0,1]. Finally,theoutputofthespikingneuroncanbedecodedasfollows: T 1 (cid:88) xˆ= z(t) (7) NT t=1 Overall,theconversionprocessleadstoadiscretizationoftheinputx. Whentheinputoftheneuronisaconstantsignal, thatisx(t)=x,wecanobservethatthequantizationfunctionofthemulti-levelspikingneuron,showninFig. 2,has exactlyN ×T +1quantizationlevels. Forthebinaryspikingneuron,thatiswhenN = 1,thereareinsteadT +1 uniformquantizationintervalsCastagnettietal.[2023a],Lietal.[2022]. Amulti-levelspikingneuroncantherefore communicateateachtimesteplog (N)bitswhilethebinaryspikingneuronislimitedtoonly1bitofinformation. 2 Withmulti-levelspikingneuronsitisthuspossibletodecreasethequantizationerrorwithoutincreasingthelatencyof theSNN,thusachievingabetterinformationcompressionwithouthindering,intheory,thecomputationalefficiencyof theSNN. 4.3 Spikespropagationinresidualconnections ResiduallearningandshortcutsconnectionshavebeenevidencedasanimportantwayofdeepeningANNsarchitectures Heetal.[2016]andarenowwidelyadoptedbyalmostanystate-of-the-artneuralnetworkSandleretal.[2019],Tan andLe[2020],Dosovitskiyetal.[2021]. Particularly,ResNetsarchitecturesHeetal.[2016]havebeenconvertedinto SNNsinseveralpreviousworksHuetal.[2023a],Fangetal.[2021a],Huetal.[2023b]. ConversionissuesfromANNs havebeenstudiedinHuetal.[2023a],whileFangetal.[2021a]focusedonthedifficultyofimplementingidentity mappingwithspikingneurons. FinallyinHuetal.[2023b]theauthorsexploredthevanishing/explodinggradient problemsthatarisewhenusingthedirecttrainingmethod. ThemainresidualarchitecturesforSNNsareshowninFig. 3. IntheoriginalSpikingResNetproposedinHuetal.[2023a]theonlyarchitecturalmodificationconsistsinreplacing theReLUnon-linearitieswithspikingneurons. SpikingResNetconvertedfromANNachievesstate-of-the-artaccuracy onnearlyalldatasets,albeitwithhighlatenciesHanetal.[2020]. Ontheotherhand,severalchallengeshavetobe addressedtoachievethesameaccuracylevelswithSpikingResNetstraineddirectlyinthespikedomain. MS-ResNet 5 --- Page 6 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks Figure2: Quantizationfunctionofamulti-levelIFwithsoft-reset(T =2,N =4,V =1.0). Wecanobservethatthe th outputoftheneuronhasexactlyN ×T +1quantizationlevelsandtheoutputsaturateswhentheinputequalsV . th Huetal.[2023b]addressesthisproblembyimplementingtheresidualconnectionsbetweenthemembranepotentialsof thespikingneurons. However,bydirectlyconnectingthemembranepotentialofthespikingneurons,theMS-ResNets cannotbeconsideredafullyevent-basednetwork. Ontheotherhand,SEW-ResNetsFangetal.[2021a]proposesto aggregatethespikesatthesummationpointoftheresidualconnection. Usingthistopology,SEW-ResNetscanindeed achievecomparableresultswithstandardResNetswhilemaintainingtheevent-basedcommunicationscheme. However, asthespikesaresequentiallytransferredfromonelayertothenext,thespikesthatcomefromtheresidualconnection areaddedtotheflowofspikescomingfromthedirectpath. Thiseffect,thatwecallspikeavalancheisillustratedinFig. 4. Inthissimplifiedexample,weconsiderthatthenumberofinputbinaryspikesinSEWResNets,thatwecallγ ,is b notmodifiedwhenprocessedbyaconvolutionallayer. Asshown,theγ spikesthatthefirstresidualblockreceivesas b inputarepropagatedthroughboththedirectandtheresidualpath. Therefore,thenumberofspikesattheoutputofthe firstresidualblockisdoubled. Inconsequence,thefollowinglayerreceives2×γ spikesatitsinput. Similarly,thetwo b flowsofspikesaddupattheoutputofthesecondresidualblockthatgenerates4×γ spikes. Thenumberofspikes b increasesexponentiallyasthenetworkbecomesdeeper,thuscreatinganavalancheofspikesthathavetobeprocessed bythefinallayersofthenetwork. ToovercomethisproblemweproposeamodificationoftheResNetarchitecturefor SNNs,thatwecallSparse-ResNetshowninFig. 3(d). 4.4 Sparse-ResNet Sparse-ResNetmakesuseofthemulti-levelspikingneuronbothinthedirectpathlikeSEWResNetandafterthe summationpointasSpikingResNet. Themulti-levelneuronthatisplacedafterthesummationpointiscalledabarrier neuron. Itsgoalistomaintainalowquantizationerrorattheoutputofthesummationpointwhilelimitingthenumber ofspikesthataregenerated. However,aspointedoutinFangetal.[2021a],placingaspikingneuronjustafterthe summationpointcausesaperformancedegradationforthedirectlytrainedspikingResNets. Thevanishing/exploding gradients,inducedbythesurrogatefunctionofthespikingneuron,areindeedthemaincausesoftheperformance degradationobservedinFangetal.[2021a]. SEW-ResNetssolvethisproblembyremovingthespikingneuronafterthe summationpoint. Therefore,sinceS =O andthegfunctionistheaddition,thegradientsthatareback-propagated l l bothinthedirect,A andtheresidualconnections,R ,areequaltotheincominggradient ∂L: l l ∂Ol ∂L ∂L ∂L = = (8) ∂A ∂R ∂O l l l IntheSparse-ResNetmodelinstead,theincominggradientfirstgoesthroughthesurrogategradientfunctionofthe spikingneuronbeforebeingpropagatedthroughthedirectandresidualconnections: ∂L ∂L ∂L ∂O ∂L = = l = σ′(S −V ) (9) ∂A ∂R ∂O ∂S ∂O l th l l l l l CommonlyusedsurrogatefunctionsNeftcietal.[2019],liketheoneusedinourstudyandshowninFig. 5,arehighly peaked at V and drops quickly as the membrane potential of the neuron, V moves away from the threshold th mem voltage. 6 --- Page 7 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks Figure3: Residualblocksin(a)SpikingResNetsHuetal.[2023a],(b)SEW-ResNetsFangetal.[2021a],(c)MS- ResNetsHuetal.[2023b]andtheproposedSparse-ResNets. SN standsforbinaryspikingneuronwhileml-SN/STEand ml-SN/SGmeansamulti-levelspikingneuronwithaStraight-ThroughEstimatorandasurrogatebackwardfunction respectively. 7 --- Page 8 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks Figure4: Spikepropagationinresidualconnections. Hereγ andγ representtheamountofbinaryandmulti-level b mv spikesattheinputofthefirstresidualconnectionofthenetwork. OnlyoneConvolutionalblockisrepresentedinthe directpath. IntheSEW-ResNetsFangetal.[2021a],theflowofspikesisaddedatthesummationpoints,thuscreating anexponentialincreaseofthenumberofspikesthathavetobeprocessedbydeeperlayers. Usingthebarrierneuron (dottedinthefigure), theSparse-ResNetscanlimitthepropagationofspikeswithouthinderingtherepresentation capacityofthenetwork. 8 --- Page 9 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks Figure5: Thederivative,σ′,ofthesigmoidsurrogateusedinourstudy: σ′(x,α)=ασ(α·x)(1−σ(α·x)). Where σ(x)isthesigmoidfunctionandαthescalingfactor. Hereα = 5andV = 1. ThethresholdvoltageV isalso th th plottedasaverticaldottedline. SincetheneuronthatfollowsthesummationpointdirectlyreceivestheunweightedspikesS (t)asinput,itsmembrane l potentialiseitherclosetozeroorhigherthanV . AsitcanbeseenfromFig. 5,inbothcasesthederivativeσ′ has th verylowamplitude,thuscreatingtheconditionsforthegradienttovanishasalreadyobservedinFangetal.[2021a]. Toovercomethisvanishinggradientproblemwereplacethesurrogategradientfunctionofthebarrierneuronswith aStraight-Through-Estimator(STE)Bengioetal.[2013]. TheSTEintroducesafarlargerdeviationfromthetrue gradientsthanasurrogatefunctionliketheoneshowninFig. 5. But,therationalbehindourchoiceisthatwecan acceptsomedeviationfromthetruegradientsinexchangeforahighergradientamplitude. Moreover,byusingtheSTE thegradientofthebarrierneuronsbecomes: ∂O l =σ′(S −V )=1 (10) ∂S l th l ByreplacingthepreviousgradientintoEq. 9wecanobservethatthegradientsthatareback-propagatedthroughthe directandtheresidualconnectionsarenowequalforbothSEW-ResNetsandSparse-ResNets. Weprovideexperimental resultsinSec. 6.3andSec. 6.4toconfirmouranalysisonthegradientsandthesparsityforbothSparse-ResNetsand SEW-ResNets. 5 ExperimentalResults 5.1 DatasetsandModels WetraintheSNNmodelsusingSGtrainingmethodontwodifferentimageclassificationdatasets(CIFAR-10,CIFAR- 100)Krizhevsky[2009]aswellasaneuromorphicdataclassificationdataset(CIFAR-10-DVS)Lietal.[2017]. TheCIFAR-10/100datasetsaremadeof32x32RGBimagesrepresenting10and100classesrespectively. ForCIFAR- 10/100wetraintheSNNsusingstochasticgradientdescent(SGD),withalearningrateof8·10−2. Thelearningrateis exponentiallydecayedwithafactorof0.9each50epochs,anddataaugmentation(randomresizeandhorizontalflip)is used. Eachnetworkistrainedfor1500epochs. TheCIFAR-10-DVSneuromorphicdatasetisobtainedbyrecordingthemovingimagesoftheCIFAR-10datasetona LCDmonitorbyaDVScamera. TheDVScamerahasaspatialresolutionof128×128. Theevent-to-frameintegration methoddescribedinFangetal.[2021b]isusedforpre-processingCIFAR-10-DVS.Theneuromorphicdataarethen representedbyT frameswhichcontainthesummationoftheevents,inbothpositiveandnegativepolarities,ineach temporalslice. Similarevent-to-frameintegrationmethodsarewidelyadoptedinSNNresearchKaiseretal.[2020], Leeetal.[2016],Xingetal.[2020]. Thedatasetiscomposedof1Kimagesperclassforatotalof10Kimages. In oursetting,80%oftheimagesareusedfortrainingandtherestfortestingthemodels. WetrainSparse-ResNet18and VGG16ontheCIFAR-10-DVSusingtheAdamoptimizerandalearningrateof10−3thatisexponentiallydecayed withafactorof0.5each50epochs. Finally,thenetworkistrainedfor500epochs. 9 --- Page 10 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks 5.2 Networkactivityandsparsity TheSNNactivityisdefinedasthetotalnumberofspikesgeneratedbytheSNNduringtheinference,thatisduringT timesteps. Letuscallz (t),ofdimension(T,C,H,W),theoutputofthelthlayerofanSNNwithtotaldepthL. Then l thetotalactivityofthelayerliscomputedasfollows: T C H W (cid:88)(cid:88)(cid:88)(cid:88) #Spikes = z (t)| (11) l l [0,1] Wherez (t)| representsthevalueofz(t)restrictedtotheinterval[0,1]. Finally,thetotalactivityoftheSNNis l [0,1] computedbysummingtheactivitiesofeachlayerasfollows: l (cid:88) #Spikes = #Spikes (12) SNN l Theactivitydefinedabovemeasuresthetotalnumberofevents,i.e. spikes,thathavetobeprocessedandtransmitted foreachinference. Asdiscussedinthenextsection,thismetricistightlyrelatedtotheSNNenergyconsumption. 5.3 Energyestimation TheSNNenergyconsumptionisestimatedusingthemetricintroducedinLemaireetal.[2023]. Theamountofenergy consumedbytheSNNduringtheinferenceiscomputedbytakingintoaccountthenetworkarchitecture,thenumberof parametersandactivationsaswellasthesparsity. Basedontheseinformation,theenergymodeloutputstheenergy consumedbythesynapticoperations,thememoryaccessesandaddressing. MemoryaccessesarecomputedbasedontheassumptionthateachSNNlayerhasitsownlocalStaticRAM(SRAM) memory. Thislocalmemorystoresthelayerparameters(weightsandbiases)andalsoactsasabufferforthespikes emittedbythespikingneurons. Synapticoperationsaretypicallyconsideredtobeaccumulations(ACC)forSNNs. ThisisindeedthecasewhenN =1 (binaryspikes)asthesynapticweightisaccumulatedatmostonceeachtimestepintothemembranepotential. Insuch acaseweconsidertheenergycostofasynapticoperationtobeequaltooneACCoperation. HoweverwhenN >1 (multi-levelspikes),thesynapticweightcouldbeaccumulateduptoN timesforeachreceivedspike. Insuchacase, weassumetheworstcasehypothesisdescribedabove,andconsiderthatthesynapticenergycostofamulti-levelspike correspondstoN ×ACCoperations,whatevertheexactvaluecarriedbyeachspike. Thisoverestimationisnegligible aswediscussinSec. 6.1and6.2. 5.4 Comparisontopreviousworksonimageclassification Inthissectionourproposedmethodiscomparedwithstate-of-the-artresultsontheCIFAR-10/100imagedatasets. The experimentalresultsareprovidedinTab. 1. Toprovideacomprehensivecomparisonweincluderesultsfromdifferent trainingmethods. InANN/QANN2SNNmethodstheresultingSNNisobtainedbyconvertingeitheranFPANNora quantizedANNrespectively. Ontheotherhand,inSGmethodstheSNNisdirectlytrainedinthespikingdomainwith surrogategradients. AsitcanbeobservedfromTab. 1,thenumberoftimesteps,i.e. thelatency,canbedrastically reducedfromANN2SNNtoQANN2SNNandSGmethods. Theseresultshighlighttheimportanceoftakinginto accountthequantizationnoiseduringthetrainingprocesstoachievebothhighaccuracyandlowlatency. Wecanalso observethat,foralldatasetsandnetworkarchitectures,theadoptionofmulti-levelneuronsallowsfurtherreducingthe latencycomparedtobinarySNNs. Ourresultsindeedshowthatusingonly1timestepwecanobtainthebestaccuracy resultscomparedtobothbinaryandothersternaryormulti-levelSNNswithequalorsimilarnetworkarchitectures. On CIFAR-100forinstanceweimprovebymorethan3%theaccuracycomparedtoIM-LossGuoetal.[2022b]while reducingthelatencybyafactorof5. SimilartrendscanbeobservedfortheCIFAR-10datasetwhere,forexample, weimprovetheaccuracyby0.35%andreducethelatencyby4,comparedtoRecDis-SNNGuoetal.[2022a]using ResNet18/19respectively. Moreover,itcanbeobservedthatSparse-ResNetachievessimilarorevenslightlybetterperformanceresultscompared toSEW-ResNet,thusvalidatingourapproachbasedonabarrierneuronwithSTEanddescribedinSec. 4.4. More results are provided inSec. 6.4 to show that Sparse-ResNet alsoimproves the sparsitycompared to SEW-ResNet. Moreover, in Sec. 6.3 we provide experimental results that validate the effectiveness of the STE in reducing the vanishinggradientprobleminspikingresidualnetworks. 5.5 ComparisontopreviousworksonNeuromorphicdataclassification TheexperimentalresultsobtainedfortheCIFAR-10-DVSdatasetareshowninTab. 2. Herewereporttheresultsfor amulti-levelSparse-ResNet18andVGG16forthe[T = 1,N = 10]configurationwhichprovides10quantization 10 --- Page 11 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks Table1: ComparisonwithSoTAmethodsonCIFAR-10/100. OursresultsaregivenforN =4. Dataset Method Type Neuron Architecture Timestep Accuracy 01-RAFIC RMP-SNNHanetal.[2020] ANN2SNN binary VGG16 2048 93.63% QFFSLietal.[2022] QANN2SNN binary VGG16 4 92.64% QCFSDingetal.[2021] QANN2SNN binary VGG16 4 93.86% DIET-SNNRathiandRoy[2021] SG binary VGG16 10 93.44% IM-LossGuoetal.[2022b] SG binary VGG16 5 93.85% ATIFCastagnettietal.[2023a] SG binary VGG16 4 93.13% Ours SG multi-level VGG16 1 94.34% S-ResNetHuetal.[2023a] ANN2SNN binary ResNet20 350 91.82% ACPLietal.[2021a] ANN2SNN binary ResNet20 4 84.70% QFFSLietal.[2022] QANN2SNN binary ResNet18 4 93.14% DIET-SNNRathiandRoy[2021] SG binary ResNet20 10 92.54% IM-LossGuoetal.[2022b] SG binary ResNet19 4 95.40% RecDis-SNNGuoetal.[2022a] SG binary ResNet19 4 95.53% TernarySpikeGuoetal.[2023] SG ternary ResNet20 4 94.96% MLFFengetal.[2022] SG multi-level ResNet20 4 94.25% MBLIFXiaoetal.[2024] SG multi-level ResNet20 1 94.59% Ours SG multi-level SEW-ResNet18 1 95.94% Ours SG multi-level Sparse-ResNet18 1 95.69% 001-RAFIC RMP-SNNHanetal.[2020] ANN2SNN binary VGG16 2048 70.93% QCFSDingetal.[2021] QANN2SNN binary VGG16 4 69.62% DIET-SNNRathiandRoy[2021] SG binary VGG16 5 69.67% IM-LossGuoetal.[2022b] SG binary VGG16 5 70.18% Ours SG multi-level VGG16 1 73.75% DIET-SNNRathiandRoy[2021] SG binary ResNet20 5 64.07% RecDis-SNNGuoetal.[2022a] SG binary ResNet19 4 74.10% TernarySpikeGuoetal.[2023] SG ternary ResNet20 4 74.02% MBLIFXiaoetal.[2024] SG multi-level ResNet20 1 75.43% Ours SG multi-level SEW-ResNet18 1 75.27% Ours SG multi-level Sparse-ResNet18 1 75.7% Table2: ComparisonwithSoTAmethodsonCIFAR-10-DVS.OursresultsaregivenforN =10. Dataset Method Type Neuron Architecture Timestep Accuracy SVD-01-RAFIC DSRMengetal.[2022] SG binary VGG11 20 77.27% DspikeLietal.[2021b] SG binary ResNet18 10 75.4% MSGLietal.[2024] SG binary ResNet18 10 79.35% SEW-ResNetFangetal.[2021a] SG binary Wide-7B-Net 16 74.4% IM-LossGuoetal.[2022b] SG binary ResNet19 10 72.6% RecDis-SNNGuoetal.[2022a] SG binary ResNet19 10 72.42% MLFFengetal.[2022] SG multi-level ResNet14 10 70.36% TernarySpikeGuoetal.[2023] SG ternary ResNet20 10 79.8% Ours SG multi-level Sparse-ResNet18 1 79.1% Ours SG multi-level VGG16 1 76.97% intervals. AswecanseefromTab. 2wecanreachclosetothestateoftheartaccuracywhiledecreasingthelatency to only 1 timestep. Previous state of the art methods, both with binary and multi-level neurons, are significantly outperformedastheyrequirealatency10to20timeshighertogetasimilarperformancelevel. MoreresultsonsparsityandenergyconsumptionwillbeprovidedinSec. 6.2,toassesswetherthereductioninlatency alsotranslatesinenergyefficiencyimprovement. 11 --- Page 12 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks 6 Ablationstudies 6.1 Energyefficiencycomparisonbetweenmulti-levelandbinaryspikingneuronsonImageclassification Someexperimentswereconductedtocomparetheenergyefficiencyofbinaryandmulti-levelSNNsontheCIFAR-10 dataset. Todoso,differentconfigurationsoftheVGG16architecturewerestudied,bothintermsoflatencyT andspikes valueN. InthefirstconfigurationwefixN =1andtraintheVGG16networkwithdifferentnumberoftimestepsin therangeT ∈[1,10]. Inthesecondconfiguration,wesetthenumberoftimesteptoT =1andthevalueofthespikes intherangeN ∈[1,10]. Thissetupallowscomparingconfigurationswithequalnumberofquantizationintervals. As anexampletheconfigurations[T = 4,N = 1]and[T = 1,N = 4]bothprovidethesamenumberofquantization intervals. However,inthefirstcasethequantizationprocesstakesplacethroughtimebyprocessingmultiplebinary spikes. Inthesecondcasethediscretizationprocessisperformedinonetimestepthroughmulti-valuedspikes. Foreach configuration,SNNsaretrainedusingthesetupdescribedinSec. 5.1. Figure6: Top-1accuracyofVGG16ontheCIFAR-10dataset. Thehorizontaldottedlinerepresentstheaccuracyof theANNFPversionofthenetwork. TheaccuracyofthedifferentSNNsconfigurationsaswellastheANNbaselineisshowninFig. 6. Asexpected,the accuracygapbetweentheANNandtheSNNsreduceswhenincreasingthenumberofquantizationintervals. Moreover, as it can be observed, there is a functional equivalence between both SNN configurations. For a given number of quantizationintervals,theaccuracyconvergestoveryclosevalues. TheseresultsthusconfirmtheanalysisofSec. 4.2, wherewehaveshownthatthesamequantizationfunctioncanbeobtainedwithdifferent[N,T]configurations. ThetotalspikingactivityaswellasthetotalenergyconsumptionareshowninFig. 7(A)andFig. 7(B)respectively. Eventhoughthedifferentbinaryandmulti-levelconfigurationsarefunctionallyequivalent,wecanobservethattheyare considerablydifferentbothintermsofspikingactivityandenergyconsumption. Asanexample,the[T =4,N =1] configuration generates 130·103 binary spikes for each inference, while the same functional configuration with multi-levelneurons,[T = 1,N = 4],onlyproduces57·103 valuedspikes. Thatis43%lessspikesthathavetobe processedforeachinputimage. Moreover,itcanbeobservedthatforthebinarySNNsthetotalactivitygrowsalmost linearlywiththetimestepswhilethereisa(sub)logarithmicincreaseforthemulti-levelconfigurations. Thespiking activitydirectlyimpactsthetotalenergyconsumptionshownFig. 7(B).ItcanbeobservedthatevenifthebinarySNNs aremoreenergyefficientthantheANNatlow-timesteps,e.g. thebinarySNNsprovides35%energyreductionwhen [T =4,N =1],thegapquicklyreducesasTincreases. AtT =8forinstance,thebinarySNNsalreadyconsumes 10%moreenergythantheANN.Ontheotherhand,themulti-levelVGG16maintainsahighlevelofenergyefficiency ateachoperatingpoint. Asanexample,when[T =1,N =4]thereisa66%energyreductioncomparedtotheANN. Atthe[T =1,N =8]operatingpointtheenergyreductionisstill60%comparedtotheANN. Finally,thecompleteenergyconsumptionbreakdownisshowninTab. 3. Hereweprovidethedetailedestimationfor theANNandtheconfigurationswith4and8quantizationintervalsforboththebinaryandthemulti-levelSNNs. As itcanbeseen,thetotalenergyconsumption,especiallyfortheSNNs,isstronglydominatedbythecostofmemory accesses. Eachspikegeneratedbyaneuronleadstothreememoryaccesses(tworeadoperationsforretrievingthe weights and the current value of the membrane potential and a write operation) and a synaptic operation (ACC or N ×ACCforthebinaryandmulti-valuedcaserespectively). Theenergycostforonememoryaccessisonaverage 10to100timeshigherthanasynapticoperationJouppietal.[2021]. Indeed,wecanobservedifferencesoftwoto 12 --- Page 13 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks Figure7: SpikingactivityandtotalenergyconsumptionofSNNsandANNfortheCIFAR-10dataset. Weprovide (A)thetotalspikingactivityofVGG16fordifferentconfigurationsofmulti-levelSNNs{T = 1, N ∈ [1,10]}and binary SNNs {T ∈ [1,10], N = 1}. (B) The total energy consumption of the different SNN configurations. The horizontaldottedlinerepresentsthetotalenergyconsumptionoftheANN. Table3: Energyconsumptionbreakdownforbinaryandmulti-levelVGG16SNNsandANNonCIFAR-10. Allenergy valuesareexpressedin[nJ]. E correspondstothetotalenergyconsumptionofthemulti-levelSNN,whileE N binary referstothebinarySNN.Thesymbol[↓]meanslowerisbetter. ANN SNN binary multi-level binary multi-level [T =4,N =1] [T =1,N =4] [T =8,N =1] [T =1,N =8] Memory Potentials - 6.0·106 3.1·106 10.0·106 3.6·106 Weights 5.58·106 2.4·106 1.3·106 4.1·106 1.54·106 Bias 6.9·102 27.8·102 6.9·102 55.7·102 6.9·102 In/Out 6.1·106 3.5·103 1.54·103 5.7·103 1.8·103 Total 11.6·106 8.4·106 4.38·106 14.2·106 5.19·106 Synaptic 1.29·106 13.3·103 27.9·103 22.4·103 65.4·103 Operations Addressing 4.9·102 9.0·102 5.5·102 14.8·102 8.7·102 Total 12.9·106 8.5·106 4.41·106 14.28·106 5.26·106 E /E [↓] 0.51 0.37 N binary E /E [↓] 0.65 0.34 1.10 0.4 SNN ANN threeordersofmagnitudebetweentheenergyconsumedbythememoryaccessesandthesynapticoperations. Wecan alsoobservethat,asexpected,theenergyconsumedbythesynapticoperationsisgreaterforthemulti-valuedspike. However,astheamountofgeneratedspikesislowerthanbinaryspikes,thisincreaseislargelycounterbalancedbythe decreaseofenergyrequiredforaccessingthememory. Theseresultsclearlyconfirmthatthetotalamountofspikes, showninFig. 7(1)isthemostimportantmetrictoconsiderforreducingtheenergyconsumptionofSNNs. Neitherthe averageactivitypertimestepnorthelatencyalonearesufficienttoproperlyevaluatetheoverallenergyconsumption. In thatsenseourresultsshowthat,usingmulti-valuedspikes,theenergyefficiencyofSNNscanbegreatlyimprovedby2 to3timesdependingonthenumberofquantizationintervals. 6.2 Energyefficiencycomparisonbetweenmulti-levelandbinaryspikingneuronsonNeuromorphicdata classification InthissectionwestudytwoconfigurationsoftheVGG16architecture,withmulti-levelandbinaryneurons,onthe CIFAR-10-DVSneuromorphicclassificationdataset. Specificallywecomparethemulti-levelVGG16networkgiven in Tab. 2 with a binary VGG16 trained on T = 10 timesteps. Both configurations thus provide 10 quantization 13 --- Page 14 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks Table4: Energyconsumptionbreakdownforbinaryandmulti-levelVGG16SNNsandANNonCIFAR-10-DVS.All energyvaluesareexpressedin[nJ]. E correspondstothetotalenergyconsumptionofthemulti-levelSNN,while N E referstothebinarySNN.Thesymbol[↓]meanslowerisbetter. binary ANN SNN binary multi-level [T =10,N =1] [T =1,N =10] Memory Potentials - 233.3·106 90.6·106 Weights 52·106 32.2·106 12.8·106 Bias 6.9·102 69.7·102 6.9·102 In/Out 186.3·106 40.5·103 15.8·103 Total 238.3·106 265.5·106 103.4·106 Synaptic 12.1·106 170.8·103 664.6·103 Operations Addressing 6.2·103 10.3·103 7.7·103 Total 250.4·106 265.7·106 104.1·106 E /E [↓] 0.39 N binary E /E [↓] 1.06 0.41 SNN ANN intervalsaswellassimilaraccuracylevels: 76.97%when{T =1, N =10}and75.5%forthebinaryconfiguration {T =10, N =1}. TheenergybreakdownforbothSNNsaswellastheANNisgiveninTab. 4. First it can be observed that the total energy consumption for both the SNNs and the ANN is almost twenty time higherthantheCIFAR-10resultsshowninTab. 3. ThehigherresolutionofCIFAR-10-DVS(128× 128)compared toCIFAR-10(32× 32)leadstoanincreaseofthenumberofmemoryaccessesandoperationstoprocesstheinput framesaswellasthefeaturemapsofhiddenlayers. However,resultsshowninTab. 4confirmtheenergyefficiency improvementsofmulti-levelSNNs,evenonneuromorphicdata. Asitcanbeobserved,themulti-levelVGG16reduces by60%theenergyconsumptioncomparedtotheANN.Theenergygainscomefromthereductionofenergyrelatedto memorytransfers. Themulti-levelSNNconsumesintotal103.4·106nJforthememoryaccesses,while238.3·106nJ arerequiredfortheANN.Inthemulti-levelSNN,mostoftheenergyisconsumedforaccessingtheneuronpotentials (90.6·106nJ)whileonly12.8·106nJarerelatedtothememoryaccessesassociatedwiththesynapticweights. On theotherhand,theANNconsumesfourtimesmoreenergyforaccessingthesynapticweights(52·106nJ)andtwice moreforaccessingthefeaturemaps,i.e. In/Out186.3·106nJ.Finally,thereisadifferenceofalmostthreeorderof magnitudeintheenergyconsumptionofthesynapticoperationsbetweentheANNandtheSNNs. However,forthe ANNtheenergyrelatedtothesynapticoperationsaccountsforlessthan5%ofthetotalenergybudget. Ourresultsare closetothosealreadyobservedinDampfhofferetal.[2023],thatistheenergycostofsynapticoperationsisverysmall comparedtotheoneoftransferringdatafromthememory. Therefore,theenergygainsobtainedbyreducingthecostof thesynapticoperationsaremarginalwhenconsideringthetotalenergyconsumption. Finally,thebinarySNNdoesnot exhibitenergygainscomparedtotheANNwhenusinglatenciesthatprovidestateoftheartaccuracyresultsonthe CIFAR-10-DVSdataset,i.e. 10ormoretimesteps. WhenT =10theSNNfires1.5·106spikeswhileonly591·103 spikesareemittedwhenN =1. AsshowninTab. 4,itrepresentsa60%reductioninthetotalspikingactivitywhichtranslatesinanequivalentenergy gainbetweenmulti-levelandbinarySNNs. Theseresultsagainconfirmthat,evenforneuromorphicdatasets,theenergy consumptioncanbeloweredbyreducingthetotalamountofspikesgeneratedbytheSNN. 6.3 GradientpropagationinspikingResNetarchitectures InthissectionwestudythegradientpropagationinthespikingresidualarchitecturesSEW-ResNetandtheSparse- ResNet that has been introduced in Sec. 4.4. To assess the improvement provided by the STE we also consider a variantofSparse-ResNetwhereallthespikingneurons,includingthebarrierneurons,usethesamesurrogatederivative functionsshowninFig. 5. Foreachblockofthethreearchitectures,SEW-REsNet18,Sparse-ResNet18withSTEand Sparse-ResNet18withoutSTEwemeasurethegradientnormonthedirectpath, ∂L,theresidualpath ∂L andafter ∂Al ∂Rl thesummationpoint ∂L. Thegradientisaveragedover104 minibatchesoftheCIFAR-10dataset. Wealsorepeat ∂Ol the simulation by training the networks 10 times starting from different initial conditions. The gradient mean and standarddeviationshowninFig. 8areprovidedforeachresidualblockofthenetwork. Theexperimentalresults 14 --- Page 15 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks Figure8: UsingSTEhelpsovercomethevanishinggradient. ThegradientsoftheresidualblocksinSEW-ResNet18, Sparse-ResNet18withSTEandSparse-ResNet18withoutSTEontheCIFAR-10dataset. Theblock0isthefirstblock aftertheinput. confirm the analysis given in Sec. 4.4. As it can be observed, the gradients norm on the direct and residual paths ofSEW-ResNetandSparse-ResNetwithSTEareclosertoeachotherandconsistentlyhigherthanthegradientsof Sparse-ResNetwithoutSTE.Indeed,whenSTEisnotusedtheupdatesofthegradientonSparse-ResNetareweaker, thusslowingdownthetraininganddecreasingtheperformance. ThisbehaviorcanbeobservedinFig. 9whereweshow thevalidationloss,computedattheendofeachtrainingepochforthethreenetworksontheCIFAR-10dataset. The lossofSparse-ResNetwithoutSTEdeclinesmoreslowlyandfinallyreachesahighervaluecomparedtoSEW-ResNet andSparse-ResNetwithSTE. 6.4 Spikespropagationinresiduallayers Inthissectionweprovidesomeexperimentalresultstoassesstheeffectivenessofthebarrierneuronsinlimitingthe spikepropagationintheresiduallayers.Todoso,wecomparetheactivityofSEW-ResNetandSparse-ResNetwithSTE measuredontheCIFAR-10dataset. Bothnetworksaretrainedusingthe[T =1,N =4]configuration,thatprovides almostthesameaccuracy,asshowninTab. 1. Wemeasurethenumberofgeneratedspikesinthedirectandtheresidual pathsforbothnetworks. Thespikesthataregeneratedatthesummationpointarecomputedasthesumofthenumberof spikescomingfromthedirectpathandthespikescomingfromtheresidualpathforSEW-ResNet. ForSparse-ResNet, wemeasureinsteadthenumberofspikesafterthebarrierneuron.Ourmeasurecorresponds,inbothcases,totheamount ofspikesthataretransferredfromonelayertothenextinthecontextofanevent-basedcommunication/processing scheme. TheexperimentalresultsareshowninFig. 10,wherewecanobservethatSEW-ResNetconsistentlygenerates morespikesinthefirstlayersofthenetworkcomparedtoSparse-ResNet. Thethenumberofgeneratedspikesstabilizes fromthemiddletothelastlayers,wherebothnetworksgeneratealmostthesameactivity. Thespikeavalancheeffect, 15 --- Page 16 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks Figure9: ComparisonofthevalidationlossofSEW-REsNet18,Sparse-ResNet18withSTEandSparse-ResNet18 withoutSTEonCIFAR-10. that we qualitatively described in Sec. 4.3, can be clearly observed in Fig. 10. The number of spikes generated bySEW-ResNetatthefirst(sum )andespeciallyatthesecond(sum )summationpointsareconsiderablyhigher 0 1 comparedtoSparse-ResNet. SEW-ResNetproduces45665spikesatthesummationpointsum whileonly37150spikesarefiredbySparse-ResNet. 0 Theavalancheeffectisevenmorevisibleatsum wherethereare67848spikesforSEW-ResNetandonly35819for 1 Sparse-ResNet. Thatis47%lessactivityinoneofthelayerthatgeneratesmostofthespikesinthewholenetwork. TheseresultsconfirmthatthebarrierneuronsofSparse-ResNetareabletopreventthespikeavalancheeffect,thus reducingthenetworkactivityaswediscussinmoredetailsinthenextsection. Figure10: Layer-wiseactivityofSEW-ResNet18andSparse-ResNet18. Top: numberofspikesgeneratedinthe directpath(block )andafterthesummationpoint(sum ). Bottom: numberofspikesgeneratedintheresidualpath i i (res ). ThevaluesareaveragedoveralltheimagesoftheCIFAR-10testset. i 6.5 AnalysisofsparsityinspikingResNetarchitectures Inthisfinalsectionweprovideexperimentalresultsonthespikingactivityformulti-levelSEW-ResNetandSparse- ResNet. TheaccuracyaswellasthetotalspikingactivityfordifferentconfigurationsonCIFAR-10,i.e. {T =1, N ∈ [1,8]},areshowninFig. 11. As it can be observed, both networks provide very similar accuracy results, with a marginally better accuracy for SEW-ResNetwhenN > 2. However, wecanobservethatSEW-ResNetgeneratesamuchhigherlevelofspiking 16 --- Page 17 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks Figure11: (A)Top-1accuracyand(B)spikingactivityofSparse-ResNet18andSEW-ResNet18ontheCIFAR-10 dataset. ForalltheconfigurationsthenumberoftimestepsT =1. Thespikingactivityiscomputedbyaveragingover alltheimagesoftheCIFAR-10testset. activity for all the configurations except when N = 1. As an example, when N = 2 SEW-ResNet fires 242662 spikes/imagewhileSparse-ResNetgenerates218343spikes/image,whichcorrespondstoa10%reductionintheoverall activitywhileprovidingverysimilaraccuracy. Thespikingactivitygapbetweenbothnetworkarchitectureswidensas Nincreases. FortheconfigurationN =4,Sparse-ResNetgenerates25%lessspikes/imagecomparedtoSEW-ResNet withonly0.25%accuracydrop. ThespikingactivityreductionprovidedbySparse-ResNetrisesto30%whenN =8 withanaccuracygaplessthan1%. AsdiscussedinSec. 4.3andshowninFig. 10,thedifferenceinspikingactivityis mainlyduetothespikeavalancheeffectcausedbythefirsttwoshortcutconnectionsintheResNet18network. This effectwouldbeexacerbatedindeepernetworks,thatiswhenmoreconsecutivelayersuseskipconnectionssuchas intheResNet34architectureHeetal.[2016],whenuptofiveconsecutiveresidualblocksuseshortcutconnections. AnotherexampleistheMobileNetV2architecturesSandleretal.[2019],whichiscomposedof17consecutivelayersof invertedresidualblocks. Ashortcutconnectstheinputwiththeoutputofeachblock. Forthespikingversionofthat particulararchitecture,thespikeavalancheeffectwouldbesignificantlyhigherifthespikesareaggregatedattheendof eachresidualconnectionsasshowninFig. 4. Inthatcasetheuseofbarrierneurons,asproposedinthispaper,could leadtoasignificantreductioninthetotalspikingactivityandthusanenergyefficiencyimprovement. 7 ConclusionandFutureworks Inthispaper,weproposedtoimprovethesparsityandenergyefficiencyofSNNsbothatneuronalandatnetworklevel. Byleveragingmulti-levelspikingneuronswehaveshownthattheenergyefficiencyofSNNscabbeimproved. By reducingthequantizationnoiseweareabletoprovidestateoftheartresultsusingonly1timestep,thereforeatthe lowestpossiblelatency. Ourapproachcanbealsoappliedtoneuromorphicdata,forwhichstateoftheartaccuracywere obtainedwhilereducingthelatencybyafactorof10. Atthearchitecturallevel,weidentifiedthespikeavalancheeffect thatimpactsmostofspikingresidualarchitectures. Sparse-ResNethasbeenproposedasasolutiontotheavalanche effect. Thisresidualarchitectureisindeedabletoprovidestateoftheartaccuracyresultsonimageclassificationwhile reducingbymorethan20%thenetworkactivity. Weidentifiedseveraldeepresidualarchitectures,widelyusedinimage processing,thatcanbeimpactedbythespikeavalancheeffect. Webelievethatitwouldbeinterestingtoextendour approachtothesearchitecturesandevaluatetheenergyefficiencygains,bothwithrealisticenergymodelsandusing realneuromorphichardware. References Chen Li, Lei Ma, and Steve Furber. Quantization Framework for Fast Spiking Neural Networks. Frontiers in Neuroscience,16,2022. ISSN1662-453X. URL 2022.918793. NitinRathiandKaushikRoy. DIET-SNN:ALow-LatencySpikingNeuralNetworkWithDirectInputEncodingand LeakageandThresholdOptimization. IEEETransactionsonNeuralNetworksandLearningSystems,pages1–9, 17 --- Page 18 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks 2021. ISSN2162-2388. doi:10.1109/TNNLS.2021.3111897. ConferenceName: IEEETransactionsonNeural NetworksandLearningSystems. Andrea Castagnetti, Alain Pegatoquet, and Benoît Miramond. Trainable quantization for Speedy Spiking Neural Networks. Frontiers in Neuroscience, 17, 2023a. ISSN 1662-453X. URL  articles/10.3389/fnins.2023.1154241. YufeiGuo,XinyiTong,YuanpeiChen,LiwenZhang,XiaodeLiu,ZheMa,andXuhuiHuang. RecDis-SNN:Rectifying MembranePotentialDistributionforDirectlyTrainingSpikingNeuralNetworks. In2022IEEE/CVFConferenceon ComputerVisionandPatternRecognition(CVPR),pages326–335,NewOrleans,LA,USA,June2022a.IEEE. ISBN 978-1-66546-946-3. doi:10.1109/CVPR52688.2022.00042. URL 9880053/. YongjunXiao,XianlongTian,YongqiDing,PeiHe,MengmengJing,andLinZuo. Multi-BitMechanism: ANovel InformationTransmissionParadigmforSpikingNeuralNetworks,July2024. URL 05739. arXiv:2407.05739[cs]version: 1. EdgarLemaire,LoïcCordone,AndreaCastagnetti,Pierre-EmmanuelNovac,JonathanCourtois,andBenoîtMiramond. AnAnalyticalEstimationofSpikingNeuralNetworksEnergyEfficiency. InMohammadTanveer,SonaliAgarwal, SeiichiOzawa,AsifEkbal,andAdamJatowt,editors,NeuralInformationProcessing,pages574–587,Cham,2023. SpringerInternationalPublishing. ISBN978-3-031-30105-6. doi:10.1007/978-3-031-30105-6_48. ManonDampfhoffer,ThomasMesquida,AlexandreValentian,andLorenaAnghel. AreSNNsReallyMoreEnergy- EfficientThanANNs?anIn-DepthHardware-AwareStudy.IEEETransactionsonEmergingTopicsinComputational Intelligence, 7(3):731–741, June 2023. ISSN 2471-285X. doi:10.1109/TETCI.2022.3214509. URL https: //ieeexplore.ieee.org/document/9927729/?arnumber=9927729. Conference Name: IEEE Transactions onEmergingTopicsinComputationalIntelligence. Wei Fang, Zhaofei Yu, Yanqi Chen, Tiejun Huang, Timothée Masquelier, and Yonghong Tian. Deep Residual LearninginSpikingNeuralNetworks. InAdvancesinNeuralInformationProcessingSystems,volume34,pages 21056–21069.CurranAssociates,Inc.,2021a. URL afe434653a898da20044041262b3ac74-Abstract.html. PeterU.Diehl,DanielNeil,JonathanBinas,MatthewCook,Shih-ChiiLiu,andMichaelPfeiffer. Fast-classifying, high-accuracyspikingdeepnetworksthroughweightandthresholdbalancing.In2015InternationalJointConference onNeuralNetworks(IJCNN),pages1–8,July2015. doi:10.1109/IJCNN.2015.7280696. ISSN:2161-4407. BodoRueckauer,Iulia-AlexandraLungu,YuhuangHu,MichaelPfeiffer,andShih-ChiiLiu. ConversionofContinuous- ValuedDeepNetworkstoEfficientEvent-DrivenNetworksforImageClassification. FrontiersinNeuroscience,11: 682,2017. ISSN1662-453X. doi:10.3389/fnins.2017.00682. URL 10.3389/fnins.2017.00682. Bing Han, Gopalakrishnan Srinivasan, and Kaushik Roy. RMP-SNN: Residual Membrane Potential Neuron for Enabling Deeper High-Accuracy and Low-Latency Spiking Neural Network. pages 13558–13567, 2020. URL  Potential_Neuron_for_Enabling_Deeper_High-Accuracy_and_CVPR_2020_paper.html. AnaStanojevic,StanisławWoz´niak,GuillaumeBellec,GiovanniCherubini,AngelikiPantazi,andWulframGerstner. An exact mapping from ReLU networks to spiking neural networks. Neural Networks, 168:74–88, November 2023. ISSN0893-6080. doi:10.1016/j.neunet.2023.09.011. URL article/pii/S0893608023005051. YuchenWang, HanwenLiu, MaluZhang, XiaolingLuo, andHongQu. AuniversalANN-to-SNNframeworkfor achieving high accuracy and low latency deep Spiking Neural Networks. Neural Networks, 174:106244, June 2024. ISSN0893-6080. doi:10.1016/j.neunet.2024.106244. URL article/pii/S0893608024001680. JianhaoDing,ZhaofeiYu,YonghongTian,andTiejunHuang. OptimalANN-SNNConversionforFastandAccurate Inference in Deep Spiking Neural Networks. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, pages 2328–2336, Montreal, Canada, August 2021. International Joint Conferences on Artificial Intelligence Organization. ISBN 978-0-9992411-9-6. doi:10.24963/ijcai.2021/321. URL https: //www.ijcai.org/proceedings/2021/321. YoshuaBengio,NicholasLéonard,andAaronCourville. EstimatingorPropagatingGradientsThroughStochastic NeuronsforConditionalComputation,August2013.URL [cs]. 18 --- Page 19 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks HongchaoYang, SuorongYang, LingmingZhang, HuiDou, FuraoShen, andJianZhao. CS-QCFS:Bridgingthe performancegapinultra-lowlatencyspikingneuralnetworks. NeuralNetworks,184:107076,April2025. ISSN 0893-6080. doi:10.1016/j.neunet.2024.107076. URL pii/S0893608024010050. EmreO.Neftci,HeshamMostafa,andFriedemannZenke. SurrogateGradientLearninginSpikingNeuralNetworks: BringingthePowerofGradient-BasedOptimizationtoSpikingNeuralNetworks. IEEESignalProcessingMagazine, 36(6):51–63,November2019. ISSN1558-0792. doi:10.1109/MSP.2019.2931595. ConferenceName: IEEESignal ProcessingMagazine. AndreaCastagnetti,AlainPegatoquet,andBenoîtMiramond. SPIDEN:deepSpikingNeuralNetworksforefficient imagedenoising. FrontiersinNeuroscience,17,2023b. ISSN1662-453X. URL org/articles/10.3389/fnins.2023.1224457. YangLi,FeifeiZhao,DongchengZhao,andYiZeng.DirectlytrainingtemporalSpikingNeuralNetworkwithsparsesur- rogategradient. NeuralNetworks,179:106499,November2024. ISSN08936080. doi:10.1016/j.neunet.2024.106499. URL YufeiGuo, YuanpeiChen, LiwenZhang, XiaodeLiu, YingleiWang, XuhuiHuang, andZheMa. IM-Loss: Infor- mation Maximization Loss for Spiking Neural Networks. Advances in Neural Information Processing Systems, 35:156–166,December2022b. URL 010c5ba0cafc743fece8be02e7adb8dd-Abstract-Conference.html. Yunhua Chen, Ren Feng, Zhimin Xiong, Jinsheng Xiao, and Jian K. Liu. High-performance deep spiking neural networks via at-most-two-spike exponential coding. Neural Networks, 176:106346, August 2024. ISSN 0893- 6080. doi:10.1016/j.neunet.2024.106346. URL S0893608024002703. Yufei Guo, Yuanpei Chen, Xiaode Liu, Weihang Peng, Yuhan Zhang, Xuhui Huang, and Zhe Ma. Ternary Spike: Learning Ternary Spikes for Spiking Neural Networks, December 2023. URL  06372. arXiv:2312.06372[cs]. Congyi Sun, Qinyu Chen, Yuxiang Fu, and Li Li. Deep Spiking Neural Network with Ternary Spikes. In 2022 IEEE Biomedical Circuits and Systems Conference (BioCAS), pages 251–254, October 2022. doi:10.1109/BioCAS54905.2022.9948581. URL ISSN: 2163-4025. LangFeng,QianhuiLiu,HuajinTang,DeMa,andGangPan. Multi-LevelFiringwithSpikingDS-ResNet: Enabling Better and Deeper Directly-Trained Spiking Neural Networks. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, pages 2471–2477, Vienna, Austria, July 2022. International Joint ConferencesonArtificialIntelligenceOrganization. ISBN978-1-956792-00-3. doi:10.24963/ijcai.2022/343. URL  XiaotingWangandYanxiangZhang. MT-SNN:EnhanceSpikingNeuralNetworkwithMultipleThresholds,October 2024. URL arXiv:2303.11127[cs]. SeijoonKim,SeongsikPark,ByunggookNa,andSungrohYoon. Spiking-YOLO:SpikingNeuralNetworkforEnergy- EfficientObjectDetection. ProceedingsoftheAAAIConferenceonArtificialIntelligence,34(07):11270–11277, April2020. ISSN2374-3468,2159-5399. doi:10.1609/aaai.v34i07.6787. URL php/AAAI/article/view/6787. KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. DeepResidualLearningforImageRecognition. In2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, Las Vegas, NV, USA, June2016.IEEE. ISBN978-1-4673-8851-1. doi:10.1109/CVPR.2016.90. URL document/7780459/. MarkSandler,AndrewHoward,MenglongZhu,AndreyZhmoginov,andLiang-ChiehChen. MobileNetV2: Inverted ResidualsandLinearBottlenecks,March2019. URL arXiv:1801.04381 [cs]. MingxingTanandQuocV.Le. EfficientNet: RethinkingModelScalingforConvolutionalNeuralNetworks,September 2020. URL arXiv:1905.11946[cs,stat]. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszkoreit,andNeilHoulsby. AnImage isWorth16x16Words: TransformersforImageRecognitionatScale,June2021. URL 2010.11929. arXiv:2010.11929[cs]. 19 --- Page 20 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks YangfanHu,HuajinTang,andGangPan. SpikingDeepResidualNetworks. IEEETransactionsonNeuralNetworks andLearningSystems,34(8):5200–5205,August2023a. ISSN2162-2388. doi:10.1109/TNNLS.2021.3119238. URL YifanHu,LeiDeng,YujieWu,ManYao,andGuoqiLi. AdvancingSpikingNeuralNetworkstowardsDeepResidual Learning,March2023b. URL arXiv:2112.08954[cs]. AlexKrizhevsky. Learningmultiplelayersoffeaturesfromtinyimages. Technicalreport,2009. Hongmin Li, Hanchao Liu, Xiangyang Ji, Guoqi Li, and Luping Shi. CIFAR10-DVS: An Event-Stream Dataset for Object Classification. Frontiers in Neuroscience, 11, May 2017. ISSN 1662-453X. doi:10.3389/fnins.2017.00309. URL 10.3389/fnins.2017.00309/full. Publisher: Frontiers. WeiFang,ZhaofeiYu,YanqiChen,TimotheeMasquelier,TiejunHuang,andYonghongTian. IncorporatingLearnable MembraneTimeConstanttoEnhanceLearningofSpikingNeuralNetworks,August2021b. URL org/abs/2007.05785. arXiv:2007.05785[cs]. Jacques Kaiser, Hesham Mostafa, and Emre Neftci. Synaptic Plasticity Dynamics for Deep Contin- uous Local Learning (DECOLLE). Frontiers in Neuroscience, 14, May 2020. ISSN 1662-453X. doi:10.3389/fnins.2020.00424. URL 10.3389/fnins.2020.00424/full. Publisher: Frontiers. JunHaengLee,TobiDelbruck,andMichaelPfeiffer. TrainingDeepSpikingNeuralNetworksUsingBackpropagation. FrontiersinNeuroscience,10,November2016. ISSN1662-453X. doi:10.3389/fnins.2016.00508. URLhttps: //www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2016.00508/full. Pub- lisher: Frontiers. YannanXing,GaetanoDiCaterina,andJohnSoraghan. ANewSpikingConvolutionalRecurrentNeuralNetwork (SCRNN)WithApplicationstoEvent-BasedHandGestureRecognition. FrontiersinNeuroscience,14,November 2020. ISSN1662-453X. doi:10.3389/fnins.2020.590164. URL neuroscience/articles/10.3389/fnins.2020.590164/full. Publisher: Frontiers. Yuhang Li, Shikuang Deng, Xin Dong, Ruihao Gong, and Shi Gu. A Free Lunch From ANN: Towards Efficient, AccurateSpikingNeuralNetworksCalibration. InProceedingsofthe38thInternationalConferenceonMachine Learning,pages6316–6325.PMLR,July2021a. URL ISSN:2640-3498. QingyanMeng,MingqingXiao,ShenYan,YisenWang,ZhouchenLin,andZhi-QuanLuo. TrainingHigh-Performance Low-LatencySpikingNeuralNetworksbyDifferentiationonSpikeRepresentation. In2022IEEE/CVFConference onComputerVisionandPatternRecognition(CVPR),pages12434–12443,NewOrleans,LA,USA,June2022. IEEE. ISBN978-1-6654-6946-3. doi:10.1109/CVPR52688.2022.01212. URL document/9878999/. YuhangLi,YufeiGuo,ShanghangZhang,ShikuangDeng,YongqingHai,andShiGu. DifferentiableSpike: Rethinking Gradient-DescentforTrainingSpikingNeuralNetworks. InAdvancesinNeuralInformationProcessingSystems, volume 34, pages 23426–23439. Curran Associates, Inc., 2021b. URL  paper/2021/hash/c4ca4238a0b923820dcc509a6f75849b-Abstract.html. NormanP.Jouppi, DoeHyunYoon, MatthewAshcraft, MarkGottscho, ThomasB.Jablin, GeorgeKurian, James Laudon,ShengLi,PeterMa,XiaoyuMa,ThomasNorrie,NishantPatil,SushmaPrasad,CliffYoung,Zongwei Zhou,andDavidPatterson. TenLessonsFromThreeGenerationsShapedGoogle’sTPUv4i: IndustrialProduct. In 2021ACM/IEEE48thAnnualInternationalSymposiumonComputerArchitecture(ISCA),pages1–14,June2021. doi:10.1109/ISCA52012.2021.00010. ISSN:2575-713X. 20