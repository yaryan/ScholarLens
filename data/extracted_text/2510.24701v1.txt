--- Page 1 --- 2025-10-29 Tongyi DeepResearch Technical Report ∗ TongyiDeepResearchTeam TongyiLab ,AlibabaGroup     Abstract WepresentTongyiDeepResearch,anagenticlargelanguagemodel,which isspecificallydesignedforlong-horizon,deepinformation-seekingresearch tasks. Toincentivizeautonomousdeepresearchagency,TongyiDeepResearch isdevelopedthroughanend-to-endtrainingframeworkthatcombinesagen- tic mid-training and agentic post-training, enabling scalable reasoning and informationseekingacrosscomplextasks. Wedesignahighlyscalabledata synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environmentsforeachstage,oursystemenablesstableandconsistentinterac- tionsthroughout. TongyiDeepResearch,featuring30.5billiontotalparameters, withonly3.3billionactivatedpertoken,achievesstate-of-the-artperformance acrossarangeofagenticdeepresearchbenchmarks,includingHumanity’sLast Exam,BrowseComp,BrowseComp-ZH,WebWalkerQA,xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, frame- work,andcompletesolutionstoempowerthecommunity. 35 30 25 20 15 DeTe(op3nR0geBys-i Aea3rBc)h DeepSeek-V3.1 ReKsiemaircher DeGeepm Riensiearch DeOeppeRneA sIearch GLM-4.5 DPeeerpplReexsiteyarch  Humanity's Last Exam BrowseComp BrowseComp-ZH WebWalkerQA 32.9 29.8 45 55 51.5 43.4 50 46.7 42.9 77 05 72.2 71.7 26.9 26.9 26.6 21.2 21.1 123 555 30.0 28.3 26.4 14.1 12.2 34 00 37.5 29.1 28.8 66 05 65.6 63.0 61.7 61.2 5 DeOeppeRneA sIearch DeTe(op3nR0geBys-i Aea3rBc)h DeepSeek-V3.1 OpenAI-o4-mini GLM-4.5 Kimi-K2 Claude-4-Sonnet20 DeTe(op3nR0geBys-i Aea3rBc)h DeOeppeRneA sIearch GLM-4.5 Claude-4-Sonnet Kimi-K2 55 DeTe(op3nR0geBys-i Aea3rBc)h OpenAI-o3 GLM-4.5 Kimi-K2 Claude-4-Sonnet DeepSeek-V3.1 70 60 50 DeTe(op3nR0geBys-i Aea3rBc)h Claude-4-Sonnet DeOeppeRneA sIearch GLM-4.5 DeepSeek-V3.1 Kimi-K2  GAIA xbench-DeepSearch FRAMES xbench-DeepSearch-2510 70.9 68.3 67.4 66.0 63.1 57.7 67 55 75.0 71.0 70.0 69.0 67.0 65.0 789 555 90.6 84.0 83.7 80.7 78.9 78.8 72.0 4578 0505 75+ 55+ 40+ 35+ 35+ 35+ 25 55 DeTe(op3nR0geBys-i Aea3rBc)h DeepSeek-V3.1 GLM-4.5 ReKsiemaircher OpenAI-o3 Claude-4-Sonnet 65 DeTe(op3nR0geBys-i Aea3rBc)h OpenAI-o3 DeepSeek-V3.1 Claude-4-Sonnet GLM-4.5 ReKsiemaircher Kimi-K2 10 ChatGPT-5-Pro DeTe(op3nR0geBys-i Aea3rBc)h SupEexrpGerrotk DeDeop uRbeasoearch Manus Agent Minimax Agent Figure1: BenchmarkperformanceofTongyiDeepResearch. ∗FullauthorlistavailableintheContributionssection. 1 5202 tcO 82 ]LC.sc[ 1v10742.0152:viXra --- Page 2 --- 1 Introduction AsweadvancetowardArtificialGeneralIntelligence(AGI),theemergenceofDeepResearchagentsoffers apromisingparadigmforaugmentingandpotentiallyliberatinghumanintellectualproductivity. Deep researchisanewagenticcapabilitythatautonomouslyconductsmulti-stepreasoningandinformation seekingontheinternetforcomplexresearchtasks. Itcanbecompletedintensofminutes,whichwould otherwise require several hours for a human (OpenAI, 2025a; Claude Team, 2025; Grok Team, 2025; GeminiTeam,2025). However,mostdeepresearchsystemsremainclosed-source,andtheirintermediate researchprocessesareinaccessible. Whilethecommunityhasmadepreliminaryexplorationsinthis area(Wuetal.,2025a;Lietal.,2025c;Taoetal.,2025),thereisstillalackofasystematicmethodologyand publiclyavailablemodelsthatcanbefullyopen-sourcedandsharedacrossthecommunity. Inthiswork,weintroduceTongyiDeepResearch,openingtheeraofopen-sourceAIresearchers. Our goalistoendowlargelanguagemodels(LLMs)withautonomousresearchcapabilitiesagency,theability toplan, search, reason, andsynthesizeknowledgeacrossextendedsequencesofactionsanddiverse informationsources. TongyiDeepResearchdeliversseveralkeyadvancements: • Weproposeanend-to-endagentictrainingparadigmthatunifiesagenticmid-trainingandagentic post-training,formingascalablefoundationfordeepreasoningandinformation-seekingbehaviors. Agenticmid-trainingcultivatesinherentagenticbiasesbyexposingthemodeltolarge-scale,high- quality agentic data, serving as a progressive transition from pre-training to post-training stages. Agenticpost-trainingfurtherunlocksthemodel’spotentialviascalablemulti-turnreinforcement learningonastrongbasemodel. Together,theyenablethemodeltograduallydevelopfrombasic interactionskillstoadvancedautonomousresearchbehaviors. • Wedesignafullyautomated,highlyscalabledatasynthesispipelinethateliminateshumanannota- tionwhilegeneratingdiverse,high-qualityagenttrajectories. Wedesignstage-specificdatasynthesis strategiestailoredtotheobjectivesofeachtrainingphase,ensuringthateverystageissupported by appropriately structured and targeted data. Synthetic data is highly scalable, fast to validate, andenablestheconstructionofsuper-human-leveldatasetswithstabledistributions. Itservesasan indispensableengineforagenttraining. • Weconstructstage-specific,customizedenvironmentsthatrelyonrobustinfrastructuretodeliver consistentinteractionsfordatasynthesisacrosstrainingstages.Theseenvironmentsallowtheagentto engageinrich,specializedinteractionsthataretightlyalignedwithitsdevelopmentalstage. Theycan takevariousforms,frompriorworldmodelstosimulatedenvironmentsandreal-worldinteractive contexts. TongyiDeepResearchestablishesanewstate-of-the-artwithsubstantiallyfewerparameters,comprising atotalof30.5billionparameterswhileactivatingonly3.3billionpertoken,buildingupontheQwen3- 30B-A3B-Basemodel(Yangetal.,2025). Empiricalevaluationsondeepresearchbenchmarksdemonstrate the effectiveness of our agent. Tongyi DeepResearch reaches 32.9 on Humanity’s Last Exam, 43.4 on BrowseComp, 46.7 on BrowseComp-ZH, 72.2 on WebWalkerQA, 70.9 on GAIA, 75.0 on xbench- DeepSearch, 90.6onFRAMESand55.0onxbench-DeepSearch-2510, outperformingstrongbaselines such as OpenAI-o3 (OpenAI, 2025b) and Deepseek-V3.1 (DeepSeek Team, 2025). We also provide a systematicanalysiscoveringagenticreinforcementlearning,syntheticdata,offeringkeyinsightsintothe developmentofdeepresearchagent. Inaddition,wepresenttheperformanceofTongyiDeepResearch ongeneralbenchmarks,includingAIME25,HMMT25andSimpleQA.Webelievethatagenticmodels representanemergingtrendforthefuture,asmodelsincreasinglyinternalizeagent-likecapabilitiesand canautonomouslyinvoketheappropriatetoolstosolveawiderangeofproblems. Inthefollowingsections,wefirstoutlinethedesignprinciplesunderlyingTongyiDeepResearch. We then describe the training pipeline, followed by a comprehensive evaluation of its performance. We 2 --- Page 3 --- releasethemodel,framework,andend-to-endsolutionstosupportandacceleratecommunityresearch. Thistechnicalreportsummarizesourmaininsightsandaimstoinspirefurtherprogresstowardscalable andcapableagenticsystems. 2 Design Principle AgentTrainingPipeline. Agenttrainingisinherentlymorecomplexandchallengingthanconventional LLMtraining. Weintroducetwostagesinouragenttrainingpipeline: mid-trainingandpost-training. Weintegratemid-trainingdirectlyintothedeepresearchtrainingprocess,andco-designtheend-to-end on-policyreinforcementlearningalgorithmanditsunderlyinginfrastructureforseamlessscalability andstability. Whilemostworkonlyappliespost-trainingphaseforDeepResearchagents,wenovelly introducemid-trainingforagenticlearning. Generalfoundationmodelsusuallylackagenticinductive bias. Mostgeneralfoundationmodelsaretypicallypretrainedonplaintextcrawledfromtheinternetand thenpost-trainedoninstruction-followingdata. Thesedatasetslackresearch-levelquestionsandagentic behaviors,resultinginthemodellearnsagenticcapabilitiesandalignmentsimultaneouslyduringthe post-trainingphase. Agenticpost-trainingonthesegeneralfoundationmodelscanresultinsub-optimal outcomesandinherentoptimizationconflicts. Mid-trainingendowsthepre-trainedbasemodelwith substantialagenticpriorknowledge,therebybridgingthegapbetweenpretrainingandagenticpost- training. Mid-trainingphaseprovidesapowerfulagenticfoundationmodeltosupporteffectiveagentic post-training. Duringpost-training,themodelfurtherinternalizesdeepresearchcapabilitiesthrough reinforcementlearningwithsupervisedfine-tuning(SFT)forcoldstart. SFTteachesthemodeltoreliably imitatecurateddemonstrations,establishingastablebehavioralbaselineforresearchworkflowsandtool use. However,behaviorcloningalonetendstoproducemimicrywithoutexploration. RLclosestheloop withtheenvironment,usingrewardsignalstorefinepoliciesandtointernalizeagenticplanningand execution. Inparticular,reinforcementlearning(1)exploresoptimalstrategiesthroughactiveinteraction withtheenvironment;(2)internalizesgoal-directedplanningandexecutioncapabilities;and3)achieves superiorsampleefficiencybyprioritizinghigh-rewardbehaviors. Theagentfirstacquiresgeneralagentic patternduringsupervisedfine-tuningphase,whilereinforcementlearningphaseeffectivelypushesthe limitsofitsagenticperformance. Synthetic Data Centric Scaling. Data serves as the foundation of training, while collecting data for DeepResearchproblemsisextremelyhard. Deepresearchproblemsrequireagents’capabilityofconnect- inginformation,reasoningacrosssourcesandvalidatingconclusions. Unlikepre-trainingdata,which isnaturallyabundant,andconventionalLLMpost-trainingdata,whichisrelativelyeasytoannotate, agenticdataisinherentlyscarce. Research-levelproblemsaredifficulttoobtainthroughnaturaltexts fromtheweb. Manuallyannotatingtheseproblemsandagentictrajectoriesisextremelytime-consuming andcostly(Weietal.,2025). Buildingontheaforementionedagenttrainingpipeline,agenticmid-training requireslarge-scale,diversetrajectoriestoalignsubsequentagentbehaviors,whileagenticpost-training depends on high-quality, verifiable data to provide reliable reward signals. As a result, it is hard to relyonnaturaldatatoscaleDeepResearchcapability. Therefore,wefocusonsyntheticdatawithlarge languagemodels. Syntheticdatacontainsseveraladvantagesoverhumanannotationsbelow: • Synthesizingresearch-levelquestionsiseasytoscale. WecanuseLLMstosynthesizequestion- answerpairefficientlycomparedtomanuallyannotating. • Thepatternanddiversityareeasytogeneralize. LLMsareeasytounderstandthestructureofhard problemsandusuallyhaverareinsightintodiversepatterns,whiletrainingannotatorstounderstand thestructureandpatternsforresearch-levelproblemsistime-consuming. • Synthesizeddataenablestargetedmeta-capabilityenhancement. Bydecomposingcomplexagent tasksintofundamentalmeta-capabilities(e.g.,planning,informationsynthesis,memorymanagement), wecangeneratesyntheticdatathatspecificallytargetsandstrengthensindividualagentskills. • Synthesizeddatacanbeverifiedeasily. Itismucheasierthanfindingthesolutiontothequestion, 3 --- Page 4 --- whichisessentialinhumanannotating. • Synthesized data can provide data flywheels in training stages. After one round of the agentic trainingpipeline,thetrainedagenticmodelcangeneratesynthesizeddatawithstrongerreasoning andplanningpatterns. Dataflywheelmakestheagenticmodelevolveiteratively. Based on these insights, we believe synthetic agentic data becomes the key to scaling deep-research agents. Thesyntheticdatainallphasesoftheagentictrainingpipelinearedesignedinthreesteps: (1) synthesizingresearch-levelquestions;(2)Generatingagenticbehaviordata;(3)Utilizingagenticdatain trainingpipeline. LearningThroughEnvironmentalInteraction. Environmentalinteractionplaysacrucialroleinagent intelligenceemergence(Silver&Sutton,2025). However,relyingsolelyonreal-worldenvironmentsfor thewholeagenttrainingstagefacesfundamentalchallenges: (1)Non-stationarity. Thedynamicnature ofenvironmentscausescontinuousdistributionshiftintrainingdata,undermininglearningstability; (2)Interactioncost. ThetangibleexpenseofeachAPIcallmakeslarge-scaleexplorationeconomically prohibitive. Thesebarriersrenderagentcapabilityacquisitionfromtherealworldaloneaformidable endeavor. InTongyiDeepResearch,weproposeafundamentalreframing: environmentsshouldnotbepassivelyviewed asexternalreality,butactivelydesignedassystemsdeeplycoupledwiththetrainingprocess. Specifically,we modelenvironmentsintothreeforms,eachstrikingadistinctbalancebetweenstability,fidelity,andcost: • PriorWorldEnvironment. Thisenvironmentprovidestaskelements, tools, andstatedefinitions, allowingagentstoautonomouslymineinteractiontrajectoriesbasedonpretrainedknowledgewithout receivingactualenvironmentalresponses.Itoffersperfectstability,zerointeractioncost,andunlimited scalability,butlacksreal-worldfeedbacksignals. • SimulatedEnvironment. Thisenvironmentconstructscontrolled,reproduciblereplicasofreal-world interactionslocally. Itprovidesstability, rapidresponse, andlowcost, enablingfastiterationand causalattributionanalysis. However, itsdatacoverageisinherentlylimited, exhibitinganotable sim-to-realgap. • Real-world Environment. This environment delivers the most authentic data distribution and feedbacksignals,servingastheultimateprovinggroundforagentcapabilities. Itsadvantageliesin absolutedistributionalfidelity;thecostisexpensiveinteractions,significantnon-stationarity,and explorationrisks. Building on this environmental insight, we adopt adaptive strategies for synthetic data generation and training. Specifically, (1) During agentic mid-training, we primarily leverage the Prior World EnvironmentandSimulatedEnvironmenttogeneratelarge-scalesyntheticdataatminimalcost,ensuring efficientagenticabilitybootstrapping;(2)Duringagenticpost-training,wevalidatetrainingstrategies andalgorithmictechniquesinthesimulatedenvironment,thendeployverifiedoptimalpoliciestothe realenvironmentforfinaltraining. Thechoiceofenvironmentsplaysacrucialrole,agenticintelligence emergesnotfromasinglewolrd,butfromcarefullychosenenvironments. Agenttrainingfundamentallydependsonsyntheticdataandenvironmentinteraction. Basedonthese designprinciples,wethenintroduceTongyiDeepResearchindetailbelow. 3 Tongyi DeepResearch 3.1 Formulation WeformallydefinetheTongyiDeepResearch’srolloutateachtimestep t throughthreefundamental components: • Thought(τ): Theinternalcognitiveprocessoftheagent. Thisincludesanalyzingthecurrentcontext, t 4 --- Page 5 --- recallinginformationfrommemory,planningsubsequentsteps,andengaginginself-reflectionto adjustitsstrategy. • Action(a ): Anexternaloperationexecutedbytheagenttointeractwithitsenvironment. Tongyi t DeepResearch is equipped with a versatile set of tools that define its action space, enabling it to interactwithawiderangeofinformationsources: Search,Visit,PythonInterpreter,GoogleScholarand FileParser. Actionsencompassallintermediatetoolcallsandthefinalresponsetotheuser. Inagiven trajectory,intermediateactions(a wheret < T)aretoolcalls,whilethefinalaction,a ,constitutes t T thegenerationofanin-depthreportfortheuser. • Observation(o ): Thefeedbackreceivedfromtheenvironmentafteranactionisperformed. Thisnew t informationisusedtoupdatetheagent’sinternalstateandinformitsnextthought. Basedonthefundamentalcomponentsabove,wedefinetwodifferentrollouttypesasfollows: ReAct. TongyiDeepResearch’sarchitectureisfundamentallybasedonthevanillaReAct(Yaoetal., 2023)framework,whichsynergizesreasoningandacting. Inthisparadigm,theagentgeneratesboth areasoningtrace(Thought)andasubsequentActioninaninterleavedmanner. Thisprocessformsa trajectory,H ,whichisasequenceofthought-action-observationtriplets: T H = (τ ,a ,o ,...,τ,a,o,...,τ ,a ), (1) T 0 0 0 i i i T T where a representsthefinalanswertothegiventask. Atanygivenstept ≤ T,theagent’spolicy,π, T generatesthecurrentthoughtτ t andactiona t basedonthehistoryofallpreviousinteractions,H t−1: τ t,a t ∼ π(·|H t−1). (2) Whilemorecomplexsingleandmulti-agentparadigmshaveemerged,ourchoiceofReActisadeliberate one,rootedinitssimplicityandalignmentwithfundamentalprinciples. Thisdecisionisinformedby "TheBitterLesson"(Sutton,2019),whichpositsthatgeneralmethodsleveragingscalablecomputation ultimatelyoutperformapproachesthatrelyoncomplex,human-engineeredknowledgeandintricate designs. Frameworksthatrequireextensive,specializedpromptengineeringorpossessrigidoperational structuresriskbecomingobsoleteastheintrinsiccapabilitiesofmodelsscale(Lietal.,2025a). ContextManagement. Theexecutionoflong-horizontasksisfundamentallyconstrainedbythefinite lengthoftheagent’scontextwindow. Tomitigatetheriskofcontextoverflowandensuretaskfocus, weproposethecontextmanagementparadigm(Qiaoetal.,2025),whichemploysadynamiccontext managementmechanismbasedonMarkovianstatereconstruction. Withinthisframework,theagent isnotconditionedonthecompletehistory. Instead,ateachstept,itisconditionedonastrategically reconstructedworkspacecontainingonlyessentialelements:thequestionq,anevolvingreportS serving t ascompressedmemory,andtheimmediatecontextfromthelastinteraction(a ando ). ThisMarkovian t t structureenablestheagenttomaintainconsistentreasoningcapacityacrossarbitraryexplorationdepths whilenaturallycircumventingthedegradation. Foreverystep0< t < T,thiscoreupdateprocesscanbe formalizedas: S t,τ t+1,a t+1 ∼ π(·|S t−1,a t,o t). (3) This context management paradigm is particularly crucial, it not only prevents context suffocation but also enforces structured reasoning by requiring the agent to explicitly synthesize and prioritize informationateachstep. Thisdesignnaturallyalignswithhumanresearchpatterns,whereperiodic synthesisandreflectionareessentialformaintainingcoherentlong-terminvestigation. 3.2 Overall Training Recipe ThesystemisinitializedfromthepretrainedbasemodelQwen3-30B-A3B-Base1. TongyiDeepResearch isdevelopedthroughanend-to-endtrainingframeworkthatintegratesagenticmid-trainingandpost- training, enabling scalable reasoning and information seeking across complex research tasks. This 1 5 --- Page 6 --- establishesanewparadigmfortrainingagenticmodels. Wefirstpresentthemid-trainingprocessin Section3.3,followedbythepost-trainingstageinSection3.4. Pre-training Mid-training Post-training Agentic CPT Agentic CPT Qwen Series Base Stage 1 Stage 2 Agentic SFT Agentic RL Models 32K 128K Figure2: TrainingpipelineofTongyiDeepResearch. 3.3 Agentic Mid-training 3.3.1 TrainingConfiguration TongyiDeepResearchemploysatwo-stageAgenticContinualPre-training(AgenticCPT)(Suetal., 2025) as its core mid-training phase. This phase functions as a critical bridge connecting pre-trained modelsandagenticpost-training. Itsprimaryobjectiveistoprovideabasemodelendowedwithastrong inductivebiasforagenticbehavior,whilesimultaneouslypreservingbroadlinguisticcompetence. To achievethis,theoptimizationprocessisdrivenbythestandardNext-TokenPredictionlossfunction. Thedesignofthisphaseisstrategicallyoptimizedforbothefficiencyandprogressivecapabilityscaling. Weinitiatewitha32Kcontextlengthinthefirststage,beforeexpandingto128Kinthesecond. This expandedcontextwindowisspecificallyleveragedinthesecondstage,whereweintroduceasubstantial corpusoflong-sequence(64K-128K)agenticbehaviordata. Thisapproachiscriticalforenhancingthe model’s capacity for coherent, long-horizon reasoning and action. Throughout both stages, a small proportionofgeneralpre-trainingdataisinterleaved,ensuringthemodelacquiresspecializedagentic competencewithoutsacrificingitsfoundationalgeneralizationcapabilities. 3.3.2 Large-scaleAgentBehaviorDataSynthesis PlanningAction ReasoningAction Hidden Process ···· Task Planning Response Decision-Making Reasoning Answer Potential Pathways QuestionSynthesis Decision-Making Action Figure3: Large-scaleagentbehaviordatasynthesisforagenticcontinualpre-training. InAgenticCPT,wesynthesizedataacrossthecompletelifecycleofagentworkflowsasshowninFigure3. Atypicalagentworkflowbeginswithaproblem,iterativelycyclesthroughreflectionandaction,and ultimatelyconvergesonafinalsolution. Tocomprehensivelycapturethisprocess,wesynthesizedata forthecriticalstepsthatconstitutetheagent’soperationalcycle: QuestionSynthesis,PlanningAction, ReasoningAction,andDecision-MakingAction. Notethatwhiledecision-makingisoftenimplicitwithin agentcycles,weexplicitlymodelitasadistinctactiontypeinoursynthesisframework. Large-scaleMulti-styleQuestionSynthesis.Groundedincontinuouslyupdatedopen-worldknowledge, weconstructanentity-anchoredopen-worldmemory. Thismemoryconsolidatesdiversereal-world knowledgesources,suchasweb-crawleddataandagentinteractiontrajectories,intostructuredrepresen- tationsofentitiesandtheirassociatedknowledge. Buildinguponthisfoundation,wesampleentities alongwiththeirrelatedknowledgetogeneratediversequestionsthatembedspecificbehavioralpattern requirements,suchasmulti-hopreasoningquestionsandnumericalcomputationquestions. 6 --- Page 7 --- Planning Action. Planning refers to problem decomposition and first-step action prediction. A key insightisthatplanningaccuracyishighlycorrelatedwithwhetheranagentcansuccessfullycompletea task. Thus,weemployopen-sourcemodelstoanalyze,decompose,andpredictinitialactionsforthe synthesizedquestions. Furthermore,weleveragetheentitiesandassociatedknowledgeusedinquestion constructionasthebasisforrejectionsampling,therebyensuringhigh-qualityplanningoutputs. ReasoningAction.Logicalreasoningandknowledgeintegrationoverheterogeneousdataisfoundational foragentssolvingcomplextasks. Whenexternaltoolsreturnmassiveunstructuredresponses,whether modelscandistillcriticalknowledgefromnoiseandconstructcoherentreasoningpathsdirectlydeter- minestaskoutcomes. Tothisend,givenaquestionanditsdependentknowledge,weguidelargemodels throughatwo-stageprocesstogeneratecompletereasoningchains,withadualfilteringmechanism basedonreasoninglengthandanswerconsistencytoensurequality. Decision-MakingAction. Eachstepofanagent’sthinkingandactionisessentiallyanimplicitdecision- makingprocess. Specifically,eachdecisionpointencompassesmultiplepotentialreasoningandaction paths,fromwhichtheagentmustselectthemostpromisingsolution. Tocapturethiscriticalmechanism, weexplicitlymodelthisdecision-makingprocess. First,basedonexistingdemonstrationtrajectories,we thoroughlyexplorethefeasibleactionspaceateachstep. Second,wereconstructtheoriginaltrajectories intomulti-stepdecisionsequenceswhilepreservingtheoriginaldecisionchoices. GeneralFunction-callingDataSynthesisviaEnvironmentScaling. Toenhanceourmodel’sgeneral agenticcapability,wesystematicallyscalethefunction-callingdatathroughenvironmentscaling. The breadthoffunction-callingcompetenceiscloselytiedtothediversityofenvironmentsinwhichagents are trained (Fang et al., 2025). We also scale up environments as a step towards advancing general agenticintelligence. Indesigningenvironmentconstructionandscaling,wefollowtheprinciplethat thecoreofanagentliesinitscapacityforenvironmentinteraction,witheachenvironmentinstantiated asaread–writedatabase. Wedesignascalableframeworkthatautomaticallyconstructsheterogeneous environmentsthatarefullysimulated,systematicallybroadeningthespaceoffunction-callingscenarios. Theproduceddataareincorporatedintothemodel’smid-trainingphase. 3.4 Agentic Post-training Thepost-trainingpipelinecomprisesthreestages: datasynthesis,supervisedfine-tuningforcoldstart, andagenticreinforcementlearning. 3.4.1 High-qualityDataSynthesis (1) GraphConstruction (2) SubgraphSampling (3) Uncertainty Injection Figure4: High-qualitydatasynthesispipeline. Wedevelopanend-to-endsolutionforsyntheticdatagenerationtogeneratecomplex,high-uncertainty and super-human level question and answer pairs (Li et al., 2025c;b), as shown in Figure 4. This fully automated process requires no human intervention to construct super-human quality datasets, designedtopushtheboundariesofagentperformance. Theprocessbeginsbyconstructingahighly interconnectedknowledgegraphviarandomwalks,leveragingwebsearchtoacquirerelevantknowledge, and isomorphic tables from real-world websites, ensuring a realistic information structure. We then 7 --- Page 8 --- samplesubgraphsandsubtablestogenerateinitialquestionsandanswers. Thepivotalstepinvolves strategicallyincreasingtheuncertaintywithinthequestiontoenhanceitsdifficulty(Wuetal.,2025a). Thispracticalapproachisgroundedinacompletetheoreticalframework,whereweformallymodelQA difficultyasaseriesofcontrollable"atomicoperations"(e.g.,mergingentitieswithsimilarattributes)on entityrelationships,allowingustosystematicallyincreasecomplexity. Tofurtherreduceinconsistencies betweentheorganizedinformationstructureandthereasoningstructureofQA,enablemorecontrollable difficultyandstructurescalingofreasoning,weproposedaformalmodelingoftheinformation-seeking problembasedonsettheory(Taoetal.,2025).Withthisformalization,wedevelopagentsthatexpandsthe probleminacontrolledmanner,andminimizesreasoningshortcutsandstructuralredundancy,leading tofurtherimprovedQAquality. Moreover,thisformalmodelingalsoallowsforefficientverificationof QAcorrectness,effectivelyaddressingthechallengeofvalidatingsyntheticinformation-seekingdatafor post-training. WealsodevelopanautomateddataenginetoscalethegenerationofPhD-levelresearchquestions(Qiao et al., 2025). Starting from a multi-disciplinary knowledge base, it creates seed QA pairs requiring multi-sourcereasoning. Theseseedsundergoiterativecomplexityupgrades,whereaquestion-crafting agent,equippedwiththecorrespondingtool,progressivelyexpandsscopeandabstraction. Eachiteration refinesandcompoundsprioroutputs,enablingasystematicandcontrollableescalationoftaskdifficulty. 3.4.2 SupervisedFine-tuningforColdStart Theinitialphaseofouragenticpost-trainingpipelineisasupervisedfine-tuning(SFT)stage,designed to equip the base model with a robust initial policy prior to reinforcement learning. Starting from oursynthesizedhigh-qualityQAdata,weobtaintrainingtrajectoriesthatcoverthecompletethought processandtoolresponsesgeneratedbyhigh-performingopen-sourcemodels,whicharethensubjected to a rigorous rejection sampling protocol. This comprehensive filtering process guarantees that only high-qualitytrajectoriesexhibitingdiverseproblem-solvingpatternsareretained. Mixed Training Paradigm. The cold stage training leverages data from two different formulations to enhance model robustness and generalization. For the React Mode, the training samples take the historicalstateH asinput,andoutputthecorrespondingthoughtτ andtoolcalla forthecurrentstep. t1 i i ForourContextManagementMode,thetrainingsamplestakeasinputthepreviousstep’strajectory summaryS t−1,toolcalla i−1,andtoolresponseo i−1,andoutputthecurrentstep’strajectorysummary, thoughtτ,andtoolcalla. TheContextManagementModedataparticularlystrengthenstheagent’s i i capabilitiesinstateanalysisandstrategicdecision-making,asitrequiresthemodeltosynthesizecomplex observationsintocoherentsummarieswhilemaintainingtaskfocusacrossextendedtrajectories. This synthesis-orientedtrainingenablesmoredeliberatereasoningpatternscomparedtopurelyReAct. We adoptatwo-stagetrainingstrategybasedoncontextlength. Inthefirststage,thecontextlengthissetto 40K,andthetrainingdataconsistofReActModesampleswithcontextlengthsshorterthan40K,along withallContextManagementModesamples(astheyareallwithin40k). Inthesecondstage,thecontext lengthisextendedto128K,andthetrainingdataincludeReActModesampleswithcontextlengths between40Kand128K,aswellasasmallportionof40Kdataforstability. 3.4.3 AgenticReinforcementLearning Toadvancethemodel’scapabilitiestowardmorerobustandreliableplanningandsearchinginacomplex webenvironment,weapplyanagenticRLframework,whichisillustratedinFigure5. Inthisframework, themodelgeneratesacompletetaskattempt(a"rollout")andreceivesarewardifitsfinalanswermatches thegroundtruth(RLVR)(Guoetal.,2025).ThroughoutthisagenticRLprocedure,themodelcontinuously interactswiththeenvironment(simulatedorreal-world),iterativelyrefiningitspolicywitheachiteration, and,inturn,usingthatimprovedpolicytocurateanew,higher-qualitysetoftrainingdata. 8 --- Page 9 --- Utilization Preservation Automatic Synthetic Data Async Call Collect Async Rollout Service Rollout Worker Trajectory RewardService Update Action Observation Simulated Environment Search Visit File Parser Real Environment Python Google Scholar Interpreter Figure5: Anoverviewofouragenticreinforcementlearningframework. Real-worldEnvironment. Ouragent’stoolkitisacomplexsystemthatintegratesseveralspecialized tools2: (1)Search,(2)Visit,(3)PythonInterpreter,(4)GoogleScholar,(5)FileParser. Theend-to-end reliability of this system is paramount. The inherent volatility of external APIs, encompassing high latency,outrightfailures,andinconsistentreturns,threatenstocorruptourtrainingtrajectories. Thisdata contaminationmakesitnearlyimpossibletodiagnoseperformanceissues,obscuringwhetherapoor outcomeiscausedbyaweaknessintheagent’spolicyorbytheinstabilityoftheenvironmentitself. To ensurereliabletooluseduringagenttrainingandevaluation,wedevelopedaunifiedsandbox. This interfaceisbuiltaroundacentralschedulingandmanagementlayerthatorchestrateseverytoolcall. For eachtool,weimplementrobustconcurrencycontrolsandfault-tolerancemechanisms,suchasproactive QPSrateconstraints,resultcaching,automatictimeout-and-retryprotocols,gracefulservicedegradation fornon-criticalfailures,andseamlessfailovertobackupdatasources(e.g.,abackupsearchAPI).This designabstractsthetoolinvocationintoadeterministicandstableinterfacefortheagentandthereby insulatesthetrainingloopfromreal-worldstochasticitywhilealsosignificantlyreducingoperational costs. Thisdesignabstractstoolinvocationintoadeterministicinterface, providingastableandfast experiencethatiscrucialforpreventingtoolerrorsfromcorruptingtheagent’slearningtrajectory. Simulated Environment. Directly utilizing real-world web environment APIs presents numerous practicalproblems3. Wefirstbuildanofflineenvironmentbasedonthe2024Wikipediadatabaseand developasuiteoflocalRAGtoolstosimulatethewebenvironment. Wethenreusethedatasynthesis pipelinetocreateahigh-quality,structurallycomplexQAspecificallyforthisofflineenvironment. This providesuswithalow-cost,high-efficiency,andfullycontrollableplatformthatenableshigh-frequency, rapidexperimentation,therebygreatlyacceleratingourdevelopmentanditerationprocess. On-PolicyAsynchronousRolloutFramework. Theiterativenatureofagenticrollouts,whichrequire numerousinteractionswiththeenvironment,createsasignificantbottleneckthatslowsdowntheentire RLtrainingprocess. Toovercomethis,weimplementacustom,step-levelasynchronousRLtraining loopbuiltontherLLMframework(Tanetal.,2025). Oursolutionutilizestwoseparateasynchronous onlineservers,withoneformodelinferenceandanotherfortoolinvocation. Acentralizedinteraction handlerthenprocessestheoutputsfromboth,formattingthefeedbackintoaunifiedmessagelist. This architectureallowsmultipleagentinstancestointeractwiththeenvironmentinparallel,eachcompleting itsrolloutindependently. 2ThedetailsforeachtoolareshowninAppendixD. 3Queriespersecond(QPS)impactsignificantlydegradeourdevelopmentefficiencyandcompromisethereliability duringourearly-stageablationstudies. 9 --- Page 10 --- RLTrainingAlgorithm. OurRLalgorithmisatailoredadaptationofGRPO(Shaoetal.,2024): J(θ) = E (q,y)∼D,{Hi} iG =1∼πθold(·|context) (cid:34) 1 ∑G | ∑Hi| min(cid:16) r (θ)Aˆ , clip(cid:16) r (θ),1−ε ,1+ε (cid:17) Aˆ (cid:17)(cid:35) , (4) ∑G |Hi| i,j i,j i,j low high i,j i=1 i=1j=1 where(q,y)isthequestion-answerpair,r (θ)istheimportancesamplingratio(remains1.0forstrictly i,j on-policytraining),and Aˆ isanestimatoroftheadvantageattokenj: i,j π (Hi,j | context) r (θ) = θ , Aˆ = R −mean({R }G ). (5) i,j π (Hi,j | context) i,j i i i=1 θold Weemployastricton-policyregimen,wheretrajectoriesareconsistentlysampledusingthemostup- to-datepolicy,ensuringthatthelearningsignalisalwaysrelevanttothemodel’scurrentcapabilities. Therewardisapure0or1signalofanswercorrectness. Wedonotincludeaformatreward(e.g.,0.1 for format correctness) because the preceding cold start stage ensures the model is already familiar with the required output format. Following DAPO (Yu et al., 2025), we apply the token-level policy gradientlossinthetrainingobjectiveandclip-higherstrategytoencouragemoreexploration. Tofurther reduce variance in the advantage estimation, we adopt a leave-one-out strategy (Chen et al., 2025). Furthermore,weobservedinpreliminaryexperimentsthatdirectlyoptimizingonanunfilteredsetof negativerolloutssignificantlydegradetrainingstabilityandcanleadtopolicycollapseafterextended training. To mitigate this, we selectively exclude certain negative samples from the loss calculation, for instance, those that do not yield a final answer because they exceed a length limit. The primary motivationforthesemodificationsisnotalgorithmicnoveltybutthepragmaticpursuitofamoreefficient andstabletrainingparadigm. AutomaticDataCuration. Weoptimizedatainrealtime,guidedbytrainingdynamicstogeneralize toout-of-distributionscenariosthroughself-exploration. Thisoptimizationisachievedthroughafully automateddatafilteringpipelinethatdynamicallyadjuststhetrainingsetbasedontheimprovedpolicy model. Specifically,ourprocessbeginswithalargedataset,D. WeusetheinitialSFTmodelasabaseline policy to sample multiple solution attempts, or rollouts, for each problem. We then create an initial trainingset,D′,byfilteringoutproblemswherethemodeleitheralwaysfailsoralwayssucceeds,as thesewilloffernolearningsignalforRLtraining. Thisleavesuswithafocusedsubsetofproblemsof moderatedifficulty. DuringRLtraining, wecontinuouslymonitortheproblemsin D′ bytheirlatest rolloutstoseeiftheyhavebecometooeasyfortheimprovedpolicymodel. Inparallel,aseparateprocess usesintermediatecheckpointsofthepolicymodeltosamplefromtheentireoriginaldataset,D. This backgroundprocessidentifiesandcollectsabackuppoolofnewproblemsthathavebecomemoderately difficult for the now-stronger model. When the training reaches a certain step count or the reward plateaus,werefreshtheactivetrainingsetD′ byremovingthemasteredproblemsandincorporating new,challengingonesfromthebackuppool. Theentiredatafilteringandrefreshmentpipelineruns independently,neverinterruptingthemainRLtrainingloop. Thisdesignallowsustoautomatically evolveboththepolicymodelanditstrainingdata,ensuringconsistentlyhightrainingefficiencyand stability. Throughourexperiments,wearriveatacriticalinsight: thesuccessofagenticRLdependsmoreon thequalityofthedataandthestabilityofthetrainingenvironmentthanonthespecificalgorithm beingused. Consequently,weconcentrateoureffortsondesigningastableenvironmentandcurating high-quality data, making only a few essential modifications to the algorithm itself, mainly for the purposeofstabilizingthetrainingprocess. 10 --- Page 11 --- 3.4.4 ModelMerging Weemploymodelmergingatthelaststageofthepipeline. Thisapproachisbuiltonthekeyinsightthat whendifferentmodelvariantsarederivedfromthesamepre-trainedmodel,theirparameterscanbe effectivelycombinedthroughaveragingorinterpolation(Wangetal.,2025). Specifically,ourprocess involvesselectingseveralmodelvariantsthatoriginatefromthesamebasemodelbutexhibitdifferent capabilitypreferences. Wethencreatethefinalmergedmodelbycomputingaweightedaverageoftheir parameters: θ = ∑ α ·θ(k) , s.t. ∑ α =1, α ≥0. (6) merged k k k k k whereθ(k) representstheparametersofthek-thmodelvariant,andα isitscorrespondingmergeweight. k Empirically,thisinterpolationstrategynotonlypreservesthecorestrengthsofeachcontributingmodel butalsoequipsthemergedmodelwithrobustgeneralizationabilities. Incomplexscenariosrequiringa synthesisofthesevariedcapabilities,themergedmodelperformscomparablytothebest-performing sourcemodelinitsrespectiveareaofstrength,allwithoutincurringadditionaloptimizationcosts. 4 Experiments 4.1 Experimental Setup Backbones. WeevaluateTongyiDeepResearchonsevenpublicinformation-seekingbenchmarksspan- ning long-term reasoning and long-horizon tool use. The model is compared against two families of systems: 1) LLM-based ReAct agents: GLM-4.5 (Zeng et al., 2025), Kimi-K2 (Team et al., 2025), DeepSeek-V3.1(DeepSeekTeam,2025),Claude-4-Sonnet(anthropic,2025),OpenAIo3/o4-mini(Ope- nAI,2025b))and2)end-to-enddeep-researchagents: OpenAIDeepResearch(OpenAI,2025a),Gemini DeepResearch(GeminiTeam,2025),KimiResearcher(Kimi,2025). Benchmarks. We follow each benchmark’s official evaluation protocol. The benchmarks cover: (1) Humanity’sLastExam(Phanetal.,2025);(2)BrowseComp(Weietal.,2025)andBrowseComp-ZH(Zhou etal.,2025);(3)GAIA(Mialonetal.,2023);(4)xBench-DeepSearch(XbenchTeam,2025);(5)WebWalk- erQA(Wuetal.,2025b);(6)FRAMES(Krishnaetal.,2025);and(7)xbench-DeepSearch-2510. Allscoresarecomputedwiththeofficialscriptsreleasedbyeachbenchmark. Thedetailsofevaluation arepresentedinAppendixB. Evaluation. Weadoptfixedinferenceparameterstoensurestabilityandreproducibilityacrossevalua- tions: temperature=0.85,repetitionpenalty=1.1,andtop-p=0.95. Amaximumof128toolinvocations isallowedpertask,andthecontextlengthisconstrainedto128Ktokens. Eachbenchmarkisevaluated  Forcom-  analysis. AllresultsareobtainedonSeptember16,2025,exceptforxbench-DeepSearch-2510,whichis evaluatedonOctober28,2025. Reproduce. Tongyi DeepResearch operates utilizing an action space that includes the Search, Visit, Python,Scholar,andFileParsertools. WereleaseofficialreproductionscriptsonGitHub4,alongwiththe completetoolimplementationsandpromptconfigurations. 4.2 Main Results Table1presentstheperformanceofTongyiDeepResearchcomparedwithabroadrangeofstate-of-the- artLLM-basedagentsandproprietarydeepresearchsystemsacrossmultiplebenchmarks,including Humanity’sLastExam,BrowseComp,BrowseComp-ZH,GAIA,xbenchDeepSearch,WebWalkerQA, andFRAMES.TongyiDeepResearchachievesthehighestscoresonnearlyallevaluatedbenchmarks, 4 11 --- Page 12 --- Table1: Performancecomparisononvariousbenchmarks. Benchmarks Humanity’s Browse Browse GAIA xbench WebWalker FRAMES LastExam Comp Comp-ZH DeepSearch QA LLM-basedReActAgent GLM4.5 21.2 26.4 37.5 66.0 70.0 65.6 78.9 KimiK2 18.1 14.1 28.8 57.7 50.0 63.0 72.0 DeepSeek-V3.1 29.8 30.0 49.2 63.1 71.0 61.2 83.7 Claude-4-Sonnet 20.3 12.2 29.1 68.3 65.0 61.7 80.7 OpenAIo3 24.9 49.7 58.1 – 67.0 71.7 84.0 OpenAIo4-mini 17.7 28.3 – 60.0 – – – DeepResearchAgent OpenAIDeepResearch 26.6 51.5 42.9 67.4 – – – GeminiDeepResearch 26.9 – – – – – – KimiResearcher 26.9 – – – 69.0 – 78.8 TongyiDeepResearch(30B-A3B) 32.9 43.4 46.7 70.9 75.0 72.2 90.6 demonstratingstronggeneralizationacrossbothEnglishandChinesetasks.Itconsistentlysurpassesboth openandclosedcommercialsystems,includingOpenAIo3,DeepSeek-V3.1,andGeminiDeepResearch. Onthenewlyreleasedxbench-DeepSearch-2510,TongyiDeepResearchranksjustbelowChatGPT-5-Pro, demonstratingcompetitiveperformanceattheforefrontofthefield. Notably,thesegainsareachieved withonly3.3billionactivatedparameterspertoken,underscoringthemodel’sefficiencyandscalability. Inaggregate,TongyiDeepResearchsetsanewstateoftheartamongopen-sourcedeepresearchagents, narrowingandinsomecasesevensurpassingtheperformanceoffrontierproprietarysystemswhile maintainingsuperiorinterpretabilityandcomputationalefficiency. 4.3 Heavy Mode 50 40 30 20 10 D(eHT eo p en aRg vey ysi eMar oc dh e) DeT e (o p 3n R 0g e By s -ie Aa 3rBc )h DeepSeek-V3.1 DeG ee pm Ri en si earch ReK siem ai rcher DeO ep pe Rn eA sI earch GLM-4.5 )%(  Humanity's Last Exam 70 60 38.3 50 32.9 29.8 40 26.9 26.9 26.6 30 21.2 20 10 D(eHT eo p en aRg vey ysi eMar oc dh e) DeT e (o p 3n R 0g e By s -ie Aa 3rBc )h DeO ep pe Rn eA sI earch DeepSeek-V3.1 GLM-4.5 Kimi-K2 Cla Su od ne n-e4 t )%(  BrowseComp 70 58.3 60 51.5 50 43.4 40 30.0 26.4 30 14.1 12.2 20 10 D(eHT eo p en aRg vey ysi eMar oc dh e) DeT e (o p 3n R 0g e By s -ie Aa 3rBc )h DeepSeek-V3.1 DeO ep pe Rn eA sI earch GLM-4.5 Cla Su od ne n-e4 t Kimi-K2 )%(  BrowseComp-ZH 58.1 46.7 49.2 42.9 37.5 29.1 28.8 Figure 6: Performance comparison between Tongyi DeepResearch Heavy Mode and state-of-the-art models. Tofurtherunlockthepotentialofdeepresearchagents,weintroducetheHeavyMode,whichleverages test-timescalingthroughaResearch-Synthesisframeworkbuiltuponthecontextmanagementparadigm. GiventhatDeepResearchinvolvesmulti-roundtoolcallsandintensivereasoning,directlyaggregating contexts from multiple trajectories is computationally prohibitive. Our Heavy Mode addresses this challengethroughstrategicparallelizationandsynthesis. ParallelResearchPhase. Wedeploynparallelagents,eachfollowingthecontextmanagementparadigm butexploringdiversesolutionpathsthroughdifferenttoolusageandreasoningstrategies. Eachagentu independentlyprocessesthequestionqandproducesafinalreportandanswer: (Su,answer ) =Agent (q), u ∈ [1,n] (7) T u u whereSu representsthefinalreportsummaryfromagentuafterTiterations,encapsulatingthecomplete T reasoningtrajectoryincompressedform. 12 --- Page 13 --- IntegrativeSynthesisPhase. Asynthesismodelconsolidatesallparallelfindingstoproducethefinal answer: answer =Synthesis({(Su,answer )}n ), (8) final T u u=1 ThekeyadvantageofthisapproachliesinthecompressednatureofcontextmanagementreportsSu. T Unliketraditionalmethodsthatwouldrequireaggregatingfulltrajectories(potentiallyexceedingcontext limits with just 2-3 agents), our approach enables the synthesis model to assess n diverse solution strategieswithinamanageablecontextwindow. EachreportSu preservestheessentialreasoninglogic T andfindingswhilediscardingredundantintermediatesteps,enablingeffectivetest-timescaling. As shown in Figure 6, our Heavy Mode achieves state-of-the-art performance on Humanity’s Last Exam (38.3%) and BrowseComp-ZH (58.1%), while remaining highly competitive on BrowseComp (58.3%). Thesesubstantialimprovementsvalidatetheeffectivenessofourheavymodebasedoncontext managementinleveragingtest-timecomputethroughparallelexplorationandintelligentaggregation. 4.4 Detailed Analysis   Giventhedynamicand  runs) and  in Figure 7. Despite the unstable evaluation environment, our final  results are consistent with the  (best result over 3 runs) results, demonstrating the robustness of our deepresearchapproach.  In particular,itachieves59.64onBrowseComp,63.67onBrowseComp-ZH,and45.9onHumanity’sLast Exam. 100 80 60 40 20 0 HLE BrowseComp Brows -Ze HComp WebWalkerQA Dexb epen Sc eh a- rch GAIA erocS   86.0 85.5  82.8 76.0 72.272.9 73.0 70.972.7 63.7 59.6 48.4 45.9 43.444.2 46.7 32.933.4 Figure7:  Training Rewards and Entropy. As shown in Figure 8, the agent’s performance exhibits a clear and significantupwardtrendwithtraining,confirmingeffectivepolicylearning. Thesustainednatureof thisimprovementunderscoresthesuccessofourdynamicdatacuration,whichpreventslearningfrom stagnatingbyconsistentlyprovidingchallengingmaterial. Concurrently, thepolicyentropyexhibits exceptionalstability,convergingtoaconsistentvalueafterabriefinitialincreaseandtherebyavoiding bothcollapseandexplosion.Thisoutcomeservesasstrongevidenceforourmethodologicalcontributions inenvironmentdesignandalgorithmmodification,whichtogethercreatethenecessaryconditionsfora remarkablystableandeffectiveRLtrainingparadigm. ContextLengthofRL.InFigure10,weanalyzetheimpactofthemodel’scontextlengthontheagentic RLtrainingprocess, comparingmodelswith32k, 48k, and64kcontextlimits. Itisimportanttonote 13 --- Page 14 --- 0.65 0.60 0.55 0.50 0.45 100 200 300 400 500 Step draweR 1.0 0.8 0.6 0.4 Original EMA Smoothed 0.2 100 200 300 400 500 Step ssoL yportnE Original EMA Smoothed Figure8: RewardandentropylossofagenticRLtraining. thatthedynamicdatacurationforallthreeexperimentalvariantswasperformedusingthesamemodel witha64kcontext. Focusingfirstontherewarddynamicsintheleftpanel,weobservethatallthree modelsdemonstrateeffectiveandstablepolicylearning,evidencedbyamonotonicallyincreasingreward. Thisconfirmstherobustnessofourtrainingframework. However,theirperformanceceilingsdiverge significantly,whichisanexpectedconsequenceofourdatacurationmethod. Becausethecurriculumis populatedwithproblemsdeemedmoderatelydifficultbythehighlycapable64kcontextmodel,manyof theseproblemsinherentlyrequirelongandcomplexreasoningtosolve. Consequently,aclearhierarchy emerges: the64kmodel,perfectlymatchedtoitsowndata,achievesthehighestreward. The48kand32k models,beingincreasinglyconstrained,areunabletosolvethemostcomplexproblemsinthecurriculum, thuscappingtheirmaximumpotentialreward. Thetrainingdynamicsintherightpanelrevealamoreinterestingstory. Themodelwitha64kcontext exhibitsasteadyincreaseinaverageresponselength,learningtoleverageitsexpansivecontexttobuild moreelaboratesolutions. Incontrast,themodelwitha48kcontextmaintainsaconsistentequilibrium, improvingitspolicywithinastablecomplexitybudget. Mostsurprisingly,themodelwitha32kcontext displaysacleardownwardtrendinresponselength. Thisobservationprovidesakeyinsight: foramodel withalimitedcontext,RLtrainingonacurriculumdesignedforamorecapablemodelcanforceitto discovermoreefficientsolutions. Thiseffectarisesbecauseourdynamicdatacurriculumiscontinuously updatedusingthe64kcontextmodel,aprocessthatpopulatesthetrainingsetwithproblemswhose optimalsolutionscanbelongerthan32ktokens. Forthemodelwitha32kcontext,attemptingthese problemsislikelytoyieldazero-rewardsignal. Thiscreatesapowerfulimplicitincentivetodiscover moreconcise,potentactionsequencesthatfitwithinitslimit,thusbecomingmoreefficientovertime. 0.65 0.60 0.55 0.50 0.45 0.40 0.35 0 50 100 150 200 250 300 350 Step draweR 35000 30000 25000 20000 32k 48k 64k 15000 0 50 100 150 200 250 300 350 Step htgneL esnopseR .gvA 32k 48k 64k Figure9: ComparisonofdifferentcontextlengthlimitsforRLtraining. InteractionTest-timeScaling. Unlikeconventionalmodels,theDeepResearchagentprimarilyrelieson interactionswiththeenvironmenttoacquireinformationandaccomplishtasks. Therefore,thenumberof 14 --- Page 15 --- 50.0 37.5 25.0 12.5 0.0 8K 16K 32K 64K 128K Context Length )%( ycaruccA 0.85 Accuracy 0.80 0.75 0.70 0.65 0.60 0.55 0.50 0.45 50 100 150 200 250 300 Step (a)InteractionturnsscalingforBrowseComp. draweR Simulated Environment EMA Smoothed (b)Rewardinthesimulatedenvironment. Figure10: Detailedanalysisoninteractionscalingandsimulatedenvironments. interactionturnswiththeenvironmentiscrucial. Whilereasoningmodelscanbescaledbyincreasingthe numberofoutputtokens,ourapproachscalesalongadifferentdimension,thenumberofenvironment interactions. Naturally, asthenumberofinteractionsincreases, theagentobtainsmoreobservations fromenvironment,resultinginalongercontext. Figure10aillustratesourscalingcurve: asthecontext lengthandnumberofinteractionsgrow,themodel’sperformanceontheBrowseCompdatasetimproves consistently. Super-humanLevelSyntheticData. Tovalidatetheeffectivenessofoursyntheticdata,weconducted astatisticalanalysisoftheSFTdataset. Over20%ofthesamplesexceed32ktokensandinvolvemore than10toolinvocations. Thisdemonstratesthehighcomplexityandrichnessofoursyntheticdata. Such high-quality,cold-startdataprovidesthemodelwithastrongfoundationfordeepreasoningandresearch capabilities,servingasanexcellentinitializationfortheRLphase. Duringreinforcementlearning,we leverageautomateddatacurationtomakemoreeffectiveuseofthesyntheticdata. FromSimulationtoReality. Torapidlyvalidateouralgorithm,webuiltasimulatedWikienvironment thatmirrorsreal-worldconditions. WetestouradaptedGRPOalgorithminthisenvironment,andthe resultingrewardcurve,showninFigure10b,closelymatchestheoneobservedintherealenvironment,as showninFigure8. ThisWikisimulationenvironmentprovidesfunctionalityanalogoustoa"windtunnel laboratory",enablingfastalgorithmiterationandsignificantlyimprovedourdevelopmentefficiency. 100 80 60 40 20 AIME25 HMMT25 SimpleQA erocS PerformanceonGeneralBenchmark. Weevaluate Qwen3-30B-A3B-Thinking-2507 Qwen3-235B-A22B-Thinking-2507 Tongyi DeepResearch three general benchmarks, AIME25, HMMT25 and 100.0 100.0 98.6 92.3 SimpleQA (OpenAI, 2025c). The results are shown 85.0 83.9 inFigure11. Experimentalresultsdemonstratethat 71.4 TongyiDeepResearchachievessubstantialimprove- ments over the base model, which relies solely on 47.1 reasoning without any tool use. On one hand, the systemcanretrieveexternalinformationviasearch, which proves particularly effective for knowledge- 19.2 intensivebenchmarks,andontheother,PythonInter- preterenablesittoenhanceperformanceonmathe- Figure11: Performanceongeneralbenchmarks. maticalreasoningtasksthroughnativecomputational support. Lookingahead,modeltrainingincreasingly convergeswithagenttraining,solvingparadigmsevolvetowardagenticarchitecturesthatintegratetool invocationandenvironmentinteraction,reflectingamorehuman-likeproblem-solvingprocess. 5 Discussion 15 --- Page 16 --- 5.1 Limitations We acknowledge several limitations in our current work: First, the current 128K context length re- mainsinsufficientforhandlingthemostcomplexlong-horizontasks,motivatingfurtherexploration ofextendedcontextwindowsormoreadvancedcontextmanagementmechanisms(Qiaoetal.,2025; Wuetal.,2025c). Second,wehavenotyetreleasedalarger-scalemodel. Althoughthesmaller-sized modelalreadydemonstratesstrongperformance,alargermodeliscurrentlyinprogress. Third,weare continuouslyimprovingreportgenerationfidelityandoptimizingforuserpreferencestoensuremore faithful,useful,andpreference-alignedoutputs(Lietal.,2025e). Fourth,weaimtoimprovetheefficiency ofourreinforcementlearningframeworkbyexploringtechniquessuchaspartialrollouts,whichwill requireaddressingoff-policytrainingchallenges,includingdistributionalshift. Finally,ourcurrentDeep Researchtrainingfocusesonspecificpromptinstructionsandpredefinedtoolsets. Weplantoenhance itsrobustnessandextendtheframeworkfromDeepResearchtobroaderagentictoolusescenarios. 5.2 Model Scale Webelievethattrainingagenticcapabilitiesonrelativelysmallmodelsishighlyvaluable(Belcaketal., 2025). Smaller models are inherently more efficient to deploy on edge devices, broaden accessibility acrossdiversereal-worldscenarios,anddeliverfaster,moreresponsiveinteractions. Thisdirectionaligns withthebroadergoalofmakingautonomousresearchagentsbothpowerfulandpracticallydeployable. 5.3 What’s Next Wehavealong-standingcommitmenttoadvancingresearchanddevelopmentindeepresearchagents. The Tongyi DeepResearch represents a significant step toward AI systems capable of autonomously transforminginformationintoinsight. Weadvocateforopen-sourcemodelswithemergentagency,which areessentialfordemocratizingagenticintelligenceanddeepeningourfundamentalunderstandingof howagencycanemergeandscaleinopensystems.Lookingahead,weaimtoevolvefromdomain-specific agentstogeneral-purposeagents,whicharecapableofreasoning,planning,andactingautonomously acrossdiversedomainswithminimalhumansupervision. Toachievethis,wearedevelopingthenext- generation agent foundation model, a unified model designed to endow AI systems with scalable reasoning,memory,andautonomy,enablingthemtooperateastrulygeneralagents. Webelieveitwill empowerindividualsandorganizationstoreachnewheightsofproductivityandinnovation. 6 Conclusion We introduced Tongyi DeepResearch, an open-source deep research agent that unifies agentic mid- training and post-training into a scalable, end-to-end paradigm. Through automated data synthesis andstage-specificenvironments,themodellearnstoplan,search,reason,andsynthesizeinformation autonomously. Despiteitsefficiency,activatingonly3.3Bparameters,TongyiDeepResearchachieves state-of-the-artresultsonmultipledeepresearchbenchmarks,surpassingstrongproprietarysystems. Thisworkestablishesafoundationforopen,reproducibleresearchintoautonomousAIagentsandmarks asteptowardmoregeneral,self-improvingintelligence. 16 --- Page 17 --- Contributions Thenamesarelistedinalphabeticalorderbyfirstname. ProjectLeader YongJiang CoreContributors BaixuanLi,BoZhang,DingchuZhang,FeiHuang,GuangyuLi,GuoxinChen,HuifengYin,JialongWu, JingrenZhou,KuanLi,LiangcaiSu,LituOu,LiwenZhang,PengjunXie,RuiYe,WenbiaoYin,Xinmiao Yu,XinyuWang,XixiWu,XuanzhongChen,YidaZhao,ZhenZhang,ZhengweiTao,ZhongwangZhang, ZileQiao Contributors ChenxiWang,DongleiYu,GangFu,HaiyangShen,JiayinYang,JunLin,JunkaiZhang,KuiZeng,Li Yang,HailongYin,MaojiaSong,MingYan,PengXia,QianXiao,RuiMin,RuixueDing,RunnanFang, ShaoweiChen,ShenHuang,ShihangWang,ShihaoCai,WeizhouShen,XiaobinWang,XinGuan,Xinyu Geng,YingchengShi,YuningWu,ZhuoChen,ZijianLi 17 --- Page 18 --- References anthropic. Introducingclaude4,2025. URL PeterBelcak,GregHeinrich,ShizheDiao,YongganFu,XinDong,SauravMuralidharan,YingyanCe- line Lin, and Pavlo Molchanov. Small language models are the future of agentic ai. arXiv preprint arXiv:2506.02153,2025. JingyiChai,ShuoTang,RuiYe,YuwenDu,XinyuZhu,MengchengZhou,YanfengWang,YuzhiZhang, LinfengZhang, SihengChen, etal. Scimaster: Towardsgeneral-purposescientificaiagents, parti. x-masterasfoundation: Canweleadonhumanity’slastexam? arXivpreprintarXiv:2507.05241,2025. KevinChen,MarcoCusumano-Towner,BrodyHuval,AlekseiPetrenko,JacksonHamburger,Vladlen Koltun,andPhilippKrähenbühl. Reinforcementlearningforlong-horizoninteractivellmagents. arXiv preprintarXiv:2502.01600,2025. ClaudeTeam. Clauderesearch,2025. URL DeepSeek Team. Introducing deepseek-v3.1: our first step toward the agent era!, 2025. URL https: //api-docs.deepseek.com/news/news250821. RunnanFang,ShihaoCai,BaixuanLi,JialongWu,GuangyuLi,WenbiaoYin,XinyuWang,XiaobinWang, LiangcaiSu,ZhenZhang,etal. Towardsgeneralagenticintelligenceviaenvironmentscaling. arXiv preprintarXiv:2509.13311,2025. GeminiTeam. Geminideepresearch,2025. URL GrokTeam. Grok-3deepersearch,2025. URL DayaGuo,DejianYang,HaoweiZhang,JunxiaoSong,RuoyuZhang,RunxinXu,QihaoZhu,ShirongMa, PeiyiWang,XiaoBi,etal. DeepSeek-R1: IncentivizingreasoningcapabilityinLLMsviareinforcement learning. arXivpreprintarXiv:2501.12948,2025. Jina.ai. Jina,2025. URL Kimi. Kimi-researcher: End-to-endrltrainingforemergingagentic,2025. URL ithub.io/Kimi-Researcher/. SatyapriyaKrishna, KalpeshKrishna, AnhadMohananey, StevenSchwarcz, AdamStambler, Shyam Upadhyay,andManaalFaruqui. Fact,fetch,andreason: Aunifiedevaluationofretrieval-augmented generation. InProceedingsofthe2025ConferenceoftheNationsoftheAmericasChapteroftheAssociationfor ComputationalLinguistics: HumanLanguageTechnologies(Volume1: LongPapers),pp.4745–4759,2025. KuanLi, LiwenZhang, YongJiang, PengjunXie, FeiHuang, ShuaiWang, andMinhaoCheng. Lara: Benchmarking retrieval-augmented generation and long-context llms–no silver bullet for lc or rag routing. arXivpreprintarXiv:2502.09977,2025a. KuanLi,ZhongwangZhang,HuifengYin,RuiYe,YidaZhao,LiwenZhang,LituOu,DingchuZhang, XixiWu,JialongWu,etal. Websailor-v2: Bridgingthechasmtoproprietaryagentsviasyntheticdata andscalablereinforcementlearning. arXivpreprintarXiv:2509.13305,2025b. KuanLi,ZhongwangZhang,HuifengYin,LiwenZhang,LituOu,JialongWu,WenbiaoYin,BaixuanLi, ZhengweiTao,XinyuWang,etal. Websailor: Navigatingsuper-humanreasoningforwebagent. arXiv preprintarXiv:2507.02592,2025c. Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability. CoRR,abs/2504.21776,2025d. doi: 10.48550/ARXIV.2504.21776. URL rXiv.2504.21776. 18 --- Page 19 --- Zijian Li, Xin Guan, Bo Zhang, Shen Huang, Houquan Zhou, Shaopeng Lai, Ming Yan, Yong Jiang, PengjunXie,FeiHuang,etal. Webweaver: Structuringweb-scaleevidencewithdynamicoutlinesfor open-endeddeepresearch. arXivpreprintarXiv:2509.13312,2025e. Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmarkforgeneralaiassistants. InTheTwelfthInternationalConferenceonLearningRepresentations, 2023. OpenAI. Deepresearchsystemcard,2025a. URL ard.pdf. OpenAI. Introducingopenaio3ando4-mini,2025b. URL 3-and-o4-mini/. OpenAI. Introducingsimpleqa,2025c. URL LongPhan,AliceGatti,ZiwenHan,NathanielLi,JosephinaHu,HughZhang,ChenBoCalvinZhang, MohamedShaaban,JohnLing,SeanShi,etal. Humanity’slastexam. arXivpreprintarXiv:2501.14249, 2025. ZileQiao,GuoxinChen,XuanzhongChen,DongleiYu,WenbiaoYin,XinyuWang,ZhenZhang,Baixuan Li, Huifeng Yin, Kuan Li, et al. Webresearcher: Unleashing unbounded reasoning capability in long-horizonagents. arXivpreprintarXiv:2509.13309,2025. ZhihongShao,PeiyiWang,QihaoZhu,RunxinXu,JunxiaoSong,XiaoBi,HaoweiZhang,Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open languagemodels. arXivpreprintarXiv:2402.03300,2024. DavidSilverandRichardSSutton. Welcometotheeraofexperience. GoogleAI,1,2025. LiangcaiSu,ZhenZhang,GuangyuLi,ZhuoChen,ChenxiWang,MaojiaSong,XinyuWang,KuanLi, JialongWu,XuanzhongChen,ZileQiao,ZhongwangZhang,HuifengYin,ShihaoCai,RunnanFang, ZhengweiTao,WenbiaoYin,etal. Scalingagentsviacontinualpre-training,2025. RichardSutton. Thebitterlesson. IncompleteIdeas(blog),13(1):38,2019. SijunTan,MichaelLuo,ColinCai,TarunVenkat,KyleMontgomery,AaronHao,TianhaoWu,Arnav Balyan,MananRoongta,ChenguangWang,LiErranLi,RalucaAdaPopa,andIonStoica. rllm: A frameworkforpost-traininglanguageagents.  -Framework-for-Post-Training-Language-Agents-21b81902c146819db63cd98a54ba5f31, 2025. NotionBlog. ZhengweiTao,JialongWu,WenbiaoYin,JunkaiZhang,BaixuanLi,HaiyangShen,KuanLi,LiwenZhang, XinyuWang,YongJiang,etal. Webshaper: Agenticallydatasynthesizingviainformation-seeking formalization. arXivpreprintarXiv:2507.15061,2025. KimiTeam,YifanBai,YipingBao,GuanduoChen,JiahaoChen,NingxinChen,RuijueChen,YanruChen, YuankunChen,YutianChen,etal. Kimik2: Openagenticintelligence. arXivpreprintarXiv:2507.20534, 2025. HaomingWang,HaoyangZou,HuatongSong,JiazhanFeng,JunjieFang,JuntingLu,LongxiangLiu, QinyuLuo,ShihaoLiang,ShijueHuang,etal. Ui-tars-2technicalreport: Advancingguiagentwith multi-turnreinforcementlearning. arXivpreprintarXiv:2509.02544,2025. JasonWei,ZhiqingSun,SpencerPapay,ScottMcKinney,JeffreyHan,IsaFulford,HyungWonChung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: A simple yet challenging benchmarkforbrowsingagents. arXivpreprintarXiv:2504.12516,2025. 19 --- Page 20 --- JialongWu,BaixuanLi,RunnanFang,WenbiaoYin,LiwenZhang,ZhengweiTao,DingchuZhang,Zekun Xi,YongJiang,PengjunXie,etal. Webdancer: Towardsautonomousinformationseekingagency. arXiv preprintarXiv:2505.22648,2025a. JialongWu,WenbiaoYin,YongJiang,ZhenglinWang,ZekunXi,RunnanFang,LinhaiZhang,Yulan He,DeyuZhou,PengjunXie,etal. Webwalker: Benchmarkingllmsinwebtraversal. arXivpreprint arXiv:2501.07572,2025b. Xixi Wu, Kuan Li, Yida Zhao, Liwen Zhang, Litu Ou, Huifeng Yin, Zhongwang Zhang, Yong Jiang, PengjunXie,FeiHuang,etal. Resum: Unlockinglong-horizonsearchintelligenceviacontextsumma- rization. arXivpreprintarXiv:2509.13313,2025c. XbenchTeam. Xbench-deepsearch,2025. URL AnYang,AnfengLi,BaosongYang,BeichenZhang,BinyuanHui,BoZheng,BowenYu,ChangGao, ChengenHuang,ChenxuLv,etal. Qwen3technicalreport. arXivpreprintarXiv:2505.09388,2025. ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikNarasimhan,andYuanCao. Re- act: Synergizing reasoning and acting in language models. In International Conference on Learning Representations(ICLR),2023. QiyingYu,ZhengZhang,RuofeiZhu,YufengYuan,XiaochenZuo,YuYue,TiantianFan,GaohongLiu, LingjunLiu,XinLiu,etal. Dapo: Anopen-sourcellmreinforcementlearningsystematscale. arXiv preprintarXiv:2503.14476,2025. AohanZeng,XinLv,QinkaiZheng,ZhenyuHou,BinChen,ChengxingXie,CunxiangWang,DaYin, HaoZeng,JiajieZhang,etal. Glm-4.5: Agentic,reasoning,andcoding(arc)foundationmodels. arXiv preprintarXiv:2508.06471,2025. PeilinZhou,BruceLeon,XiangYing,CanZhang,YifanShao,QichenYe,DadingChong,ZhilingJin, ChenxuanXie,MengCao,etal.Browsecomp-zh:Benchmarkingwebbrowsingabilityoflargelanguage modelsinchinese. arXivpreprintarXiv:2504.19314,2025. 20 --- Page 21 --- A Rollout Details SystemPrompt You are a deep research assistant. Your core function is to conduct thorough, multi-source investigationsintoanytopic. Youmusthandlebothbroad,open-domaininquiriesandqueries within specialized academic fields. For every request, synthesize information from credible, diversesourcestodeliveracomprehensive,accurate,andobjectiveresponse. Whenyouhave gatheredsufficientinformationandarereadytoprovidethedefinitiveresponse,youmustenclose theentirefinalanswerwithin<answer></answer>tags. #Tools Youmaycalloneormorefunctionstoassistwiththeuserquery. Youareprovidedwithfunctionsignatureswithin<tools></tools>XMLtags: <tools> {"type": "function","function": {"name": "search","description": "PerformGooglewebsearches thenreturnsastringofthetopsearchresults. Acceptsmultiplequeries.","parameters": {"type": "object", "properties": {"query": {"type": "array", "items": {"type": "string", "description": "The searchquery."},"minItems": 1,"description": "Thelistofsearchqueries."}},"required": ["query"]}}} {"type": "function","function": {"name": "visit","description": "Visitwebpage(s)andreturnthe summaryofthecontent.", "parameters": {"type": "object", "properties": {"url": {"type": "array", "items": {"type": "string"},"description": "TheURL(s)ofthewebpage(s)tovisit. Canbeasingle URLoranarrayofURLs."},"goal": {"type": "string","description": "Thespecificinformationgoal forvisitingwebpage(s)."}},"required": ["url","goal"]}}} {"type": "function", "function": {"name": "PythonInterpreter", "description": "Executes Python codeinasandboxedenvironment. Tousethistool,youmustfollowthisformat: 1. The’arguments’JSONobjectmustbeempty: {}. 2. ThePythoncodetobeexecutedmustbeplacedimmediatelyaftertheJSONblock,enclosed within<code>and</code>tags. IMPORTANT: Any output you want to see MUST be printed to standard output using theprint()function. Exampleofacorrectcall: <tool_call>{"name": "PythonInterpreter","arguments": {}} <code>importnumpyasnp#Yourcodehereprint(f"Theresultis: np.mean([1,2,3])")</code> </tool_call>","parameters": {"type": "object","properties": {},"required": []}}} {"type": "function","function": {"name": "google_scholar","description": "LeverageGoogleScholar to retrieve relevant information from academic publications. Accepts multiple queries. This tool will also return results from google search", "parameters": {"type": "object", "properties": {"query": {"type": "array","items": {"type": "string","description": "Thesearchquery."},"minItems": 1,"description": "ThelistofsearchqueriesforGoogleScholar."}},"required": ["query"]}}}{"type": "function", "function": {"name": "parse_file", "description": "This is a tool that can be used to parsemultipleuseruploadedlocalfilessuchasPDF,DOCX,PPTX,TXT,CSV,XLSX,DOC,ZIP, MP4,MP3.","parameters": {"type": "object","properties": {"files": {"type": "array","items": {"type": "string"},"description": "Thefilenameoftheuseruploadedlocalfilestobeparsed."}},"required": ["files"]}}} </tools> For each function call, return a json object with function name and arguments within <tool_call></tool_call>XMLtags: <tool_call>{"name": <function-name>,"arguments": <args- json-object>}</tool_call> Currentdate: 21 --- Page 22 --- TheaboveconstitutesthesystempromptofourReActrollout. B Evaluation Details ForGAIAandWebWalkerQA,followingtheevaluationprotocolofLietal.(2025d),weadoptQwen2. 5-72B-Instruct as the judging model. The evaluation prompt is kept identical to that used in their worktoensureconsistencyandcomparability. Forxbench-DeepSearchandxbench-DeepSearch-2510,we adoptGemini-2.0-Flash-001asthejudgemodel. ForBrowseCompandBrowseComp-ZH,weemploy GPT-4o-2024-08-06 as the judge model. For Humanity’s Last Exam, we evaluate the 2,154 text-only questions following Chai et al. (2025). The evaluation prompt follows the official protocol, with the o3-miniservingastheevaluator. Theevaluationpromptforthesebenchmarksiskeptconsistentwith thatdescribedintheoriginalpapertoensurealignmentandreproducibility. Theevaluationprompts usedforeachbenchmarkisprovidedindetailonourGitHubrepository5. Forgeneralbenchmarks,weadoptdifferentevaluationstrategiesbasedontasktype. Formathematical problems,sinceoursystemoutputsadetailedreportanddatasetssuchasAIME25andHMMT25are relativelysmallinscale,weemploymanualevaluationtoensureaccuracyandfairness. Forknowledge- based problems, we utilize the official evaluation script of SimpleQA to maintain consistency with establishedbenchmarks. C Post-training Synthetic Data Case Question: A military officer, who also served as governor in a western North American territory, commanded a mounted infantry unit during a period of significant mineral discovery in the region. His official report on the discovery prompted the minting of a special commemorative coin in a certain year in the mid-19th century. During that same year, the unit he commanded was involved in a military conflict against a neighboring country. Just over a decade later, this unit was officially redesignated and would be assigned to a new division in the early 1920s. In the 1930s, this redesignated regiment was involved in an organizational swap. Which other regiment was it exchanged for? Answer: 12th Cavalry Regiment Question: An 18th-century travelogue, later adapted for a radio series, describes a port town in southeastern England as notable for its rampant illicit trade. This town was also the home of a 16th-century gentleman whose murder led to his wife’s execution. Centuries later, another resident of the same town was granted letters patent providing special commercial privileges in a particular year of the early 19th century. During that same year, a collector, whose large collection of manuscript poems was later auctioned, secured a patent for a method of grinding inks. In that year, a patent of nobility was issued to a German family; what is the German term for the princely status it conferred? Answer: Fürstenstand Question: In trisilylamine (N(SiH ) ), the Si-N bond length is 1.736 Å. Substituting one silyl 3 3 group with methyl to form (CH )N(SiH ) elongates the Si-N bond to 1.752 Å. Calculate the 3 3 2 percentage increase in bond length due to diminished hyperconjugation, and identify which specific orbital interaction weakens most significantly. Use covalent radii: Si=1.11 Å, N=0.70 Å, C=0.77 Å. Answer: n → σ∗ Si−C Thefirsttwocasesabovearesyntheticallygeneratedhigh-quality,high-uncertainty,superhumanques- tion–answerpairs,examplesofacaliberthatisexceptionallydifficulttoproduceviahumanannotation. ThethirdcaserepresentsaPhD-levelresearchquestion,demandingdeepdomainexpertise,multi-step reasoning. 5 22 --- Page 23 --- D Environment Details WeutilizefivetoolsforTongyiDeepResearch,namelySearch,Visit,PythonInterpreter,GoogleScholar, andFileParser6: • SearchleveragestheGooglesearchengineforinformationretrieval. Thetoolacceptsalistofoneor moresearchqueriestobeexecutedconcurrently. Foreachquery,itreturnsthetop-10rankedresults, witheachresultcomprisingatitle,adescriptivesnippet,anditscorrespondingURL. • Visitisdesignedfortargetedinformationextractionfromwebpages. Thetooltakesasinputaset ofwebpages, whereeachpageispairedwithadedicatedinformation-seekinggoal. Theprocess beginsbyemployingJina(Jina.ai,2025)toparsethefullcontentofagivenwebpage. Subsequently,a summarymodelprocessesthiscontenttoextractonlytheinformationpertinenttothatpage’sspecific goal. • PythonInterpreterisusedtoexecutePythoncodewithinasandboxedenvironment. Theinputisa stringofPythoncode,whichmustbeenclosedwithin<code>tagsforproperexecution. Thetoolruns theprovidedcodeandcapturesitsstandardoutput;therefore,anyresultsorvaluesintendedtobe seenmustbeexplicitlypassedtotheprint()function. Thiscapabilityenablesdynamiccomputation, datamanipulation,andtheuseofvariousPythonlibrariesinasecureandisolatedmanner. • GoogleScholarisusedtoretrieveinformationfromacademicpublications. Theinputconsistsofa listofoneormoresearchqueries,allowingformultiple,distinctsearcheswithinasingletoolcall. The toolleveragestheGoogleScholarsearchenginetoexecuteeachqueryandgatherrelevantscholarly literature,suchasarticles,papers,andcitations. • FileParseranswersuserqueriesbyanalyzingamixofdocuments,webpages,andmultimediafiles (e.g.,PDF,DOCX,MP4)fromlocalorURLsources. Itworksintwosteps: first,itconvertsallinput intoplaintext,transcribingaudio/videocontentwhennecessary. Second,asummarymodelreads thisunifiedtexttogenerateadirectanswertotheuser’squestion 6SinceoursystemreliesonseveralinternalAPIsandfallbackstrategies(asdescribedinSection3.4.3),weprovide alternativeopenimplementationsinouropen-sourceGitHubrepositorytofacilitatepublicuse.Wehaveverified throughextensivetestingthatthesesubstitutionscanfaithfullyreproduceourresults. 23