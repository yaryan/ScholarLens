{
  "abstract": "Abstract Object binding, the brain\u2019s ability to bind the many features that collectively representanobjectintoacoherentwhole,iscentraltohumancognition. Itgroups low-levelperceptualfeaturesintohigh-levelobjectrepresentations,storesthose objectsefficientlyandcompositionallyinmemory,andsupportshumanreasoning aboutindividualobjectinstances. Whilepriorworkoftenimposesobject-centric attention(e.g.,SlotAttention)explicitlytoprobethesebenefits,itremainsunclear whetherthisabilitynaturallyemergesinpre-trainedVisionTransformers(ViTs). Intuitively,theycould:recognizingwhichpatchesbelongtothesameobjectshould beusefulfordownstreampredictionandthusguideattention. Motivatedbythe quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term IsSameObject. We decodeIsSameObjectfrompatchembeddingsacrossViTlayersusingasimilarity probe,whichreachesover90%accuracy. Crucially,thisobject-bindingcapability emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker in ImageNet-supervised models, suggesting that binding is not a trivial architecturalartifact,butanabilityacquiredthroughspecificpretrainingobjectives. WefurtherdiscoverthatIsSameObjectisencodedinalow-dimensionalsubspace ontopofobjectfeatures,andthatthissignalactivelyguidesattention. Ablating IsSameObjectfrommodelactivationsdegradesdownstreamperformanceandworks against the learning objective, implying that emergent object binding naturally servesthepretrainingobjective. OurfindingschallengetheviewthatViTslack object binding and highlight how symbolic knowledge of \u201cwhich parts belong together\u201demergesnaturallyinaconnectionistsystem. 1",
  "introduction": "1 Introduction Humansnaturallyparsescenesintocoherentobjects[1](e.g.,groupingfeaturessuchasrounded shape,smoothsurface,andmutedcolorintothemug)andfurthergroundtheiridentitiesincontext (e.g., recognizing my coffee mug on the desk rather than just a mug). This is assumed to be made possible by what cognitive scientists call object binding [2], the brain\u2019s ability to group an object\u2019s low-level features (color, shape, motion, etc.) into a unified representation. This in turnenablesobjectstobestoredefficientlyandcompositionallyinmemoryandusedashigh-level symbolsforreasoning. Thebindingproblemisagenuinecomputationalchallenge,asevidenced byhumans\u2019limitedcompetenceinconjunction-searchtasks[3]andclinicaldissociationssuchas Balint\u2019s syndrome, where feature perception remains intact but binding breaks down [4]. If AI systemscouldreplicatethehumanabilityforobjectbinding,thatmayhelpthemgroundsymbolsfor 1Codeavailableat: 39thConferenceonNeuralInformationProcessingSystems(NeurIPS2025). 5202 tcO 82 ]VC.sc[ 1v90742.0152:viXra --- Page 2 --- perceptionandexploitingcompositionality[5]. Thekeyquestionis: docurrentAIsystemssolvethe bindingproblem? Figure 1: Assessing object binding in ViTs with IsSameObject. (a) We use a probe to decode IsSameObject, with scores near 1 for same-object pairs and near 0 for different-object pairs. (b) Downstreamtasksthatbenefitfromstrongobjectbindingincludeinstancesegmentationandvisual reasoning(e.g., locatingandcountingobjectswithspecificfeatures), wherepatchestriggeredby certainfeaturesareboundtotherestoftheirobjecttoallowextractionoftheentireobject. ObjectbindinghasreceivedlittleattentioninmainstreamAIresearch. Cognition-inspiredmodels[6, 7]buildinhuman-likeobject-basedattention. Bycontrast,mainstreamvisionmodelsareassumedto implicitlylearntohandlemultipleobjectsfromthetrainingset,yetempiricalstudiesshowtheyoften \"attend\"onlytothemostsalientregionsandoverlooktherest[8]. ViTscomputeattentionscores fromfeature-levelsimilarities,whichinprincipleshouldhelpwithobjectbinding. Butexperiments showthatself-attentionbehavesmorelikeasimilarity-basedgroupingmechanismthanselective attention,whichgroupsimagepatchesbylow-levelfeaturesimilarity(e.g. colorortexture),rather thanisolatingdistinctobjects[9].Object-centricmethodslikeSlotAttention[10]fixthisbyallocating asmallsetoflearnableslotsthatcompetefortokenfeatures,enforcingbindingbydesign. However, whetherAIvisionmodels,especiallyleadingViTs,canachieverobustobjectbindingwithoutexplicit mechanismsremainsanopenquestion. CognitivescientistshavequestionedwhetherViTscanbindobjectsatall: arguingthattheylack mechanismsfordynamicallyandflexiblygroupingfeatures[5];theylackrecurrencenecessaryfor iterative refinement of object representations in the cognitive science field [11, 7]; and as purely connectionist models, they appear incapable of true symbolic processing [8]. However, these architecturallimitationsdonotprecludebindingfromemergingthroughlearning. Ifamodelencodes whether two patches belong to the same object (IsSameObject), this signal can guide attention andimproveprediction[12,13]. Human-labeleddataalsoreflectsobject-levelstructure,soViTs canacquirebindingbyimitation. ThissuggeststhatViTsmaylearntobindobjectsdirectlyfrom large-scaletrainingdata,withoutrequiringexplicitarchitecturalinductivebiases. Here, weaskwhetherobjectbindingnaturallyemergesinlarge, pretrainedVisionTransformers, which is a question that matters for both cognitive science and AI. We propose IsSameObject (whethertwopatchesbelongtothesameobject)andshowthatitisreliablydecodable(with90.20% accuracy)usingaquadraticsimilarityprobestartingfrommid-layersofthetransformerlayers. This effectisrobustacrossself-supervisedmodelssuchasDINO,MAE,andCLIP,butlargelyabsent in ImageNet-supervised ViTs, suggesting that binding is an acquired ability rather than a trivial architecturalartifact. AcrosstheViT\u2019slayerhierarchy,itprogressivelyencodesIsSameObjectina low-dimensionalprojection-spaceontopofthefeaturesoftheobject,anditguidesself-attention. AblatingIsSameObjectfrommodelactivationshurtsdownstreamperformanceandworksagainstthe pretrainingobjective. 2 --- Page 3 --- Ourmaincontributionsareasfollows: (i)Wedemonstratethatobjectbindingnaturallyemerges inlarge, pretrainedVisionTransformers, challengingthecognitive-scienceassumptionthatsuch bindingisn\u2019tpossiblegiventheirarchitecture. (ii)WeshowthatViTsencodealow-dimensional signatureofIsSameObject(whethertwopatchesbelongtothesameobject)ontopoftheirfeature representations. (iii)Wesuggestthatlearning-objective\u2013basedinductivebiasescanenableobject binding,pointingfutureworktowardimplicitlylearnedobject-basedrepresentations. 2 RelatedWork ObjectBindinginCognitiveScienceandNeuroscience. Theobjectbindingproblemaskshow the brain integrates features that are processed across many distinct cortical areas into coherent object representations [14]. The concept of binding2 rests on three key hypotheses: First, visual processingiswidelyunderstoodtobehierarchical,parallel,anddistributedacrossthecortex[16\u201320]. Second,weperceivetheworldprimarilyintermsofobjects,ratherthanasacollectionofscattered features[11,1]. Thisabstractionisfundamentaltobothperceptionandinteractionwiththeworld, allowingustorecognize,reasonabout,andmanipulateourenvironmenteffectively[21\u201323]. Third, featurebindingrequiresamechanismthatcorrectlyassignsfeatures,representedinspatiallydistinct corticalareas,totheircorrespondingobject[15,24,2]. Thisthirdhypothesisiswherethecoreof thebindingproblemlies,andithasbeenalongstandingpointofdebateamongneuroscientistsand cognitivescientists[25\u201327]. Despite their substantial difference, vision transformers (ViTs) share several key computational parallelswiththemammalianvisualsystem: theybothrelyonparallel,distributedandhierarchical processes. Moreimportantly,ViTsdohavetwoofthethreearchitecturalandcomputationalelements hypothesized to enable binding in the brain. The explicit position embeddings in ViTs resemble spatial tagging and the spatiotopic organization observed in the ventral stream [28, 26]; and the self-attentionmechanismisakintodynamictuningandattentionalmodulation,whicharethought tobeprimarymechanismsforobjectbinding[26,29,30](althoughattentionisbelievedtobeof recurrent nature in the brain [31, 32]). These parallels position ViTs as potential computational modelsforexploringobjectbindinginbothartificialandbiologicalsystems. Object-CentricLearning. Motivatedbyhowhumansnaturallyreasonaboutindividualobjects, Object-CentricLearning(OCL[10])aimstorepresentasceneasacompositionofdisentangledobject representations. Whilesegmentationonlypartitionsanimageintoobjectmasks,OCLgoesfurtherby encodingeachobjectintoitsownrepresentation[33]. UnsupervisedapproachessuchasMONet[34], IODINE[33],andespeciallySlotAttention[10]encodescenesintoasmall,permutation-invariantset of\u201cslots\u201dthatareiterativelyrefined,producingrobustobjectrepresentationsonbothsynthetic[10,35] and real-world data [36, 37] and enabling compositional generation and manipulation [38\u201340]. However,slot-basedmodelsimposeafixedslotbudgetandrequiremultiplerefinementiterations thatslowsinference,andbecauseSlotAttentionisboltedontoratherthanbuiltintothetransformer, scalingandtrainingbecomeharder[41]. Otherexplicitobject-centricapproachesincludeTensor ProductRepresentations[42]andCapsuleNetworks[43]. Instead of object-centric approaches that explicitly enforce object-level attention, we propose an alternativeviewthatViTsmayalreadyencodeimplicitobject-levelstructure. Priorworkhasassumed thisandattemptedtogrouppatchesintoobjectsdirectlyfromactivationsorattentionmapsViTs, usingmethodslikeclustering[44]orGraphCut [45]. Otherstudiesdesignself-supervisedobjectives (e.g.,object\u201cmovability\u201d[46]orpatch-levelcontrastivelearning[47])andtrainmodelstostrengthen object-levelgrouping. Incontrast,ourstudydirectlyvalidatesthisassumptionbyshowingthatViT patchembeddingsintrinsicallyencodewhetheranytwopatchesbelongtothesameobject. BindinginTransformers. Bindinghasreceivedgrowingrecognitionintransformer-basedmachine learningresearchandbindingfailuresareseenasexamplesofperformancebreakdownsinmodern applications [48\u201351]. Diffusion models rely on binding attributes to entities, and failures cause attributeleakage(e.g.,bothadogandacatendupwearingsunglassesandasun-hat)[49,48]. Vision- languagemodelsfacesimilarbindingchallenges,strugglingwithdifferentiatingmultipleobjectswith featureconjunctions[51]. Despitethesebindingfailures,transformersstilldemonstratesomebinding 2ThetermbindingwasintroducedtoneurosciencebyChristophvonderMalsburgin1981,inspiredbythe notionofvariablebindingincomputerscience[15]. 3 --- Page 4 --- capability,yettheunderlyingmechanismisnotwellunderstood. FengandSteinhardt[52],Daietal. [53]studybindinginlanguagemodels,showingthatattributes(e.g.,\u201clivesinShanghai\u201d)arelinked totheirsubjects(e.g.,\"Alice\")viaalow-dimensionalbinding-IDcodethatisaddedtotheactivation andcanbeeditedtoswaporredirectrelations. Bindingmechanismsinvisiontransformersremain unexplored,andourstudyaimstofillthisgap. 3 AssessingObjectBindinginViTsthroughIsSameObject 3.1 ProbingIsSameObjectrepresentations VisionTransformers(ViTs)tokenizeimagesbydividingthemintoagridoffixed-sizepatches[54]. Becausethetokenistheminimalrepresentationalunit,anygroupingoffeaturesintoobjectsmust arisethroughrelationsbetweentokens,notwithinthem. TheonlymechanismViTshaveforsuch cross-token interaction is scaled dot-product attention, where attention scores can be viewed as dynamic edge weights in a graph that route information between tokens [5]. Therefore, if ViTs performanyformofobjectbinding,weexpecttoobserveapairwisetoken-levelrepresentationthat indicateswhethertwopatchesbelongtothesameobject,whichwetermIsSameObject. Sinceobjectbindingistheabilitytogroupanobject\u2019sfeaturestogether, decodingIsSameObject reliably from ViT patch embeddings would provide direct evidence of object binding (and its representation)inthemodel. Weadoptprobing,whichtakesmeasurementsofViTactivationswith lightweight classifiers [55], to determine whether IsSameObject is encoded or unrecoverable by simpleoperations. s(\u2113) =LayerNorm(cid:0) h(\u2113)+MultiHeadAttention(h(\u2113))(cid:1) (1) h(\u2113+1) =LayerNorm(cid:0) s(\u2113)+FFN(s(\u2113))(cid:1) (2) Transformerspropagateinformationacrosslayersaccordingtotheaboveequations12,whereh(\u2113)is theresidual-streamoutput(whichwe\u2019llcallthepatchembedding)atlayer\u2113. Formally,wedefinetheIsSameObjectpredicateonapairoftokenembeddings(x(\u2113),x(\u2113))atlayer\u2113 i j by IsSameObject(cid:0) x(\u2113),x(\u2113)(cid:1) =\u03d5(cid:0) x(\u2113),x(\u2113)(cid:1) , \u03d5:Rd\u00d7Rd \u2192[0,1], i j i j where\u03d5scorestheprobabilitythattokensiandj belongtothesameobject. We consider the following hypotheses about how IsSameObject may be encoded in the model\u2019s activations: \u2022 Itmaybelinear(recoverablebyaweightedsumoffeatures)orfundamentallyquadratic (recoverableonlythroughpairwisefeatureinteractions). \u2022 Itisapairwiserelationshipversusapointwisemapping(i.e.themodelfirstmapseach patchtoanobjectidentityorclass,thencompares). \u2022 The model tells objects apart using only broad class labels or object identities\u2013i.e., it may solve binding by recognizing \u201cthis is a dog vs. this is a chair\u201d without actually representingwhichspecificpixelsbelongtoeachobject. Suchasolutionispossiblebecause classlabelsalreadyprovideacoarsenotionofobjectidentitywithoutrequiringpixel-level grounding. \u2022 The signal is stored in a few specialized dimensions versus distributed across many dimensions. Intheformercase,bindinginformationwouldbeisolatedtoasmallsubsetof channels,whileinthelatteritwouldbeencodeddiffusely(e.g.,asrotatedcombinationsof features)suchthatnosingledimensioncarriesthesignalonitsown. Totestthesehypotheses,wedecodeIsSameObjectusingseveralprobearchitectures,eachdefined byalearnableparametermatrixW: 1. Linearprobe: \u03d5 (x,y) = Wx + Wy,whereW \u2208R1\u00d7d. lin 2. Diagonalquadraticprobe(specializeddims)\u03d5 (x,y) = x\u22a4diag(w)y,withw \u2208Rd. diag 3. Quadraticprobe(distributed)\u03d5 (x,y)= x\u22a4W\u22a4W y, W \u2208Rk\u00d7d, k \u226ad. quad 4 --- Page 5 --- 4. Object-class/identityprobes(pointwise)Wefirstmapeachembeddingx,ytoclassdistributions p=softmax(W x),q =softmax(W y)andtrainW withmulticlasscross-entropyonobject-class c c c labels(andsimilarlyW onobject-identitylabels). ThepointwiseIsSameObjectscoreisthen N (cid:88)Nc \u03d5 (x,y)=p\u22a4q = p(c)q(c)=softmax(W x)\u22a4softmax(W y). class/identity N N c=1 5. Object-classprobes(pairwise)Alternatively,wecantreatIsSameObjectasabinarylabeland optimizep\u22a4qdirectlywithbinarycross-entropyloss.Thisversionispairwisebecausethesupervision isapplieddirectlyto(x,y)asapair. Themodelisthusoptimizedtoencodetherelationshipbetween twopatches,nottheirindividualclassmemberships. 3.2 IsSameObjectisbestdecodableinquadraticform Figure2: Quadraticprobesexcelatdecodingthebindingsignal. Layer-wiseaccuracyofthe IsSameObject probe on DINOv2-Large. The quadratic probe consistently outperforms all other probesfrommiddlelayersonward. ResultsforadditionalmodelsareshowninAppendixA.2. We extract DINOv2-Large [13] activations at each layer and train the probes on the ADE20K dataset[56]usingcross-entropylossforallpairwiseprobestoclassifysame-objectvs.different-object patch pairs. Figure 2 shows probe accuracy across layers. To test our hypotheses about how IsSameObjectisrepresented,wecompare: \u2022 Linearvs. quadraticprobes: Quadraticprobes(evenwiththesameparametercountas indiagonalquadratic)significantlyoutperformlinearones,suggestingthatIsSameObject isaquadraticrepresentation,consistentwiththequadraticformusedbytheself-attention mechanism. \u2022 Quadraticvs.object-classprobes: Trainedobjectclassprobesfallshortofthequadratic probe directly optimized for distinguishing same and different object instances. This indicatesthatthemodelencodesnotjustwhethertwopatchesshareanobjectclassbutalso subtleridentitycues\u2014e.g.,distinguishingtwoidenticalcarsofthesamemakeandmodel. \u2022 Fullvs. diagonalquadraticprobes: Thefullquadraticprobeoutperformsitsdiagonal variant, implying that IsSameObject information is more distributed across dimensions ratherthanrestrictedtospecificchannels. \u2022 Pointwiseobjectclassprobevs. pairwiseobjectclassprobe: Mappingeachpatchtoan objectclassviasoftmaxandthencomparingclasses(pointwise)underperformsdirectly comparingtheirembeddings(pairwise),asthepointwiseapproachlosesinformationwhen itcollapsesembeddingsintodiscreteclasses. 3.3 Objectbindingemergesbroadlyacrossself-supervisedViTs WeextendouranalysisbeyondDINOv2toabroadersetofpretrainedVisionTransformers,including CLIP,MAE,andfullysupervisedViTs. Toenabledirectcomparison,westandardizeinputpatch coveragebyresizingallinputssothateachmodelprocessesthesamespatialpatchdivisions. Under 5 --- Page 6 --- thissetup,everyprobestartsfromthesametrivialbaselineof72.6%accuracy,whichcorrespondsto alwayspredicting\u201cdifferent\u201d,reflectingtheclassimbalancethatmostpatchpairsdonotbelongto thesameobjectinthedataset. Table1reportsIsSameObjectdecodingaccuracyacrossmodels. DINOmodelsshowthestrongest bindingsignal,withlargeandgiantvariantsexceeding+16percentagepointsoverbaseline. CLIP andMAEalsoexhibitclearobject-bindingability,thoughtoalesserdegree. Incontrast,ImageNet- supervisedViTyieldspoorobject-bindingperformance,suggestingthatbindingisanacquiredability underspecificpretrainingobjectivesratherthanbeingauniversalpropertyofallvisionmodels. Table1: Bindingisconsistentlyrepresentedinself-supervisedViTs,butlesssoinsupervised ViTs. ProbeaccuracyonIsSameObjectacrosspretrainedViTs. \u2206isreportedinpercentagepoints (pp),andthepeaklayerindexisnormalizedto[0,1]withineachmodel. Model HighestAccuracy(%) \u2206overBaseline(pp) PeakLayer(0\u20131) DINOv2-Small 86.7 +14.1 1.00 DINOv2-Base 87.5 +14.9 0.82 DINOv2-Large 90.2 +17.6 0.78 DINOv2-Giant 88.8 +16.2 0.77 CLIP(ViT-L) 84.2 +11.6 0.39 MAE(ViT-L) 82.9 +10.3 0.65 Supervised(ViT-L) 76.3 +3.7 0.13 Our findings thus produce a much wider coverage of ViTs and we provide an understanding of potentialreasonswhybindingemerges: \u2022 DINO.Thecontrastiveteacher\u2013studentlossenforcesconsistencyacrossaugmentedviews containing the same objects. This objective encourages the model to learn object-level featuresthatpersistunderaugmentedviews[12]. \u2022 CLIP. By aligning images with text captions, CLIP effectively assigns each object a symbolic label (e.g., \u201cthe red car\u201d), which can act like a pointer that pulls together all patchesofthatobject. Thissupervisionlikelyencouragespatchesfromthesameobjectto clusterinfeaturespace. \u2022 MAE.Themaskedautoencoderobjectiverequiresthemodeltoreconstructamissingpatch from its surroundings. When the masked patch sits between multiple objects, correctly predictingitscontentforcesthemodeltoinferwhichobjectitbelongsto,thuspromoting thegroupingofpatchesfromthesameobject. \u2022 SupervisedImageNettraining. BecauseImageNetsupervisionlabelsonlythedominant objectineachimage[57],itrewardsclass-levelcategorizationbutnotgroupingofpatches intoobjectinstances,leadingtoaweakerbindingsignal. 4 ExtractingtheBindingSubspaceofViTRepresentations 4.1 DecomposingIsSameObjectfromfeatures Followingthelinearfeaturehypothesis[58],andsimilarto[52],weassumethatatlayer\u2113eachtoken embeddingdecomposesintoa\u201cfeature\u201dpartanda\u201cbinding\u201dpart: h(\u2113)(x )=f(\u2113)(x ,c) + b(\u2113)(x ), t t t wheref(\u2113)(x ,c) \u2208 Rd encodesallattributesoftokenx (texture,shape,etc.) givencontextc = t t {x ,...,x },excludinganyinformationaboutwhichothertokensitbindswith,andb(\u2113)(x )\u2208Rd 1 T t encodesthebindinginformationthatdetermineswhichothertokensbelongtothesameobject(i.e., theIsSameObjectrelation). Considertwoidenticalpatchesx andx atcorrespondingpositionsofidenticalobjectsAandBin Ai Bi thesameimage,andlettheirresidualbe\u2206 . Itmaybetemptingtocancelthefeaturetermdirectly. ABi Indeed,withoutpositionalencoding(seeproofinAppendixA.4.1),wehavef(\u2113)(x )=f(\u2113)(x ), Ai Bi 6 --- Page 7 --- Figure3:ThegeometryofIsSameObjectrepresentation. Patchembeddingsh andh areprojected i j ontotheIsSameObjectsubspacebyW ,producingbindingvectorsb andb ,whosesimilarityis probe i j computedbyadotproduct. because foridentical tokensthe positionalencoding is theonly signalthat candifferentiate their cross-tokeninteractions. Wecanapproximatef(\u2113)(x )\u2248f(\u2113)(x ),sincethetwopatchesarevisuallyidentical,appearin Ai Bi nearlythesamecontext,andanypositionaldifferencecanbeoffloadedintothebindingcomponent. Thisyields: (cid:2) (cid:3) (cid:2) (cid:3) \u2206 =h(x )\u2212h(x )= f(x )\u2212f(x ) + b(x )\u2212b(x ) \u2248b(x )\u2212b(x ). ABi Ai Bi Ai Bi Ai Bi Ai Bi If\u2206 remainsroughlyconsistentacrosspatchpairswiththesameindexi,thenb(x )andb(x ) ABi Ai Bi canformlinearlyseparableclusters,whichcanthusserveasobjectidentityrepresentations. However, thisbecomesproblematicinnaturalimages,whereidenticalpatchesarerare. Instead,wetakeasupervised approachtodecodingthebindingcomponent. Ourquadraticprobe servesasatoolforseparatingbindingfromfeatureinformationwithineachtoken(Fig.3). Con- ceptually, thequadraticprobecanbeviewedasprojectinganactivationhintotheIsSameObject subspace,yieldingb(\u2113)(x)=h(\u2113)(x)\u22a4W,andthenmeasuresthedot-productsimilaritybetweentwo projectedvectors. Giventhatnaturalimagedatasetscontainnumerousobjectswherebistheprimary distinguishingfactor,theprobeshouldbeoptimizedtodiscoveradirectionthatisolatesb. Withthis strategywecanseparatethebindingsignalfromtherestoftherepresentation. Theobservationthatbindingvectorsremainmeaningfulunderlinearcombination,andbecomehard to discriminate when they are close together, is consistent with this interpretation [52]. In later ablationstudies,weuseourtrainedquadraticprobeviab(\u2113)(x)=h(\u2113)(x)\u22a4W. 4.2 AToyExperiment: distinguishingidenticalobjectsandsimilarlookingobjects ToprobethelimitsofobjectbindinginViTs,weconstructatestimagewithtwoidenticalredcars,a thirdredcarofadifferentbrand,andaredboat. ThissetupletsustrackIsSameObjectrepresentations acrosslayersbyevaluatingthreedistinctions:differentobject-classbutsimilarappearance,sameclass withsubtledifferences,andexactduplicates. Asexpected,thesedistinctionsbecomeprogressively harder. WechosenaturalobjectsratherthanabstractshapesbecauseboththeViTandourprobeare trainedonreal-worldimages,whichallowsustoanalyzebindinginanontrivialsetting. Toanalyzewherebindingemerges,weplottheIsSameObjectscorespredictedbyourtrainedquadratic probe(Figure4). Sincetheprobeperformsreliably(seeSection3.2),itsoutputsapproximatethe internal IsSameObject the model encodes. Weobservethat, from early tomid-layers, the model increasingly discerns the local object (the one to which each patch belongs). Surprisingly, from mid-layerstolaterlayers,themodelshiftstowardclass-basedgrouping,increasinglytreatingallred carsasthesame. Bindingemergesinthemiddleofthenetworkandisthenprogressivelylosttowards thetop. 7 --- Page 8 --- Figure4: Layer-wisevisualizationofIsSameObjectpredictionsonthetestimage. Weusedthree redcarsandoneredboattomakebindingdeliberatelydifficult. Earlylayersattendtosimilarsurface features(e.g.,theredcarorboathull),mid-layersfocusonlocalobjects,andhigherlayersshiftto groupingpatchesbyobjectclass. TheIsSameObjectrepresentationislow-dimensional. Weusefouridenticalred-carimagesand spliteachoneintopatchesusingexactlythesamegridalignment. Weperformprincipalcomponent analysis(PCA)ontheresidualssets{\u2206 ,\u2206 ,\u2206 },where\u2206 =h \u2212h \u2248b \u2212b BA CA DA BA Bi Ai Bi Ai and visualize the first three components (see Figure 5). \u2206 ,\u2206 ,\u2206 fall into three linearly BA CA DA separableclustersinthefirstthreeprincipalcomponentspace,showingthatb ,b ,b formobject- B C D levelrepresentations. Theseparationoftheseclustersinaverysmallnumberofprincipaldirections demonstratesthatIsSameObjectliesinalow-dimensionalsubspace: patchesfromthesameobject instancemaptocloselyalignedbindingvectors,anddifferentinstancesarelinearlyseparablewith largemargins. Mid-layerscapturelocalobjects,andhigherlayersshifttowardsgroupingpatchesbyobject class. Asurprisingobservationisthesuddenincreaseinthecross-objectIsSameObjectscore(fig.4) inthemid-layersoftheDINOV2modelforinstancesofthesameclass(Fig.4). Thisisconsistent withpriorworkshowingthatViTsrepresentdifferenttypesofinformationatdifferentlayers[59]. At thesametime,token-positiondecodabilitydropsindeeperlayers(seeAppendixA.4.3),suggesting thatthemodelisdeliberatelydiscardingpositionalinformation. Ourinterpretationisthatthenetwork initially relies on positional cues to support binding, since location is necessary to disambiguate tokensthatsharesimilarfeaturecontent. However,theDINOtrainingobjectiveenforcesposition invarianceattheoutputlayer[12],whichimplicitlyencouragesthenetworktoremovepositional signalonceitisnolongerusefulandtorepurposecapacityforsemanticallyrelevantobjectstructure. Ourfindingsareconsistentwithexperimentalevidencefromtheventralstreaminthebrain,showing thatwhiletheretinotopicorganizationofearlyventralareasisnecessaryforperceptionandbinding, globalspatialinformationisinsteadprocessedandmaintainedbythedorsalstream[60\u201362]. 4.3 Attentionweights(query-keysimilarity)correlatewithIsSameObject InSection3.2weshowedthatIsSameObjectisbestdecodedquadratically. Sinceself-attentionis alsoaquadraticinteraction,bindinginformationintheresidualstreamatlayer\u2113caninprinciple guidewhereattentionisallocatedatlayer\u2113+1,allowingthemodeltoselectivelyrouteattention withinthesameobjecttobuildacoherentobject-levelrepresentation. Totestthis,wecomputethePearsoncorrelationbetweenattentionweightsandtheIsSameObject scores(seeFig.6andAppendixA.5).Inmid-levellayers,weobserveapositivebutmodestcorrelation, indicatingthatthemodeldoesmakeuseoftheIsSameObjectsignalwhenallocatingattention. The modeststrengthoftheeffectisexpected,becauseattentionservesmanyrolesbeyondbinding. 8 --- Page 9 --- Figure 5: Identical objects form distinct object-level representations. The first 3 principal componentsofthefouridenticalcars,withthethreeclustersdenoting\u2206 ,\u2206 ,\u2206DA. Thethree BA CA linearlyseparableclusterssuggestthatidenticalobjects\u2019bindingvectorsformdistinctobject-level representations. Thepercentageinparenthesesindicatesthevarianceexplainedbythatprincipal component. Figure6: AttentionweightsarecorrelatedwithIsSameObject. Dotsizeisproportionaltothe Euclideandistancebetweenpatches. AttentionweightscorrelatewithIsSameObjectinmiddlelayers: (a)Pearsonr =0.163,(b)Pearsonr =0.201. 4.4 AblationofIsSameObjecthurtsdownstreamperformanceandworksagainstthe minimizationofthepretrainingloss Weconductablationstudiesandevaluatetheimpactondownstreamsegmentationperformanceand pretrainingloss. InsteadofdirectlysubtractingtheIsSameObjectrepresentationb(x )fromh(x ), i i weuselessaggressiveapproaches: \u2022 UninformedAblation: Randomlyshuffleb(x )acrosspatchesintheimageataspecified i ratio 9 --- Page 10 --- Table 2: Ablations demonstrate the functional role of IsSameObject. Segmentation accuracy, instance accuracy, and DINO loss on layer 18 under uninformed (random shuffle) and informed (ground-truthinjection)ablations. Higheruninformedratiosmeanmoreaggressiveshuffling;lower informed\u03b1valuesmeanstrongerinjectionoftrueobjectlabels. Uninformed/ratio Informed/\u03b1 0 0.5 1 1 0.5 0 SegmentationAcc.(%) 44.14 41.03 39.20 44.14 44.91 43.59 InstanceAcc.(%) 35.14 31.39 28.19 35.14 36.37 37.02 DINOLoss 0.6182 0.6591 0.6749 0.6182 \u2014 \u2014 \u2022 InformedAblation/Injection: Usingground-truthinstancemasks,weablateorinjectthe trueIsSameObjectsignalbylinearlycombiningthemeanobjectdirectionwitheachpatch\u2019s bindingvectorb :\u02dcb =(1\u2212\u03b1) 1 (cid:80) b +\u03b1b . i i |I| j\u2208I object,j object,i Weevaluatethesemanticandinstancesegmentationperformancewithretrainedsegmentationheads onasubsetofADE20Kunderthesevariations. Wealsoevaluatetheteacher\u2013studentself-distillation lossasemployedinDINO(seeAppendixA.6fordetails). Resultsshowthatuninformedablation,whichrandomlyshufflesthebindingvector,reducesseg- mentationperformance,whereasinjectingthemeanobjectdirectionimprovesaccuracy. Ablating IsSameObjectwithrandomshufflingleadstoanoticeablegradualincreaseintheDINOloss,suggest- ingthatablationofIsSameObjectworksagainstthispretrainingloss. 5 Limitations We assume the trained probe cleanly splits each patch embedding into \u201cfeature\u201d and \u201cbinding\u201d components,asimplificationthatwouldbenefitfromfurtherempiricalexploration. Additionally, whilerandomizingtheIsSameObjectsubspaceclearlyincreasesDINO\u2019spretrainingloss,thisindirect evidencefallsshortofdemonstratingacausallinktodownstreamtaskperformanceorisolatingits precisecontribution. Finally,ourdownstreamevaluationsfocusonlyonsegmentation,leavingopen whethertheseemergentbindingsignalsalsobenefitothervisiontaskssuchasvisualreasoning. More broadly,thispaperstudiesobject-identitybinding,i.e.,whethertwotokensbelongtothesameobject; compositionalbinding,suchasreusingobjectstructureacrosscontexts,isnotexploredhereandis leftforfuturework. 6",
  "conclusion": "Conclusion Inthispaper,weshowthatobjectbindingnaturallyemergesinlarge,pretrainedvisiontransformers, especiallyinDINOv2,andthiseffectisconsistentacrossmultiplemodels. Wealsoshowthatitis anacquiredratherthaninnateabilitythroughcomparisonswithsupervisedmodels. IsSameObject, whethertwopatchesbelongtothesameobject,isreliablydecodableandliesinalow-dimensional latentspace. Ablatingthissignalbothdegradesdownstreamsegmentationandopposesthemodel\u2019s pretrainingloss,suggestingthatemergentobjectbindingisanaturalsolutiontotheself-supervised objective. Ourstudybridgeswhatpsychologistsidentifyasobjectbindingwithemergentbehaviorin ViTs,challengesthebeliefthatViTslackthisabilityanddemonstrateshowsymbolicprocessingcan ariseinconnectionistmodels. Moving forward, we propose that AI researchers seeking to address binding failures adopt an alternativetoexplicitbindingmodules(suchasSlotAttention[10])byenhancingViTs\u2019inherent object-bindingthroughtailoredtrainingobjectivesorminimalarchitecturaltweaks.Futureworkcould extendourapproachtovideomodels,inwhichobjectpersistenceacrosstimeisessential[11,63];such modelsshouldexhibitsimilarspatialbindingseeninViTsandadditionallydeveloptemporalbinding tomaintainobjectidentityacrossframes. Alsoofinterestwillbehowtheseobjectrepresentations formed by binding interact with one another, with one possibility being that they interact with low-dimensional\u201cobjectfiles\u201d[64]. Together,theseeffortswilldeepenourunderstandingofhow symbolicprocessingofobjectscanemergeinconnectionistmodels. 10 --- Page 11 ---",
  "references": "References [1] BrianJScholl. Objectsandattention: Thestateoftheart. Cognition,80(1-2):1\u201346,2001. [2] AnneTreisman. Thebindingproblem. Currentopinioninneurobiology,6(2):171\u2013178,1996. [3] AnneTreismanandHilarySchmidt.Illusoryconjunctionsintheperceptionofobjects.Cognitive psychology,14(1):107\u2013141,1982. [4] LynnRobertson,AnneTreisman,StaciaFriedman-Hill,andMarciaGrabowecky. Theinterac- tionofspatialandobjectpathways: Evidencefrombalint\u2019ssyndrome. JournalofCognitive Neuroscience,9(3):295\u2013317,1997. [5] Klaus Greff, Sjoerd Van Steenkiste, and J\u00fcrgen Schmidhuber. On the binding problem in artificialneuralnetworks. arXivpreprintarXiv:2012.05208,2020. [6] DrewLinsley,DanShiebler,SvenEberhardt,andThomasSerre. Learningwhatandwhereto attend. arXivpreprintarXiv:1805.08819,2018. [7] Saeed Salehi, Jordan Lei, Ari S Benjamin, Klaus-Robert M\u00fcller, and Konrad P Kording. Modelingattentionandbindinginthebrainthroughbidirectionalrecurrentgating. bioRxiv, pages2024\u201309,2024. [8] TarunKhajuria, BraianOlmiroDias, andJaanAru. Howstructuredaretherepresentations intransformer-basedvisionencoders? ananalysisofmulti-objectrepresentationsinvision- languagemodels. arXivpreprintarXiv:2406.09067,2024. [9] PariaMehraniandJohnKTsotsos. Self-attentioninvisiontransformersperformsperceptual grouping,notattention. FrontiersinComputerScience,5:1178450,2023. [10] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,JakobUszkoreit,AlexeyDosovitskiy,andThomasKipf. Object-centriclearningwith slotattention. Advancesinneuralinformationprocessingsystems,33:11525\u201311538,2020. [11] Benjamin Peters and Nikolaus Kriegeskorte. Capturing the objects of vision with neural networks. Naturehumanbehaviour,5(9):1127\u20131144,2021. [12] MathildeCaron,HugoTouvron,IshanMisra,Herv\u00e9J\u00e9gou,JulienMairal,PiotrBojanowski, andArmandJoulin. Emergingpropertiesinself-supervisedvisiontransformers. InProceedings oftheIEEE/CVFinternationalconferenceoncomputervision,pages9650\u20139660,2021. [13] MaximeOquab,Timoth\u00e9eDarcet,Th\u00e9oMoutakanni,HuyVo,MarcSzafraniec,VasilKhalidov, PierreFernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal.Dinov2:Learning robustvisualfeatureswithoutsupervision. arXivpreprintarXiv:2304.07193,2023. [14] ChristophVonderMalsburg. Thewhatandwhyofbinding: themodeler\u2019sperspective. Neuron, 24(1):95\u2013104,1999. [15] JeromeFeldman. Theneuralbindingproblem(s). Cognitiveneurodynamics,7:1\u201311,2013. [16] SemirMZeki. Functionalspecialisationinthevisualcortexoftherhesusmonkey. Nature,274 (5670):423\u2013428,1978. [17] Margaret Livingstone and David Hubel. Segregation of form, color, movement, and depth: anatomy,physiology,andperception. Science,240(4853):740\u2013749,1988. [18] MortimerMishkin,LeslieGUngerleider,andKathleenAMacko. Objectvisionandspatial vision: twocorticalpathways. Trendsinneurosciences,6:414\u2013417,1983. [19] DanielJFellemanandDavidCVanEssen. Distributedhierarchicalprocessingintheprimate cerebralcortex. Cerebralcortex(NewYork,NY:1991),1(1):1\u201347,1991. [20] KalanitGrill-SpectorandKevinSWeiner. Thefunctionalarchitectureoftheventraltemporal cortexanditsroleincategorization. NatureReviewsNeuroscience,15(8):536\u2013548,2014. 11 --- Page 12 --- [21] GaetanoKanizsa,PaoloLegrenzi,andPaoloBozzi. Organizationinvision: Essaysongestalt perception. (NoTitle),1979. [22] StephenEPalmer. Hierarchicalstructureinperceptualrepresentation. Cognitivepsychology,9 (4):441\u2013474,1977. [23] Irving Biederman. Recognition-by-components: a theory of human image understanding. Psychologicalreview,94(2):115,1987. [24] ChristophVonDerMalsburg. Thecorrelationtheoryofbrainfunction. InModelsofneural networks: Temporalaspectsofcodingandinformationprocessinginbiologicalsystems,pages 95\u2013119.Springer,1994. [25] HStevenScholteandEdwardHFdeHaan. Beyondbinding: frommodulartonaturalvision. TrendsinCognitiveSciences,2025. [26] Lynn C Robertson. Binding, spatial attention and perceptual awareness. Nature Reviews Neuroscience,4(2):93\u2013102,2003. [27] AdinaLRoskies. Thebindingproblem. Neuron,24(1):7\u20139,1999. [28] AnneTreisman. Focusedattentionintheperceptionandretrievalofmultidimensionalstimuli. Perception&Psychophysics,22:1\u201311,1977. [29] JohnHReynoldsandRobertDesimone. Theroleofneuralmechanismsofattentioninsolving thebindingproblem. Neuron,24(1):19\u201329,1999. [30] PieterRRoelfsema. Solvingthebindingproblem: Assembliesformwhenneuronsenhance theirfiringrate\u2014theydon\u2019tneedtooscillateorsynchronize. Neuron,111(7):1003\u20131019,2023. [31] RubenSvanBergenandNikolausKriegeskorte. Goingincirclesisthewayforward: therole ofrecurrenceinvisualinference. CurrentOpinioninNeurobiology,65:176\u2013193,2020. [32] Kohitij Kar, Jonas Kubilius, Kailyn Schmidt, Elias B Issa, and James J DiCarlo. Evidence thatrecurrentcircuitsarecriticaltotheventralstream\u2019sexecutionofcoreobjectrecognition behavior. Natureneuroscience,22(6):974\u2013983,2019. [33] Klaus Greff, Rapha\u00ebl Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, DanielZoran,LoicMatthey,MatthewBotvinick,andAlexanderLerchner. Multi-objectrepre- sentationlearningwithiterativevariationalinference. InInternationalconferenceonmachine learning,pages2424\u20132433.PMLR,2019. [34] ChristopherPBurgess,LoicMatthey,NicholasWatters,RishabhKabra,IrinaHiggins,Matt Botvinick,andAlexanderLerchner. Monet: Unsupervisedscenedecompositionandrepresenta- tion. arXivpreprintarXiv:1901.11390,2019. URL [35] ThomasKipf,GamaleldinFElsayed,AravindhMahendran,AustinStone,SaraSabour,Georg Heigold,RicoJonschkowski,AlexeyDosovitskiy,andKlausGreff. Conditionalobject-centric learningfromvideo. arXivpreprintarXiv:2111.12594,2021. [36] Maximilian Seitzer, Max Horn, Andrii Zadaianchuk, Dominik Zietlow, Tianjun Xiao, Carl- Johann Simon-Gabriel, Tong He, Zheng Zhang, Bernhard Sch\u00f6lkopf, Thomas Brox, et al. Bridgingthegaptoreal-worldobject-centriclearning. arXivpreprintarXiv:2209.14860,2022. [37] GautamSingh,Yi-FuWu,andSungjinAhn. Simpleunsupervisedobject-centriclearningfor complex and naturalistic videos. Advances in Neural Information Processing Systems, 35: 18181\u201318196,2022. [38] JindongJiang,FeiDeng,GautamSingh,andSungjinAhn. Object-centricslotdiffusion. arXiv preprintarXiv:2303.10834,2023. [39] WhieJung,JaehoonYoo,SungjinAhn,andSeunghoonHong. Learningtocompose:Improving objectcentriclearningbyinjectingcompositionality. arXivpreprintarXiv:2405.00646,2024. 12 --- Page 13 --- [40] JinwooKim,JanghyukChoi,JaehyunKang,ChangyeonLee,Ho-JinChoi,andSeonJooKim. Leveragingimageaugmentationforobjectmanipulation: Towardsinterpretablecontrollability inobject-centriclearning. arXivpreprintarXiv:2310.08929,2023. [41] AlexanderRubinstein,AmeyaPrabhu,MatthiasBethge,andSeongJoonOh. Arewedonewith object-centriclearning? arXivpreprintarXiv:2504.07092,2025. [42] WeiYuenTeh,ChernHongLim,MeiKuanLim,andIanKTTan. Towardsdiscreteobject representationsinvisiontransformerswithtensorproducts. InProceedingsofthe20237th International Conference on Computer Science and Artificial Intelligence, pages 190\u2013194, 2023. [43] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. Advancesinneuralinformationprocessingsystems,30,2017. [44] JianingQian,AnastasiosPanagopoulos,andDineshJayaraman. Recastinggenericpretrained visiontransformersasobject-centricsceneencodersformanipulationpolicies. In2024IEEE International Conference on Robotics and Automation (ICRA), pages 17544\u201317552. IEEE, 2024. [45] YangtaoWang,XiShen,YuanYuan,YumingDu,MaomaoLi,ShellXuHu,JamesLCrowley, andDominiqueVaufreydaz. Tokencut: Segmentingobjectsinimagesandvideoswithself- supervisedtransformerandnormalizedcut. IEEEtransactionsonpatternanalysisandmachine intelligence,45(12):15790\u201315801,2023. [46] Adam Bielski and Paolo Favaro. Move: Unsupervised movable object segmentation and detection. AdvancesinNeuralInformationProcessingSystems,35:33371\u201333386,2022. [47] Jian Ding, Enze Xie, Hang Xu, Chenhan Jiang, Zhenguo Li, Ping Luo, and Gui-Song Xia. Deeplyunsupervisedpatchre-identificationforpre-trainingobjectdetectors.IEEETransactions onPatternAnalysisandMachineIntelligence,46(3):1348\u20131361,2022. [48] MariaMihaelaTrusca,WolfNuyts,JonathanThomm,RobertHonig,ThomasHofmann,Tinne Tuytelaars,andMarie-FrancineMoens. Object-attributebindingintext-to-imagegeneration: Evaluationandcontrol. arXivpreprintarXiv:2404.13766,2024. [49] TaihangHu,LinxuanLi,JoostvandeWeijer,HongchengGao,FahadShahbazKhan,JianYang, Ming-MingCheng,KaiWang,andYaxingWang. Tokenmergingfortraining-freesemantic bindingintext-to-imagesynthesis. AdvancesinNeuralInformationProcessingSystems,37: 137646\u2013137672,2024. [50] BoshiWangandHuanSun. Isthereversalcurseabindingproblem? uncoveringlimitationsof transformersfromabasicgeneralizationfailure. arXivpreprintarXiv:2504.01928,2025. [51] DeclanCampbell,SunayanaRane,TylerGiallanza,CamilloNicol\u00f2DeSabbata,KiaGhods, AmoghJoshi,AlexanderKu,StevenFrankland,TomGriffiths,JonathanDCohen,etal. Under- standingthelimitsofvisionlanguagemodelsthroughthelensofthebindingproblem.Advances inNeuralInformationProcessingSystems,37:113436\u2013113460,2024. [52] JiahaiFengandJacobSteinhardt. Howdolanguagemodelsbindentitiesincontext? arXiv preprintarXiv:2310.17191,2023. [53] QinDai,BenjaminHeinzerling,andKentaroInui. Representationalanalysisofbindinginlarge languagemodels. arXive-prints,pagesarXiv\u20132409,2024. [54] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai, ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,etal. Animageisworth16x16words: Transformersforimagerecognitionatscale. arXivpreprint arXiv:2010.11929,2020. [55] GuillaumeAlainandYoshuaBengio. Understandingintermediatelayersusinglinearclassifier probes. arXivpreprintarXiv:1610.01644,2016. 13 --- Page 14 --- [56] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Sceneparsingthroughade20kdataset. InProceedingsoftheIEEEconferenceoncomputer visionandpatternrecognition,pages633\u2013641,2017. [57] OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,SanjeevSatheesh,SeanMa,Zhiheng Huang,AndrejKarpathy,AdityaKhosla,MichaelBernstein,etal. Imagenetlargescalevisual recognitionchallenge. Internationaljournalofcomputervision,115(3):211\u2013252,2015. [58] KihoPark,YoJoongChoe,andVictorVeitch. Thelinearrepresentationhypothesisandthe geometryoflargelanguagemodels. arXivpreprintarXiv:2311.03658,2023. [59] ShirAmir,YossiGandelsman,ShaiBagon,andTaliDekel. Deepvitfeaturesasdensevisual descriptors. arXivpreprintarXiv:2112.05814,2(3):4,2021. [60] MichaelJArcaro,StephanieAMcMains,BenjaminDSinger,andSabineKastner. Retinotopic organizationofhumanventralvisualcortex. Journalofneuroscience,29(34):10638\u201310652, 2009. [61] L.G.UngerleiderandL.Pessoa. Whatandwherepathways. Scholarpedia,3(11):5342,2008. doi: 10.4249/scholarpedia.5342. revision#91940. [62] VladislavAyzenbergandMarleneBehrmann. Thedorsalvisualpathwayrepresentsobject- centeredspatialrelationsforobjectrecognition. JournalofNeuroscience,42(23):4693\u20134710, 2022. [63] NikhilaRavi,ValentinGabeur,Yuan-TingHu,RonghangHu,ChaitanyaRyali,TengyuMa, HaithamKhedr,RomanR\u00e4dle,ChloeRolland,LauraGustafson,etal.Sam2:Segmentanything inimagesandvideos. arXivpreprintarXiv:2408.00714,2024. [64] DanielKahneman,AnneTreisman,andBrianJGibbs. Thereviewingofobjectfiles: Object- specificintegrationofinformation. Cognitivepsychology,24(2):175\u2013219,1992. [65] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez, \u0141ukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformation processingsystems,30,2017. 14 --- Page 15 --- A Appendix A.1 ExperimentalSetup DatasetandPreprocessing. FollowingtheDINOv2standardsetup,weusetheADE20Kdataset withimagesresizedandcroppedto512\u00d7512pixels,thenpaddedto518\u00d7518pixels. Weemploya patchsizeof14\u00d714,resultinginatotalof1,369patchesperimage. Allcomputationsareperformed usingfloat32precisiononaNVIDIARTX4090GPU. Training Configuration. We use the Adam optimizer with a learning rate of 0.001 and a step learningrateschedulerwithstepsizeof8epochsandgammadecayfactorof0.2. Allprobesaretrainedfor16epochswithabatchsizeof256. Foreachsampleimage,werandomly select64patchesandtrainwithsupervision. Forpairwiseprobes,weapplysupervisionovertheuppertriangularportionofthe64\u00d764pairwise matrix. Themodelisoptimizedusingbinarycross-entropyloss. Forpointwiseprobes,weconsidertwotasks: \u2022 Semantic Segmentation: We use standard cross-entropy loss for pixel-level object class classification. \u2022 InstanceSegmentation: FollowingtheDETRframework,weemployHungarianmatching forobjectassignment. Thetotallossiscomputedas: L =L +\u03bb L +\u03bb L (3) total cls mask mask dice dice whereL isreplacedwithaweightedbinarycross-entropylossdistinguishingbetween cls objectandno-objectclasses: L =\u2212w log(p )\u2212w log(1\u2212p ) (4) cls obj obj no-obj obj The hyperparameters follow the DETR configuration: no_object_weight = 0.1, mask_weight (\u03bb ) = 5.0, dice_weight (\u03bb ) = 5.0, and num_object_queries = mask dice 100. A.2 ProbePerformance We train and evaluate quadratic probes on various ViTs including the complete DINOv2 family (small,base,large,giant)andCLIP-L14. Allmodelsutilizeapatchsizeof14\u00d714pixels. CLIP-L/14 processes224\u00d7224inputimages,resultingin256patchesperimage(16\u00d716grid). Overall,weobservethatDINOv2modelsachieveconsistenthighclassificationaccuracyat88%, andprogressalonglayers,whileCLIPmodelachievesalower84.8%anddisplaysnosuchgrowing trend. DINOmodelsseemtohaveobjectbindingbetterthanCLIP,althoughitispossiblethatless fine-grainedgridleadstoworseIsSameObject,sinceforeachpatch,thesemanticsareentangled. A.3 Cross-LayerBinding Cross-layerbindingmayactivelyhappenundertheseconditions: \u2022 Layer-specific information processing: ViTs represent different types of information at differentlayers. Althoughtheself-attentionimplementsa\"fullrange\"interaction(alltokens toalltokensinteraction), themodellearnstolimitthe\"interaction\"betweenthecorrect tokensandintegratefeaturesoftheobjecttheybelongtoaswegodeeper \u2022 Informationretrievalfromearlierlayers: ViTsmayaccessinformationfromearlierlayers whencurrentlayerslacknecessarybindingcues. AsshowninAppendixA.4.3positional informationisgraduallyremovedasthenetworkdeepens,andotherlow-levelfeaturesmay alsobediscarded. Thisinformationlossmaytriggercross-layerretrievalasacomputational strategy, which rather than maintaining all binding-relevant information in every layer, themodelefficientlyaccesseswhatitneedsfromearlierrepresentations. Thishypothesis requiresdeeperanalysisinfutureresearch. 15 --- Page 16 --- Figure7: Layer-wiseIsSameObjectclassificationaccuracyacrossvisiontransformerarchitec- tures. DINOv2modelsconsistentlyachieve 88%accuracy,demonstratingrobustdevelopmentof IsSameObjectrepresentationsacrossallmodelsizes. CLIP-L/14reaches 85%accuracyandperforms closertothebaselinelevel. Cross-layerinteractionsinViTscanoccurviatheresidualskipconnections,sowealsotrainquadratic probes between non-adjacent layers. Specifically, we compute phi(x,y) = xTWTW2y, where 1 W and W are the learned projection matrices from layer 15 and layer 18, respectively. The 1 2 probeaccuraciesare: layer15: 89.0%,layer18: 90.1%,layer15-layer18: 83.3%,showingdecent cross-layerobjectbinding,andacertaindegreeofobjectbindingacrossthesystem. A.4 PositionalInformation A.4.1 PositionalEncodingDistinguishesIdenticalObjects Asingletransformerencoderlayercanbeviewedascomprisingtwocomplementarytypesofcompu- tations: token-wise(i.e.,position-wise)operations,whichactlocallyonindividualtokensandcan beexecutedinparalleltoextractfeaturesandshort-rangeinteractions;andcross-tokenoperations, implementedthroughscaled-dot-productattention,whichenableslong-rangeinteractionsbyinte- gratingcontextualinformationacrossalltokens. Wheninspectingthemathematicalformulation, wecanalsoshowthatforidenticaltokens(i.e.,identicalpatches),positionalencodingistheonly informationthatcanguidethecross-tokeninteractions. Herewereviewtheoperationsforatrans- formerencoderwithasingleheadfrom[65]inorder. Forsimplicity,wealsoassumethatquery-key vectorshavethesamedimensionasthemodel(i.e.,d =d). Weusebluecolorforthetoken-wise k operationsandredcolorforcross-tokenoperations. Forasequenceofinputtokenst \u2208Rk where i k =n-channels\u00d7patch-height\u00d7patchwidth: 16 --- Page 17 --- pre-processing: embedding:e =t W i i E addingpositionembedding:x =e +p i i i encoderlayer: Query-Key-Value:q =x W , k =x W , v =x W i i Q i i K i i V (cid:18) Q(K)\u22ba(cid:19) self-attention:U=softmax \u221a V d projectionMLP:y =u W i i O residualconnection:y =x +y i i i normalization:z =LayerNorm(y ) i i feed-forwardnetwork:z =ReLU(z W +b )W +b2 i i 1 1 2 residualconnection:z =z +y i i i where W \u2208 Rk\u00d7d is the embedding layer, W \u2208 Rd\u00d7d, W \u2208 Rd\u00d7d, and WV \u2208 Rd\u00d7d are E Q K the Query, Key, and Value layers, W \u2208 Rd\u00d7d is the linear projection layer, and W \u2208 Rd\u00d7m, O 1 W \u2208Rm\u00d7darethefeed-forwardweights. 2 Our goal is to show that for two identical tokens (i.e., two patches with identical features), the transformerhastousethepositiontaggingascueforbinding. Sincemostoperationsaretoken-wise (positionagnostic),weonlyneedtoshowtheresultsfortheself-attentionoperation.Wewillshowthat iftwoinputtokensareidenticalwithnopositionalembedding(orwithequalpositionalembedding), thenduetothesymmetryoftheattentionmechanism,theiroutputvectorsafterself-attentionwillbe identical. Formally,ift =t i\u0338=j andp =p wewanttoshowthatu =u . i j i j i j Assumingt =t andp =p : i j i j x =t W +p =t W +p =x i i E i j E j j ifx =x then: i j q =q , k =k , v =v i j i j i j Thustheattentionscorecomputedbyq andq againstallkeyswouldbethesame: i j \u22ba \u22ba q k =q k \u2200n i n j n Sotheattentionweights(aftersoftmax)forrowsiandj arethesame: (cid:18) q K\u22a4(cid:19) a =a \u2200n where:a =softmax \u221ai i,n j,n i d k AndsincethevaluesVarethesameacrossallinputsforthesamex ,theweightedsumofvalues n willalsobeidentical: N N (cid:88) (cid:88) u = a v = a v =u i i,n n j,n n j n=1 n=1 A.4.2 QuantifyingtheDegreeofDistinguishingIdenticalObjects Here,weuseasimplifiedformofourproposedtoyexamplecontainingtwoidenticalcarsandone redboat. We quantify the model\u2019s ability to distinguish identical objects by examining the kernel density estimationofIsSameObjectscoresbetweenpatchpairsfromthesameobject(CarA,CarB,Boat,or Background)acrosslayersinDINOv2-LargeinFigure8. Ideally,patchpairsfromthesameobjectshouldachieveIsSameObjectscoresapproaching1.0. In earlylayers,thedistributionsclusteraround0.5,indicatingthemodelcannotreliablydistinguish 17 --- Page 18 --- Figure8: Layer-wisevisualizationofIsSameObjectpredictionsonthetestimage(twoidentical redcarsandoneredboat.) same-objectfromdifferent-objectpatchpairs. Asprocessingprogressesthroughlaterlayers,these distributionsshifttoward1.0. However,somesame-objectpatchpairscontinuetoscorenear0.0even indeeperlayers,representingindistinguishabletokenpairs. WealsoanalyzedpatchpairsfromdifferentobjectsinFigure10,whereweexpectIsSameObject scorestoapproach0.0. Inlayersbefore12,thedistributionscorrectlyclusternear0.0,showingthe modelcandistinguishdifferentobjects. However,asthemodellearnstogrouppatcheswithinthe sameobject(asshowninthepreviousanalysis), itsimultaneouslylosesitsabilitytotellthetwo identicalcarsapart. Thistrade-offisvisibleintheCar1-Car2distribution,whichgraduallyshifts upwardthroughthelayersanddevelopsastrongpeakat1.0bythefinallayer. A.4.3 PositionInformationDecay Wehypothesizethatthetransitionfrommiddlelayers\u2019capacitytodistinguishidenticalobjectstolater layers\u2019failurestemsfromthegradualdiffusionofprecisepositionalinformationintomoreglobal, semantically-focusedrepresentations. Totestthishypothesis,wetrainedlinearprobestodecodethe (x,y)coordinatesofeachpatchfromthemodel\u2019sinternalrepresentations(Figure11). Weobservea markedincreaseinprobeRMSEatlayer21,whichsupportsourhypothesis. A.5 Attentionweights(query-keysimilarity)vs. IsSameObject Weinvestigatetherelationshipbetweenattentionmechanismsandobjectidentityrepresentationsby comparingattentionweightswithIsSameObjectscores. Attentionweightsarecomputedas: (cid:32) (cid:33) Q KT Attention =softmax \u221ai j (5) ij d k whereQ andK representthequeryandkeyvectorsforpatchesiandj,respectively,andd isthe i j k keydimension. WethencomputethePearsoncorrelationbetweenattentionweightsatlayer\u2113+1andIsSameObject scoresderivedfromthequadraticprobeatlayer\u2113: \u03c1=corr(cid:0) Attention(\u2113+1),IsSameObject(\u2113)(cid:1) . (6) ij ij Usingthesimplifiedtwo-carscenariofromFigure8, weexaminehowattentionweightsatlayer \u2113+1correlatewithIsSameObjectscoresatlayerl. Inearlylayers,weobserveminimalcorrelation 18 --- Page 19 --- Figure9: Bindingstrengthenswithdepthforsame-objectpatchpairs. Kerneldensityestimation ofIsSameObjectscoresforpatchpairswithinthesameobjectacrossdifferentlayers. betweenthesetwomeasures,whichislikelybecauseIsSameObjectrepresentationhasnotyetfully developed. Indeeperlayers,certainpatchpairsreceivehighattentionweightsdespitehavinglowIsSameObject scores(indicatingthemodelbelievestheybelongtodifferentobjects). Thisphenomenonmaybe explainedbybackgroundpatchesbeingrepurposedforinternalcomputationalprocesses,asidentified inpriorworkonDINOregistertokens.Futureresearchcouldfurtherinvestigatehowthesespecialized backgroundpatchescontributetoobjectrepresentationandtheirroleinmaintainingdistinct\u201cobject files\u201d. ThePearsoncorrelationsinFig.6arestatisticallysignificant(p<0.001underpermutationtest). A.6 ImplementationofAblationStudies. Weconductablationexperimentsatlayer18ofDINOv2-Large,whereIsSameObjectrepresentation achievesthebestdecodability.Weapplybothuninformedandinformedablationmethodsasdescribed inSection3.3. SegmentationEvaluation. Forbothsemanticandinstancesegmentationtasks,weretrainlinear segmentation heads with ablated representations. The uninformed ablation randomly permutes 19 --- Page 20 --- Figure10: Identicalobjectscollapseinrepresentationatdeeperlayers. Kerneldensityestimation ofIsSameObjectscoresforpatchpairsfromdifferentobjectsacrosslayers. bindingvectorsacrosspatches,whiletheinformedablationinjectsobject-averagedbindingvectors usingground-truthmasks. Theselinearheadsuseidenticalconfigurationstothepointwiseprobes describedinSectionA.1,whichareeffectivelypointwiseprobesappliedtothefinaltransformerlayer. DINOLossEvaluation. Toassesstheimpactonthepretrainingobjective,weevaluateDINOloss using the pretrained model as both student and teacher networks. For computational simplicity, weexcludetheiBOTandKoLeolosscomponentsfromthisanalysis. Notethatinformedablation cannotbeevaluatedunderDINOloss,astheuseoflocalcropsaltersthepatchdivisions,making object-averagedbindingvectorsundefinedforthecroppedregions. 20 --- Page 21 --- Figure11: Positionalinformationdecaysinlaterlayers. Layer-wisedecodingperformancefor patch(x,y)coordinates,comparedwiththesuccessrateofdistinguishingpatchesfromCarAandCar B. Figure12: Attentionvs.IsSameObjectinearlylayers. Scatterplotscomparingattentionweights toIsSameObjectscoresforpatchpairs;correlationisstillweak,indicatingthatbindinghasnotyet developedsufficientlytoinfluenceattention. Figure13: Attentionvs.IsSameObjectinlaterlayers. Attentionissometimesallocatedtolow- IsSameObjectbackgroundtokens,suggestingthesetokensmightberepurposedforinternalcomputa- tion. 21"
}