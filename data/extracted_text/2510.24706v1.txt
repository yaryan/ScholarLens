--- Page 1 ---  COMBOBENCH: CAN LLMS MANIPULATE PHYSI- CAL DEVICES TO PLAY VIRTUAL REALITY GAMES? ShuqingLi1 JiayiYan1∗ ChenyuNiu1∗ Jen-tseHuang1 YunPeng1 WenxuanWang1† YepangLiu2 MichaelR.Lyu1 1ChineseUniversityofHongKong 2SouthernUniversityofScienceandTechnology ABSTRACT VirtualReality(VR)gamesrequireplayerstotranslatehigh-levelsemanticactions into precise device manipulations using controllers and head-mounted displays (HMDs).Whilehumansintuitivelyperformthistranslationbasedoncommonsense andembodiedunderstanding,whetherLargeLanguageModels(LLMs)caneffec- tivelyreplicatethisabilityremainsunderexplored. Thispaperintroducesabench- mark,ComboBench,evaluatingLLMs’capabilitytotranslatesemanticactionsinto VRdevicemanipulationsequencesacross262scenariosfromfourpopularVR games:Half-Life:Alyx,IntotheRadius,Moss:BookII,andVivecraft.Weevaluate sevenLLMs,includingGPT-3.5,GPT-4,GPT-4o,Gemini-1.5-Pro,LLaMA-3-8B, Mixtral-8x7B,andGLM-4-Flash,comparedagainstannotatedgroundtruthandhu- manperformance.Ourresultsrevealthatwhiletop-performingmodelslikeGemini- 1.5-Prodemonstratestrongtaskdecompositioncapabilities,theystillstrugglewith proceduralreasoningandspatialunderstandingcomparedtohumans. Performance variessignificantlyacrossgames,suggestingsensitivitytointeractioncomplexity. Few-shot examples substantially improve performance, indicating potential for targeted enhancement of LLMs’ VR manipulation capabilities. We release all materialsat 1 INTRODUCTION Large Language Models (LLMs) have demonstrated remarkable proficiency in general-purpose tasksolving(Qinetal.,2023),conqueringcomplexdomainssuchascode(Leeetal.,2024;Lam etal.,2025)ormath(Luetal.,2024)problems. Whiletheyexhibitincreasinglymorehuman-like characteristics(Huangetal.,2024;Liangetal.,2023),anessentialattributeofhumanintelligenceis stillunderexplored: theabilitytorapidlylearnandapplyunfamiliarconceptsbyleveragingcommon sense,priorexperiences,andarepertoireofcognitiveskills. Thisisparticularlyevidentinnovelinteractiveenvironmentslikevideogames,whereplayersquickly masterdevicemanipulations(atomicactions)andcombinethemtoachievecomplexsemanticgoals. VirtualReality(VR)gameselevatethischallenge. Theydemandnotonlytheexecutionofatomic actionsviaphysicaldevices(e.g., Head-MountedDisplays(HMDs)andcontrollers)butalsothe inferenceofcomplex,oftenuninstructed,semanticactions. Forinstance,inHalf-Life: Alyx(Valve, 2020),whenaskedto“surrender,”playersmightinstinctivelyraisetheircontroller-heldhandsevenif notexplicitlytaught. Such translation of high-level intent into a sequence of physical device manipulations engages a suiteofcognitiveabilities: (1)Taskdecomposition: Breakingdownahigh-levelsemanticaction (e.g.,“tamethehorse”and“plantwheat”)intoacoherentseriesofintermediatesteps. (2)Procedural reasoning: Understanding the logical and temporal order of these steps, including prerequisite conditionsorconcurrentactions(e.g.,theneedtotillsoilbeforeplantingseeds).(3)Spatialreasoning &contextualawareness: Interpretinginstructionswithina3Dspatialcontext(e.g.,“moveHMD towardstheCreeper”and“crouchthroughthegap”)andunderstandingenvironmentalcuesorobject ∗Equalcontributions. †Correspondingauthor. 1 5202 tcO 82 ]LC.sc[ 1v60742.0152:viXra --- Page 2 ---  states(e.g.,recognizingadoorisopen/closedandactingaccordingly). (4)Objectinteraction&tool useunderstanding: Correctlymappingintendedsub-actionstospecificVRdevicemanipulations (e.g.,knowingwhichbuttontopressto“use”anitem,andhowtomanipulateacontrollertosimulate “swinging” a tool like a pickaxe). This involves understanding the affordances of virtual objects andtools. (5)Motoractionmapping&VRproceduraltransfer: Translatingabstractactions(e.g., “press,”“move,”and“trigger”)intospecific,executableVRcontrollercommands,potentiallyby adaptingfromprovidedexamplesorgeneralknowledgeofVRinteractionparadigms. Thistouches uponaformofsimulatedembodiedreasoning. (6)Judgmentoftermination/continuationconditions: Recognizingwhenasub-taskoraloopedactioniscomplete(e.g.,“mineuntiltheblockbreaks”and “wateruntiltheplantgrows”). Therefore,playingVRgamesservesasarichtestbedforevaluatingif LLMscanbridgethisgapbetweenabstractunderstandingandgrounded,physicalinteraction. To systematically evaluate LLMs’ ability to perform this crucial translation, we introduce Com- boBench,whichstandsforCognitive-OrientedManipulationBenchmarkforgamecombosusing physicalVRdevices.Itcomprises262scenariosderivedfromfourpopularVRgames:Vivecraft(Vive- craft,2013)(MinecraftinVR),Half-Life: Alyx(Valve,2020),Moss: BookII(Polyarc,2022),and IntotheRadius(CMGames,2019). Eachscenariopresentsahigh-levelsemanticaction, andthe groundtruthconsistsofafine-grainedsequenceofVRdevicemanipulationsrequiredtoachieveit. ThesesequencesareannotatedbyexperiencedVRplayers,allowingustoanalyzeLLM-generated outputsatthestep-levelandmaptheirsuccessesandfailurestotheaforementionedcognitiveabilities. Forexample,failingto“presstheXbutton”after“movingtheHMDtowardstheCreeper”might indicatealapseinproceduralreasoningorobjectinteractionunderstandingforthatspecificstep. WeevaluatesevenLLMs,includingGPT-3.5(OpenAI,2022),GPT-4(OpenAI,2023),GPT-4o(Hurst etal.,2024),Gemini-1.5-Pro(Teametal.,2024),LLaMA-3-8B(Grattafiorietal.,2024),Mixtral- 8x7B(Jiangetal.,2023),andGLM-4-Flash(GLMetal.,2024). Wedesignamulti-dimensional scoringapproachthatassesses: (1)high-levelsemanticactionunderstanding,(2)proceduralstep correctness, and (3) device-specific manipulation accuracy, allowing for fine-grained analysis of whereeachmodelsucceedsorstrugglesinthetranslationprocess. Ourfindingsrevealsignificant variationinmodelperformanceacrosscognitivecapabilities. Allmodelsdemonstratestrongtask decompositionabilitiesbutshowpronouncedweaknessesinmotoractionmappingandprocedural reasoning. Gemini-1.5-Proexhibitsthemostbalancedperformanceacrosscapabilities,whileeven advanced models like GPT-4 struggle with spatial reasoning compared to human performance. Few-shotexamplessubstantiallyimproveoutcomes,particularlyforproceduralunderstanding,with diminishingreturnsbeyondthreeexamples. Performancealsovariesconsiderablyacrossgames, withmodelsgenerallyperformingbetterinenvironmentswithmoreconsistentinteractionpatterns (Vivecraft)thanthoserequiringnuancedcontrollermanipulations(Half-Life: Alyx). Theseresults highlightspecificcognitivegapsincurrentLLMs’abilitytoperformsimulatedembodiedreasoning forVRinteractionsandidentifytargetedareasforimprovementtowardmorecapablevirtualagents. Ourcontributionsare: • WeintroduceComboBench,thefirstbenchmarkdesignedtoevaluateLLMs’fine-grainedcognitive abilitiesintranslatinghigh-levelsemanticactionsintoVRdevicemanipulations,comprising262 human-annotatedscenariosfromfourdiverseVRgames. • WedefineasetofkeycognitiveabilitiescrucialforVRinteractionanddesignComboBenchto enablestep-levelanalysisofLLMperformanceagainstthesedimensions. • Weconductacomprehensiveevaluationofsixstate-of-the-artLLMs,providinganuancedanalysis oftheirstrengthsandweaknessesacrossthesecognitiveabilitiesandofferinginsightsintothe currentfrontiersofLLM-drivenVRinteraction. 2 COMBOBENCH: DESIGN AND CURATION TheComboBenchdatasetismeticulouslycuratedtoevaluatethecapabilityofLLMsintranslating high-levelsemanticactionsintosequencesofphysicalVRdevicemanipulations. Thissectiondetails thegameselectionprocess,scenariodefinition,andtheannotationmethodology. 2 --- Page 3 ---  2.1 COGNITIVECAPABILITYTAXONOMYDEVELOPMENT ToestablishatheoreticallygroundedframeworkforevaluatingLLMsinVRcontexts,weconducted structuredinterviewswiththreedomainexpertsspecializingincognitivescienceandeducational psychology. The experts were selected based on their research backgrounds in spatial cognition, procedurallearning,andembodiedinteraction,whichareasthatarehighlyrelevanttoVRinteraction. ExpertInterview. Eachexpertparticipatedina90-minutesemi-structuredinterviewfocusedon identifyingandcategorizingthecognitiveabilitiesrequiredfortranslatingsemanticgoalsintophysical actionsinvirtualenvironments. Theinterviewsfollowedathree-phasestructure: (1)open-ended discussionaboutcognitiveprocessesinVRinteraction,(2)systematicreviewofpreliminarycapability categoriesidentifiedfromliterature,and(3)expertsuggestionsforrefinementandexpansionofthese categories. TaxonomyRefinement. Followingtheinterviews,wesynthesizedtheexperts’insightsthrough thematicanalysis. Areasofconsensusweredirectlyincorporatedintoourtaxonomy,whiledivergent perspectiveswerereconciledthroughfollow-upconsultations. Thisiterativeprocessresultedinthe identificationofsixcorecapabilitydimensionsthatcomprehensivelycapturethecognitivedemandsof VRinteraction: (1)Taskdecomposition: Theabilitytobreakdownhigh-levelgoalsintosequentially orderedsub-tasks. (2)Proceduralreasoning: Understandingcausalrelationshipsbetweenactions andtheirtemporaldependencies. (3)Spatialreasoning&contextualAwareness: Processingspatial relationshipsandinterpretingenvironmentalcuesforactionselection. (4)Objectinteraction&tool use understanding: Comprehending affordances and functional properties of virtual objects. (5) Motoractionmapping&VRproceduraltransfer:Translatingconceptualactionsintospecificphysical devicemanipulations. (6)Judgmentoftermination/continuationconditions: Recognizingcompletion statesorconditionsrequiringrepeatedaction. 2.2 GAMESELECTIONCRITERIAANDPROCESS To ensure a diverse and relevant set of VR interaction paradigms, we selected games based on a systematicprocess. First,wequeriedtheSteamstore(web,2023)filteringfortitlestaggedas“VR Only”andavailablein“English,”sortingtheresultsbyuserreviewscoresindescendingorder. We theniterativelyexaminedgamesfromthisrankedlist,focusingontheirprimarygenreascategorized bySteam. Toensuregenrediversity,weprioritizedgamesfromgenresnotyetrepresentedinour collection. Acrucialselectioncriterionwastheavailabilityofcomprehensivetextualwalkthroughs. Foreachcandidategame,wesearchedfordetailedguidesusingkeywordssuchas“walkthrough,” “guide,”or“tutorial.” Awalkthroughwasdeemedsufficientlydetailedifitprovidedunambiguous, step-by-step instructions enabling the completion of core game objectives or specific complex tasks. Followingthismethodology, weselectedfourpopularandcriticallyacclaimedVRgames representingdistinctgenresandinteractionstylesforComboBench: (1)Vivecraft(Vivecraft,2013) (Open-world sandbox, crafting) (2) Half-Life: Alyx (Valve, 2020) (First-person shooter, puzzle- solving,physics-basedinteraction)(3)Moss:BookII(Polyarc,2022)(Third-personaction-adventure, puzzle-platformer)(4)IntotheRadius(CMGames,2019)(First-personsurvivalshooter,exploration) SuchselectionprovidesarichvarietyofcontrolschemesandtaskcomplexitiesforevaluatingLLMs. 2.3 SCENARIODEFINITION: SEMANTICACTIONIDENTIFICATION Forallselectedgames,eightdataannotators,comprisingundergraduateandpostgraduatecomputer sciencestudentswithatleasttwoyearsofprogrammingexperienceandsufficientknowledgeabout VR games, manually identified salient semantic actions from the collected textual walkthroughs. Semanticactionsweredefinedashigh-level,goal-orientedtasksdescribedinthewalkthroughs(e.g., “tamethehorse,” “killthecreeper,” “solvethegravityglovepuzzle”)thatnecessitateasequence offine-grainedVRdevicemanipulationstoaccomplish. Wefocusedonscenariosthat: (1)involve complexinteractionsnotalwaysexplicitlydetailedinin-gametutorials,(2)oftenconstituteessential stepsorobjectivesrequiredforgameprogression. Thisprocessresultedintheidentificationof262 distinctscenariosacrossthefourgames. 3 --- Page 4 ---  2.4 ANNOTATIONOFVRDEVICEMANIPULATIONS ExperiencedVRusersfromourannotationteamthenplayedthrougheachidentifiedsemanticaction intherespectivegamesusingOculusQuest2VRhardware. Theobjectivewastorecordtheprecise sequenceofdevicemanipulationsrequiredtocompleteeachsemanticaction. Theannotationprocess capturedthefollowingdetailsforeachstepwithinamanipulationsequence: • Deviceused: SpecificationofwhethertheHMDoracontrollerwasused. • Controllerspecificity: Ifacontrollerwasused,andtheactionwashand-specific(e.g.,primary handforatool),theannotationindicatedwhethertheleftorrightcontrollerwasrequired. Ifeither controllercouldperformtheaction,thiswasnotedas“leftorrightcontroller.” • Operationtypeandparameters: – Movement: Foractionsinvolvingdevicemovement(HMDorcontroller), thedirection(e.g., “towardstheCreeper,”“upwards”)ortargetpositionwasrecorded. – Buttonpresses: Thespecificbuttoninvolvedandtheaction(e.g., “pressXbutton,” “release trigger”)werenoted. – Joystick/thumbstickmanipulation: Thedirectionofjoystickpush(e.g.,“pushleftthumbstick forward”)wasrecorded. • Sequentialcomposition: Forcomplexsemanticactionscomposedofmultiple,distinctsub-actions thatmighthavebeenannotatedindividually,thesequenceandcompositionofthesesimpleractions wereexplicitlyrecorded. 2.5 COGNITIVECAPABILITYLABELINGUSINGLLMS A critical aspect of ComboBench is the annotation of each manipulation step with the specific cognitivecapabilitiesitengages. Thisfine-grainedlabelingenablespreciseanalysisofwhereLLMs succeedorfailintheVRinteractiontranslationprocess. InitialHumanAnnotation. Tobegin,ourannotatorsmanuallylabeledasubsetof50manipulation sequences(approximately20%ofthedataset),assigningrelevantcapabilitycategoriestoeachstep basedonthetaxonomydescribedinSection2.1. Forexample,inthesequencerequiredto"tamea horse"inVivecraft,thestep"equipthesaddlebypressingtheYbuttonwhilelookingattheinventory slotcontainingthesaddle"waslabeledwith"ObjectInteraction&ToolUseUnderstanding"and "MotorActionMapping." LLM-AssistedAnnotationPipeline. WethendevelopedanLLM-assistedannotationpipeline toscalethisprocesstotheentiredataset. Specifically: ①Weusedthehuman-annotatedexamples asfew-shotdemonstrationsforGPT-4o. ②Foreachunlabeledmanipulationstep,weprovidedthe LLMwith: [2.a]Thesemanticactioncontext(e.g.,"tamingahorseinVivecraft"). [2.b]Thespecific manipulationsteptolabel. [2.c]Theprecedingandfollowingsteps(whenavailable). [2.d]Detailed descriptionsofeachcapabilitycategory. [2.e]Threefew-shotexampleswithexplanationsofwhy eachcapabilitywasassigned. ③TheLLMgeneratedcapabilitylabelsalongwithjustificationsfor eachassignment. ④HumanannotatorsreviewedtheLLM-generatedlabels,makingcorrectionswhen necessary. Thereviewprocessrevealedan89.7%agreementratebetweenLLM-assignedlabelsand humanjudgments. Multi-label Distribution. Most manipulation steps engaged multiple cognitive capabilities si- multaneously. Onaverage,eachstepwasassociatedwith2.3capabilitycategories(σ =0.8). The mostfrequentlyco-occurringcapabilitieswere"MotorActionMapping"and"ObjectInteraction & Tool Use Understanding" (present together in 68% of steps), reflecting the inherent coupling betweenunderstandingvirtualobjectaffordancesandtranslatingthisunderstandingintophysical manipulations. 2.6 CONTEXTUALIZATIONANDVERIFICATION Tofurthercontextualizetheannotatedactionsandaidinverification,wesourcedorrecordedgameplay videos corresponding to the textual walkthroughs for each game. For each annotated semantic actionanditsconstituentmanipulationsteps,werecordedthecorrespondingtimestampsinthese videos. Thisallowsforvisualverificationoftheannotatedsequencesandprovidesrichercontextfor 4 --- Page 5 ---  understandingtheactions. Ifsuitablepublicgameplayvideosmatchingtheexactwalkthroughsteps wereunavailable,ourannotatorsrecordedtheirowngameplaysessionswhileperformingtheactions. 3 EXPERIMENTS 3.1 MODELSELECTION We evaluate six state-of-the-art LLMs: GPT-4o, GPT-4-turbo, GPT-3.5-turbo, Gemini-1.5-Pro, LLaMA-3-8B,andMixtral-8x7B.Wealsoperformhumanevaluationtovalidatetheaveragehuman capabilities for comparison, when humans are given exactly the same input as LLMs. For all experiments,weusedtheofficialAPIsforproprietarymodelsandHuggingFaceimplementations foropen-sourcemodels. Temperaturewassetto0acrossallmodelstominimizenon-deterministic outputs. Forembeddingcalculations,weutilizedOpenAI’stext-embedding-3-largemodelviatheir API. 3.2 EVALUATIONMETRICS TocomprehensivelyevaluatethecapabilityofLLMsintranslatingsemanticactionsintoVRdevice manipulations, we propose a multi-dimensional evaluation framework with four distinct metrics. ThesemetricscollectivelycapturedifferentaspectsofmodelperformanceinComboBench,ranging fromstrictmatchingtomoreflexiblesemanticalignment. Strict Step-by-Step Matching (SSM). Our first metric evaluates the exact matching between model-generatedandgroundtruthsteps,enforcingbothsequencelengthequivalenceandsemantic alignment: SSM= Numberofcorrectlypredictedsequences. Asequenceisconsideredcorrectlypredictedonly Totalnumberofsequences when: thenumberofstepsinthegeneratedsequenceequalsthatofthegroundtruth,andeverystep inthegeneratedsequencehasacosinesimilarityaboveathresholdof0.8387withitscorresponding stepinthegroundtruthThisstrictmetricservesasameasureofprecisioninreproducingexactdevice manipulationsequencesandrewardsmodelsthatcangeneratecomplete,step-accurateinstructions. CommonSubsequenceEvaluation. Wefurtherintroducetwocomplementarymetricsbasedon commonsubsequencealignmenttoassesspartialcorrectness:(1)NormalizedStepAlignmentScore (NSAS)Thismetricquantifiesthealignmentbetweenthemodel-generatedsequenceandgroundtruth whileaccountingformissingandadditionalsteps: NSAS = (|C|−|M|−|A|)−minall_samples,where: |C| |G|·(maxall_samples−minall_samples) representsthecountofcorrectlymatchedstepsinthecommonsubsequence,|M|representsmissing stepsfromthegroundtruth,|A|representsadditionalstepsgeneratedbythemodel,|G|represents thetotalnumberofstepsinthegroundtruth,min andmax representtheminimum all_samples all_samples and maximum raw scores across all evaluations, enabling consistent normalization This score is normalizedacrosstheentiredatasettoensurefaircomparisonacrossdifferentmodelsandscenarios. (2)SequentialOrderPreservation(SOP)TheSOPmetricspecificallyassessesthemodel’sability tomaintainthecorrectproceduralorderingofsteps: SOP= |Stepscorrectlyorderedandmatched|. Thismetric |G| evaluateswhetherthestepsinthematchedsubsequencemaintaintheirordinalpositions(e.g.,step1 followedbystep2,etc.) inboththegroundtruthandmodeloutput,capturingthemodel’sprocedural reasoningcapabilities. SemanticStepCoverage(SSC). Ourfinalmetricadoptsamoreflexiblematchingapproachto evaluate semantic coverage of critical actions: SSC = |MRstepsmatchedtoanyGTstep|, where a model |MR| result (MR) step is considered matched if it has a cosine similarity above the threshold (0.8387) withanystepinthegroundtruth(GT).Thismetriccomputestheproportionofgeneratedstepsthat semanticallyalignwithatleastonegroundtruthstep,regardlessofposition. 3.3 EXPERIMENTALRESULTS WeanalyzeandanswerthefollowingResearchQuestions(RQs): (RQ1)Howdostate-of-the-art LLMsperformintranslatingsemanticactionsintoVRdevicemanipulationsacrossdifferentVR games? (RQ2) How does the number of few-shot examples affect LLMs’ ability to execute this 5 --- Page 6 ---  Table1: OverallperformancecomparisonofLLMsacrossVRgames(5-shotsetting). Bestmodel performancepermetricisbolded,secondbestisunderlined. Half-Life:Alyx IntotheRadius Moss:BookII Vivecraft Model NSAS↑SOP↑F1SOP↑SSC↑NSAS↑SOP↑F1SOP↑SSC↑NSAS↑SOP↑F1SOP↑SSC↑NSAS↑SOP↑F1SOP↑SSC↑ GPT-3.5 0.858 0.123 0.287 0.143 0.662 0.169 0.226 0.137 0.782 0.169 0.207 0.186 0.922 0.043 0.098 0.067 GPT-4 0.853 0.125 0.258 0.172 0.693 0.189 0.328 0.177 0.824 0.218 0.336 0.220 0.927 0.137 0.437 0.081 GPT-4o 0.804 0.022 0.075 0.167 0.698 0.291 0.414 0.190 0.824 0.300 0.342 0.222 0.931 0.190 0.489 0.096 Mixtral 0.839 0.126 0.246 0.147 0.666 0.123 0.228 0.097 0.756 0.117 0.191 0.121 0.926 0.060 0.239 0.070 LLaMA-3 0.848 0.126 0.279 0.162 0.644 0.242 0.317 0.168 0.823 0.283 0.349 0.200 0.929 0.039 0.122 0.042 GLM-4 0.836 0.076 0.183 0.149 0.618 0.096 0.186 0.149 0.749 0.087 0.174 0.165 0.909 0.000 0.045 0.061 Human 0.845 0.090 0.240 0.110 0.684 0.148 0.257 0.181 0.817 0.112 0.328 0.174 0.935 0.122 0.482 0.084 Table2: OverallperformanceacrossVRgamesandsettings. Wereporttheaveragescoresforour fourevaluationmetrics: StrictStep-by-StepMatching(SSM),NormalizedStepAlignmentScore (NSAS),SequentialOrderPreservation(SOP),andSemanticStepCoverage(SSC).Higherisbetter forallmetrics. Boldindicatesbestmodelperformance,underlineindicatessecondbest. AverageAcrossSettings Zero-Shot 5-Shot Model SSM(%) NSAS SOP SSC SSM(%) NSAS SOP SSC SSM(%) NSAS SOP SSC GPT-3.5 1.4 0.781 0.063 0.066 0.8 0.771 0.003 0.046 2.1 0.791 0.128 0.095 GPT-4 3.7 0.806 0.107 0.124 1.0 0.788 0.015 0.107 8.8 0.825 0.184 0.140 GPT-4o 5.3 0.797 0.138 0.141 0.6 0.785 0.015 0.108 10.9 0.806 0.228 0.161 Gemini-1.5 5.8 0.813 0.146 0.142 2.1 0.795 0.010 0.124 11.7 0.832 0.236 0.162 Mixtral 1.1 0.784 0.068 0.079 0.0 0.777 0.002 0.040 2.2 0.796 0.105 0.107 LLaMA-3 1.2 0.787 0.088 0.111 0.1 0.783 0.011 0.088 1.8 0.794 0.163 0.132 GLM-4 0.0 0.761 0.038 0.077 0.0 0.762 0.006 0.052 0.0 0.765 0.071 0.120 Human 1.2 0.833 0.122 0.159 – – – – – – – – translation? (RQ3)DoLLMandhumanperformanceexhibitsignificantvariationsacrossthefour differentVRgames,potentiallyindicatingsensitivitytogamemechanicsandinteractioncomplexity? (RQ4)WhichcognitivecapabilitiesdocurrentLLMsexcelat,andwheredotheystruggle? (RQ5) HowdoLLMscomparetohumanperformanceinVRdevicemanipulationtasks? 3.4 RQ1&RQ3: LLMPERFORMANCEACROSSVRGAMES Tables1,2,and3presentcomprehensiveperfor- Table3: Cross-gameperformancevariation(stan- mancemetricsforallevaluatedLLMsacrossthe darddeviationacrossgames)w/5-shotexamples. fourVRgames. Ouranalysisrevealssubstan- Model NSASσ↓ SOPσ↓ F1 σ↓ GameGap↓ tialvariationsinmodelcapabilitiesandgame- SOP specificchallenges. Gemini-1.5-Proemergesas GPT-3.5 0.110 0.061 0.084 0.085 GPT-4 0.059 0.051 0.081 0.074 the strongest performer overall, achieving the GPT-4o 0.068 0.137 0.184 0.127 highestNSASscoresinthreeofthefourgames Gemini-1.5 0.099 0.093 0.127 0.095 (Half-Life: Alyx: 0.863,Moss: BookII:0.848, Mixtral 0.114 0.031 0.065 0.070 Vivecraft: 0.938), while maintaining compet- LLaMA-3 0.112 0.103 0.120 0.113 itive performance in Into the Radius (0.682). GLM-4 0.135 0.049 0.069 0.084 GPT-4odemonstratesparticularstrengthinInto Human 0.105 0.029 0.117 0.084 theRadiuswiththehighestSOPscore(0.291) andF1 (0.414),suggestingsuperiorproceduralreasoningcapabilitiesinthisspecificgamecontext. SOP GPT-4-turbo maintains consistently strong performance across all games, positioning itself as a reliablegeneral-purposemodelforVRinteractiontranslation. A striking pattern emerges in the SOP metrics, which vary dramatically across both models and games (0.00-0.30 range). While NSAS scores remain relatively high (mostly >0.75), indicating modelscanidentifyrelevantsteps,thelowSOPvaluesrevealfundamentaldifficultiesinmaintaining correcttemporalordering. ThisdiscrepancyisparticularlypronouncedinVivecraft,wheremodels achievehighNSASscores(0.909-0.938)butstrugglewithstepordering(SOP:0.00-0.19),suggesting thatsimplerinteractionpatternsmayparadoxicallyleadtooverconfidenceinstepsequencing. 6 --- Page 7 ---  Analysisofperformancevariations(Table3)revealssignificantgame-dependenteffects. Vivecraft exhibits the highest average performance across models (NSAS: 0.922-0.938), likely due to its consistentblock-basedinteractionparadigminheritedfromMinecraft. Incontrast,IntotheRadius presentsthegreatestchallenge,withnotablylowerNSASscores(0.618-0.698)andhighperformance variance. This pattern suggests that games featuring realistic physics simulations and complex inventorymanagementposeparticulardifficultiesforcurrentLLMs. Interestingly,differentmodelsexhibitdistinctstrengthsacrossgametypes.GPT-4oshowsremarkable adaptabilityinIntotheRadius(SOP:0.291)comparedtoothermodels,whilestrugglinginHalf-Life: Alyx(SOP:0.022). Gemini-1.5-Promaintainsthemostbalancedperformanceprofileacrossgames (GameGap: 0.095),suggestingmorerobustgeneralizationcapabilities. SmallermodelslikeMixtral- 8x7BandGLM-4-flashshowdisproportionateperformancedegradationincomplexenvironments, withGLM-4-flashachievingzeroSOPinVivecraftdespitereasonableNSASscores. Thesubstantial performancevariationsacrossgameshighlighttheimpactofinteractiondesignonLLMcapabilities. Gameswithdiscrete,well-definedactions(Vivecraft)enablehighermodelperformance,whilethose requiringnuancedcontrollermanipulationandspatialreasoning(Half-Life: Alyx,IntotheRadius) exposecurrentlimitations. Thecorrelationbetweengamecomplexityandperformancedegradation isnon-linear, moderatecomplexity(Moss: BookII)sometimesyieldsbetterresultsthansimpler environments,suggestingthatmodelsmaybenefitfromrichercontextualcuesincertainscenarios. These findings collectively demonstrate that while state-of-the-art LLMs have made significant progressinunderstandingVRinteractions,theirperformanceremainshighlysensitivetospecific game mechanics and interaction paradigms. The gap between high NSAS scores and low SOP valuesacrossallgamesindicatesthatcurrentmodelscanidentifyrelevantactionsbutstrugglewith theproceduralreasoningrequiredtosequencethemcorrectly,whichisanimportantcapabilityfor successfulVRinteraction. 3.5 RQ2: IMPACTOFFEW-SHOTEXAMPLES Table 4 demonstrates that few- Table 4: Effect of few-shot examples on model performance shot examples substantially im- (averageacrossallgames) proveLLMperformanceinVR Zero-shot 3-shot 5-shot devicemanipulationtasks,with Model NSAS↑ SOP↑ SSM↑ NSAS↑ SOP↑ SSM↑ NSAS↑ SOP↑ SSM↑ themostdramaticgainsobserved inSequentialOrderPreservation GPT-3.5 0.781 0.003 0.006 0.783 0.113 0.013 0.806 0.128 0.022 GPT-4 0.799 0.015 0.012 0.807 0.106 0.039 0.824 0.167 0.066 (SOP), where scores increase GPT-4o 0.785 0.015 0.012 0.802 0.166 0.059 0.815 0.159 0.077 by10–20xfromnear-zerobase- Gemini-1.5 0.797 0.010 0.016 0.827 0.174 0.048 0.833 0.207 0.056 lines. Allmodelsbenefitfromin- Mixtral 0.784 0.002 0.000 0.766 0.103 0.010 0.797 0.106 0.021 LLaMA-3 0.786 0.011 0.001 0.787 0.139 0.014 0.811 0.172 0.020 context examples, though with GLM-4 0.747 0.005 0.000 0.756 0.036 0.000 0.778 0.065 0.000 diminishingreturns,theimprove- ment from zero-shot to 3-shot (average NSAS gain: 2.1%, SOP: 10-fold increase) significantly exceeds that from 3-shot to 5-shot (NSAS: 1.4%, SOP: 20-50% relative gain). Gemini-1.5-Pro exhibitsthestrongestadaptability,achievingthehighest5-shotperformance(NSAS:0.833,SOP: 0.207),whilemaintainingconsistentimprovementsacrossallmetrics. Thedifferentialimpactacross metricsrevealsthatfew-shotexamplesprimarilyaddressproceduralsequencingchallenges(massive SOPimprovements)moreeffectivelythanexactstepmatching(modestSSMgains),suggestingthat demonstrationshelpmodelsunderstandtemporaldependenciesinVRinteractionsbutdonotfully resolvethecomplexityoftranslatingsemanticactionsintoprecisedevicemanipulations. 3.6 RQ4: COGNITIVECAPABILITIESANALYSIS WeanalyzedmodelperformanceacrosssixcognitivecapabilitiesrequiredforeffectiveVRinteraction (Figure1). Bymappingevaluationmetricstocapabilityscores(0-10scale),weidentifiedspecific strengthsandlimitationsinhowLLMsapproachspatial-mechanicalreasoningtasks. AreasofStrength:AllevaluatedLLMsdemonstratestrongtaskdecompositioncapabilities(7.8-8.5), withminimalperformancegapcomparedtohumans(8.2). Gemini-1.5-Proleadswithascoreof 8.5,whileevensmallermodelslikeMixtral-8x7B(8.0)andGLM-4-flash(7.8)performadmirably. Thissuggeststhatsegmentinghigh-levelactionsintocomponentstepsalignswellwiththesequential reasoningabilitiesdevelopedduringlanguagemodelpre-training. 7 --- Page 8 ---  AreasofWeakness: Motoractionmappingemergesasthemostsignificantchallenge(0.5-4.5),with allmodelsstrugglingtopreciselytranslateabstractactionsintospecificVRcontrolmanipulations. GPT-4operformsbestinthisdimension(4.5),butstillfallsshortofrobustcapability. Procedural reasoningalsoshowssubstantialvariation(2.3-7.0),withonlyGemini-1.5-Proapproachingadequate performance. Judgment of termination conditions represents another challenge area, with most modelsscoringbelow5.0(exceptGemini-1.5-Proat6.0),comparedtohumanperformance(6.5). Model Comparison: Gemini-1.5-Pro demonstrates the most balanced performance profile, con- sistently outperforming other models in procedural reasoning (7.0), spatial reasoning (7.5), and terminationjudgment(6.0). GPT-4variantsshowstrongtaskdecompositionandobjectinteraction (5.3-5.7)butlaginproceduralsequencing.LLaMA-3-8Bshowssurprisinglycompetitiveperformance inproceduralreasoning(5.7),outperforminglargermodelslikeGPT-3.5-Turbo(4.3),suggesting architecturedifferencesmaybeasimportantasscale. 3.7 RQ5: COMPARISONWITHHUMANPERFORMANCE Tocontextualizeourfindings,wecompareLLM Ta1s0k Decomposition performanceagainsthumanbaselinesacrossour 8 evaluationmetrics. AsshowninTables1and2, 6 Judgment of Procedural state-of-the-artLLMsdemonstratecompetitive Termination 4 Reasoning GPT-3.5-turbo performance with humans on several key di- GPT-4-turbo 2 GPT-4o mensions. Ourresultsrevealanuancedperfor- Gemini-1.5-Pro mancelandscape. Whiletop-performingmodels Mixtral-8x7B Llama-3-8B (Gemini-1.5-Pro,GPT-4o)achievecomparable GLM-4-flash orsuperiorNSASscoresrelativetohumansin Motor Action Spatial Human Mapping Reasoning certaingames(e.g.,Vivecraft: 0.931-0.938vs. 0.935forhumans),humanparticipantsmaintain Object adecisiveadvantageinSequentialOrderPreser- Interaction vation,particularlyforgamesrequiringcomplex interactionsequences. InHalf-Life: Alyx,hu- Figure1: CognitivecapabilitiesofLLMsandhu- mansachieveonly0.090SOPcomparedtothe mansintranslatingsemanticactionstoVRdevice bestmodelperformanceof0.209(Gemini-1.5- manipulations. Higherscores(0-10scale)indicate Pro),yetthisreflectsthechallengingnatureof strongerabilities. thetaskratherthansuperiormodelperformance,bothhumansandmodelsstrugglewiththeintricate proceduralrequirementsofthisgame. Analysisofperformancevarianceacrossgames(Table3)revealsstrikingsimilaritiesbetweenhuman andhigh-performingmodelbehavior. Thestandarddeviationofhumanperformance(0.084)closely aligns with that of GPT-4 (0.074) and Mixtral-8x7B (0.070), suggesting that both humans and advancedLLMsexhibitsimilarsensitivitypatternstogame-specificinteractioncomplexities. This convergenceisparticularlyevidentinstructuredenvironmentslikeVivecraft,wheretheconsistency gapbetweenhumansandLLMshassubstantiallynarrowed. Figure1illustratesthecapability-wise performancecomparison,revealingcriticalgapsinembodiedreasoning. Humansmaintainsuperior performance in spatial reasoning (8.3 vs. 7.5 for Gemini-1.5-Pro) and judgment of termination conditions(6.5vs. 6.0). Thesedifferencesarestatisticallysignificant(p<0.05,Wilcoxonsigned- ranktest)andpersistacrossallevaluatedmodels. ThisperformancegapsuggeststhatwhileLLMs haveachievedremarkableprogressinunderstandingVRinteractionsemantics,theylackthegrounded physicalintuitionthathumansnaturallyapplywhenreasoningaboutthree-dimensionalmanipulations anddeterminingactioncompletionstates. TheconvergenceofhumanandLLMperformanceoncertainmetrics,coupledwithpersistentgapsin spatialandterminationreasoning,indicatesthatcurrentlanguagemodelscaneffectivelydecompose VR tasks but struggle with aspects requiring embodied experience. This finding has important implicationsforthedevelopmentoffutureVR-capableAIsystems,suggestingtheneedfortraining paradigmsthatbetterincorporatespatialandphysicalreasoningcapabilities. 8 --- Page 9 ---  4 RELATED WORK Recent work has explored the use of large language models (LLMs) as generalist agents for em- bodiedandinteractivereasoningtasks. Inrobotics,SayCan(Ahnetal.,2022)andPaLM-E(Driess etal.,2023)combineLLMswithaffordance-basedskillmodelsormultimodalinputstoplanand executerobotactionsinreal-worldsettings. ThesemethodsdemonstratethatLLMscandecompose high-level goals into actionable steps when grounded in sensory input and executable primitives. Similar capabilities have been investigated in virtual domains such as Minecraft through agents likeVoyager(Wangetal.,2023)andsimulationplatformslikeMineDojo(Fanetal.,2022),which showcasein-contextlearningandautonomousskillacquisitionbypromptingLLMstogenerateand refinecodeorsub-goalsbasedonenvironmentalfeedback. However,thesesystemsaretypically tunedforcode-levelorsymbolicoutputsanddonotfocusonphysicaldevicemanipulationorspatially groundedmotorcontrolasrequiredinVRenvironments. Taskdecompositionandproceduralreasoninghavebeenstudiedextensivelyviapromptingstrategies suchasChain-of-Thought(Weietal.,2022)andReAct(Yaoetal.,2022),whichinterleavereasoning with action selection to improve coherence in multi-step planning. LLMs have also been used to generate structured action sequences or API calls from natural instructions in domains like householdtasks(Shridharetal.,2020)andscientificprocedures(Wangetal.,2022). Code-as-policy paradigms(Liangetal.,2022)showthatLLMscanoutputexecutablepolicycodethatintegrates logicalcontrolflow, enablingconditionalanditerativeactions. However, theseapproachesoften abstractawaythecomplexityofphysicalorspatialexecution,makingthemlesssuitedforevaluating embodiedskillsinvolvingreal-timedeviceinput,objectaffordances,or3Dspatialconstraints. Toassessgroundedreasoning,severalbenchmarkshavebeenproposedininteractivesettings.Animal- AI(Mecattafetal.,2024)evaluatesembodiedcognitionthroughphysics-basedtasksadaptedfrom animal intelligence experiments, highlighting LLMs’ partial competence in navigation, tool use, and physical causality. Similarly, platforms like ALFWorld (Shridhar et al., 2021) and Science- World(Wangetal.,2022)testinstruction-followingviatextorsymbolicinterfaces,whileMacGyver- style tasks (Tian et al., 2024) probe object-use innovation in constrained settings. These works underscoreknownlimitationsinspatialreasoning,persistence,andtool-usegeneralizationamong LLMs. Concurrently, capability-oriented embodied evaluations for multimodal/embodied LLMs have emerged. EmbodiedBench(Yangetal.,2025)unifieshigh-andlow-leveltaskswithfine-grained errortaxonomies,whileVLABench(Zhangetal.,2024a)targetslong-horizon,language-conditioned manipulationforVLApolicies;theEmbodiedAgentInterface(EAI)(Lietal.,2025)standardizes modules and metrics for step-level diagnostics. In parallel, GUI/OS/mobile control benchmarks, includingOSWorld(Xieetal.,2024), SPA-Bench(Zhangetal.,2024b), WebArena(Zhouetal., 2023),Mind2Web(Dengetal.,2023),AndroidEnv(Toyamaetal.,2021),andAppAgent/AppA- gent v2 (Zhang et al., 2023; Li et al., 2024b), evaluate precise, procedure-level device interac- tions(click/tap/drag/typing)withprogrammaticsuccesschecks,offeringacomplementaryviewof groundedactioncompetence. Ontheroboticsside,VLApoliciessuchasRT-1/RT-2(Brohanetal., 2022;2023)andOpenVLA(Kimetal.,2024)mapvisualobservationsandlanguagetoactiontokens, improvingsemanticgeneralizationinmanipulation;large-scale3DsuiteslikeHabitat2.0/HAB(Savva etal.,2021),BEHAVIOR-1K/OmniGibson(Lietal.,2024a),andCALVIN(Meesetal.,2021)stress long-horizonrearrangementandphysics. Incontrast,ourbenchmark,ComboBench,targetsthetranslationofsemanticgoalsintofine-grained, physicallygroundedVRdevicemanipulations,enablingamoreprecisestep-levelanalysisofembod- iedcognitiveabilitiescriticalforreal-worldinteraction. 5 CONCLUSION WepresentComboBench,acomprehensivebenchmarkevaluatingsevenstate-of-the-artLLMson theirabilitytotranslatesemanticactionsintoVRdevicemanipulationsacross262scenariosfrom fourpopularVRgames. OurevaluationrevealsthatwhileadvancedmodelslikeGemini-1.5-Pro demonstratestrongtaskdecompositioncapabilities(NSAS>0.8),theyexhibitsignificantweaknesses inmotoractionmappingandproceduralreasoning,withSequentialOrderPreservationscoresoften below0.3eveninthebestcases. Few-shotexamplesdramaticallyimproveproceduralunderstanding 9 --- Page 10 ---  (10-20x increase in SOP scores) but provide limited benefit for exact step matching, suggesting that in-context learning helps models understand action relationships but cannot fully bridge the gap in physical manipulation reasoning. The pronounced performance variations across games andcognitivecapabilitiesindicatethatcurrenttext-trainedLLMslacktheembodiedunderstanding necessaryforreliableVRinteraction,pointingtotheneedformultimodaltrainingapproachesthat incorporatespatial,visual,andhapticinformation. Thesefindingshighlightfundamentallimitations inhowlanguagemodelsrepresentphysicalinteractionsandsuggestthatachievinghuman-levelVR manipulationcapabilitieswillrequirearchitecturalinnovationsbeyondscalingcurrentapproaches, withimportantimplicationsforthedevelopmentofembodiedAIsystemsinvirtualandaugmented realityapplications. REFERENCES VR Content on Steam App Store.  ?vrsupport=401,2023. MichaelAhn,AnthonyBrohan,NoahBrown,YevgenChebotar,OmarCortes,ByronDavid,Chelsea Finn,ChuyuanFu,KeerthanaGopalakrishnan,KarolHausman,etal. Doasican,notasisay: Groundinglanguageinroboticaffordances. arXivpreprintarXiv:2204.01691,2022. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, KeerthanaGopalakrishnan,KarolHausman,AlexHerzog,JasmineHsu,JulianIbarz,BrianIchter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, IgorMordatch, OfirNachum, CarolinaParada, JodilynPeralta, EmilyPerez, Karl Pertsch,JornellQuiambao,KanishkaRao,MichaelRyoo,GreciaSalazar,PannagSanketi,Kevin Sayed,JaspiarSingh,SumedhSontakke,AustinStone,ClaytonTan,HuongTran,VincentVan- houcke,SteveVega,QuanVuong,FeiXia,TedXiao,PengXu,SichunXu,TianheYu,andBrianna Zitkovich. Rt-1: Roboticstransformerforreal-worldcontrolatscale,2022. AnthonyBrohan,NoahBrown,JusticeCarbajal,YevgenChebotar,XiChen,KrzysztofChoromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, et al. Rt-2: Vision- language-actionmodelstransferwebknowledgetoroboticcontrol,2023. CMGames. IntotheRadius,2019. URL Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and YuSu. Mind2web: Towardsageneralistagentfortheweb,2023. Danny Driess, Fei Xia, Arjun Srinivas, Wenlong Huang, Julius Müller, Roberto Martín-Martín, TobiasBücheler,YevgenChebotarDu,KarolHausman,SaranTunyasuvunakool,etal. Palm-e: Anembodiedmultimodallanguagemodel. arXivpreprintarXiv:2303.03378,2023. LinxiFan,GuanzhiWang,YunfanJiang,AjayMandlekar,YuncongYang,HaoyiZhu,AndrewTang, De-AnHuang,YukeZhu,andAnimaAnandkumar. Minedojo: Buildingopen-endedembodied agentswithinternet-scaleknowledge. arXivpreprintarXiv:2206.08853,2022. TeamGLM,AohanZeng,BinXu,BowenWang,ChenhuiZhang,DaYin,DanZhang,DiegoRojas, GuanyuFeng,HanlinZhao,etal. Chatglm: Afamilyoflargelanguagemodelsfromglm-130bto glm-4alltools. arXivpreprintarXiv:2406.12793,2024. AaronGrattafiori,AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,Ahmad Al-Dahle,AieshaLetman,AkhilMathur,AlanSchelten,AlexVaughan,etal. Thellama3herdof models. arXivpreprintarXiv:2407.21783,2024. Jen-tseHuang,WenxuanWang,EricJohnLi,ManHoLam,ShujieRen,YouliangYuan,Wenxiang Jiao,ZhaopengTu,andMichaelLyu. Onthehumanityofconversationalai: Evaluatingthepsycho- logicalportrayalofllms. InTheTwelfthInternationalConferenceonLearningRepresentations, 2024. AaronHurst,AdamLerer,AdamPGoucher,AdamPerelman,AdityaRamesh,AidanClark,AJOs- trow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276,2024. 10 --- Page 11 ---  AlbertQJiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,DevendraSinghChaplot, DiegodelasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,LucileSaulnier,etal. Mistral7b. arXivpreprintarXiv:2310.06825,2023. MooJinKim,KarlPertsch,SiddharthKaramcheti,TedXiao,AshwinBalakrishna,SurajNair,Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel,RussTedrake,DorsaSadigh,SergeyLevine,PercyLiang,andChelseaFinn. Openvla: Anopen-sourcevision-language-actionmodel,2024. ManHoLam,ChaozhengWang,Jen-tseHuang,andMichaelRLyu. Codecrash: Stresstestingllm reasoningunderstructuralandsemanticperturbations. arXivpreprintarXiv:2504.14119,2025. CherylLee,ChunqiuStevenXia,Jen-tseHuang,ZhouruixinZhu,LingmingZhang,andMichaelR Lyu. A unified debugging approach via llm-based multi-agent synergy. arXiv preprint arXiv:2404.17153,2024. ChengshuLi,JosiahWong,MichaelLingelbach,RobertoMartín-Martín,JimFan,etal. Behavior-1k: Ahuman-centered,embodiedaibenchmarkwith1,000everydayactivitiesandrealisticsimulation, 2024a. ManlingLi,ShiyuZhao,QinengWang,KangruiWang,YuZhou,SanjanaSrivastava,CemGokmen, TonyLee, LiErranLi, RuohanZhang, WeiyuLiu, PercyLiang, LiFei-Fei, JiayuanMao, and JiajunWu. Embodiedagentinterface: Benchmarkingllmsforembodieddecisionmaking,2025. YandaLi,ChiZhang,WanqiYang,BinFu,PeiCheng,XinChen,LingChen,andYunchaoWei. Appagentv2: Advancedagentforflexiblemobileinteractions,2024b. JackyLiang,WenlongHuang,FeiXia,PengXu,KarolHausman,BrianIchter,PeteFlorence,and AndyZeng. Codeaspolicies: Languagemodelprogramsforembodiedcontrol. arXivpreprint arXiv:2209.07753,2022. TianLiang,ZhiweiHe,WenxiangJiao,XingWang,YanWang,RuiWang,YujiuYang,ZhaopengTu, andShumingShi. Encouragingdivergentthinkinginlargelanguagemodelsthroughmulti-agent debate. arXivpreprintarXiv:2305.19118,2023. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-WeiChang,MichelGalley,andJianfengGao. Mathvista: Evaluatingmathematicalreasoning offoundationmodelsinvisualcontexts. InTheTwelfthInternationalConferenceonLearning Representations,2024. MatteoG.Mecattaf,BenSlater,MarkoTešic´,JonathanPrunty,KonstantinosVoudouris,andLucyG. Cheke. Alittlelessconversation,alittlemoreaction,please: Investigatingthephysicalcommon- senseofllmsina3dembodiedenvironment. arXivpreprintarXiv:2410.23242,2024. OierMees,LukasHermann,ErickRosete-Beas,andWolframBurgard. Calvin: Abenchmarkfor language-conditionedpolicylearningforlong-horizonrobotmanipulationtasks,2021. OpenAI. Introducingchatgpt. OpenAIBlogNov302022,2022. URL index/chatgpt/. OpenAI. Gpt-4technicalreport. arXivpreprintarXiv:2303.08774,2023. Polyarc. Moss: Book II, 2022. URL  moss-book-ii. Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476,2023. ManolisSavva,AbhishekKadian,ErikWijmans,ShengyiQian,AngelChang,etal. Habitat2.0: Traininghomeassistantstorearrangetheirhabitat,2021. 11 --- Page 12 ---  MohitShridhar,JesseThomason,DanielGordon,YonatanBisk,WinsonHan,RoozbehMottaghi, LukeZettlemoyer,andDieterFox. Alfred: Abenchmarkforinterpretinggroundedinstructions foreverydaytasks. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern Recognition,pp.10740–10749,2020. MohitShridhar,XingdiYuan,Marc-AlexandreCôté,YonatanBisk,AdamTrischler,andMatthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. In ProceedingsoftheInternationalConferenceonLearningRepresentations(ICLR),2021. URL  Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer,DamienVincent,ZhufengPan,ShiboWang,etal. Gemini1.5: Unlockingmultimodal understandingacrossmillionsoftokensofcontext. arXivpreprintarXiv:2403.05530,2024. YufeiTian,AbhilashaRavichander,LianhuiQin,RonanLeBras,RamiMarjieh,NanyunPeng,Yejin Choi,ThomasLGriffiths,andFaezeBrahman. Macgyver: Arelargelanguagemodelscreative problemsolvers? InProceedingsofthe2024ConferenceoftheNorthAmericanChapterofthe AssociationforComputationalLinguistics: HumanLanguageTechnologies,pp.5303–5324,2024. DanielToyama,PhilippeHamel,AnitaGergely,GheorgheComanici,AmeliaGlaese,ZafaraliAhmed, TylerJackson,ShiblMourad,andDoinaPrecup. Androidenv: Areinforcementlearningplatform forandroid,2021. Valve. Half-Life: Alyx,2020. URL Vivecraft. Vivecraft – Virtual Reality Minecraft for SteamVR, 2013. URL  vivecraft.org/. GuanzhiWang,YuqiXie,YunfanJiang,AjayMandlekar,ChaoweiXiao,YukeZhu,LinxiFan,and AnimaAnandkumar. Voyager: Anopen-endedembodiedagentwithlargelanguagemodels. arXiv preprintarXiv:2305.16291,2023. YujiaWang,TusharKhot,AshishSabharwal,andPeterClark. Scienceworld: Isyouragentsmarter thana5thgrader? arXivpreprintarXiv:2203.07540,2022. JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,BrianIchter,FeiXia,EdChi,QuocV Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. arXivpreprintarXiv:2201.11903,2022. TianbaoXie,DanyangZhang,JixuanChen,XiaochuanLi,SihengZhao,RuishengCao,TohJing Hua,ZhoujunCheng,DongchanShin,FangyuLei,YitaoLiu,YihengXu,ShuyanZhou,Silvio Savarese,CaimingXiong,VictorZhong,andTaoYu. Osworld: Benchmarkingmultimodalagents foropen-endedtasksinrealcomputerenvironments,2024. RuiYang,HanyangChen,JunyuZhang,MarkZhao,ChengQian,KangruiWang,QinengWang, TejaVenkatKoripella,MarziyehMovahedi,ManlingLi,HengJi,HuanZhang,andTongZhang. Embodiedbench: Comprehensivebenchmarkingmulti-modallargelanguagemodelsforvision- drivenembodiedagents,2025. ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikNarasimhan,andYuanCao. React: Synergizingreasoningandactinginlanguagemodels. arXivpreprintarXiv:2210.03629, 2022. ChiZhang,ZhaoYang,JiaxuanLiu,YuchengHan,XinChen,ZebiaoHuang,BinFu,andGangYu. Appagent: Multimodalagentsassmartphoneusers,2023. Shiduo Zhang, Zhe Xu, Peiju Liu, Xiaopeng Yu, Yuan Li, Qinghui Gao, Zhaoye Fei, Zhangyue Yin, Zuxuan Wu, Yu-Gang Jiang, and Xipeng Qiu. Vlabench: A large-scale benchmark for language-conditionedroboticsmanipulationwithlong-horizonreasoningtasks,2024a. ZhaofengZhang,YiyanQi,JinjieNi,JiayiYuan,FangkaiYang,etal. Spa-bench: Acomprehensive benchmarkforsmartphoneagentevaluation,2024b. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, TianyueOu,YonatanBisk,DanielFried,UriAlon,andGrahamNeubig. Webarena: Arealistic webenvironmentforbuildingautonomousagents,2023. 12 --- Page 13 ---  A PRELIMINARIES ON VIRTUAL REALITY VirtualReality(VR)representsafundamentallydistinctparadigmofhuman-computerinteraction thattranscendstraditionalinterfaceboundaries. Unlikeconventionalcomputingsystemsthatrely onindirectmanipulationthroughkeyboards,mice,andtwo-dimensionaldisplays,VRcreatesim- mersivedigitalenvironmentswhereusersexperiencepresenceandembodiment. Thisparadigmshift necessitatesacomprehensiveunderstandingofboththetechnologicalinfrastructureandthecognitive demandsplacedonuserswhomusttranslateabstractintentionsintoconcretephysicalmanipulations withinvirtualspaces. TheevolutionofVRtechnologyhasprogressedthroughseveralgenerations, fromearlytethered systemsrequiringsubstantialcomputationalinfrastructuretomodernstandalonedevicesthatintegrate processing,display,andtrackingcapabilitieswithincompactformfactors.ContemporaryVRsystems canbebroadlycategorizedintothreearchitecturalapproaches.PC-tetheredheadsetsleverageexternal computational resources to deliver high-fidelity experiences with complex graphics and physics simulations. Standaloneheadsets,exemplifiedbydevicesliketheMetaQuestseries,incorporate integrated processors that balance performance with portability. Mobile-phone-based solutions representanaccessibleentrypoint,utilizingsmartphonesasbothdisplayandprocessor,thoughwith inherentlimitationsintrackingprecisionandcomputationalcapability. ThecorehardwarecomponentsenablingVRinteractionformanintegratedecosystemofsensory inputandoutputdevices. Head-MountedDisplays(HMDs)serveastheprimaryvisualinterface, providingstereoscopicrenderingthatcreatesdepthperceptionwhilesimultaneouslytrackinghead orientationandpositionthroughintegratedsensors. Thistrackingenablesnaturalviewingbehaviors whereuserscanexaminevirtualobjectsbyphysicallymovingtheirheads,mirroringreal-worldvisual explorationpatterns. Motioncontrollers,typicallydeployedinpairstorepresentbothhands,enable directmanipulationofvirtualobjectsthroughacombinationofpositionaltracking,buttoninputs, triggermechanisms,andthumbstickcontrols. Thesedevicesmustbalanceergonomicconsiderations withfunctionalcomplexity,providingsufficientinputchannelswhilemaintainingintuitiveoperation. Spatial tracking systems, whether implemented through external sensors (outside-in tracking) or integrated cameras (inside-out tracking), monitor user movements with six degrees of freedom, capturingbothtranslationalandrotationalmotiontoenablenaturallocomotionandinteractionwithin virtualenvironments. TheongoingevolutionofVRhardwarecontinuestointroducenovelinteractionmodalities. Haptic glovespromisetodelivertactilefeedbackthroughactuatorsthatsimulatetexture,resistance,and temperature. Full-body tracking systems capture skeletal motion to enable more nuanced avatar controlandgesturerecognition. Specializedperipherals,fromsteeringwheelsforracingsimulations toweaponreplicasforcombatgames,demonstratethetrendtowardapplication-specificcontrollers thatenhanceimmersionthroughphysicalaffordancesthatmatchvirtualinteractions. A.1 INTERACTIONPARADIGMSANDDESIGNPRINCIPLES The design of VR interaction paradigms represents a delicate balance between leveraging users’ existingmotorskillsandintroducingnovelcontrolschemesthatexploittheuniquecapabilitiesof virtual environments. Direct manipulation forms the foundation of most VR interactions, where usersemployhandcontrollerstosimulatenaturalactionslikegrasping,throwing,andpushing. This approachcapitalizesonusers’lifetimeofexperiencewithphysicalobjectmanipulationbutrequires carefulcalibrationofvirtualphysicstomatchexpectations. Themappingbetweencontrollerinputs andvirtualhandmovementsmustaccountfortheabsenceoftactilefeedback,oftenemployingvisual orauditorycuestoconfirmsuccessfulinteractions. Ray-castingemergedasanelegantsolutiontothefundamentalchallengeofinteractingwithobjects beyondphysicalreach. Byprojectingvirtualraysfromcontrollers,userscanselect,manipulate,and activatedistantobjectswithoutlocomotion. ThistechniqueexemplifieshowVRinteractiondesign oftenaugmentsnaturalhumancapabilitiesratherthanstrictlysimulatingphysicalconstraints. Ad- vancedray-castingimplementationsincorporatefeatureslikeraycurvatureforimprovedergonomics, variable ray length based on context, and visual feedback mechanisms that indicate interaction possibilities. 13 --- Page 14 ---  Gesturerecognitionsystemsinterprettemporalpatternsofcontrollerorhandmovementasdiscrete commands,enablingarichvocabularyofinteractionswithoutrelyingonbuttoncombinations. These systemsmustbalancerecognitionaccuracywithusercomfort,avoidinggesturesthatcausefatigue orrequireprecisemovementsdifficulttoperformconsistently. Machinelearningapproacheshave enhancedgesturerecognitioncapabilities,allowingformorenaturalandvariedinputpatternswhile maintainingreliabledetectionrates. Symbolicinputmechanismsaddressscenarioswheredirectphysicalanaloguesareimpracticalor inefficient. VirtualkeyboardspresentuniquechallengesinVR,asuserslacktactilefeedbackand mustrelyonvisualconfirmationofkeypresses.Solutionsrangefromlaser-pointerselectionofvirtual keystogesture-basedtextentrysystemsthatmaphandmovementstocharacters. Voicecommands offeranalternativeinputmodalitythatbypassesmanualinteractionentirely,thoughtheyintroduce considerationsaroundrecognitionaccuracy,latency,andsocialacceptabilityinsharedspaces. A.2 DEVELOPMENTPLATFORMSANDTECHNICALCONSIDERATIONS ThecreationofVRapplicationsreliesonsophisticateddevelopmentecosystemsthatabstracthardware complexitywhileprovidingfine-grainedcontroloverinteractionmechanics.UnityandUnrealEngine haveemergedasdominantplatforms,offeringcomprehensivetoolsetsthathandlerenderingpipelines, physicssimulation,spatialaudio,andcross-platformdeployment. Theseenginesprovidespecialized VRinteractionframeworksthatstandardizecommonpatternslikeobjectgrabbing,teleportation,and menusystems,significantlyreducingdevelopmentcomplexity. Hardwaresoftwaredevelopmentkits(SDKs)serveasthebridgebetweenhigh-levelapplicationlogic anddevice-specificcapabilities. Meta’sOpenXRinitiativerepresentsanindustryefforttostandardize VR/ARinterfaces,enablingapplicationstotargetmultiplehardwareplatformswithoutextensive modifications. Platform-specificSDKslikeSteamVRandOculusSDKcontinuetoplayimportant roles,offeringaccesstoproprietaryfeaturesandoptimizationsthatenhanceperformanceonparticular hardware. TechnicalconstraintsfundamentallyshapeVRinteractiondesigndecisions. Maintainingconsistent frame rates above 72Hz (and preferably 90Hz or higher) prevents motion sickness and ensures responsiveinteractions. Thisperformancerequirementinfluenceseveryaspectofapplicationdesign, from polygon counts and texture resolution to the complexity of physics simulations. Tracking precisionvariesacrosshardwareplatformsandenvironmentalconditions,necessitatinginteraction designsthataccommodateoccasionaltrackinglossesorreducedaccuracy. Developersmustalso considerthediversecomputationalcapabilitiesacrosstheVRecosystem, implementingscalable solutionsthatprovideacceptableexperiencesonentry-levelhardwarewhileleveragingthecapabilities ofhigh-endsystems. A.3 CHALLENGESINVRINTERACTION Despiteremarkabletechnologicalprogress,VRinteractioncontinuestofacefundamentalchallenges thatimpactuserexperienceandlimitapplicationdomains. Thelocomotionproblemexemplifiesthe tensionbetweenphysicalandvirtualspaces. Whileusersmayexplorevastvirtualenvironments,they remainconstrainedbyfinitephysicalplayareas. Teleportationoffersapracticalsolutionbutbreaks immersionandcancausespatialdisorientation. Artificiallocomotionthroughthumbstickcontrol risksmotionsicknessinsusceptibleusers. Moreexoticsolutionslikeomnidirectionaltreadmillsor redirectedwalkingtechniquesremainimpracticalforconsumerapplications. Theabsenceofcomprehensivehapticfeedbackrepresentsperhapsthemostsignificantlimitationin currentVRsystems. Whilecontrollersprovidebasicvibrationfeedback,theycannotsimulatethe richtactileexperiencesofreal-worldinteraction: theweightofobjects,surfacetextures,temperature variations,orresistancetomovement. Thissensorygapcreatesafundamentaldisconnectbetween visualexpectationsandphysicalsensations,requiringuserstoadapttheirinteractionstrategiesand oftenleadingtoreducedprecisioninmanipulationtasks. InteractiondiscoverabilityposesongoingchallengesasVRapplicationslackstandardizedinterface conventionscomparabletodesktopormobileplatforms. UsersencounteringnewVRexperiences mustoftenlearnapplication-specificcontrolschemes, gesturesets, andinteractionpatterns. The absenceofpersistentvisualUIelements(tomaintainimmersion)exacerbatesthischallenge,asusers 14 --- Page 15 ---  cannoteasilyreferencecontrolschemesduringgameplay. Thislackofstandardizationincreases cognitiveloadandcreatesbarrierstoentryfornewusers. Precisionmanipulationtaskshighlightthelimitationsofcurrenttrackingsystemsandinputdevices. Tasksrequiringfinemotorcontrol,suchasthreadingavirtualneedleormanipulatingsmallcompo- nents,provechallengingduetotrackingjitter,lackofphysicalsurfacesforhandstabilization,and absenceoftactileconfirmation.TheselimitationsrestrictthetypesofapplicationssuitableforVRand influenceinteractiondesigntowardlarger,moreforgivingtargetsizesandsimplifiedmanipulation schemes. Figure2: OverviewofStrictStep-by-StepMatching(SSM)Calculation Figure3: OverviewofCommonSubsequenceEvaluation B EXPLANATION OF EVALUATION METRICS B.1 STRICTSTEP-BY-STEPMATCHING(SSM) Figure2illustratestheStrictStep-by-StepMatching(SSM)calculationprocess. SSMrepresentsour moststringentevaluationmetric,requiringexactcorrespondencebetweenmodel-generatedsequences andgroundtruthannotations. Thecalculationprocessoperatesasfollows: Intheleftpanel, weobserveascenariowherethegroundtruthcontains4stepswhilethemodel resultcontains5steps. ForSSMtoregisteramatch,twoconditionsmustbesatisfied: (1)thenumber ofstepsmustbeidenticalbetweengroundtruthandmodeloutput,and(2)eachstepmusthavea cosinesimilarityscoreaboveourthresholdof0.8387withitscorrespondinggroundtruthstep. In thisexample,thelengthmismatchalonedisqualifiesthesequencefrombeingcountedascorrect, resultinginanSSMscoreof0. TheorangeXsymbolonthefifthmodelstepvisuallyindicatesthis lengthmismatchfailure. TherightpaneldemonstratesasuccessfulSSMmatchwherebothsequencescontain4steps. Each modelstepiscomparedwithitscorrespondinggroundtruthstepusingcosinesimilarityoftheirtext embeddings. Thegreencheckmarksindicatethatallfoursteppairsexceedthesimilaritythreshold, resultinginasuccessfulmatchandcontributing1totheSSMscore. Thismetric’sstrictnessexplains 15 --- Page 16 ---  whyevenhigh-performingmodelsachieverelativelylowSSMscores—anydeviationinsequence lengthorindividualstepsimilarityresultsincompletefailureforthatsequence. B.2 COMMONSUBSEQUENCEEVALUATION Figure3detailsourCommonSubsequenceEvaluationapproach,whichunderliestheNormalized StepAlignmentScore(NSAS)andSequentialOrderPreservation(SOP)metrics. Thisevaluation methodprovidesmorenuancedassessmentthanSSMbyidentifyingpartialmatchesandpreserved orderingwithinsequences. Theprocessbeginswithcomparingeachstepinthegroundtruthandmodelresultsequencesusing cosine similarity, as shown by the crossing blue lines in the leftmost panel. Unlike SSM’s strict position-based matching, this approach allows steps to match regardless of their positions in the sequences. Thealgorithmthenidentifiesthetop3longestcommonsubsequenceswherematched stepsmaintaintheirrelativeordering. Intheexampleshown,multiplecandidatesubsequencesaregenerated,eachrepresentingdifferent waysstepsfrombothsequencescanbealignedwhilepreservingorder. Themodel(shownasGPT- 4o)thenselectsthemostmatchingsubsequencebasedonthehighestcumulativesimilarityscores. ThefinalselectedsubsequenceshowsGTSteps2, 3, and4matchingwithMRSteps1, 4, and5 respectively. Thisflexiblematchingapproachallowsthemetricstocapturesemanticcorrectnesseven whenmodelsincludeadditionalstepsorpresentstepsinslightlydifferentpositions. TheNSASmetriciscalculatedbyconsideringthecorrectlymatchedsteps(|C|),missingstepsfrom groundtruth(|M|),andadditionalstepsinthemodeloutput(|A|),normalizedbythetotalgroundtruth stepsandscaledacrossthedataset. TheSOPmetricspecificallyevaluateswhethermatchedsteps maintaintheirsequentialorder,providinginsightintothemodel’sproceduralreasoningcapabilities. C DETAILED EXPERIMENT RESULTS Thissectionprovidescomprehensiveanalysisofourexperimentalresults,includingdetailedperfor- mancebreakdownsacrossmodels,games,andexperimentalconditions. Wepresentbothaggregated metricsandfine-grainedanalysesthatilluminatespecificstrengthsandweaknessesincurrentLLMs’ abilitytoreasonaboutVRdevicemanipulations. C.1 OVERALLPERFORMANCEANALYSIS Thetable5belowpresentsaholisticviewofmodelperformanceacrossallexperimentalconditions. Theresultsrevealaclearperformancehierarchy,withGemini-1.5-Proachievingthehighestaverage NormalizedStepAlignmentScore(NSAS)of0.845,followedcloselybyGPT-4o(0.832)andGPT-4 (0.824). Notably,eventhebest-performingmodelsachieverelativelymodestStrictStep-by-Step Matching(SSM)scores,withGemini-1.5-Proreachingonly8.7%exactsequencematches. This discrepancybetweenNSASandSSMscoresindicatesthatwhilemodelscanidentifyappropriate actions,theystrugglewithprecisesequencingandcompletereproductionofmanipulationsequences. TheSequentialOrderPreservation(SOP)scoresrevealperhapsthemostsignificantchallengefacing current LLMs. Even top-performing models achieve SOP scores below 0.3, indicating difficulty inmaintainingcorrectproceduralorderingofsteps. Thislimitationisparticularlypronouncedin zero-shotsettings, whereSOPscoresapproachzeroformostmodels, suggestingthatprocedural reasoning for VR interactions requires exposure to examples rather than emerging from general languageunderstanding. Humanperformanceprovidesanimportantbaselineforcontextualizingmodelachievements. While humansachievecomparableNSASscores(0.817)totopLLMs,theyshownotablylowerSOPscores (0.124)thanleadingmodels. Thiscounterintuitiveresultreflectsthechallengingnatureofthetasks evenforexperiencedVRusersandsuggeststhatperfectproceduralrecallmaybelessimportantthan adaptiveproblem-solvinginreal-worldVRinteraction. 16 --- Page 17 ---  Table5: PerformanceofLLMsacrossVRGames(BestFew-ShotSetting) Model NSAS SOP SSC SSM BestFS Gemini-1.5-Pro 0.845 0.251 0.151 0.087 5 GPT-4o 0.832 0.291 0.190 0.135 5 GPT-4 0.824 0.218 0.177 0.095 5 LLaMA-3-8B 0.823 0.283 0.200 0.040 5 Human 0.817 0.124 0.174 0.021 - Mixtral-8x7B 0.790 0.123 0.142 0.039 5 GPT-3.5 0.778 0.169 0.137 0.037 5 GLM-4-Flash 0.749 0.096 0.165 0.000 5 C.2 GAME-SPECIFICPERFORMANCEPATTERNS Thetable6belowrevealssubstantialvariationsinmodelperformanceacrossdifferentVRgames, highlighting how game design and interaction complexity influence LLM reasoning capabilities. Vivecraftconsistentlyyieldsthehighestperformanceacrossallmodels,withNSASscoresranging from 0.909 to 0.938. This strong performance likely reflects the game’s discrete, block-based interactionparadigminheritedfromMinecraft,whichprovidesclearaction-objectmappingsthat alignwellwithlinguisticdescriptions. Incontrast,IntotheRadiusprovesmostchallenging,withNSASscoresdroppingto0.618-0.698 acrossmodels.Thisgame’semphasisonrealisticphysicssimulation,complexinventorymanagement, andweaponmanipulationrequiresunderstandingofnuancedspatialrelationshipsandmulti-step procedures that current LLMs struggle to capture. The high standard deviation in performance (0.135forGLM-4-flash)indicatesinconsistentmodelbehaviorwhenconfrontingcomplexinteraction scenarios. Half-Life: AlyxandMoss: BookIIoccupyintermediatepositionsinthedifficultyspectrum. Half- Life:Alyx’sphysics-basedpuzzlesandcombatscenariosrequireprecisetimingandspatialreasoning, reflectedinextremelylowSOPscores(0.022forGPT-4o). Moss: BookII’sthird-personperspective andpuzzle-platformingelementsintroduceuniquechallengesintranslatingcamera-relativedirections into controller movements, though models show more consistent performance than in Half-Life: Alyx. Table6:PerformancecomparisonacrossdifferentVRgames(5-shotsetting). WereportNSASscores (primarymetric)andSOPscores(inparentheses). Model Half-Life:Alyx Radius Moss Vivecraft GPT-3.5-turbo 0.858(0.123) 0.662(0.169) 0.782(0.169) 0.922(0.043) GPT-4-turbo 0.852(0.125) 0.693(0.189) 0.824(0.218) 0.927(0.137) GPT-4o 0.804(0.022) 0.698(0.291) 0.824(0.300) 0.931(0.190) Gemini-1.5-Pro 0.863(0.209) 0.682(0.102) 0.848(0.265) 0.938(0.250) Mixtral-8x7B 0.839(0.126) 0.666(0.123) 0.756(0.117) 0.926(0.060) LLaMA-3-8B 0.848(0.126) 0.644(0.242) 0.823(0.283) 0.929(0.039) GLM-4-flash 0.836(0.076) 0.618(0.096) 0.749(0.087) 0.909(0.000) Human 0.845(0.090) 0.684(0.148) 0.817(0.112) 0.935(0.122) C.3 IMPACTOFFEW-SHOTLEARNING Thetable7belowdemonstratesthetransformativeeffectoffew-shotexamplesonmodelperformance. ThemostdramaticimprovementsoccurinSOPscores,whichincreasebyfactorsof10-20xfrom zero-shotto5-shotsettings. GPT-3.5-turboexemplifiesthispattern,improvingfrom0.036to0.226 in SOP F1 score, representing a 527.8% relative gain. This massive improvement suggests that examplesprimarilyhelpmodelsunderstandtheexpectedformatandlevelofdetailforprocedural instructionsratherthanteachingfundamentalVRinteractionprinciples. Thediminishingreturnspatternisconsistentacrossmodels,withthelargestgainsoccurringbetween zero-shotand1-shotconditions. Thejumpfrom3-shotto5-shotprovidesminimaladditionalbenefit, 17 --- Page 18 ---  Table7: Impactoffew-shotexamplesonmodelperformance. WereportF1scoresforSequential OrderPreservation(SOP)acrossdifferentnumberofexamples. Higherisbetter. Model Zero-shot 1-shot 3-shot 5-shot RelativeGain(%) GPT-3.5-turbo 0.036 0.096 0.190 0.226 527.8 GPT-4-turbo 0.102 0.171 0.254 0.301 195.1 GPT-4o 0.112 0.224 0.257 0.287 156.3 Gemini-1.5-Pro 0.085 0.187 0.260 0.330 288.2 Mixtral-8x7B 0.031 0.110 0.204 0.201 548.4 LLaMA-3-8B 0.090 0.165 0.254 0.299 232.2 GLM-4-flash 0.033 0.069 0.121 0.146 342.4 indicatingthatmodelsquicklyextractrelevantpatternsfromlimitedexamples.Gemini-1.5-Proshows themostefficientfew-shotlearning,achievingtopperformancewithfewerexamplesthancompeting models,suggestingsuperiorin-contextlearningcapabilitiesforproceduraltasks. Interestingly,few-shotexampleshavedifferentialeffectsacrossgametypes. Complexgameslike IntotheRadiusshowcontinuedimprovementwithadditionalexamples,whilesimplerenvironments likeVivecraftplateauquickly. Thispatternindicatesthatfew-shotlearningismostbeneficialwhen dealingwithdiverseinteractionpatternsandcomplexproceduralsequences. C.4 COGNITIVECAPABILITYANALYSIS Thefigure1showsmodelperformanceacrosssixcognitivedimensions,revealingdistinctcapability profiles. All models demonstrate strong task decomposition abilities (7.8-8.5), indicating that breakingdownhigh-levelgoalsintosubtasksalignswellwithLLMs’trainingonhierarchicaltext structures. Gemini-1.5-Proleadsinthisdimensionwithascoreof8.5,thoughevensmallermodels likeMixtral-8x7Bachieverespectablescoresof8.0. Motor action mapping emerges as the most challenging capability across all models (0.5-4.5), highlighting the difficulty of translating abstract action concepts into specific button presses and controller movements. This limitation likely stems from the absence of embodied experience in text-basedtrainingdata.GPT-4operformsbestinthisdimensionbutstillfallsfarshortofhuman-level capability,suggestingafundamentalgapincurrentarchitectures. Proceduralreasoningshowshighvarianceacrossmodels(2.3-7.0),withGemini-1.5-Proagainleading. Thecorrelationbetweenproceduralreasoningscoresandfew-shotlearninggainssuggeststhatthis capabilitycanbepartiallyaddressedthroughexamples,thoughtheceilingremainswellbelowhuman performance. Spatial reasoning capabilities (4.8-7.5) reveal another significant gap, particularly evidentingamesrequiring3Dnavigationandobjectmanipulation. C.5 STATISTICALSIGNIFICANCEANDVARIANCEANALYSIS Thetables8,9,10,11,12,13,14,15,16,17,18,and19belowprovidedetailedstatisticalanalysesof modelperformance,revealingimportantpatternsinconsistencyandreliability. Andthefigures4,5,6 The standard deviation measurements across different games and shot settings illuminate which modelsmaintainstableperformanceversusthoseexhibitinghighvariability.Forinstance,inVivecraft, GPT-3.5-turboshowsremarkablyconsistentNSASscoresinzero-shotsettings(std=0.0248),butthis consistencydeteriorateswithfew-shotexamples(std=0.0734at3-shot),suggestingthatadditional examplesintroduceuncertaintyinthemodel’sapproachtotaskcompletion. The variance patterns differ significantly between metrics. NSAS scores generally show lower standarddeviations(0.02-0.21range)comparedtoSOPscores(0.00-0.34range), indicatingthat models more consistently identify relevant steps than maintain proper ordering. This pattern is particularly pronounced in complex games like Into the Radius, where SOP standard deviations exceed0.3forseveralmodelsinfew-shotsettings. Suchhighvariancesuggeststhatmodelsemploy differentstrategiesacrossdifferentruns,sometimesachievingcorrectorderingbychanceratherthan throughsystematicunderstanding. 18 --- Page 19 ---  Comparisonwithhumanvarianceprovidescrucialcontextforinterpretingmodelstability. Human annotatorsshowstandarddeviationscomparabletomid-tiermodels(0.084incross-gameperfor- mance),suggestingthatsomedegreeofvarianceisinherenttothetaskratherthanamodellimitation. However,humansmaintainmoreconsistentSOPperformance(std=0.029)comparedtoallmodels exceptMixtral-8x7B,indicatingmorereliableproceduralreasoningdespiteoveralllowerscores. Table 8: Average and standard deviation of Normalized Step Alignment Score (NSAS) scores comparisonofLLMsonVivecraftunderdifferentshotsettings. Model GPT-3.5-turbo GPT-4-turbo GPT-4o Gemini-1.5-Pro Mixtral-8x7B LLaMA-3-8b Metrics avg std avg std avg std avg std avg std avg std Zero-shot 0.9258 0.0248 0.9255 0.0238 0.9191 0.0306 0.9209 0.0334 0.9312 0.0207 0.9244 0.0329 1-shot 0.921 0.0309 0.9349 0.0506 0.9358 0.0735 0.9362 0.0553 0.9219 0.0636 0.9101 0.0765 3-shot 0.9284 0.0734 0.914 0.1167 0.9212 0.1115 0.9381 0.0781 0.9005 0.1125 0.9022 0.1051 5-shot 0.9218 0.0385 0.9274 0.0674 0.9305 0.0689 0.9378 0.0708 0.9256 0.0477 0.9289 0.0364 Table9: AverageandstandarddeviationofSequentialOrderPreservation(SOP)scorescomparison ofLLMsonVivecraftunderdifferentshotsettings. Model GPT-3.5-turbo GPT-4-turbo GPT-4o Gemini-1.5-Pro Mixtral-8x7B LLaMA-3-8b Metrics avg std avg std avg std avg std avg std avg std Zero-Shot 0.0029 0.0312 0.0007 0.0078 0.0012 0.0125 0.0 0.0 0.0 0.0 0.0059 0.0624 1-shot 0.015 0.0734 0.1203 0.2157 0.1568 0.2337 0.1794 0.291 0.0812 0.164 0.0351 0.1215 3-shot 0.1302 0.2352 0.1143 0.2136 0.2826 0.3417 0.2335 0.3417 0.0986 0.2024 0.1124 0.2026 5-shot 0.0395 0.1226 0.1366 0.2388 0.1837 0.278 0.2495 0.3358 0.0553 0.158 0.0374 0.1371 Table10: AverageandstandarddeviationofSemanticStepCoverage(SSC)scorescomparisonof LLMsonVivecraftunderdifferentshotsettings. Model GPT-3.5-turbo GPT-4-turbo GPT-4o Gemini-1.5-Pro Mixtral-8x7B LLaMA-3-8b Metrics avg std avg std avg std avg std avg std avg std Zero-Shot 0.049 0.11 0.1301 0.1443 0.1272 0.1511 0.2221 0.2151 0.024 0.0999 0.1088 0.1549 1-shot 0.1274 0.1988 0.544 0.3672 0.6598 0.3322 0.5747 0.3359 0.4914 0.3617 0.3165 0.3415 3-shot 0.4755 0.3526 0.6486 0.3373 0.6817 0.3204 0.6538 0.3373 0.5414 0.37 0.5299 0.3785 5-shot 0.18 0.2337 0.5035 0.3772 0.6183 0.3416 0.608 0.3546 0.3579 0.3555 0.1606 0.2605 Table 11: Average and standard deviation of Normalized Step Alignment Score (NSAS) scores comparisonofLLMsonHalf-Life: Alyxunderdifferentshotsettings Model GPT-3.5-turbo GPT-4-turbo GPT-4o Gemini-1.5-Pro Mixtral-8x7B LLaMA-3-8b Metrics avg std avg std avg std avg std avg std avg std Zero-Shot 0.838 0.0413 0.8456 0.0366 0.8376 0.0424 0.8447 0.032 0.8376 0.0331 0.848 0.0317 1-shot 0.8354 0.0582 0.8427 0.0489 0.8472 0.0629 0.8627 0.0482 0.807 0.1099 0.8131 0.1289 3-shot 0.8452 0.0551 0.845 0.0467 0.838 0.0757 0.8701 0.0603 0.8255 0.0819 0.8449 0.0707 5-shot 0.8577 0.0773 0.8523 0.0613 0.8039 0.0694 0.8625 0.0691 0.8394 0.0834 0.848 0.0976 19 --- Page 20 ---  Table12: AverageandstandarddeviationofSequentialOrderPreservation(SOP)scorescomparison ofLLMsonHalfLife: Alyxunderdifferentshotsettings. Model GPT-3.5-turbo GPT-4-turbo GPT-4o Gemini-1.5-Pro Mixtral-8x7B LLaMA-3-8b Metrics avg std avg std avg std avg std avg std avg std Zero-Shot 0.0098 0.0802 0.0252 0.1265 0.0396 0.1745 0.0082 0.0669 0.0019 0.0158 0.0123 0.0704 1-shot 0.0447 0.0764 0.0402 0.1224 0.024 0.0733 0.0198 0.1263 0.0425 0.0816 0.0447 0.0967 3-shot 0.0725 0.1159 0.0312 0.0725 0.0701 0.1261 0.1349 0.2187 0.0703 0.1094 0.087 0.1687 5-shot 0.123 0.1834 0.1248 0.2382 0.0216 0.0809 0.2089 0.2938 0.1257 0.2409 0.1259 0.2385 Table13: AverageandstandarddeviationofSemanticStepCoverage(SSC)scorescomparisonof LLMsonHalfLife: Alyxunderdifferentshotsettings. Model GPT-3.5-turbo GPT-4-turbo GPT-4o Gemini-1.5-Pro Mixtral-8x7B LLaMA-3-8b Metrics avg std avg std avg std avg std avg std avg std Zero-Shot 0.0785 0.1843 0.2231 0.2089 0.2424 0.2111 0.1989 0.1982 0.0716 0.1187 0.1662 0.172 1-shot 0.2562 0.235 0.3485 0.2336 0.4184 0.2413 0.3859 0.2654 0.3256 0.1934 0.3872 0.2058 3-shot 0.3072 0.2444 0.3648 0.2414 0.5611 0.229 0.5494 0.2887 0.3544 0.2202 0.4599 0.2371 5-shot 0.425 0.2814 0.6127 0.2856 0.6934 0.2359 0.6299 0.315 0.4642 0.2957 0.5152 0.2708 Table14: NormalizedStepAlignmentScore(NSAS)scorescomparisonofLLMson Moss: BookII underdifferentshotsettings Model GPT-3.5-turbo GPT-4-turbo GPT-4o Gemini-1.5-Pro Mixtral-8x7B LLaMA-3-8b Metrics avg std avg std avg std avg std avg std avg std Zero-Shot 0.7819 0.0403 0.8055 0.0717 0.7871 0.0596 0.7994 0.0548 0.7913 0.0595 0.7916 0.0572 1-shot 0.776 0.0616 0.7993 0.0771 0.803 0.0924 0.8139 0.0778 0.7663 0.0793 0.7938 0.0763 3-shot 0.7776 0.0889 0.818 0.0925 0.8016 0.1242 0.8302 0.0935 0.7613 0.1341 0.7895 0.1371 5-shot 0.782 0.0952 0.8243 0.102 0.8237 0.1092 0.8478 0.1017 0.756 0.1469 0.8232 0.105 Table15: AverageandstandarddeviationofSequentialOrderPreservation(SOP)scorescomparison ofLLMsonMoss: BookIIunderdifferentshotsettings. Model GPT-3.5-turbo GPT-4-turbo GPT-4o Gemini-1.5-Pro Mixtral-8x7B LLaMA-3-8b Metrics avg std avg std avg std avg std avg std avg std Zero-Shot 0.0091 0.0581 0.0263 0.1533 0.0113 0.0494 0.0197 0.1029 0.0052 0.033 0.0 0.0 1-shot 0.034 0.1568 0.0663 0.2044 0.1084 0.2486 0.1145 0.2495 0.0739 0.1721 0.0578 0.1486 3-shot 0.1581 0.242 0.1678 0.2522 0.2324 0.3185 0.2272 0.3324 0.1351 0.252 0.2584 0.3089 5-shot 0.1686 0.244 0.2182 0.2801 0.2998 0.3062 0.2652 0.3596 0.1169 0.247 0.2831 0.3097 20 --- Page 21 ---  Table16: AverageandstandarddeviationofSemanticStepCoverage(SSC)scorescomparisonof LLMsonMoss: BookIIunderdifferentshotsettings. Model GPT-3.5-turbo GPT-4-turbo GPT-4o Gemini-1.5-Pro Mixtral-8x7B LLaMA-3-8b Metrics avg std avg std avg std avg std avg std avg std Zero-Shot 0.0715 0.1792 0.2491 0.2844 0.2313 0.2567 0.1763 0.221 0.0407 0.0991 0.1208 0.1779 1-shot 0.0748 0.1719 0.259 0.2771 0.3682 0.3018 0.3449 0.3396 0.1749 0.2393 0.2319 0.293 3-shot 0.3349 0.3069 0.4593 0.3309 0.5001 0.3444 0.5238 0.3738 0.3207 0.3105 0.4689 0.3399 5-shot 0.3737 0.3213 0.4951 0.3373 0.5562 0.3319 0.6091 0.3476 0.2974 0.3385 0.4567 0.3173 Table 17: Average and standard deviation of Normalized Step Alignment Score (NSAS) scores comparisonofLLMsonVRgameIntotheRadiusunderdifferentshotsettings Model GPT-3.5-turbo GPT-4-turbo GPT-4o Gemini-1.5-Pro Mixtral-8x7B LLaMA-3-8b Metrics avg std avg std avg std avg std avg std avg std Zero-Shot 0.6165 0.0755 0.6408 0.0955 0.5939 0.1306 0.6492 0.1018 0.6644 0.0718 0.6447 0.0882 1-shot 0.641 0.1177 0.6519 0.1421 0.6282 0.1687 0.6875 0.1159 0.6285 0.1684 0.6285 0.1346 3-shot 0.6305 0.128 0.6802 0.1645 0.6491 0.2057 0.6634 0.1638 0.618 0.1633 0.6479 0.1606 5-shot 0.6621 0.1291 0.6927 0.1721 0.6984 0.2136 0.6818 0.1191 0.666 0.1495 0.6443 0.211 Table18: AverageandstandarddeviationofSequentialOrderPreservation(SOP)scorescomparison ofLLMsonIntotheRadiusunderdifferentshotsettings. Model GPT-3.5-turbo GPT-4-turbo GPT-4o Gemini-1.5-Pro Mixtral-8x7B LLaMA-3-8b Metrics avg std avg std avg std avg std avg std avg std Zero-Shot 0.0091 0.0581 0.0263 0.1533 0.0113 0.0494 0.0197 0.1029 0.0052 0.033 0.0 0.0 1-shot 0.034 0.1568 0.0663 0.2044 0.1084 0.2486 0.1145 0.2495 0.0739 0.1721 0.0578 0.1486 3-shot 0.1581 0.242 0.1678 0.2522 0.2324 0.3185 0.2272 0.3324 0.1351 0.252 0.2584 0.3089 5-shot 0.1686 0.244 0.2182 0.2801 0.2998 0.3062 0.2652 0.3596 0.1169 0.247 0.2831 0.3097 Table19: AverageandstandarddeviationofSemanticStepCoverage(SSC)scorescomparisonof LLMsonIntotheRadiusunderdifferentshotsettings. Model GPT-3.5-turbo GPT-4-turbo GPT-4o Gemini-1.5-Pro Mixtral-8x7B LLaMA-3-8b Metrics avg std avg std avg std avg std avg std avg std Zero-Shot 0.0354 0.0982 0.1528 0.1774 0.2199 0.1745 0.1783 0.2107 0.0406 0.0899 0.1243 0.1655 1-shot 0.1511 0.2246 0.304 0.2983 0.4102 0.2585 0.2823 0.3277 0.2593 0.3249 0.3171 0.269 3-shot 0.2321 0.2713 0.4463 0.3099 0.5623 0.2976 0.3402 0.3379 0.3544 0.2621 0.5319 0.2382 5-shot 0.3302 0.3115 0.5082 0.3063 0.6194 0.2877 0.2971 0.3187 0.285 0.3384 0.5314 0.2886 21 --- Page 22 ---  Figure4: LLMsNSAS(avg)byDifferentShotSettingAcrossFourVRGames 22 --- Page 23 ---  Figure5: LLMsSOP(avg)byDifferentShotSettingAcrossFourVRGames 23 --- Page 24 ---  Figure6: LLMsSSC(avg)byDifferentShotSettingAcrossFourVRGames 24 --- Page 25 ---  C.6 CROSS-GAMEGENERALIZATIONPATTERNS Thecross-gameperformanceanalysisrevealsimportantinsightsaboutmodelgeneralizationcapabili- ties. Modelsthatperformwellononegamedonotnecessarilymaintaintheiradvantageacrossothers. Forexample,whileGPT-4oachievesthehighestSOPscoreinIntotheRadius(0.291),itperforms poorlyinHalf-Life: Alyx(0.022). Thisgame-specificvariationsuggeststhatmodelsmayoverfitto particularinteractionpatternsratherthandevelopinggeneralVRmanipulationcapabilities. The"GameGap"metricinthetable3quantifiesthisgeneralizationchallenge. Lowervaluesindicate more consistent cross-game performance. Mixtral-8x7B achieves the lowest Game Gap (0.070), despite not leading in any individual game. This consistency might make it more suitable for applicationsrequiringreliableperformanceacrossdiverseVRexperiences. Incontrast,GPT-4o’s highGameGap(0.127)reflectsitsspecializedstrengthsandweaknessesacrossdifferentinteraction paradigms. Analysisofconfusionpatternsrevealsthatmodelsstrugglemostwhentransitioningbetweengames withdifferentcontrolschemes. TheshiftfromVivecraft’sdiscreteblockinteractionstoHalf-Life: Alyx’scontinuousphysicsmanipulationrepresentsafundamentalchangeinhowactionsmapto controllerinputs. Modelstrainedprimarilyontextlacktheembodiedexperiencetonavigatethese transitionssmoothly,oftenapplyinginappropriateinteractionpatternslearnedfromonecontextto another. C.7 TEMPORALDYNAMICSINSEQUENTIALTASKS Detailedexaminationofstep-by-stepperformancerevealshowmodelshandletemporaldependencies inVRinteractions. Earlystepsinsequencesgenerallyshowhigheraccuracy(NSAS>0.9)across allmodels,withperformancedegradingforlatersteps. Thisdegradationisparticularlyseverefor stepsthatdependonthesuccessfulcompletionofpreviousactions. Forinstance,inasequencelike "pickupobject,aimattarget,throwobject,"modelsmaycorrectlyidentifyallthreeactionsbutfail torecognizethataimingrequiressuccessfullycompletingthepickupactionfirst. TheSOPmetricspecificallycapturesthesetemporaldependencies, andthelowscoresacrossall modelshighlightafundamentallimitationincurrentarchitectures. Evenwithfew-shotexamplesthat demonstratecorrectordering,modelsstruggletointernalizethecausalrelationshipsbetweensteps. Thissuggeststhatimprovedperformancemayrequirearchitecturalinnovationsthatbettercapture temporalandcausalreasoning,ratherthansimplyscalingexistingapproaches. Erroranalysisrevealscommonpatternsintemporalmistakes. Modelsfrequentlysuggestparallel actionsthatmustbeperformedsequentially(e.g.,"presstriggerwhilereachingforobject"whenthe triggercanonlybemeaningfullypressedaftergrasping). Theyalsostrugglewithiterativeprocesses, oftenomittingloopconditionsorterminationcriteria. Thesepatternsindicatethatmodelslackan understandingofthephysicalconstraintsthatgovernVRinteractions. C.8 DETAILEDPERFORMANCETABLESANDVISUALIZATIONS Thetable2providesgranulardataforresearchersseekingtounderstandspecificmodelbehaviors. Thesetablesrevealseveralnoteworthypatterns. First,therelationshipbetweendifferentmetricsis non-linear. HighNSASscoresdonotguaranteegoodSOPperformance,andmodelswithsimilar averagescoresmayachievethemthroughdifferentstrengths. Thismultidimensionalperformance landscapesuggeststhatselectingmodelsforspecificapplicationsrequirescarefulconsiderationof whichcapabilitiesaremostcritical. Thetable2illustratesthestrictmatchingprocess,highlightingwhySSMscoresremainloweven for generally capable models. The requirement for exact sequence length and step-by-step cor- respondenceprovesextremelydemanding. Evenminorvariationsinphrasingorstepgranularity resultinmatchfailures. ThisvisualizationhelpsexplainwhySSMmaybeoverlystrictforpractical applications,wherefunctionalequivalencemattersmorethanexactreplication. Thetable2demonstratesthemorenuancedevaluationapproachthatunderliesourNSASandSOP metrics. Byidentifyingthelongestcommonsubsequenceswithsemanticmatching,thesemetrics bettercapturefunctionalunderstandingwhilestillpenalizingsignificantdeviationsfromgroundtruth. 25 --- Page 26 ---  ThevisualizationshowshowmodelsmightachievereasonableNSASscoresbyidentifyingmost relevantactionswhilestillfailingSOPevaluationduetoorderingerrors. Theheatmapsofmodelperformanceacrossgame-taskcombinationsrevealclusteringofdifficulty. Certaintasktypes(e.g.,combatsequencesinHalf-Life: Alyx,inventorymanagementinIntothe Radius)consistentlychallengeallmodels,whileothers(e.g.,blockplacementinVivecraft)show near-ceilingperformance. Thesepatternssuggestthattargetedimprovementsforspecificinteraction typesmightyieldbetterresultsthangeneralcapabilityenhancement. C.9 IMPLICATIONSFORFUTURERESEARCH ThedetailedexperimentalresultspaintacomplexpictureofcurrentLLMcapabilitiesandlimitations inVRinteractionreasoning. Whilemodelsdemonstratecompetenceinidentifyingrelevantactions and decomposing high-level goals, they consistently struggle with the procedural and embodied aspects of VR interaction. The strong effect of few-shot examples suggests that current models possesslatentcapabilitiesthatcanbeactivatedthroughappropriateprompting,butfundamentalarchi- tecturallimitationspreventthemfromachievinghuman-likeunderstandingofphysicalmanipulation sequences. Thehighvarianceinperformanceacrossgamesandtasksindicatesthatrobustnessremainsasig- nificantchallenge. Modelsthatexcelinonecontextmayfaildramaticallyinanother,limitingtheir practicalapplicability. Thisbrittlenesslikelystemsfromthediscretenatureoftext-basedtraining, whichlacksthecontinuous,embodiedexperiencethathumansleveragewhenlearningnewphysical tasks. Movingforward,theseresultssuggestseveralpromisingresearchdirections. Multimodalmodelsthat incorporatevisualandproprioceptiveinformationalongsidetextmaybettercapturetheembodied natureofVRinteractions. Explicitmodelingoftemporalandcausalrelationshipscouldaddressthe proceduralreasoninggapsidentifiedinourexperiments. Finally,trainingonsyntheticVRinteraction data or through simulated embodiment might provide models with the experiential knowledge currentlylackingintext-onlyapproaches. The detailed results also highlight the importance of comprehensive evaluation frameworks that assess multiple dimensions of capability. Single metrics fail to capture the complexity of VR interactionreasoning,andfuturebenchmarksshouldcontinuetoembracemultidimensionalevaluation approachesthatcanidentifyspecificstrengthsandweaknessesinmodelcapabilities. D DISCUSSION, LIMITATIONS & BROADER IMPACTS Our investigation into LLMs’ ability to translate semantic actions into VR device manipulations revealsbothpromisingcapabilitiesandfundamentallimitationsthatreflectbroaderchallengesin bridginglinguisticunderstandingandembodiedinteraction. TherelativelylowSequentialOrder Preservation (SOP) scores across all evaluated models indicate that current LLMs struggle with thetemporalreasoningrequiredforcomplexproceduraltasks. Thislimitationsuggeststhatwhile LLMscanidentifyrelevantactionsandunderstandtheirpurposes,theylacktheembodiedexperience necessarytoaccuratelysequencephysicalmanipulations. ThesubstantialperformancevariationsacrossdifferentVRgameshighlighthowinteractioncom- plexityandconsistencyimpactmodelperformance. Gameswithstandardized,discreteactions(like Vivecraft’sblock-basedinteractions)provemoreamenabletoLLMreasoningthanthoserequiring nuancedcontrollermovementsorcomplexspatialreasoning(likeHalf-Life: Alyx). Thispattern suggeststhatcurrentlanguagemodelsmaybenefitfrommorestructuredrepresentationsofphysical actionsandexplicittrainingonproceduralsequences. Thesignificantimprovementfromfew-shotexamplesdemonstratesthatLLMspossesslatentcapa- bilitiesforVRinteractionreasoningthatcanbeactivatedthroughappropriateprompting. However, the fact that performance plateaus with additional examples indicates fundamental architectural limitations rather than simple lack of exposure to relevant examples. This finding suggests that advancesinVR-capableAImayrequirenewtrainingparadigmsthatincorporatespatialandtemporal reasoningmoredirectly. 26 --- Page 27 ---  Fromabroaderperspective,thisworkcarriesimportantimplicationsforthefutureofhuman-computer interactionandAIdevelopment. Onthepositiveside,LLMsthatcaneffectivelyreasonaboutVR interactionscoulddramaticallyimproveaccessibilityforuserswithmotorimpairments,enablemore intuitivenaturallanguageinterfacesforVRapplications,andacceleratethedevelopmentofintelligent tutoringsystemsforVRtrainingscenarios. Thepotentialtransferofthesecapabilitiestorobotic systems could enable more sophisticated human-robot collaboration in both virtual and physical environments. However,wemustalsoconsiderpotentialnegativeimplications. AsLLMsgaingreateragencyin controllingvirtual(andpotentiallyphysical)systems,questionsofsafety,security,anduserautonomy becomeparamount.Theabilitytotranslatehigh-levelcommandsintodetailedmanipulationsequences could be exploited for unauthorized system control or social engineering attacks. Additionally, thecomputationalresourcesrequiredfortraininganddeployingsuchmodelsraiseenvironmental concernsthatmustbebalancedagainsttheirbenefits. The digital divide may be exacerbated as advanced VR-AI systems require substantial hardware investmentsandtechnicalexpertise. Ensuringequitableaccesstothesetechnologieswillrequire conscious effort from researchers, developers, and policymakers. Privacy concerns also emerge asthesesystemsnecessarilymonitorandanalyzedetailedusermovementpatternsandinteraction behaviors. Movingforward,thefieldmustpursueresponsibledevelopmentpracticesthatprioritizeusersafety, privacy,andautonomywhileadvancingthetechnicalcapabilitiesofVR-AIsystems. Thisincludes developingrobustevaluationframeworksthatassessnotonlytaskperformancebutalsofailuremodes, implementingtransparentsystemsthatuserscanunderstandandcontrol,andensuringthatadvances inVRinteractionAIservetoaugmentratherthanreplacehumanagencyinvirtualenvironments. E LARGE LANGUAGE MODELS USAGE STATEMENT This work incorporated LLMs to aid in editorial refinement and linguistic improvement of the manuscript. Themodelsprovidedassistancewithstylisticenhancementsandclarityoptimization, includingtaskssuchasrephrasingsentencesandcorrectinggrammaticalerrors. WeexplicitlynotethatLLMsplayednoroleintheconceptualization,theoreticaldevelopment,or experimentaldesignaspectsofthisresearch. Theauthorsretainfullresponsibilityfortheentiretyof themanuscript’scontent,includingsectionsimprovedwithLLMsupport. AllLLM-assistedtexthas beencarefullyreviewedtoensureadherencetoacademicstandardsandethicalresearchpractices. 27