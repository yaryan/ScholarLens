--- Page 1 --- ROUTING MATTERS IN MOE: SCALING DIFFUSION TRANSFORMERS WITH EXPLICIT ROUTING GUIDANCE YujieWei1,ShiweiZhang2‚àó,HangjieYuan3,YujinHan4,ZhekaiChen4,5,JiayuWang2, DifanZou4,XihuiLiu4,5,YingyaZhang2,YuLiu2,HongmingShan1‚Ä† 1FudanUniversity 2TongyiLab,AlibabaGroup 3ZhejiangUniversity 4TheUniversityofHongKong 5MMLab ABSTRACT Mixture-of-Experts(MoE)hasemergedasapowerfulparadigmforscalingmodel capacity while preserving computational efficiency. Despite its notable success in large language models (LLMs), existing attempts to apply MoE to Diffusion Transformers (DiTs) have yielded limited gains. We attribute this gap to fun- damental differences between language and visual tokens. Language tokens are semanticallydensewithpronouncedinter-tokenvariation,whilevisualtokensex- hibit spatial redundancy and functional heterogeneity, hindering expert special- ization in vision MoE. To this end, we present ProMoE, an MoE framework featuring a two-step router with explicit routing guidance that promotes expert specialization. Specifically, this guidance encourages the router to first partition image tokens into conditional and unconditional sets via conditional routing ac- cordingtotheirfunctionalroles,andsecondrefinetheassignmentsofconditional imagetokensthroughprototypicalroutingwithlearnableprototypesbasedonse- mantic content. Moreover, the similarity-based expert allocation in latent space enabledbyprototypicalroutingoffersanaturalmechanismforincorporatingex- plicitsemanticguidance,andwevalidatethatsuchguidanceiscrucialforvision MoE. Building on this, we propose a routing contrastive loss that explicitly en- hances the prototypical routing process, promoting intra-expert coherence and inter-expert diversity. Extensive experiments on ImageNet benchmark demon- stratethatProMoEsurpassesstate-of-the-artmethodsunderbothRectifiedFlow andDDPMtrainingobjectives. Codeandmodelswillbemadepubliclyavailable. 1 INTRODUCTION Diffusionmodels(Hoetal.,2020)havemadesubstantialadvancesforvisualsynthesis(Rombach etal.,2022b;Yangetal.,2024;Weietal.,2024a;Wanetal.,2025). Drivenbythegrowingdemand for higher fidelity and quality, research has focused on scaling up diffusion models (Esser et al., 2024b)andpropelledanarchitecturalshiftfromU-Net(Ronnebergeretal.,2015)backbonestothe now-prevalentDiffusionTransformers(DiTs)(Peebles&Xie,2023). Despitetheproveneffective- nessofDiT-basedmodels(Esseretal.,2024a),theirdenseactivationofallparametersirrespective oftaskorinputincurssubstantialcomputationaloverhead,therebyhinderingfurtherscalability. To scale toward larger and more capable models, the large language model (LLM) community has widely adopted the Mixture-of-Experts (MoE) (Jacobs et al., 1991; Shazeer et al., 2017) paradigm, which expands model capacity while maintaining computational efficiency. Conceptu- ally, an MoE layer dispatches input tokens to specialized ‚Äúexpert‚Äù sub-networks via a router and returns a weighted sum of the selected experts‚Äô outputs. Despite MoE‚Äôs profound success in lan- guagemodeling(Jiangetal.,2024;Daietal.,2024), recenteffortstointegrateitintoDiTmodels havenotyieldedthesignificantgainsobservedinLLMs. Specifically,DiT-MoE(Feietal.,2024), which employs token-to-expert routing, often underperforms dense counterparts despite activating thesamenumberofparameters. Incontrast,EC-DiT(Sunetal.,2024),whichassignseachexpert a fixed quota of tokens, delivers only marginal gains even with extended training. More recently, ‚àóProjectLeader ‚Ä†CorrespondingAuthor 1 5202 tcO 82 ]VC.sc[ 1v11742.0152:viXra --- Page 2 --- LLM (Llama-3 8B) Visual DiT (DiT-XL/2) Expert Subspace Similarity (Lower implies Higher Diversity) Avg(inter‚àíclass distance) Avg(inter‚àíclass distance) = ùüèùüó.ùüêùüñùüë = 0.748 Avg(intra‚àíclass distance) Avg(intra‚àíclass distance) (a) t-SNE visualization of language and visual tokens (b) Comparison of expert diversity across MoE layers Figure 1: (a) We randomly sample 1k intermediate-layer tokens from 110 ImageNet classes for 10-cluster k-means clustering (differentiated by color). With class names/labels as inputs, LLM tokensformcompact,well-separatedclusterswithhighsemanticdensity,whereasvisualtokensare diffuse.Thisdisparityisquantifiedbytheratioofinter-tointra-classdistance(19.283‚â´0.748).(b) Wemeasure inter-expertdiversity usingsingular valuedecomposition oneach MoElayer‚Äôs expert weight matrices and computing the mean similarity of the subspaces spanned by their top-k left singularvectors(Huetal.,2021). Incorporatingroutingguidance(Ours)enhancesexpertdiversity. DiffMoE (Shi et al., 2025), which introduces a global token-distribution routing scheme, still re- portsrelativelylimitedimprovements. ThispronouncedgapbetweenMoE‚Äôstransformativeimpact in LLMs and its modest returns in DiT models motivates a fundamental question: What are the underlyingfactorsthatimpedetheeffectivenessofMoEinDiTmodels? Toanswerthisquestion,weexaminehowlinguisticandvisualinputsdifferinmodelsandhighlight the following two distinctive properties of visual inputs. 1) High Spatial Redundancy. Unlike discretetexttokens,whicharesemanticallydensewithsalientinter-tokendifferences,visualtokens (i.e.,imagepatches)arecontinuous,spatiallycoupled,andsubstantiallyredundant(Fig.1(a)). The highcorrelationbetweenpatchesoftenleadsexpertstolearnhomogeneousfeatures. 2)Functional Heterogeneity. The practice of classifier-free guidance (Ho & Salimans, 2022) in diffusion mod- els inherently introduces two functionally distinct input types: conditional and unconditional. A naiveMoEtreatsthemuniformlywithundifferentiatedrouting,ignoringtheirdifferentroles. These propertiescollectivelyimpedeeffectiveexpertdiversityandspecialization(Fig.1(b)). Motivatedbytheseobservations, werevisitthefoundationalprincipleofMoEdesign: expertspe- cialization,inwhicheachexpertacquiresfocusedandnon-overlappingknowledge(Daietal.,2024; Caietal.,2025). Wedecomposethisobjectiveintotwocriteria: Intra-ExpertCoherence, which ensuresthatanexpertconsistentlyprocessessimilarpatterns, maintainingastablefunctionalrole; and Inter-Expert Diversity, which encourages different experts to specialize in distinct tasks to achieve functional differentiation. In language modeling, the semantic density and separability of discretetexttokensprovideapotentinductivebiasthatnaturallyfostersexpertspecialization, sat- isfying both criteria. In contrast, for visual inputs, the combination of intrinsic redundancy and extrinsicfunctionalheterogeneitymakesexpertspecializationnon-trivial. Therefore,inthispaper, wemovebeyondimplicitexpertallocation,andintroduceexplicitroutingguidancedesignstopro- motebothintra-expertcoherenceandinter-expertdiversity. Tothisend,wepresentProMoE,aMixture-of-Expertsframeworkfeaturingatwo-steprouterwith explicitroutingguidancetopromoteexpertspecialization. Specifically,thisguidanceprovidestwo distinctroutingsignals:thetoken‚Äôsfunctionalroleanditssemanticcontent.Guidedbythesesignals, the router implements two steps: conditional routing and prototypical routing. First, conditional routingaddressesfunctionalheterogeneitybypartitioningvisualtokensintounconditionalandcon- ditional sets. Unconditional image tokens, derived from image patches under null conditioning (e.g., empty labels or texts), are processed by dedicated unconditional experts. In contrast, condi- tionalimagetokens,obtainedfrompatchesunderspecificconditioning,aredispatchedtostandard MoEexperts.Thishardroutingmechanismenforcesfunctionalsegregation,fosteringspecialization acrossunconditionalandstandardexperts. Second,prototypicalroutingfurtherassignsconditional imagetokensusingasetoflearnableprototypes,eachassociatedwithaspecificexpert,bycomput- ingcosinesimilaritybetweentokenembeddingsandtheprototypesinlatentspace. While prototypical routing is flexible and effective, it still relies on implicit learning from token semantics. Fortunately,itssimilarity-basedallocationinlatentspaceprovidesanaturalmechanism forinjectingexplicitsemanticroutingguidance.Wevalidatetheimportanceofsemanticguidancein systematicexperiments(Sec.4.2),wherebothexplicit(classification-based)andimplicit(clustering- based)guidanceyieldclearimprovements. Buildingonthis,weproposearoutingcontrastiveloss 2 --- Page 3 --- that explicitly enhances the prototypical routing process by assigning semantically similar tokens to the same expert while preserving distinct token distributions across experts. Compared with alternativeguidancestrategies,theproposedcontrastivelossrequiresnomanuallabelsandismore robust,promotingintra-expertcoherenceandinter-expertdiversityinvisionMoE. ExtensiveexperimentalresultsdemonstrateProMoE‚Äôssuperiorperformanceandeffectivescalability on both Flow Matching and DDPM paradigms. Notably, ProMoE achieves significant gains over densemodelsdespiteusingfeweractivatedparameters,andsurpassesstate-of-the-artmethodsthat have1.7√ómoretotalparametersthanours. Insummary,ourcontributionsarefourfold: 1)Byanalyzingdifferencesbetweenlanguageandvi- sualtokens,wepresentProMoE,anMoEframeworkwithexplicitroutingguidanceforDiTmodels. 2)Wedesignatwo-steprouter,whereconditionalroutingfirstpartitionsimagetokensbyfunctional roles,andprototypicalroutingthenrefinesassignmentsusinglearnableprototypesbasedonseman- tic content. 3) We propose a routing contrastive loss that enhances prototypical routing, explicitly enforcing intra-expert coherence and inter-expert diversity. 4) Extensive experiments demonstrate thatProMoEoutperformsdensemodelsandstate-of-the-artMoEmethodsacrossdiversesettings. 2 RELATED WORK Diffusion Models. Diffusion models (Ho et al., 2020; Nichol & Dhariwal, 2021) have made re- markableprogressinvisualsynthesis. Earlywork(Rombachetal.,2022a;Podelletal.,2023;Wei etal.,2024b)primarilyuseU-Net(Ronnebergeretal.,2015)architecturestrainedwiththeDDPM objective (Ho et al., 2020; Song et al., 2020). Recent models (Chen et al., 2023; Ma et al., 2024; Hatamizadehetal.,2024;Chuetal.,2024;Esseretal.,2024a;Weietal.,2025)haveshiftedtoDiffu- sionTransformers(DiT)(Peebles&Xie,2023),offeringsuperiorscalabilityandgenerativequality, and are trained with the more effective Rectified Flow (RF) (Liu et al., 2022), a flow-matching formulation (Lipman et al., 2022) that constructs a straight-line path between data and noise dis- tributions. In this work, we adopt a standard DiT backbone and train with both DDPM and RF objectives,demonstratingtheeffectivenessofourapproachacrossdifferenttrainingparadigms. Mixture of Experts. Mixture-of-Experts (MoE) (Jacobs et al., 1991; Shazeer et al., 2017; Lep- ikhinetal.,2020)aredesignedtoexpandmodelcapacitywhileminimizingcomputationaloverhead by sparsely activating sub-networks for distinct inputs. Inspired by MoE successes in LLMs (Dai etal.,2024;Liuetal.,2024;Lietal.,2025;Muennighoffetal.,2024),recentworkhasintegrated MoE to scale diffusion models to improve generative quality (Riquelme et al., 2021). Early MoE applicationsinU-Net-baseddiffusionmodels(Leeetal.,2024;Balajietal.,2022;Fengetal.,2023; Xueetal.,2023;Parketal.,2023;2024;Zhaoetal.,2024)oftenassignexpertsbydiffusiontimestep ranges,showingstrongscalingpotential. However,adaptingMoEtoDiTarchitecture(Shenetal., 2025;Sehwagetal.,2025;Chengetal.,2025)facesseverallimitations.Token-choiceroutingmeth- ods(e.g.,DiT-MoE(Feietal.,2024))sufferpoorexpertspecializationduetoimbalancedtokenas- signments,whereasexpert-choicemethods(e.g.,EC-DiT(Sunetal.,2024))thatfixtokenquotasper expertyieldonlymarginalgains. Morerecently,DiffMoE(Shietal.,2025)andExpertRace(Yuan etal.,2025)explorebatch-levelglobaltokenselectionandmutualexpert‚Äìtokenrouting,yetstillrely onimplicitexpertlearningandstrugglewithlimitedexpertspecializationduetotheredundancyand functionalheterogeneityofvisualtokens.Incontrast,weanalyzelanguage‚Äìvisiontokendifferences andintroduceexplicitroutingguidancetotheMoErouterbasedonthetoken‚Äôsfunctionalroleandits semanticcontent. Wefurtherenhancetheroutingprocessthroughtheproposedroutingcontrastive loss,promotingintra-expertcoherenceandinter-expertdiversity. 3 PRELIMINARIES Diffusion Models. Diffusion models are generative models that learn data distributions by re- versing a forward noising process. The continuous-time forward process can be formulated as x = Œ± x +œÉ œµ,witht ‚àà U(0,1)andœµ ‚àº N(0,I). Œ± andœÉ aremonotonicallydecreasingand t t 0 t t t increasingfunctionsoft,respectively. Forthereverseprocess,adenoisingnetworkF istrainedto Œ∏ predictthetargetyateachtimestept,conditionedonc(e.g.,classlabelsortextprompts): (cid:104) (cid:105) L=E ‚à•y‚àíF (x ,c,t)‚à•2 , (1) x0,c,œµ,t Œ∏ t 2 3 --- Page 4 --- + ProMoE Block + X Routing Scores Scale ProMoE Unconditional Expert 1 Expert 2 Expert 3 Shared Expert Expert Block Scale,Shift LayerNorm Two-Step Router + Step2: Semantic Routing PrototypicalRouting Guidance Scale Unconditional LatentSpace Multi-Head ImageTokens Self-Attention Scale,Shift MLP LayerNorm Conditional Routing Contrastive Image Tokens Learning Step1: ConditionalRouting Conditioning InputTokens Input Diffusion Transformer Block Tokens Unconditional Image Tokens Conditional Image Tokens Learnable Prototypes For Routing Figure2:OverviewofProMoEarchitecture.Theinputtokensaresplitbyconditionalroutinginto unconditionalandconditionalsubsets. Unconditionalimagetokensareprocessedbyunconditional experts. Conditional image tokens are assigned by prototypical routing with learnable prototypes. Theroutingcontrastivelearningexplicitlyenhancessemanticguidanceinprototypicalrouting. wherethetrainingtargetycanbetheGaussiannoiseœµforDDPMmodels(Hoetal.,2020),orthe vectorfield(œµ‚àíx )forRectifiedFlowmodels(Liuetal.,2022). 0 Mixture of Experts. Mixture-of-Experts (MoE) is an architectural paradigm that scales model capacity while preserving computational efficiency by selectively activating a subset of ‚Äúexperts‚Äù sub-networks. AstandardMoElayercomprisesN expertsandatrainablerouterR. Eachexpert E E is implemented as a feed-forward network (FFN). Given input X ‚àà RB√óL√óD, where B is the i batch size, L is the token length, D is the hidden dimension, the router R maps the input X to token‚ÄìexpertaffinityscoresS‚ààRB√óL√óNE viaanactivationfunctionA: S=A(R(X)). (2) Ateachforwardpass,therouteractivatesthetop-Khighest-scoringexpertsanddispatchestheinput tothem.Thefinaloutputistheweightedsumoftheactivatedexperts‚Äôoutputswithagatingfunction: (cid:26) S, ifS‚ààTopK(S) (cid:88)NE G= , MoE(X)= G ‚àóE (X), (3) 0, Otherwise i i i=1 whereG‚ààRB√óL√óNE isthefinalgatingtensor. TherearetwocommonroutingparadigmsinMoE: Token-Choice (TC) and Expert-Choice (EC). In TC, each token independently selects its top-K experts;inEC,eachexpertselectsafixednumberoftop-K tokens. 4 PROMOE In this section, we present ProMoE, an MoE framework for DiTs that integrates a two-step router withexplicitroutingguidance. TheoverallpipelineisdepictedinFig.2. Wefirstdetailthetwo-step routerinSec.4.1. WethenvalidatetheimportanceofsemanticroutingguidanceinvisualMoEsin Sec.4.2andfurtherproposeroutingcontrastivelearningtoenhancesemanticguidanceinSec.4.3. 4.1 TWO-STEPROUTER TheProMoErouteroperatesintwosteps: conditionalroutingbasedonthetoken‚Äôsfunctionalrole, followedbyfine-grainedprototypicalroutingbasedontokensemantics. 4 --- Page 5 --- Conditional Routing. Unlike LLMs, diffusion models typically employ classifier-free guidance (CFG)(Ho&Salimans,2022)atinferencetoenhancesamplequality. Specifically,CFGsteersthe generationprocessbycombiningthemodel‚Äôsconditionalandunconditionalnoisepredictions. This paradigmnaturallydefinestwofunctionallyheterogeneoustokens: 1)unconditionalimagetokens, derivedfromimagepatchesundernullconditioning(e.g.,emptylabelsortexts);and2)conditional imagetokens,obtainedfrompatchesunderspecificconditioning(e.g.,classlabelsortexts). Tohandledifferenttokentypes,thefirststepoftheProMoErouteremployshardroutingbasedon input conditioning. Specifically, unconditional image tokens are deterministically assigned to N u unconditionalexperts, eachimplementedasafeed-forwardnetwork(FFN),analogoustostandard experts. Conversely,conditionalimagetokensarepassedtothesecondstepforfine-grainedrouting amongstandardMoEexperts. Thisexplicitpartitioningencouragesexpertstolearnthefunctional disparitybetweentokentypes,facilitatingthespecializationofunconditionalandstandardexperts. PrototypicalRouting. ThesecondstepofourProMoErouteristodispatchconditionalimageto- kensforfine-grainedexpertallocation. Concretely,weintroduceanovelprototypicalroutingmech- anismwheretheroutingweightsareparameterizedbyasetoflearnableprototypesP ‚àà RNE√óD, asillustratedinFig.2. Eachprototypep correspondstoanexpertE andistrainedtorepresentthe i i sharedcharacteristicsofaclusterofsemanticallysimilartokens. Compared with standard MoE token assignment, which computes pre-activation scores Z ‚àà RB√óL√óNE viaalinearlayer,weassigntokensusingcosinesimilarity,whichismoreeffectiveand naturallysuitedformeasuringsemanticsimilarityinlatentspacebetweentokensandprototypes: x p‚ä§ Z =[R(X)] =Œ± i j , (4) i,j i,j ‚à•x ‚à•‚à•p ‚à• i j wherex andp arethei-thtokeninXandthej-thprototypeinP. Œ±isascalingfactor. i j Then, the activation function A transforms the pre-activation scores Z into token‚Äìexpert affinity scoresS. Insteadofsoftmax,whichiscomputationallyexpensiveandsensitivetosequencelength, weoptforasimplemonotonicfunctionthatpreservesrelativerankings. Weevaluatebothsigmoid and identity functions, finding that the identity A(Z) = Z performs best in practice, as shown in Tab. 7. We argue that the identity activation enables direct top-K selection and provides stable training,thusimprovingperformance.Consequently,weadoptidentityactivationasS=Z.Finally, eachconditionalimagetokenisroutedtothetop-K expertswithgatingscoresG,asinEq.(3). Forward Process. Besides unconditional and standard experts, we also incorporate N shared s expertsthatprocessalltokenstolearnsharedknowledge(Daietal.,2024;Chengetal.,2025). For eachtoken, theoutputofourMoEblockisdefinedasthesumofthesharedexperts‚Äôoutputanda selectiveoutputdeterminedbythetokentype(conditionalorunconditional): (cid:88)Ns Ô£± Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤(cid:88)NE G j ‚àóE j(x) ifx‚ààX c MoE(x)= ES(x)+ j=1 , (5) (cid:124)i=1 (cid:123)(cid:122)i (cid:125) Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥(cid:88)Nu E kU(x) ifx‚ààX u Shared k=1 whereES,E ,andEUaretheshared,standard,andunconditionalexperts,respectively. X andX i j k c u areconditionalandunconditionalimagetokensets,respectively,andX ‚à™X =X. c u To maintain a constant number of activated parameters, MoE models often employ fine-grained expertsegmentation(Daietal.,2024),wheretheinnerhiddendimensionofeachexpertisdivided bythenumberofactivatedexperts. Inourmostsettings,eachforwardpassofourmodelactivates exactly two experts: the single shared expert and one expert selected from the combined pool of standardandunconditionalexperts. Therefore, tomatchthecomputationalcostofadensemodel, wedividethehiddendimensionofeachexpert‚Äôsintermediatelayerbyafactoroftwo. 4.2 SEMANTICROUTINGGUIDANCE Duetotheinherenthighspatialredundancyofvisualtokens,anaiveMoErouterfailstosufficiently distinguish tokens for effective routing, leading experts to learn homogeneous features. Conse- quently, additional semantic routing guidance is required to promote intra-expert coherence and 5 --- Page 6 --- inter-expertdiversity. Tovalidatethis,weconductexperimentsbyaugmentingtheMoErouterwith twoguidancetypes: 1)ExplicitRoutingGuidanceand2)ImplicitRoutingGuidance. Explicit Routing Guidance. We design a routing classification loss that uses class labels to ex- plicitly guide token assignment. Specifically, we manually partition the 1K ImageNet classes into N superclasses based on coarse labels in (Feng & Patras, 2023), and allocate one expert per su- c perclass. Sincelabelsaresample-level,weinstantiatetherouterasaclassifierC: weaverage-pool theinputXoverthetokenlengthdimensiontoobtainX¬Ø,feedX¬Ø intoC toproducesample‚Äìexpert affinityscoresS¬Ø ‚ààRB√óNc,andassigntheexpertwithhighestscore. Duringtraining,wesupervise theroutingprocesswithacross-entropylossL =CE(S¬Ø,c¬Ø),wherec¬Øisthesuperclasslabel. cls Implicit Routing Guidance. We replace the standard MoE router with k-means clustering, as- signingalltokensinaclustertoasingleexpert. Unliketheroutingclassificationlossthatprovides explicitsupervision,thisdesignoffersimplicitguidancebymeasuringtokensimilarity,encouraging semanticallysimilartokenstobeco-assigned.Concretely,weinitializeN clustercentroidsbyran- E domlysamplingtokens. Ateachforwardpass, wecomputeeachtoken‚Äôsdistancestoallcentroids toobtaindistance-basedtoken‚Äìexpertaffinityscores. Eachtokenisthenassignedtoitsnearestcen- troidandthusroutedtothecorrespondingexpert. Duringtraining,centroidsareupdatediteratively byreplacingeachwiththemeanoftheircurrentlyassignedtokens. Results for both routing guidance are reported Table1:ComparisonresultsunderRectifiedFlowon in Tab. 1, with all MoE models having the ImageNet (256√ó256) after 500K training steps, evalu- same activated parameters and comparable to- atedwithCFG=1.5. tal parameters to ensure fairness. On the base Model(500K) FID50K‚Üì IS‚Üë model size, DiT-MoE (Fei et al., 2024) and Dense-DiT-B-Flow 9.02 131.13 DiffMoE(Shietal.,2025)yieldlimitedperfor- DiT-MoE-B-Flow 8.94 131.66 manceimprovements.Incontrast,addingeither DiffMoE-B-Flow 8.22 137.46 explicitorimplicitguidanceproducessubstan- tialgains.Notably,forbothguidancestrategies, Classification-basedRouting 5.91 165.45 wedisabletheload-balancinglosstoisolateits K-Means-basedRouting 6.24 159.77 routingeffects; despiteitsimportanceforTCrouting,guidancealonestillmarkedlyimprovesper- formance. ThesefindingshighlightthepivotalroleofsemanticroutingguidanceinvisionMoEs. 4.3 ENHANCINGSEMANTICROUTINGGUIDANCEVIAROUTINGCONTRASTIVELEARNING WhiletheroutingguidancestrategiesinSec.4.2areeffective,theysufferfromkeylimitations: 1) The classification-based routing loss is defined at the sample level, restricting token-level flexibil- ityandrequiringcostlymanualannotations, hinderinggeneralization. 2)Clustering-basedrouting supports only top-1 assignment, and struggles to scale to top-K, as methods like k-means rely on disjointclusters,makingmulti-centroidassignmentdifficult. Moreover,k-meansissensitivetothe numberofclustersandtheclusterinitialization(Arthur&Vassilvitskii,2006),reducingrobustness. Toaddresstheselimitations,weproposetheRoutingContrastiveLoss(RCL),asillustratedinFig.2, toexplicitlyenhancesemanticguidanceinprototypicalrouting. Givenamini-batchofconditional image tokens, RCL encourages semantically similar tokens to be routed to the same expert and pushes dissimilar tokens toward different experts, prompting expert specialization in MoE. Con- cretely,foreachprototypep associatedwithexpertE ,tokensassignedtop formthepositiveset, i i i representing a cluster of semantically similar tokens, while tokens dispatched to other prototypes constitutethenegativesets,comprisingmultipleclusterswithsemanticsdifferentfromp . i Next, RCL pulls each prototype p toward the centroid of its positive token set to enforce intra- i expert coherence, while pushing it away from the centroids of negative sets to encourage inter- expert diversity. Let X denote the tokens assigned to expert E in a mini-batch, its centroid m i i i is computed as the token mean: m = 1 (cid:80) x‚ààX . The RCL loss is then computed over the i |Xi| i prototypesofN expertsthatareassignedtokensinanonlinemanner: a L =‚àí 1 (cid:88)Na log exp(sim(p i,m i)/œÑ) , (6) RCL N a i=1 (cid:80)N j=a 1exp(sim(p i,m j)/œÑ) wheresim(a,b) = ab‚ä§ denotescosinesimilarity, andœÑ isatemperaturehyperparameter. Fur- ‚à•a‚à•‚à•b‚à• thermore, weempiricallyfindthatthepush-awayoperationinRCLactsasaload-balancingregu- 6 --- Page 7 --- Table2:ModelconfigurationsofProMoEwithdifferentmodelsizes,aligningwithDiT(Peebles &Xie,2023). ‚ÄúE14A1S1U1‚Äùdenotesthatatotalof14expertsareused,with1expertactivatedfor eachtoken,1expertsharedbyalltokens,and1unconditionalexpertforunconditionalimagetokens. ModelConfig #ActivatedParams. #TotalParams. #Experts #BlocksL #Hiddendim.D #Headn ProMoE-S 33M 75M E14A1S1U1 12 384 6 ProMoE-B 130M 300M E14A1S1U1 12 768 12 ProMoE-L 458M 1.063B E14A1S1U1 24 1024 16 ProMoE-XL 675M 1.568B E14A1S1U1 28 1152 16 Table 3: Quantitative comparison with Dense DiTs under Rectified Flow on ImageNet (256√ó256)after500Ktrainingsteps,evaluatedwithCFGscalesof1.0and1.5. Activated Total cfg=1.0 cfg=1.5 Model(500K) # # Params. Params. FID50K‚Üì IS‚Üë FID50K‚Üì IS‚Üë Dense-DiT-B-Flow 130M 130M 30.61 49.89 9.02 131.13 ProMoE-B-Flow 130M 300M 24.44 60.38 6.39 154.21 Dense-DiT-L-Flow 458M 458M 15.44 84.20 3.56 209.03 ProMoE-L-Flow 458M 1.063B 11.61 100.82 2.79 244.21 Dense-DiT-XL-Flow 675M 675M 13.38 91.57 3.23 227.05 ProMoE-XL-Flow 675M 1.568B 9.44 114.94 2.59 265.62 larizerbasedontokensemantics,andismoreeffectivethantraditionalload-balancingloss(Shazeer etal.,2017)(seeAppendixE.1). ThefinaltraininglossofProMoEisthecombinationofEqs.(1) and(6),weightingL byafactorŒª . RCL RCL 5 EXPERIMENT 5.1 EXPERIMENTALSETUP Baselineandmodelconfigurations. WecompareagainstDense-DiT(Peebles&Xie,2023)and MoEbaselines,includingDiT-MoE(Feietal.,2024),EC-DiT(Sunetal.,2024),andDiffMoE(Shi etal.,2025). Forafaircomparison,allMoEmodelsareevaluatedwithequivalentactivatedparame- terstothedensemodelandcomparabletotalparameters,trainingwithbothDDPM(Hoetal.,2020) andRectifiedFlow(Esseretal.,2024a)objectives. WescaleProMoEacrossfoursizes(S/B/L/XL) to align with established DiT benchmarks, as shown in Tab. 2. Models are named as: [Model]- [Size]-[Training Type], with an additional expert configuration. For instance, expert configuration E14A1S1U1denotes14totalexperts(E14),top-1activation(A1)over12standardexperts,1shared expert(S1),and1unconditionalexpert(U1). MoredetailsareprovidedinAppendixA. Implementation details. We conduct experiments on class-conditional image generation using the ImageNet (Deng et al., 2009) dataset, which contains 1,281,167 training images across 1,000 classes. Following (Peebles & Xie, 2023), we train all models with the AdamW optimizer with a learning rate of 1e-4. The batch size is 256, and weight decay is 0. We use horizontal flips as the only data augmentation, and a pretrained VAE from Stable Diffusion (Rombach et al., 2022b) to encode and decode images. We also maintain an exponential moving average (EMA) of model parametersduringtrainingwithadecayrateof0.9999,andallreportedresultsusetheEMAmode. Evaluationmetrics. WeevaluateimagegenerationqualityofallmethodsusingFre¬¥chetInception Distance (FID) (Heusel et al., 2017; Dhariwal & Nichol, 2021), calculated over 50K generated sampleswith250DDPMorFlowMatchingEulersamplingsteps. WealsoreportInceptionScore (IS)(Salimansetal.,2016)tomeasurethediversityofgeneratedimages. 5.2 MAINRESULTS ComparisonwithDenseDiT. TheresultsinFig.3(a)andTabs.3and4drawthreeconclusions: 1) ProMoE consistently surpasses dense counterparts at equivalent activated parameters across all sizes,objectives,andCFGsettings,demonstratingstrongeffectiveness,scalability,andgeneraliza- tion. 2)GainsaremorepronouncedunderRectifiedFlow,thecurrentdominanttrainingparadigm, highlightingProMoE‚Äôsabilitytoscalemoderndiffusionmodels. Comparedtodensemodels,with- 7 --- Page 8 --- Table4: QuantitativecomparisonwithDenseDiTsunderDDPMonImageNet(256√ó256)after 500Ktrainingsteps,evaluatedwithCFGscalesof1.0and1.5. Activated Total cfg=1.0 cfg=1.5 Model(500K) # # Params. Params. FID50K‚Üì IS‚Üë FID50K‚Üì IS‚Üë Dense-DiT-B-DDPM 130M 130M 41.19 35.94 18.61 78.71 ProMoE-B-DDPM 130M 300M 40.37 37.84 17.90 82.65 Dense-DiT-L-DDPM 458M 458M 20.81 65.51 6.29 148.38 ProMoE-L-DDPM 458M 1.063B 18.75 73.07 5.12 168.91 Dense-DiT-XL-DDPM 675M 675M 17.67 74.05 5.07 165.81 ProMoE-XL-DDPM 675M 1.568B 15.87 81.90 4.11 187.86 cfg=1.0 cfg=1.0 cfg=1.0 cfg=1.5 (a) Comparison with dense model and MoE SOTAs (b) Scaling model size (c) Scaling the number of experts Figure3: Comparisonsandscalingresultsacrossdiversesettings. Figure4: SamplesgeneratedbyProMoE-XL-Flowafter2Miterationswithcfg=4.0. out CFG, ProMoE-L-Flow reduces FID by 24.8% and increases IS by 19.7%; at the largest scale, ProMoE-XL-FlowreducesFIDby29.4%. WithCFG=1.5,ProMoE-B-FlowreducesFIDby29.2%, whileProMoE-XL-FlowreducesFIDby19.8%. 3)ProMoEisnotablyparameter-efficient; ituses feweractivatedparametersyetoutperformsdensemodelswithmore. Specifically,ProMoE-L-Flow achievesFID11.61/2.79atCFG1.0/1.5,versus13.38/3.23forDense-DiT-XL-Flow. ComparisonwithMoESOTAs. TheresultsinFig.3(a), Tab.5andAppendixTab.9showthat ProMoE outperforms all baselines across both objectives at equivalent activated parameters, with andwithoutCFG.WithoutCFG,ProMoE-L-FlowreducesFIDby19.7%andincreasesISby15.2% relative to DiffMoE-L-Flow; with CFG=1.5, it reduces FID by 20.5% and increases IS by 14.8%. Notably, ProMoE-L-Flow (1.063B params) surpasses the larger DiffMoE-L-Flow with 16 experts (1.846Bparams),despitefewertotalparameters,underscoringtheeffectivenessofourmethod. Visualization Results. Fig. 4 shows the samples generated by ProMoE-XL-Flow on ImageNet (256√ó256)after2MtrainingstepswithCFG=4.0;seeAppendixC.3formoreanalysesandresults. Comparison of training losses. Fig. 5 shows training loss curves for our method, the dense model, and MoE baselines. ProMoE attains lower loss and faster convergence, even at the largest scale with ex- tendedtraining(upto1.2Msteps). Figure5: Traininglosscurvecomparisons. 5.3 SCALINGBEHAVIOR Scalingthemodelsize. AsshowninFig.3(b),ProMoEexhibitsconsistentperformanceimprove- mentsoverDense-DiTwhenscalingfrombase(B)tolarge(L)toXL,with130M,458M,and675M activatedparameters,respectively,therebyvalidatingthescalabilityofourmethod. 8 --- Page 9 --- Table 5: Quantitative comparison with MoE baselines under Rectified Flow on ImageNet (256√ó256)after500Ktrainingsteps,evaluatedwithCFGscalesof1.0and1.5. Activated Total cfg=1.0 cfg=1.5 Model(500K) #Experts # # Params. Params. FID50K‚Üì IS‚Üë FID50K‚Üì IS‚Üë DiT-MoE-L-Flow E8A1S0N0 458M 1.163B 16.57 80.25 4.10 199.05 EC-DiT-L-Flow E8A1S0N0 458M 1.163B 15.58 84.11 3.65 209.06 DiffMoE-L-Flow E8A1S0N0 458M 1.095B 14.46 87.55 3.51 212.78 DiffMoE-L-Flow E16A1S0N0 458M 1.846B 13.55 92.33 3.30 222.40 ProMoE-L-Flow E14A1S1N1 458M 1.063B 11.61 100.82 2.79 244.21 Table 6: Ablation study of each component on ImageNet (256√ó256) after 500K training steps, trainedwithRectifiedFlowandevaluatedwithCFGscalesof1.0and1.5. cfg=1.0 cfg=1.5 Model(500K) FID50K‚Üì IS‚Üë FID50K‚Üì IS‚Üë Dense-DiT-B-Flow 30.61 49.89 9.02 131.13 +PrototypicalRouting 27.93 53.35 7.92 140.86 +RoutingContrastiveLearning 24.97 58.59 6.75 150.15 +ConditionalRouting 24.44 60.38 6.39 154.21 Table 7: Ablation of activation functions, Table 8: Ablation of conditional routing in trained with Rectified Flow on ImageNet K-Means-based Routing, trained with Rectified (256√ó256)for500Ksteps. FlowonImageNet(256√ó256)for500Ksteps. Activation cfg=1.0 cfg=1.5 K-Means-based cfg=1.0 cfg=1.5 (500K) FID‚Üì IS‚Üë FID‚Üì IS‚Üë Routing(500K) FID‚Üì IS‚Üë FID‚Üì IS‚Üë Softmax 25.74 58.04 6.92 149.11 w/oCond. 30.12 50.47 8.75 133.14 Sigmoid 25.49 58.51 6.63 150.94 w/Cond. 25.61 59.76 6.24 159.77 Identity 24.44 60.38 6.39 154.21 Scalingthenumberofexperts. Fig.3(c)showsmonotonicgainsastheexpertnumberincreases from4to16,with1sharedand1unconditionalexpertpersetup.Forafaircomparison,wemaintain comparabletotalparametersoverMoEbaselinesanduse14expertsacrosssettings. 5.4 ABLATIONSTUDIES Ablation on each component. The results in Tab. 6 show that using prototypical routing alone improvesperformanceandalreadysurpassesDiT-MoE-B-FlowandDiffMoE-B-Flow(seeTab.1). Addingroutingcontrastivelearningyieldssubstantialgains,reducingFIDby10.6%andincreasing IS by 9.8%, highlighting the importance of semantic routing guidance. Incorporating conditional routingfurtherlowersFIDandraisesIS.Theseresultsvalidatetheeffectivenessofeachcomponent. Ablation on score activation function. Since our prototypical routing computes similarities in latent space, choosing an appropriate activation to map similarities into routing scores is crucial. AsshowninTab.7,theidentitymappingyieldsthebestperformance,sigmoidissecond-best,and softmaxperformsworst. Consequently,weadopttheidentityfunctionasthescoreactivation. Ablationonconditionalrouting. Weemphasizethattheproposedconditionalrouting isagen- eral, method-agnostic component that can benefit other routing schemes. We ablate it within the K-Means‚Äìbased Routing method in Sec. 4.2. As shown in Tab. 8, removing conditional routing significantlydegradesperformance,underscoringitsimportance. 6 CONCLUSION Inthispaper,wepresentProMoE,aMixture-of-Expertsframeworkfeaturingatwo-steprouterwith explicit routing guidance to promote expert specialization. We analyze differences between lan- guageandvisiontokens: discretetexttokensaresemanticallydense,whereasvisualtokensexhibit high spatial redundancy and functional heterogeneity, hindering the effectiveness of MoE in DiT models. Toaddressthis,weintroduceroutingguidancebasedonthetoken‚Äôsfunctionalroleandse- manticcontent,yieldingatwo-steproutercomprisingconditionalroutingandprototypicalrouting. 9 --- Page 10 --- Furthermore,weproposearoutingcontrastivelossthatenhancessemanticguidanceinprototypical routing, explicitly promoting intra-expert coherence and inter-expert diversity. Extensive experi- mentsdemonstratethatProMoEoutperformsdenseDiTandexistingMoESOTAs,evenwithfewer activatedortotalparameters,providingarobustsolutionforapplyingMoEtoDiTmodels. Limitations. WhilewefollowstandardevaluationprotocolsandreportFID50KandIS,thesemet- rics may not fully capture fine-grained perceptual quality or semantic faithfulness. Moreover, we validateProMoEonlyonimagegeneration;extendingittomultiplemodalitiesremainsanopenand meaningfuldirectionthatweleaveforfuturework. ETHICS STATEMENT Our method achieves substantial improvements on the ImageNet benchmark over dense DiT and state-of-the-artMoEmethods,providinganeffectivesolutionforscalingDiTwithMoE.Nonethe- less,itinheritscommonrisksofgenerativemodels,suchasthepotentialtocreatefakedata. Robust imageforgerydetectionmayhelpmitigatetheseconcerns. Inaddition,weadheretoethicalguide- linesinallexperiments. REPRODUCIBILITY STATEMENT We make the following efforts to ensure the reproducibility of ProMoE: (1) All experiments are conducted on the publicly available ImageNet-1K benchmark. (2) Our code and trained model weights will be made publicly available. (3) We provide implementation details in Sec. 5.1 and AppendixA. REFERENCES David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. Technical report,Stanford,2006. YogeshBalaji,SeungjunNah,XunHuang,ArashVahdat,JiamingSong,QinshengZhang,Karsten Kreis,MiikaAittala,TimoAila,SamuliLaine,etal. ediff-i:Text-to-imagediffusionmodelswith anensembleofexpertdenoisers. arXivpreprintarXiv:2211.01324,2022. Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. A survey on mixtureofexpertsinlargelanguagemodels. IEEETransactionsonKnowledgeandDataEngi- neering,2025. JunsongChen,JinchengYu,ChongjianGe,LeweiYao,EnzeXie,YueWu,ZhongdaoWang,James Kwok,PingLuo,HuchuanLu,etal. Pixart-Œ±: Fasttrainingofdiffusiontransformerforphotore- alistictext-to-imagesynthesis. arXivpreprintarXiv:2310.00426,2023. Kun Cheng, Xiao He, Lei Yu, Zhijun Tu, Mingrui Zhu, Nannan Wang, Xinbo Gao, and Jie Hu. Diff-moe: Diffusion transformer with time-aware and space-adaptive experts. In Forty-second InternationalConferenceonMachineLearning,2025. XiangxiangChu,JianlinSu,BoZhang,andChunhuaShen. Visionllama: Aunifiedllamabackbone forvisiontasks. InEuropeanConferenceonComputerVision,pp.1‚Äì18.Springer,2024. DamaiDai,ChengqiDeng,ChenggangZhao,RXXu,HuazuoGao,DeliChen,JiashiLi,Wangding Zeng,XingkaiYu,YuWu,etal.Deepseekmoe:Towardsultimateexpertspecializationinmixture- of-expertslanguagemodels. arXivpreprintarXiv:2401.06066,2024. JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei. Imagenet: Alarge-scalehi- erarchicalimagedatabase. In2009IEEEconferenceoncomputervisionandpatternrecognition, pp.248‚Äì255.Ieee,2009. PrafullaDhariwalandAlexanderNichol. Diffusionmodelsbeatgansonimagesynthesis. Advances inneuralinformationprocessingsystems,34:8780‚Äì8794,2021. 10 --- Page 11 --- AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,Aiesha Letman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal. Thellama3herdofmodels. arXive-prints,pp.arXiv‚Äì2407,2024. PatrickEsser, SumithKulal, AndreasBlattmann, RahimEntezari, JonasMu¬®ller, HarrySaini, Yam Levi,DominikLorenz,AxelSauer,FredericBoesel,etal. Scalingrectifiedflowtransformersfor high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024a. PatrickEsser, SumithKulal, AndreasBlattmann, RahimEntezari, JonasMu¬®ller, HarrySaini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers forhigh-resolutionimagesynthesis. InForty-firstinternationalconferenceonmachinelearning, 2024b. Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Scaling diffusion transformersto16billionparameters. arXivpreprintarXiv:2407.11633,2024. ChenFengandIoannisPatras. Maskcon: Maskedcontrastivelearningforcoarse-labelleddataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19913‚Äì19922,2023. Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxi- ang Liu, Weichong Yin, Shikun Feng, et al. Ernie-vilg 2.0: Improving text-to-image diffusion modelwithknowledge-enhancedmixture-of-denoising-experts. InProceedingsoftheIEEE/CVF ConferenceonComputerVisionandPatternRecognition,pp.10135‚Äì10145,2023. AliHatamizadeh,JiamingSong,GuilinLiu,JanKautz,andArashVahdat. Diffit: Diffusionvision transformers for image generation. In European Conference on Computer Vision, pp. 37‚Äì55. Springer,2024. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Ganstrainedbyatwotime-scaleupdateruleconvergetoalocalnashequilibrium. Advancesin neuralinformationprocessingsystems,30,2017. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598,2022. JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin neuralinformationprocessingsystems,33:6840‚Äì6851,2020. EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,2021. RobertAJacobs,MichaelIJordan,StevenJNowlan,andGeoffreyEHinton. Adaptivemixturesof localexperts. Neuralcomputation,3(1):79‚Äì87,1991. AlbertQJiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,BlancheSavary,ChrisBam- ford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtralofexperts. arXivpreprintarXiv:2401.04088,2024. Yunsung Lee, JinYoung Kim, Hyojun Go, Myeongho Jeong, Shinhyeok Oh, and Seungtaek Choi. Multi-architecture multi-expert diffusion models. In Proceedings of the AAAI Conference on ArtificialIntelligence,volume38,pp.13427‚Äì13436,2024. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, MaximKrikun,NoamShazeer,andZhifengChen.Gshard:Scalinggiantmodelswithconditional computationandautomaticsharding. arXivpreprintarXiv:2006.16668,2020. AonianLi,BangweiGong,BoYang,BojiShan,ChangLiu,ChengZhu,ChunhaoZhang,Congchao Guo,DaChen,DongLi,etal. Minimax-01: Scalingfoundationmodelswithlightningattention. arXivpreprintarXiv:2501.08313,2025. 11 --- Page 12 --- YaronLipman,RickyTQChen,HeliBen-Hamu,MaximilianNickel,andMattLe. Flowmatching forgenerativemodeling. arXivpreprintarXiv:2210.02747,2022. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, ChengqiDeng,ChenyuZhang,ChongRuan,etal. Deepseek-v3technicalreport. arXivpreprint arXiv:2412.19437,2024. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transferdatawithrectifiedflow. arXivpreprintarXiv:2209.03003,2022. NanyeMa,MarkGoldstein,MichaelSAlbergo,NicholasMBoffi,EricVanden-Eijnden,andSain- ing Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. InEuropeanConferenceonComputerVision,pp.23‚Äì40.Springer,2024. NiklasMuennighoff,LucaSoldaini,DirkGroeneveld,KyleLo,JacobMorrison,SewonMin,Wei- jia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, et al. Olmoe: Open mixture-of-experts languagemodels. arXivpreprintarXiv:2409.02060,2024. AlexanderQuinnNicholandPrafullaDhariwal. Improveddenoisingdiffusionprobabilisticmodels. InInternationalconferenceonmachinelearning,pp.8162‚Äì8171.PMLR,2021. Byeongjun Park, Sangmin Woo, Hyojun Go, Jin-Young Kim, and Changick Kim. Denoising task routingfordiffusionmodels. arXivpreprintarXiv:2310.07138,2023. Byeongjun Park, Hyojun Go, Jin-Young Kim, Sangmin Woo, Seokil Ham, and Changick Kim. Switch diffusion transformer: Synergizing denoising tasks with sparse mixture-of-experts. In EuropeanConferenceonComputerVision,pp.461‚Äì477.Springer,2024. WilliamPeeblesandSainingXie. Scalablediffusionmodelswithtransformers. InProceedingsof theIEEE/CVFInternationalConferenceonComputerVision,pp.4195‚Äì4205,2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Mu¬®ller, Joe Penna,andRobinRombach. Sdxl: Improvinglatentdiffusionmodelsforhigh-resolutionimage synthesis. arXivpreprintarXiv:2307.01952,2023. Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andre¬¥ SusanoPinto,DanielKeysers,andNeilHoulsby. Scalingvisionwithsparsemixtureofexperts. AdvancesinNeuralInformationProcessingSystems,34:8583‚Äì8595,2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¬®rn Ommer. High- resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconfer- enceoncomputervisionandpatternrecognition,pp.10684‚Äì10695,2022a. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¬®rn Ommer. High- resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFCon- ferenceonComputerVisionandPatternRecognition,pp.10684‚Äì10695,2022b. OlafRonneberger,PhilippFischer,andThomasBrox. U-net: Convolutionalnetworksforbiomedi- calimagesegmentation.InInternationalConferenceonMedicalimagecomputingandcomputer- assistedintervention,pp.234‚Äì241.Springer,2015. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improvedtechniquesfortraininggans. Advancesinneuralinformationprocessingsystems, 29, 2016. VikashSehwag,XianghaoKong,JingtaoLi,MichaelSpranger,andLingjuanLyu. Stretchingeach dollar:Diffusiontrainingfromscratchonamicro-budget. InProceedingsoftheComputerVision andPatternRecognitionConference,pp.28596‚Äì28608,2025. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, andJeffDean. Outrageouslylargeneuralnetworks: Thesparsely-gatedmixture-of-expertslayer. arXivpreprintarXiv:1701.06538,2017. 12 --- Page 13 --- YingShen,ZhiyangXu,JiuhaiChen,ShizheDiao,JiaxinZhang,YuguangYao,JoyRimchala,Is- miniLourentzou,andLifuHuang.Latte-flow:Layerwisetimestep-expertflow-basedtransformer. arXivpreprintarXiv:2506.06952,2025. MingleiShi,ZiyangYuan,HaotianYang,XintaoWang,MingwuZheng,XinTao,WenliangZhao, Wenzhao Zheng, Jie Zhou, Jiwen Lu, et al. Diffmoe: Dynamic token selection for scalable diffusiontransformers. arXivpreprintarXiv:2503.14487,2025. YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,andBen Poole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. arXivpreprint arXiv:2011.13456,2020. HaotianSun,TaoLei,BowenZhang,YanghaoLi,HaoshuoHuang,RuomingPang,BoDai,andNan Du. Ec-dit: Scaling diffusion transformers with adaptive expert-choice routing. arXiv preprint arXiv:2410.02098,2024. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXivpreprintarXiv:2503.20314,2025. YujieWei,ShiweiZhang,ZhiwuQing,HangjieYuan,ZhihengLiu,YuLiu,YingyaZhang,Jingren Zhou, andHongmingShan. Dreamvideo: Composingyourdreamvideoswithcustomizedsub- ject and motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pp.6537‚Äì6549,2024a. YujieWei,ShiweiZhang,HangjieYuan,XiangWang,HaonanQiu,RuiZhao,YutongFeng,Feng Liu,ZhizhongHuang,JiaxinYe,etal. Dreamvideo-2: Zero-shotsubject-drivenvideocustomiza- tionwithprecisemotioncontrol. arXivpreprintarXiv:2410.13830,2024b. YujieWei, ShiweiZhang, HangjieYuan, BiaoGong, LongxiangTang, XiangWang, HaonanQiu, HengjiaLi,ShuaiTan,YingyaZhang,etal.Dreamrelation:Relation-centricvideocustomization. arXivpreprintarXiv:2503.07602,2025. EnzeXie,JunsongChen,JunyuChen,HanCai,HaotianTang,YujunLin,ZhekaiZhang,Muyang Li,LigengZhu,YaoLu,etal. Sana: Efficienthigh-resolutionimagesynthesiswithlineardiffu- siontransformers. arXivpreprintarXiv:2410.10629,2024. Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo. Raphael: Text-to-image generation via large mixture of diffusion paths. Advances in Neural InformationProcessingSystems,36:41693‚Äì41706,2023. ZhuoyiYang,JiayanTeng,WendiZheng,MingDing,ShiyuHuang,JiazhengXu,YuanmingYang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models withanexperttransformer. arXivpreprintarXiv:2408.06072,2024. Yike Yuan, Ziyu Wang, Zihao Huang, Defa Zhu, Xun Zhou, Jingyi Yu, and Qiyang Min. Expert race: Aflexibleroutingstrategyforscalingdiffusiontransformerwithmixtureofexperts. arXiv preprintarXiv:2503.16057,2025. Wangbo Zhao, Yizeng Han, Jiasheng Tang, Kai Wang, Yibing Song, Gao Huang, Fan Wang, and YangYou. Dynamicdiffusiontransformer. arXivpreprintarXiv:2410.03456,2024. 13 --- Page 14 --- APPENDIX A EXPERIMENTAL SETUP Baselines. We compare against open-source state-of-the-art DiT-based MoE methods, with acti- vated parameters equivalent to the dense model and comparable total parameter counts. We im- plement DiT-MoE (Fei et al., 2024), EC-DiT (Sun et al., 2024), and DiffMoE (Shi et al., 2025) followingtheiroriginalpapersandreferringtotheopen-sourcerepository1. Allmethodsaretrained withthesametrainingsettings,includinglearningrate,batchsize,anddataaugmentation. Implementation details. We train ProMoE with two objectives: the standard DDPM objec- tive (Ho et al., 2020), and Rectified Flow with Logit-Normal sampling from SD3 (Esser et al., 2024a)toalignbetterwithmodernDiTtrainingparadigms(e.g.,Sana(Xieetal.,2024)). Besides the hyperparameters in Sec. 5.1, we provide additional details as follows. In prototypical routing, Œ±inEq.(4)issetto1. Forroutingcontrastivelearning,thetemperatureissetto0.07,andtheloss weightŒª is1unlessstatedotherwise. RCL Evaluationmetrics. WefollowthestandardDiTevaluationprotocol(Peebles&Xie,2023),com- putingFIDandISon50,000generatedimages2atclassifier-freeguidancescalesof1.0and1.5. B IMPLEMENTATION ALGORITHMS TheimplementationalgorithmofProMoEisprovidedinAlgorithm1. Inthealgorithm,inputclass labels are used solely to distinguish conditional image tokens from unconditional image tokens. Duringinference,ifclassifier-freeguidance(CFG)isdisabled,alltokensareroutedtothestandard expertsandthesharedexpert;theunconditionalexpertisnotused. IfCFGisenabled,classlabels are replaced with a batch-level binary mask indicating which samples receive conditioning (i.e., treatedasconditionalimagetokens). Noroutingcontrastivelossiscomputedduringinference. C MORE RESULTS C.1 MORET-SNEVISUALIZATIONSOFLANGUAGEANDVISUALTOKENS To further validate the findings on differences between language and visual tokens in Sec. 1, we extendthevisualizationresultsinFig.1(a). Figs.10and11presentt-SNEvisualizationsoftoken embeddings from DiT-XL/2 (Peebles & Xie, 2023) and Llama-3 8B (Dubey et al., 2024) across differentlayers;forDiT-XL/2,wealsovisualizetokensatdifferentdiffusiontimesteps. Tofacilitate comparison, we cluster token embeddings into 10 groups using k-means. For model inputs, we randomly sample 110 ImageNet classes, feed the corresponding class labels to DiT-XL/2 and the class names to Llama-3 8B, and randomly select 1K intermediate-layer tokens for visualization. TheresultsinFigs.10and11furtherconfirmthatlanguagetokensaresemanticallydensewithhigh inter-tokendifferences,whereasvisualtokensexhibithighspatialredundancy. C.2 T-SNEVISUALIZATIONSOFTOKENASSIGNMENTS Toassesstheimpactofvisual-tokenredundancyonMoEexpertselection,asindicatedbyFig.1(a) andFigs.10and11,wevisualizeintermediate-layertokenassignmentsofProMoE-L-FlowandDiT- MoE-L-Flowat500Ktrainingstepswithoutclassifier-freeguidance,asshowninFig.6. Following Sec. C.1, we randomly sample 110 ImageNet classes, feed the corresponding class labels to both ProMoE and DiT-MoE, and randomly select 2,560 tokens from an intermediate-layer MoE block to visualize the expert selection of each token. Compared with token-choice MoE methods such as DiT-MoE, our approach assigns experts according to token semantics, producing well-formed clustersinthetoken-embeddingspace: semanticallysimilartokensformcompactclustersandare routedtothesameexpert,whereasclustersassignedtodifferentexpertsareclearlyseparated.These 1 2 14 --- Page 15 --- results further corroborate the importance of explicit routing guidance for visual MoE, and our methodachieveseffectiveintra-expertcoherenceandinter-expertdiversity. ProMoE-L-Flow DiT-MoE-L-Flow Figure 6: t-SNE visualization results of ProMoE and DiT-MoE on expert allocation (token assignment). Eachcolorcorrespondstoasingleexpert. C.3 MOREVISUALIZATIONRESULTS We provide additional generation results in Fig. 12. Our method produces high-quality images acrossbothsimpleandchallengingcategories. C.4 MORECOMPARISONRESULTS WeprovideadditionalcomparisonswithdensemodelsandMoESOTAs. BesidestheFIDresultsin Fig.3(a), wealsoreportInceptionScorecomparisons, asshowninFig.7. Inaddition,wepresent quantitativecomparisonswithMoESOTAsundertheDDPMobjectiveinTab.9. Acrossbothtrain- ingobjectivesandCFGsettings,ourmethodconsistentlyoutperformsthedensemodelandexisting MoESOTAs,demonstratingitseffectiveness. Table 9: Quantitative comparison with MoE baselines under DDPM on ImageNet (256√ó256) after500Ktrainingsteps,andevaluatedatCFGscalesof1.0and1.5. Activated Total cfg=1.0 cfg=1.5 Model(500K) #Experts # # Params. Params. FID50K‚Üì IS‚Üë FID50K‚Üì IS‚Üë DiT-MoE-L-DDPM E8A1S0N0 458M 1.163B 23.12 60.08 7.55 133.63 EC-DiT-L-DDPM E8A1S0N0 458M 1.163B 20.76 64.77 6.48 146.49 DiffMoE-L-DDPM E8A1S0N0 458M 1.095B 19.45 70.93 5.47 158.30 ProMoE-L-DDPM E14A1S1N1 458M 1.063B 18.75 73.07 5.12 168.91 cfg=1.5 cfg=1.0 Figure7: ComparisonwithdensemodelandMoESOTAsonInceptionScore. 15 --- Page 16 --- C.5 COMPARISONWITHDENSEDITONMORETRAININGSTEPS We provide comparison results between our ProMoE and Dense DiT on more training steps in Tab.10. WeobservethatProMoE-L-Flowat500KstepssurpassesDense-DiT-L-Flowat1Msteps on FID, and ProMoE-XL-Flow at 500K steps surpasses Dense-DiT-XL-Flow at 1M steps on both FIDandIS.Withlongertraining, ProMoE-L-Flowat1MstepsoutperformsDense-DiT-L-Flowat 2MstepsandDense-DiT-XL-Flowat1Msteps. ThesefindingsareconsistentwiththoseinSec.5.2, demonstratingfasterconvergenceandscalabilityofourmethod. Table 10: Quantitative comparison with Dense DiTs under Rectified Flow on ImageNet (256√ó256)aftermoretrainingsteps,evaluatedwithCFGscalesof1.0and1.5. Activated Total cfg=1.0 cfg=1.5 Model(TrainingSteps) # # Params. Params. FID50K‚Üì IS‚Üë FID50K‚Üì IS‚Üë Dense-DiT-L-Flow(1M) 458M 458M 12.21 100.97 2.97 245.63 ProMoE-L-Flow(500K) 458M 1.063B 11.61 100.82 2.79 244.21 ProMoE-L-Flow(1M) 458M 1.063B 9.88 118.91 2.75 278.22 Dense-DiT-L-Flow(2M) 458M 458M 10.55 112.55 2.81 266.24 ProMoE-L-Flow(2M) 458M 1.063B 9.67 125.88 2.22 290.61 Dense-DiT-XL-Flow(1M) 675M 675M 10.67 107.68 2.82 260.61 ProMoE-XL-Flow(500K) 675M 1.568B 9.44 114.94 2.59 265.62 ProMoE-XL-Flow(1M) 675M 1.568B 8.34 128.58 2.53 292.38 C.6 INCREASINGTHENUMBEROFACTIVATEDEXPERTS AsdiscussedinSec.4.2,classification-andclustering-basedroutinginherentlydonotsupporttop-k assignment,permittingonlytop-1. Incontrast,ProMoEismoreflexibleandscalable,andsupports top-kassignment.Tovalidatethis,weincreasethenumberofactivatedstandardexpertsfrom1to3, whichraisestheactivatedparameterswhilekeepingthetotalparametercountunchanged.Asshown inTab.11,thisincreaseyieldsimprovedperformance,confirmingtheeffectivenessandscalability ofourmethod. Table 11: Results of increasing the number of activated standard experts on ImageNet (256√ó256)after500Ksteps,trainedwithRectifiedFlowandevaluatedatCFGscales1.0and1.5. Activated Total cfg=1.0 cfg=1.5 Model(500K) #Experts # # Params. Params. FID50K‚Üì IS‚Üë FID50K‚Üì IS‚Üë ProMoE-L-Flow E14A1S1N1 458M 1.063B 11.61 100.82 2.79 244.21 ProMoE-L-Flow E14A3S1N1 558M 1.063B 11.40 103.78 2.72 246.96 D MORE RESULTS ON SCALING BEHAVIOR D.1 SCALINGMODELSIZE Fig.3(b)showsFIDresultsformodelsizescalingatCFG=1.0. WeadditionallyreportFIDresults at CFG=1.5 and Inception Score at CFG=1.0 and 1.5, as shown in Fig. 8. ProMoE consistently outperformsitsdensecounterparts,andProMoE-L-FlowsurpassesDense-XL-FlowintermsofFID and Inception Score at both CFG=1.0 and 1.5, despite using fewer activated parameters. These observationsareconsistentwiththoseinSec.5.2. D.2 SCALINGTHENUMBEROFEXPERTS We report Inception Score for scaling the number of experts at CFG=1.0 and 1.5, and FID at CFG=1.5,asshowninFig.9. PerformanceofProMoEimprovesasthenumberofexpertsincreases, demonstratingthescalabilityofourapproach. 16 --- Page 17 --- cfg=1.0 cfg=1.5 cfg=1.5 Figure8: Morescalingresultsonmodelsize. cfg=1.0 cfg=1.5 cfg=1.5 Figure9: Morescalingresultsonthenumberofexperts. E MORE ABLATION STUDIES E.1 ABLATIONONLOAD-BALANCINGLOSS As discussed in Sec. 4.3, the push-away term in our routing contrastive learning (RCL) serves a role similar to load balancing. We verify this with an ablation in Tab. 12. Adding a conventional load-balancinglossontopofourmethodslightlydegradesperformance. WeattributethistoRCL‚Äôs explicit semantic guidance: it leverages token semantics to maintain diverse expert assignments, whereas load balancing loss only regularizes token counts and ignores assignment quality and se- mantics, thereby interfering with RCL. These results indicate that the semantic routing guidance fromRCLismoreeffectivethantraditionalload-balancinglosses. Table 12: Ablation study of using load-balancing loss under Rectified Flow on ImageNet (256√ó256)after500Ktrainingsteps. cfg=1.0 cfg=1.5 Model(500K) FID50K‚Üì IS‚Üë FID50K‚Üì IS‚Üë w/load-balancingloss 24.98 59.04 6.53 151.37 w/oload-balancingloss 24.44 60.38 6.39 154.21 E.2 ABLATIONONLOSSWEIGHTOFROUTINGCONTRASTIVELEARNING Wevarythelossweightofroutingcontrastivelearning(RCL)andreporttheresultsinTab.13. We observethatRCLisinsensitivetothelossweight,asincreasingitfrom1to10yieldsonlymarginal gains. Therefore, we use a default weight of 1 for all experiments, except for ProMoE-B-DDPM, whichusesaweightof10basedonthisablationstudy. F USAGE OF LARGE LANGUAGE MODELS (LLMS) In accordance with the ICLR 2026 policy, we report our use of a large language model (LLM) in preparing this manuscript. The LLM‚Äôs role was strictly confined to language polishing, such as correcting grammar, refining wording, and improving readability. All scientific contributions, including the ideation, methodology, experimental design, and final conclusions, are entirely our own.TheLLMwasusedsolelyasawriting-enhancementtoolanddidnotcontributetothescientific aspectsofthework. Wehavereviewedthemanuscriptandtakefullresponsibilityforitscontent. 17 --- Page 18 --- Table13:AblationstudyofŒª inProMoE-B-DDPMonImageNet(256√ó256)after500Ktrain- RCL ingsteps. cfg=1.0 cfg=1.5 Œª (500K) RCL FID50K‚Üì IS‚Üë FID50K‚Üì IS‚Üë 1 40.48 36.77 18.34 80.07 2 40.37 37.46 18.01 81.88 5 40.33 37.08 18.03 81.1 10 40.37 37.84 17.90 82.65 Algorithm1ProMoELayer(Training) Input: X‚ààRB√óL√óD (inputsequence),c‚ààZB (batchlabels) Variables: N (number of standard experts), K (number of activated standard experts), P ‚àà E RNE√óD (Learnable prototypes for routing), E (List of standard expert FFNs), EU (Unconditional expertFFN),ES(SharedexpertFFN),Œª (coefofRoutingcontrastiveloss),œÑ (temperature) RCL 1: Initialize: O‚Üêzeros like(X) ‚ñ∑Initializefinaloutput 2: /***Step1. FunctionalRouting***/ 3: M u ‚Üê(c==emptyconditioning) ‚ñ∑Getmaskofunconditionalimagetokens 4: M c ‚Üê¬¨M u ‚ñ∑Getmaskofconditionalimagetokens 5: X u ‚ÜêX[expand(M u)] ‚ñ∑Getunconditionalimagetokens 6: X c ‚ÜêX[expand(M c)] ‚ñ∑Getconditionalimagetokens 7: /***Step2. UnconditionalImageTokensProcessing***/ 8: O U ‚ÜêEU(X u) 9: O[M u]‚ÜêO U 10: ifany(M c)then 11: /***Step3. PrototypicalRouting***/ 12: X‚Ä≤ c ‚Üêreshape(X c,(‚àí1,D)) ‚ñ∑Flattenconditionalimagetokens 13: n ‚ÜêX‚Ä≤.shape[0] ‚ñ∑Getnumberofconditionalimagetokens c c 14: Z‚ààRnc√óNE ‚ÜêL2 Normalize(X‚Ä≤)√óL2 Normalize(P)‚ä§ ‚ñ∑Getpre-activationscores c 15: S‚ÜêIdentity(Z) ‚ñ∑Gettoken‚Äìexpertaffinityscores 16: G‚ààRnc√óK,indices‚ààZnc√óK ‚ÜêTopK(S,K) ‚ñ∑Getgatingtensorandindices 17: /***Step4. ConditionalImageTokensProcessing***/ 18: O‚Ä≤ ‚Üêzeros like(X‚Ä≤) C c 19: fori‚Üê0toN E ‚àí1do 20: m i ‚Üê(indices==i).any(dim=1) ‚ñ∑Maskoftokensroutedtoexperti 21: ifany(m i)then 22: G i ‚Üêsum(G[m i]√ó(indices[m i]==i),dim=1) ‚ñ∑Finalgatingscores 23: O‚Ä≤ C[m i]‚ÜêO‚Ä≤ C[m i]+G i.unsqueeze(1)√óE i(X‚Ä≤ c[m i]) ‚ñ∑Updatefinaloutput 24: endif 25: endfor 26: OC ‚Üêreshape(O‚Ä≤ C,X c.shape) 27: O[M c]‚ÜêO[M c]+OC 28: /***Step5. RoutingContrastiveLearning***/ 29: aux loss‚ÜêŒª RCL√óL RCL(X c,indices,P,œÑ) 30: endif 31: /***Step6. SharedExpertProcessing***/ 32: O‚ÜêO+ES(X) 33: Return: O,aux loss 18 --- Page 19 --- ùê¥ùë£ùëî(ùëñùëõùë°ùëíùëü‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) ùê¥ùë£ùëî(ùëñùëõùë°ùëíùëü‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) ùê¥ùë£ùëî(ùëñùëõùë°ùëíùëü‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) = 26.657 = 23.001 = 21.743 ùê¥ùë£ùëî(ùëñùëõùë°ùëüùëé‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) ùê¥ùë£ùëî(ùëñùëõùë°ùëüùëé‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) ùê¥ùë£ùëî(ùëñùëõùë°ùëüùëé‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) LayerNumber: 7 LayerNumber: 9 LayerNumber: 11 ùê¥ùë£ùëî(ùëñùëõùë°ùëíùëü‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) ùê¥ùë£ùëî(ùëñùëõùë°ùëíùëü‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) ùê¥ùë£ùëî(ùëñùëõùë°ùëíùëü‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) = 20.418 = 18.087 = 14.202 ùê¥ùë£ùëî(ùëñùëõùë°ùëüùëé‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) ùê¥ùë£ùëî(ùëñùëõùë°ùëüùëé‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) ùê¥ùë£ùëî(ùëñùëõùë°ùëüùëé‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) LayerNumber: 13 LayerNumber: 15 LayerNumber: 17 Figure10: Moret-SNEvisualizationresultsofLlama-38Bondifferentlayers. ùê¥ùë£ùëî(ùëñùëõùë°ùëíùëü‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) ùê¥ùë£ùëî(ùëñùëõùë°ùëíùëü‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) ùê¥ùë£ùëî(ùëñùëõùë°ùëíùëü‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) = 0.873 = 0.748 = 0.731 ùê¥ùë£ùëî(ùëñùëõùë°ùëüùëé‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) ùê¥ùë£ùëî(ùëñùëõùë°ùëüùëé‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) ùê¥ùë£ùëî(ùëñùëõùë°ùëüùëé‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) LayerNumber: 2 (Shallow Layer) LayerNumber: 12 (Middle Layer) LayerNumber: 24 (Deep Layer) DiffusionTimestep: 50(Last Step) DiffusionTimestep: 50(Last Step) DiffusionTimestep: 50(Last Step) ùê¥ùë£ùëî(ùëñùëõùë°ùëíùëü‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) ùê¥ùë£ùëî(ùëñùëõùë°ùëíùëü‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) ùê¥ùë£ùëî(ùëñùëõùë°ùëíùëü‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) = 0.784 = 0.704 = 0.752 ùê¥ùë£ùëî(ùëñùëõùë°ùëüùëé‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) ùê¥ùë£ùëî(ùëñùëõùë°ùëüùëé‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) ùê¥ùë£ùëî(ùëñùëõùë°ùëüùëé‚àíùëêùëôùëéùë†ùë† ùëëùëñùë†ùë°ùëéùëõùëêùëí) LayerNumber: 12 LayerNumber: 12 LayerNumber: 12 DiffusionTimestep: 10 (Early) DiffusionTimestep: 25 (Middle) DiffusionTimestep: 40 (Late) Figure 11: More t-SNE visualization results of DiT-XL/2 on different layers and diffusion timesteps. 19 --- Page 20 --- Figure12: MoresamplesgeneratedbyProMoE-XL-Flowafter2Miterationswithcfg=4.0. 20