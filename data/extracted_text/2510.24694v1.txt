--- Page 1 --- 2025-10-29 Repurposing Synthetic Data for Fine-grained Search Agent Supervision YidaZhao,KuanLi,XixiWu,LiwenZhang((cid:0)),DingchuZhang,BaixuanLi, MaojiaSong,ZhuoChen,ChenxiWang,XinyuWang,KeweiTu((cid:0)), PengjunXie,JingrenZhou,YongJiang((cid:0)) TongyiLab ,AlibabaGroup   Abstract LLM-basedsearchagentsareincreasinglytrainedonentity-centricsynthetic datatosolvecomplex,knowledge-intensivetasks. However,prevailingtrain- ing methods like Group Relative Policy Optimization (GRPO) discard this rich entity information, relying instead on sparse, outcome-based rewards. Thiscriticallimitationrendersthemunabletodistinguishinformative"near- miss"samples‚Äîthosewithsubstantiallycorrectreasoningbutaflawedfinal answer‚Äîfromcompletefailures,thusdiscardingvaluablelearningsignals. We address this by leveraging the very entities discarded during training. Our empiricalanalysisrevealsastrongpositivecorrelationbetweenthenumberof ground-truthentitiesidentifiedduringanagent‚Äôsreasoningprocessandfinal answeraccuracy. Buildingonthisinsight,weintroduceEntity-awareGroup RelativePolicyOptimization(E-GRPO),anovelframeworkthatformulates a dense entity-aware reward function. E-GRPO assigns partial rewards to incorrectsamplesproportionaltotheirentitymatchrate,enablingthemodel toeffectivelylearnfromthese"near-misses". Experimentsondiversequestion- answering(QA)anddeepresearchbenchmarksshowthatE-GRPOconsistently andsignificantlyoutperformstheGRPObaseline. Furthermore,ouranalysis revealsthatE-GRPOnotonlyachievessuperioraccuracybutalsoinducesmore efficientreasoningpoliciesthatrequirefewertoolcalls,demonstratingamore effectiveandsample-efficientapproachtoaligningsearchagents. 1 Introduction The advent of Large Language Models (LLMs) has catalyzed the development of sophisticated au- tonomousagents,withsearchagentsemergingasaprominentclassforsolvingcomplex,knowledge- intensive tasks (Yao et al., 2023; Wang et al., 2024; Xi et al., 2025). Training these agents to navigate thevast,noisywebeffectivelyrequiresabundantandchallengingdata(GoogleTeam,2025;OpenAI, 2025;xAITeam,2025;MoonshotAI, 2025). To meetthisdemand, a dominantparadigmofsynthetic datagenerationhasemerged(Wuetal.,2025b;Lietal.,2025b;Gaoetal.,2025). Inthisparadigm,as shown in Figure 1 (left), complex questions are often created by systematically transforming simple "seed"questionsthroughoperationslikefactinjectionordeliberateobfuscation. Thisprocesscreatesan intricateproblemstructure,pavedwithkeyentitiesthatformthefactualbackboneofthecorrectanswer. Thissyntheticdataisthenusedtotrainagentswithinthenow-dominantreinforcementlearning(Wen (cid:0) Correspondingauthors.{zlw439616,   1 5202 tcO 82 ]LC.sc[ 1v49642.0152:viXra --- Page 2 --- etal.,2024;Singhetal.,2025),especiallywithGroupRelativePolicyOptimization(GRPO)(Shaoetal., 2024)anditsnumerousvariants(Yuetal.,2025;Dongetal.,2025;Hu,2025;Xuetal.,2024;Zhaoetal., 2025). Thesemethodstypicallyrelyonoutcome-basedrewards,utilizingonlythefinalanswerwhile discarding the intermediate entity information meticulously embedded during data synthesis. This mechanismleadstotherewardsparsityproblem(Qianetal.,2025;Dengetal.,2025),whichmanifests criticallyforsearchagents(Songetal.,2025;Wuetal.,2025a;Jinetal.,2025;Lietal.,2025c;Zhengetal., 2025;Zhangetal.,2025a;Lietal.,2025b;Gaoetal.,2025): bytreatingallnegativesamplesuniformly, GRPO fails to distinguish a "near-miss"‚Äîa response with correct intermediate reasoning steps but a flawedanswer‚Äîfromacompletefailure. Forinstance,inansweringWhowasthedirectorofthe1997film starringtheactorwhowontheAcademyAwardforBestActorforthefilm‚ÄôTheRevenant‚Äô?,a"near-miss"that correctly identifies the actor (Leonardo) and the film (Titanic) but fails on the final answer is far more informativethanonethatmisunderstandsthequeryentirely. Bypenalizingbothequally,standardGRPO discardscruciallearningsignalsembeddedinpartiallycorrectreasoning,forcingthemodeltore-learn stepsithadalreadymastered. Onenaturalapproachtoaddressthissparserewardproblemistoincorporatefine-grained,process-level supervision. In domains such as mathematics and code, this is achieved either by evaluating each intermediatestepwithaProcessRewardModel(PRM)(Fanetal.,2025;Anonymous,2025;Zhangetal., 2025b) or by employing complex sampling mechanisms (e.g., tree-based search) to derive step-level advantages(Yangetal.,2025;Houetal.,2025). However,thesemethodsareill-suitedfortheopen-ended natureofwebsearch. Thesheerscaleanddynamicnatureofwebcontentrendertheannotationrequired for a PRM prohibitively expensive. Similarly, the extensive length of search agent trajectories, often involvingdozensoftoolcallsandreasoningsteps,makesintricatesamplingstrategiescomputationally intractable. Thisleavesacriticalgap: howcanweobtainafine-grained,informative,yetcomputationallyefficient reward signal for search agents? The answer, we find, lies hidden in plain sight: within the very entity-centric information from synthetic data generation that GRPO-like methods discard. These entities,formingthefactualbackbonefortheanswer,intuitivelyrepresentanuntappedsourceoffine- grainedsupervision. Tovalidatethepotentialoftheseground-truthentities,weanalyzetherelationship betweenagentperformanceandthenumberofentitiesmatchedduringreasoning(entitymatchrate). As illustratedinFigure1(right),thestrongpositivecorrelationweobserved(furtherdiscussedinSection3.1) validatesourcorehypothesis: theentitymatchrateservesasapowerfulproxyforfactualcorrectness andcanberepurposedasafine-grainedrewardsignalthatstandardGRPOlacks. Basedonthiscoreinsight,weproposeEntity-awareGroupRelativePolicyOptimization(E-GRPO),a novelRLframeworkthatenhancespolicyoptimizationbyformulatingadense,entity-awarereward functionfromtheentitieswithinthesynthetictrainingdata. Specifically,insteadofapplyingauniform penalty,ourmethodassignsabonustonegativesamplesproportionaltotheirentitymatchrate. Bydoing so,a"near-miss"sample,whichcontainsmanycorrectentitiesandishighlyinformativeforlearning, receives a better reward than a complete failure. This fine-grained reward, obtained with negligible computationalcost,compelsthemodeltomovebeyondsimplyavoidingerrorsandtowardsmastering theprocessofidentifyingandsynthesizingkeyinformation,therebyaddressingthelimitationofstandard GRPOincomplexsearchtasks. Ourcomprehensiveevaluationon11benchmarks,spanningdiversemodelsandenvironments,demon- strates that E-GRPO significantly and consistently surpasses the GRPO baseline. Critically, beyond superioraccuracy,E-GRPOalsoenablesmoreefficientreasoningpoliciesthatconsistentlyrequirefewer toolcalls. Furtheranalysesvalidateourcorehypothesis,confirmingtheimportanceoftheentity-aware reward. Insummary,thekeycontributionsofthisworkareasfollows: 2 --- Page 3 --- Initial Question: Questions are categorized into three groups based on Who was the director of Titanic ? the comparison of the average entity match rate, e.g., "Correct Rate > Wrong Rate" means that for a question, its correct rollouts generally identify more entities (higher entity match rate) than failed ones. Fact Injection (‚ÄúTitanic‚Äù discarded) Who was the director of the 1997 film starring Leonardo ? Fact Injection (‚ÄúLeonardo‚Äù discarded) Final Question: Who was the director of the 1997 film starring the actor who won the Academy Award for Best Actor for the film ‚ÄúThe Revenant‚Äù? Figure1: Left: AnExampleofentity-centricsyntheticdatageneration. Right: Analysisofthecorrelation betweenentitymatchingandagentperformance. ‚Ä¢ Weidentifythe"near-miss"probleminGRPO-basedtrainingandproposethecoreinsightthatentities fromsyntheticdatacanberepurposedasafine-grainedrewardsignal,supportedbyempiricalanalysis revealingastrongcorrelationbetweenentitymatchrateandtaskaccuracy. ‚Ä¢ WeintroduceE-GRPO,anovelframeworkthatenhancespolicyoptimizationbyformulatinganentity- awarerewardfunctiontodifferentiatethequalityofnegativesamplesandprovidemoregranular supervision. ‚Ä¢ WeconductexperimentsonmultipleQAanddeepresearchbenchmarks,demonstratingthatE-GRPO notonlyoutperformstheGRPObaselineinaccuracybutalsolearnsmoreefficientpolicies. 2 Preliminary Inthissection,weprovideabriefoverviewofkeyconceptsinsearchagentsandareviewofentity-centric datasynthesismethods. MorediscussionofrelatedworkisavailableinSection5. 2.1 Search Agents Multi-turnRollout. WeadopttheReAct(Yaoetal.,2023)paradigmforsearchagents. TheLLMagent iterativelyperformsthoughtandaction,andreceivesobservationfromtheenvironment. Specifically, in each iteration, the LLM agent generates a free-form thought (œÑ) and executes a valid action (e.g., atoolcall a). Thenitwaitsfortheenvironment‚Äôsfeedbackastheobservation(o). Inthewebsearch setting,theactionspacetypicallyconsistsofsearchingqueries,webpagebrowsing,andgeneratingthe finalanswer. TheiterationterminateswhentheLLMgeneratesafinalanswer. AcompleterolloutwithT iterationscanbedefinedas: (cid:0) (cid:1) H = œÑ ,a ,o ,...,œÑ,a ,o ,...,œÑ ,a , 1 1 1 t t t T T whereœÑ,a ,o representthought,actionandobservationatstept,withœÑ,a sampledfromapolicyœÄ t t t t t Œ∏ basedonallpreviouscontextas(œÑ t,a t) ‚àº œÄ Œ∏(¬∑ | q,œÑ 1,a 1,o 1,...,œÑ t‚àí1,a t‚àí1,o t‚àí1). Thespecificformatof multi-turnrolloutisdetailedinAppendixA. ToolDesign. Followingexistingsearchagentstudies(Lietal.,2025b;Gaoetal.,2025),wedefinethe agent‚Äôswebexplorationactionspacewithtwoessentialtools: 3 --- Page 4 --- ‚Ä¢ Search: Asearchenginethatacceptsmultiplequeriesandretrievesthetop-10relevantresultsper query,includingtitles,snippets,andthecorrespondingURLs. ‚Ä¢ Visit: A browser agent that accesses several web pages simultaneously, given the corresponding URLsandbrowsinggoals. ItfirstretrievesthefullwebpageandthenusesQwen3-30B-A3B-Instruct- 2507(Team,2025a)toextractrelevantinformationbasedonthebrowsinggoal. 2.2 Entity-centric Data Synthesis A significant line of research has focused on the autonomous generation of complex and grounded question-answer (QA) pairs (Wu et al., 2025a;b; Li et al., 2025b; Gao et al., 2025), sharing a common threadintheirentity-centricapproach. Webrieflysummarizetwostate-of-the-art(SOTA)methodsthat exemplifythisparadigmbelow. ‚Ä¢ ASearcher(Gaoetal.,2025): Startingwithaseedquestion,ASearcher‚Äôssynthesisagentiteratively increasesdifficultyviatwoentity-focusedoperations: Injection,whichreplacesnamedentitieswith descriptive facts, and Fuzzing, which substitutes specific entities with more ambiguous, general descriptions. ‚Ä¢ SailorFog-QA(Lietal.,2025b):SailorFog-QAfirstconstructsacomplexknowledgegraphviaarandom walkfromaseedentity,creatingintricateentitycouplings. Itthengeneratesquestionsbysampling subgraphsandapplyinginformationobfuscation,whichinvolvesreplacingspecificentityattributes withvaguedescriptions. 3 Methodology In this section, we first give a detailed analysis of the correlation between agent performance and synthetic-dataentitymatching. Then,weproposetheE-GRPOalgorithm,designedtoimproveGRPO withafine-grainedentity-awarerewardfunction. 3.1 Analyzing Entity Matching in Agentic Reasoning Inspired by the entity-centric approach for data generation, where entities are intuitively the factual backboneofthesyntheticdata,weconductanempiricalanalysistoinvestigatehowtheseentitiescorrelate withtheperformanceofasearchagent. Metrics. To quantify this correlation, we first define the entity match rate. Given a synthetic QA pair (q,gt), weretainallthe m ground-truthentitiesduringQAgeneration E = (cid:8) e(1),e(2),...,e(m)(cid:9) , q andsampleagroupofGrollouts(cid:8) H(1),H(2),...,H(G)(cid:9) . ForeachrolloutH(i) inthegroup,letT(i) = (cid:110) (cid:111) (i) (i) (i) œÑ ,œÑ ,...,œÑ bethecollectionofallthoughtsinrollouti. Weidentifythesetofentitiesmatched 1 2 Ti withinthethoughtsas: (cid:110) (cid:111) E(i) = e ‚àà E | ‚àÉt ‚àà {1,...,T},eismentionedinœÑ(i) , (1) matched q i t Anentityisconsidered"mentioned"ifitsfullphraseappearsasanexactstringmatchinthethought‚Äôs text(morediscussionavailableinAppendixB).Theentitymatchrateforrollouti,denotedasŒ≥,isthen i calculatedastheratioofmatchedentitiestothetotal: (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)E(i) (cid:12) (cid:12)E(i) (cid:12) (cid:12) matched(cid:12) (cid:12) matched(cid:12) Œ≥ i = (cid:12) (cid:12) = (2) (cid:12)E q(cid:12) m Furthermore,toenablerobustcomparisonacrossdifferentquestionswhichmayhavevaryingdifficulty, weintroducethenormalizedentitymatchrate, Œ≥ÀÜ . ThisiscalculatedbynormalizingtherawrateŒ≥ i i againstthemaximumrate,Œ≥ ,observedwithinitsquestiongroup: max 4 --- Page 5 --- üí° üí° Tool Tool Tool ‚Ä¶ ‚úÖ GRPO E-GRPO üôãUserQuery: W of h tho e w 1a 9s 9 t 7h e fi ld mir e stc at ro rr i ng < <tt oh oin l_k> ca‚Ä¶ ll> < ./ .t .h <i /n tk o> o l_call> < <t /o too ol_ l_re res sp po on ns se e> >‚Ä¶ < <t toh oin l_k> ca‚Ä¶ ll> Le ...o <n /a tord olo _ c< a/ lt lh >ink> < <t /o too ol_ l_re resp spo on nse se> >‚Ä¶ < <t toh oin l_k> ca‚Ä¶ ll> T .i .t .a <n /tic o o< l/ _t ch ain ll>k> < Ca an ms ew re or n> <J /a am ne sws er> 1.0 1.0 the actor who won the A Beca std Aem cty o rA fw ora r td h efo fr il m Tool Tool Tool ‚Ä¶ ‚ùå GRPO E-GRPO ‚ÄúThe Revenant‚Äù? < <t toh oin l_k> ca‚Ä¶ ll> < ./ .t .h <i /n tk o> o l_call> < <t /o too ol_ l_re resp spo on nse se> >‚Ä¶ < <t toh oin l_k> ca‚Ä¶ ll> B .e ..r n <a /tr od o< l_/t ch ai ln l>k> < <t /o too ol_ l_re resp spo on nse se> >‚Ä¶ < <t toh oin l_k> ca‚Ä¶ ll> < ./ .t .h <i /n tk o> o l_call> < Lua hn rs mwe ar n> n B <a /z a nswer> 0.0 0.0 üìëGold Answer: üí° James Cameron Tool Tool Tool ‚Ä¶ ‚ùå GRPO E-GRPO üìëGold Entities: A Fic lmto :r : T L ite ao nn icardo < <tt oh oin l_k> ca‚Ä¶ ll> < ./ .t .h <i /n tk o> o l_call> < <t /o too ol_ l_re resp spo on nse se> >‚Ä¶ < <t toh oin l_k> ca‚Ä¶ ll> Le ...o <n /a tord olo _ c< a/ lt lh >ink> < <t /o too ol_ l_re res sp po on ns se e> >‚Ä¶ < <t toh oin l_k> ca‚Ä¶ ll> < ./ .t .h <i /n tk o> o l_call> < Saa nn ts w </e ar n> sG wu es r >Van 0.0 ùú∂¬∑0.5 Figure 2: Comparison of GRPO and E-GRPO. GRPO applies outcome-based reward, while E-GRPO additionallyassignsabonustonegativesproportionaltotheirnormalizedentitymatchrate. Thethree rolloutsillustrateasuccess,acompletefailure,anda"near-miss",respectively. Ô£± Œ≥ Ô£≤ i ifŒ≥ max >0 Œ≥ÀÜ i = Œ≥ max where Œ≥ max = max Œ≥ j. (3) Ô£≥0 otherwise j‚àà{1,...,G} Thisnormalizationallowsustoaggregatethematchrateofallrolloutsonacommon0-to-1scale. Analysis. Toinvestigatethecorrelationbetweenentitymatchrateandaccuracy,wefirstconductaper- questionanalysisonasampledsubsetofSailorFog-QA(Lietal.,2025b)usingtheWebSailor-7Bagent(Li etal.,2025b). Foreachquestion,weperform8rolloutsandcalculatetheaverageentitymatchratesof correctlysolvedandfailedrollouts,respectively. AsshowninFigure1(upperright),theaverageentity matchrateofcorrectrolloutswashigherthanthatoffailedonesinthevastmajorityofthequestions, outnumberingthereversescenariobyaclear4-to-1margin(1939vs. 487questions). Thisestablishesa strongcorrelationbetweentheentitymatchrateandthecorrectnessofthefinalanswer. Movingbeyondthisaggregate,per-questionview,weanalyzethedistributionofthenormalizedentity matchrateacrossallindividualrollouts. AsshowninFigure1(bottomright),thedistributionsforcorrect andincorrectrolloutsdivergesignificantly. Thedistributionofcorrectsamples(green)peakssharplyat anormalizedrateof1.0. Incontrast,incorrectsamples(red)showabimodaldistribution: alargepeak at0.0,andanotablespreadacrossthemid-to-highrange. Thislattergrouprepresentstheinformative "near-misses",wheremostentitieswerefoundbutthefinalreasoningfailed. Thisanalysisshowsthattheentitymatchrateismorethanjustapass/failindicator. Instead,itprovides agranularscaletodistinguishvaluable"near-misses"fromcompletefailures,offeringarichersignalof anagent‚Äôsreasoningquality. 3.2 Entity-aware Group Relative Policy Optimization Theprecedinganalysisshowsthatentitymatchrateoffersafine-grainedsignalofanagent‚Äôsreasoning quality. Conventionalpolicyoptimizationmethods,however,largelyignorethissignalbyrelyingona sparse,outcome-basedrewardtiedonlytoanswercorrectness,therebytreatingallfailuresasequally undesirable. Therefore, weintroduceEntity-awareGroupRelativePolicyOptimization(E-GRPO), aframeworkthatdirectlyincorporatestheentitymatchrateintoitsrewardfunctiontoguidepolicy learningbetter. LimitationsofRewardFormulationinGRPO. ExistingGRPO-likeframeworks(Shaoetal.,2024)for searchagentstypicallyemployoutcome-basedreward. Specifically,therewardR forarolloutH(i) is i definedsimplyas1ifitleadstoacorrectanswer,and0otherwise. Thisrewardisthenusedtocomputea group-relativeadvantage. Thisadvantagevalue,denotedas AÀÜ ,iscalculatedoncefortheentirerollouti i,j 5 --- Page 6 --- andthenappliedtoeverytokenjwithinit,servingasthecorelearningsignal: R ‚àímean({R }G ) AÀÜ = i k k=1 . (4) i,j std({R }G ) k k=1 Thelimitationofthisformulationisevident: asshowninFigure2,standardGRPOassignsanidentical rewardof0tobothacompletefailure(middlerollout,0entitiesmatched)andaninformative"near-miss" (bottomrollout,1entitymatched),thusrenderingtheirdifferentreasoningqualitiesindistinguishable. Entity-awareRewardFormulation. E-GRPOaddressesthelimitationofoutcome-basedrewardsby redefiningtherewardfunctionwithanentity-awarebonus. Weutilizethenormalizedentitymatchrate Œ≥ÀÜ ratherthantherawrate,asitsconsistent0-to-1scaleisessentialforastableadvantagecalculation i acrossdifferentgroups. Ourentity-awarerewardisthusdefinedas: Ô£± Ô£¥Ô£¥Ô£≤1 ifH(i) iscorrect R i = Œ±¬∑Œ≥ÀÜ i ifH(i) iswrong , (5) Ô£¥Ô£¥Ô£≥0 iferror1occursinH(i) whereŒ± ‚àà [0,1]isahyperparameterbalancingthevalueofaccuracyandentitymatching. Thisformula- tionyieldstwosignificantadvantages. (1)Itcreatesadenserewardspectrumtodistinguishthequality of incorrect rollouts. As shown in Figure 2, a "near-miss" that identifies a correct entity (Leonardo) is rewarded(Œ±¬∑0.5),unlikeacompletefailurewhichreceiveszero. (2)Itprovidesameaningfultraining signaleveninall-wronggroupswherestandardGRPOoffersnogradient. OverallTrainingObjective. Withourentity-awarerewarddefined,wecannowformalizethecomplete E-GRPOobjective. First,therefinedrewardfromEq.5isusedtocomputeamoreinformativeadvantage AÀÜ viaEq.4. ThepolicyisthenoptimizedbymaximizingtheGRPOobjectiveJ(Œ∏),definedas: i,j J(Œ∏) =E (q,gt) ‚àº D, {H(i)}G ‚àº œÄ i=1 Œ∏old (cid:34) |H(i)| (cid:35) (6) ‚àë iG =11 (cid:12) (cid:12)H(i)(cid:12) (cid:12) i‚àë =G 1 j‚àë =1 min(cid:16) r i,j(Œ∏)AÀÜ i,j,clip(cid:0) r i,j(Œ∏),1‚àíŒµ low,1+Œµ high(cid:1) AÀÜ i,j(cid:17) , wherer (Œ∏) = œÄŒ∏(H( ji)|q,H( j‚àíi) 1) istheimportancesamplingratio. i,j œÄŒ∏old(H( ji)|q,H( j‚àíi) 1) ImplementationDetails. Basedonthisobjective,weadditionallyapplythefollowingpracticalmodifi- cationstothetrainingofallmodels(bothourmethodandthebaselines): ‚Ä¢ KL-FreeObjectiveandPolicyExploration. FollowingDAPO(Yuetal.,2025), weremovetheKL- divergence regularization term in GRPO and apply the "clip-higher" method, which increases the upperclippingboundŒµ ,tobetterencouragepolicyexploration. high ‚Ä¢ HandlingFormatErrors. Rolloutswithformaterrors(definedinAppendixA)areassignedareward of0. ThisstrictpenaltyisjustifiedbecauseourRLtrainingisprecededbyacold-startSFTphasethat ensuresthemodelisalreadyfamiliarwiththerequiredoutputformat. ‚Ä¢ HandlingOverlengthRollouts. Overlengthrollouts(i.e.,thoseexceedingtokenortool-calllimits) arealsoassignedarewardof0. Weobservedinpreliminaryexperimentsthatdirectlyoptimizingon theserolloutscanleadtopolicycollapse. Therefore,weadoptaspecifichandlingstrategy: whilethese rolloutscontributetotheadvantagenormalization(i.e.,computingthegroup‚Äôsmeanandstandard deviation),theyareexcludedfromthefinallosscomputationtopreventinstability. 1Errors(formatandoverlengthproblems)aredetailedinthesubsequentparagraphImplementationDetails. 6 --- Page 7 --- 4 Experiments 4.1 Experiment Setup Benchmarks. Our evaluation spans a diverse set of 11 benchmarks to comprehensively assess E- GRPO‚Äôseffectiveness. Forquestion-answeringtasks,weusethreesingle-hopdatasets: NaturalQuestions (NQ)(Kwiatkowskietal.,2019),TriviaQA(TQ)(Joshietal.,2017),andPopQA(Mallenetal.,2022);and fourmulti-hopdatasets: 2WikiMultiHopQA(2Wiki.)(Hoetal.,2020),HotpotQA(HQA)(Yangetal., 2018),Bamboogle(Bamb.)(Pressetal.,2022),andMuSiQue(Musi.)(Trivedietal.,2022). Wefurthertest ouragentonfourchallengingdeepresearchbenchmarks: GAIA(Mialonetal.,2023),BrowseComp(Wei etal.,2025),BrowseComp-ZH(Zhouetal.,2025),andxbench-DeepSearch(xbench-DS)(Xbench-Team, 2025). FollowingAsearcher(Gaoetal.,2025),weuse1000sampledinstancesfromthevalidationsetsof HQA,2Wiki.,andMusi. ForGAIA,weusethe103-sampletext-onlyvalidationsubset(Lietal.,2025c). Forallotherbenchmarks,weusetheirfulltestsets. Baselines. OurprimarybaselineisthedirectcounterparttrainedwithGRPO(Shaoetal.,2024),allowing foracontrolledcomparisonofthealgorithmicenhancement. WealsocompareagainstasuiteofReAct- basedagents. ForQAbenchmarks, thisincludesR1-Searcher-7B(Songetal.,2025), DeepResearcher- 7B(Zhengetal.,2025),Search-R1-32B(Jinetal.,2025),Simple-DS-QwQ(Sunetal.,2025),andASearcher- 14B (Gao et al., 2025). For deep research benchmarks, we include advanced models like OpenAI-o3, Claude-4-Sonnet(Anthropic,2025),Kimi-K2(Teametal.,2025),andDeepSeek-V3.1(Liuetal.,2024), alongsideopen-sourcemodelswithnomorethan32BparameterssuchasR1-Searcher-7B,WebThinker- RL(Lietal.,2025d),WebDancer-QwQ(Wuetal.,2025a),andWebSailor-7B/32B(Lietal.,2025b). EnvironmentSettings. WeconducttrainingintwodistinctenvironmentstovalidateE-GRPO‚Äôsro- bustness: a closed-world local knowledge base (Local) and an open-world web exploration (Web) environment. IntheLocalsetting,searchandvisittoolsaresimulatedviainformationretrievalovera Wikipedia2024corpus(Karpukhinetal.,2020;Gaoetal.,2025). IntheWebsetting,theagentinteracts withthelivewebusingGoogleSearchandJina(Jina.ai,2025)forpagefetching. TrainingDetails. OurexperimentsarebasedonQwen2.5-7B-Instruct(Yangetal.,2024)andQwen3- 30B-A3B-Instruct-2507(Team,2025a),coveringdifferentmodelsizesandarchitectures(denseandMoE). ItisimportanttonotethatourstudyaimstovalidatetheeffectivenessofE-GRPOatthealgorithmic level,notmerelytopursuestate-of-the-artperformance.Therefore,weuselimiteddatatoensuretraining efficiencywhilestillenablingperformancecomparison. ‚Ä¢ Cold-startSFT:Wefirstfine-tunethebasemodelson11KsamplesfromSailorFog-QA(Lietal.,2025b). Thisstep,followingDongetal.(2025),mitigatesrewardcollapseandensuresthemodelunderstands theagenticformatbeforeRL. ‚Ä¢ RL: We generate two distinct 1k-sample datasets for RL training. For the Local environment, we synthesizedatausingtheAsearcher(Gaoetal.,2025)methodoverthe2024Wikipediacorpus. For theWebenvironment,weusetheSailorFog-QAdatagenerationpipeline. Crucially,forbothdatasets, weretainallground-truthentitiesgeneratedduringthesynthesisprocesstoenableE-GRPO.Wetrain the7Bmodelinbothenvironments,whilethe30BmodelistrainedonlyinthemorecomplexWeb environment. Foreachsetup,weapplybothGRPOandE-GRPO. We denote our models by their training environment, model sizes, and the training algorithm, e.g., Local-7B-GRPO.DetailedhyperparametersarepresentedinAppendixC. EvaluationMetrics. Modelanswers,extractedfromthemodeloutputenclosedin<answer>and</ answer>tags(detailedinAppendixA),areevaluatedforcorrectnessusingQwen2.5-72B-Instructunder 7 --- Page 8 --- Table1:  Resultswith‚Ä°aresourcedfromGao etal.(2025). Thetopscoresofeachevaluationenvironmentarebolded. Multi-HopQA Single-HopQA Environment Model Avg 2Wiki. HQA Bamb. Musi. NQ TQ PopQA ComparisonamongOurModels Local-7B-SFT 74.0 66.7 72.8 30.2 49.8 78.4 49.6 60.2 Local Local-7B-GRPO 75.1 65.1 74.4 31.2 51.5 82.0 50.4 61.4 Local-7B-E-GRPO 79.6 69.0 78.4 32.8 55.8 83.9 50.2 64.2 ComparisonwithOtherBaselines R1-Searcher-7B‚Ä° 69.4 61.6 72.0 25.3 48.7 79.5 45.2 57.4 DeepResearcher-7B‚Ä° 64.1 61.0 76.8 24.5 52.9 82.8 45.7 58.3 Search-R1-32B‚Ä° 69.3 64.2 81.6 30.8 51.1 86.6 53.6 62.5 Simple-DS-QwQ‚Ä° 80.4 67.5 83.2 32.9 55.3 90.2 47.8 65.3 Web ASearcher-14B‚Ä° 79.8 70.5 80.8 33.8 55.4 88.5 50.5 65.6 Local-7B-SFT 76.8 70.7 80.2 32.2 55.4 88.7 48.9 64.7 Local-7B-GRPO 77.2 73.8 82.4 34.9 55.9 89.3 50.1 66.2 Local-7B-E-GRPO 80.4 73.7 85.6 34.9 59.1 90.4 50.2 67.8 theLLM-as-Judgesetting.  acrossthreerollouts. 4.2 Main Results Wepresenttheexperimentresultsacrossthreeevaluationsettings: (1)7Bmodelstrainedandevaluated withtheLocalenvironmentonstandardQAbenchmarks,(2)thesame7Bmodelsevaluatedwiththe Web environment on the same benchmarks, and (3) all models trained and evaluated with the Web environmentondeepresearchbenchmarks. PerformanceintheLocalEnvironmentonQAbenchmarks. ThetopblockofTable1presentsthe resultsformodelstrainedandevaluatedwithinthecontrolledLocalenvironment. OurLocal-7B-E-GRPO modelachievesthehighestaveragescoreof64.2,markingasubstantialimprovementof2.8pointsover theGRPObaselineand4.0pointsovertheinitialSFTmodel. Thissuperiorityisconsistentacrossmost individualbenchmarks,demonstratingthattheentity-awarerewardallowsthemodeltolearnamore effectivereasoningpolicythantheoutcome-basedreward. PerformanceintheWebEnvironmentonQAbenchmarks. AsshowninthesecondblockofTable1, evenwhenevaluatedwiththeunfamiliarwebenvironment,ourLocal-7B-E-GRPOmodelagainachieves thehighestaveragescoreamongitspeersat67.8,outperformingtheGRPOcounterpartandotheropen- sourcebaselineswithlargersizes. Thisresultstronglyvalidatesthegeneralizabilityandrobustnessof ourmethod,allowingalocallytrainedmodeltoachievesuperiorperformanceinacompletelydifferent, real-worldsetting. PerformanceonDeepResearchBenchmarks. AspresentedinTable2,resultsondeepresearchbench- marksconsistentlyunderscorethesuperiorityofE-GRPO.Acrossboth7Band30Bscales,ourE-GRPO models significantly outperform their GRPO counterparts. Notably, Web-30B-E-GRPO achieves the bestperformanceamongopen-sourceagentsonBrowseComp(12.9)andBrowseComp-ZH(26.4),even surpassingadvancedmodelslikeClaude-4-SonnetonBrowseComp,andnarrowsthegapwithothers.  WhileGRPOoffersminimal 8 --- Page 9 --- Table2: Overallperformanceonfourchallengingdeepresearchbenchmarks. Resultswith‚Ä†aresourced fromWuetal.(2025c).  Thetop  GAIA BrowseComp BrowseComp-ZH xbench-DS Model         AdvancedModels OpenAI-o3‚Ä† 70.5 - 50.9 - 58.1 - 66.7 - Claude-4-Sonnet‚Ä† 68.3 - 12.2 - 29.1 - 64.6 - Kimi-K2‚Ä† 57.7 - 14.1 - 28.8 - 50.0 - DeepSeek-V3.1‚Ä† 63.1 - 30.0 - 49.2 - 71.0 - Open-sourceAgents‚â§32B R1-Searcher-7B 20.4 - 0.4 - 0.6 - 4.0 - WebThinker-RL 48.5 - 2.8 - 7.3 - 24.0 - WebDancer-QwQ 51.5 - 3.8 - 18.0 - 39.0 - WebSailor-7B 37.9 - 6.7 - 14.2 - 34.3 - WebSailor-32B 53.2 - 10.5 - 25.5 - 53.3 - OurAgents Web-7B-SFT 31.7 44.7 5.7 10.5 13.2 25.6 37.3 55.0 Web-7B-GRPO 33.0 44.7 6.3 11.7 17.5 31.5 40.7 56.0 Web-7B-E-GRPO 36.9 51.5 9.3 16.1 18.1 32.1 42.0 59.0 Web-30B-SFT 45.0 60.2 10.8 18.5 23.8 38.1 43.7 63.0 Web-30B-GRPO 47.6 62.1 12.3 18.9 25.7 38.8 45.3 65.0 Web-30B-E-GRPO 48.5 65.1 12.9 21.0 26.4 41.2 46.7 66.0 gainsovertheSFTbaseline(e.g.,44.7onGAIA),E-GRPOdeliverssubstantialimprovements(e.g.,a6.8- pointjumpto51.5). Thisstemsfromakeyalgorithmicdifference: GRPO‚Äôsoutcome-basedrewardtends torefineexistingsuccessfulstrategies,whereasE-GRPO‚Äôsentity-awarerewardexplicitlyencourages exploringpromisingbutincompletepaths. Thisallowstheagenttobuildamorediversesetofsolutions, whichdirectlyincreasesitschancesofsucceedingwithinafewattemptsandexplainsthesignificant  4.3 Analysis Training Dynamics. We begin by analyzing the training dynamics of E-GRPO against the GRPO baseline. AsshowninFigure3,E-GRPOdemonstratessuperiorlearningefficiencyandeffectiveness. TheleftpanelshowsthatE-GRPO(purple)consistentlyachieveshighertrainingaccuracy,showinga steadierandmorepronouncedupwardtrendthantheGRPObaseline(blue). Thissuggeststhatthedense, entity-awarerewardprovidesamoreeffectiveandstablelearningsignal. Simultaneously,themiddle panelrevealsthatE-GRPOlearnsamoreefficientreasoningpolicy,consistentlyusingfewertoolcalls perrollout. Thisefficiencycanbeattributedtorewardingthediscoveryofkeyentities,whichguidesthe agenttowardsmoredirectandinformativesolutionsteps. TofurthervalidateE-GRPO‚Äôsmechanism,we analyzetherelationshipbetweentheentitymatchrateandtheaccuracyduringtraining,asillustratedin therightpanelofFigure3. Astrongpositivecorrelationisevident: forbothGRPOandE-GRPO,the curvesoftheentitymatchrateandaccuracyriseintandem. Thisconfirmsourcorehypothesisthatthe entitymatchrateservesasaneffectiveproxyforfinalansweraccuracy. Crucially,theplotrevealsthe directimpactofourentity-awarereward: byexplicitlyincentivizingahigherentitymatchrate,E-GRPO (orange)consistentlyoutperformstheGRPObaseline(green)onthismetric. Thisadvantage,inturn, directlytranslatesintosuperiorfinalansweraccuracy(purplevs. blue),validatingthatmasteringthe sub-goalofentitymatchingleadstobetteroverallperformance. 9 --- Page 10 --- 0.65 0.60 0.55 0.50 0.45 0 10 20 30 40 50 60 70 80 Training Step ycaruccA gniniarT egarevA Comparison of Training Accuracy GRPO Accuracy E-GRPO Accuracy 13.0 12.5 12.0 11.5 11.0 0 10 20 30 40 50 60 70 80 Training Step spetS llaC looT egarevA Comparison of Tool Call Steps GRPO Tool Call Steps E-GRPO Tool Call Steps 0.50 0.45 0.40 0.35 0.30 0 10 20 30 40 50 60 70 80 Training Step etaR hctaM ytitnE egarevA Comparison of Entity Matching and Accuracy E-GRPO Entity Match Rate 0.65 GRPO Entity Match Rate E-GRPO Accuracy 0.60 GRPO Accuracy 0.55 0.50 0.45 0.40 Average Accuracy Figure3: Trainingdynamicsof30BmodelswiththeWebenvironment, includingthecomparisonof E-GRPOandGRPOovertrainingaccuracy,toolcallsteps,andtheanalysisbetweenentitymatchingand trainingaccuracy. AdetailedcasestudyinAppendixDprovidesaqualitativeillustrationofthesedynamics,concretely demonstratinghowE-GRPO‚Äôsfocusonentitymatchingleadstoamoreefficientandaccuratereasoning path. 45 40.7 41.7 42.0 40 36.9 38.3 3533.0 35.3 34.0 30 2017.5 17.9 18.1 15.1 15 10 9.3 6.3 7.2 6.3 5 0 0.0 0.1 0.3 0.5 Entity Matching Weight ()  AblationsofEntityMatchingWeights. Weconductanablation study on the hyperparameter Œ±, which balances the outcome- GAIA BrowseComp BrowseComp-ZH XBench-DS basedrewardandtheentity-matchingbonus. AsshowninFig- ure4,settingŒ± =0.0reducesourmethodtotheGRPObaseline. Forallfourbenchmarks,performanceconsistentlyimprovesas Œ± increases from 0.0, peaking at 0.3. This clearly demonstrates thebenefitofincorporatingtheentity-awarereward. However, a further increase to Œ± = 0.5 leads to a performance drop on most benchmarks, suggesting that an excessively strong entity- matchingbonuscandistractthemodelfromtheprimarygoalof generatingacorrectfinalanswer. Thishighlightstheimportance Figure4: Comparisonofdifferenten- ofbalancingthetworewardcomponents,withamoderateŒ±value titymatchingweights. yieldingtheoptimalpolicy. 5 Related Work SearchAgents. ThecapabilitiesofLargeLanguageModels(LLMs)havefueledasurgeinresearchon autonomousagentsthatcaninteractwithexternalenvironmentstosolvecomplextasks. Afoundational paradigminthisareaistheReActframework(Yaoetal.,2023),whichinterleavesreasoning(thought) andactionsteps. Buildingonthis,aprominentlineofresearchhasfocusedonsearchagentsdesignedto navigatetheweb(Songetal.,2025;Zhengetal.,2025;Lietal.,2025c;Zhangetal.,2025a;Sunetal.,2025). AdvancedmodelslikeGeminiDeepResearch(GoogleTeam,2025),OpenAIDeepResearch(OpenAI, 2025),GrokDeepSearch(xAITeam,2025),alongwithsmalleropen-sourcemodelssuchasAsearcher(Gao etal.,2025),WebThinker(Lietal.,2025d),WebWalker(Wuetal.,2025b),WebDancer(Wuetal.,2025a), andWebSailor(Lietal.,2025b;a)havedemonstratedincreasingproficiencyinretrievingandsynthesizing informationfromnoisy,real-worldwebsources. Ourworkdirectlycontributestoimprovingthetraining methodologyforthisclassofagents,addressingthechallengeoflearningrobustpoliciesincomplexweb environments. SyntheticDataGenerationforSearchAgents. Thenow-dominantparadigmfortrainingsearchagents relies heavily on high-quality synthetic data (Team, 2025b). A common thread in these generation methods is an entity-centric approach to complexity generation (Gao et al., 2025; Wu et al., 2025b;a; 10 --- Page 11 --- Li et al., 2025b;a; Tao et al., 2025; Wu et al., 2025c). During data synthesis, a rich set of ground-truth entitiesthatformthefactualbackboneofthecorrectansweraresystematicallydiscarded. Priorworkhas exclusivelyusedthefinalquestion-answerpairsfromthisprocessforpost-training(Dongetal.,2025; Wuetal.,2025a;Lietal.,2025b). Incontrast,ourworkisthefirst,toourknowledge,torecognizethese discardedentitiesnotasabyproduct,butasanuntappedsourceoffine-grained,factualsupervision. Wepioneertheideaofrepurposingthis‚Äúwaste‚Äùmaterialtoformulateanentity-awarerewardfunction, therebybridgingthegapbetweenthedatagenerationprocessandtheRLalignmentphaseinanoveland efficientmanner. Reinforcement Learning for Search Agents. Group Relative Policy Optimization (GRPO) and its variants(Shaoetal.,2024;Yuetal.,2025;Xuetal.,2024;Zhaoetal.,2025;Hu,2025;Xueetal.,2025;Su etal.,2025)havebecomeadominantparadigmforaligningsearchagents. Notableadvancementswithin thisparadigm,suchasARPO(Dongetal.,2025),haveadaptedtheframeworkwithanentropy-based rolloutmechanismforcomplexmulti-turnwebsearchsettings. Despitetheserefinements,theentire familyofGRPO-likemethodsisfundamentallyconstrainedbyitsrelianceonasparse,outcome-based rewardsignal. Whileconventionalsolutionstosuchsparsity,likeProcessRewardModels(PRMs)(Fan et al., 2025; Anonymous, 2025; Zhang et al., 2025b) or tree-based sampling (Yang et al., 2025; Hou etal.,2025),existinrelateddomains,theyareill-suitedforopen-endedwebsearchduetoprohibitive annotationcostsandcomputationalintractability. Ourwork,E-GRPO,divergesfromtheseapproachesby proposingarewardsignalthatisbothfine-grainedandcomputationallyefficient,requiringnoadditional annotation,modeltraining,orcomplexsampling. 6 Conclusion Inconclusion,weproposeEntity-awareGroupRelativePolicyOptimization(E-GRPO),anovelframe- workdesignedtoenhancepolicyoptimizationforsearchagents. Ouranalysisrevealsthattheground- truthentitiesdiscardedduringsyntheticdatagenerationserveasapowerfulproxyforfactualcorrectness, offeringafine-grainedrewardsignalthatstandardmethodsignore. E-GRPOleveragesthisinsightby formulatinganentity-awarerewardfunction,assigningpartialcredittonegativesamplesbasedontheir entitymatchratetoencouragemeaningfulexploration. AcrossawidearrayofQAanddeepresearch benchmarks,E-GRPOconsistentlyandsignificantlyoutperformstheGRPObaseline. Remarkably,itnot onlyachievessuperioraccuracybutalsolearnsmoreefficientpolicieswithfewertoolcalls,offeringa moreeffectiveandsample-efficientsolutionforaligningsearchagentsincomplex,knowledge-intensive tasks. 11 --- Page 12 --- References Anonymous. LSRL:Process-supervisedGRPOonlatentrecurrentstatesimprovesmathematicalreason- ing. InSubmittedtoACLRollingReview-May2025,2025. URL NcrDaFJfQk. underreview. Anthropic. Introducingclaude4,2025. URL YongDeng,GuoqingWang,ZhenzheYing,XiaofengWu,JinzhenLin,WenwenXiong,YuqinDai,Shuo Yang,ZhanweiZhang,QiwenWang,YangQin,YuanWang,QuanxingZha,SunhaoDai,andChanghua Meng. Atom-searcher: Enhancingagenticdeepresearchviafine-grainedatomicthoughtreward,2025. URL GuantingDong, HangyuMao, KaiMa, LichengBao, YifeiChen, ZhongyuanWang, ZhongxiaChen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprintarXiv:2507.19849,2025. Lishui Fan, Yu Zhang, Mouxiang Chen, and Zhongxin Liu. Posterior-grpo: Rewarding reasoning processesincodegeneration,2025. URL Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, and Yi Wu. Beyondtenturns: Unlockinglong-horizonagenticsearchwithlarge-scaleasynchronousrl,2025. URL  GoogleTeam. Introducinggeminideepresearch.  2025. XanhHo,Anh-KhoaDuongNguyen,SakuSugawara,andAkikoAizawa. Constructingamulti-hopqa datasetforcomprehensiveevaluationofreasoningsteps. arXivpreprintarXiv:2011.01060,2020. ZhenyuHou,ZiniuHu,YujiangLi,RuiLu,JieTang,andYuxiaoDong.Treerl:Llmreinforcementlearning withon-policytreesearch,2025. URL JianHu. Reinforce++: Asimpleandefficientapproachforaligninglargelanguagemodels. arXivpreprint arXiv:2501.03262,2025. BowenJin,HansiZeng,ZhenruiYue,JinsungYoon,SercanArik,DongWang,HamedZamani,andJiawei Han. Search-r1: Trainingllmstoreasonandleveragesearchengineswithreinforcementlearning. arXiv preprintarXiv:2503.09516,2025. Jina.ai. Jina,2025. URL Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervisedchallengedatasetforreadingcomprehension. arXivpreprintarXiv:1705.03551,2017. VladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen, andWen-tauYih. Densepassageretrievalforopen-domainquestionanswering. InBonnieWebber, TrevorCohn,YulanHe,andYangLiu(eds.),Proceedingsofthe2020ConferenceonEmpiricalMethods in Natural Language Processing (EMNLP), pp. 6769‚Äì6781, Online, November 2020. Association for ComputationalLinguistics. doi: 10.18653/v1/2020.emnlp-main.550. URL rg/2020.emnlp-main.550/. TomKwiatkowski,JennimariaPalomaki,OliviaRedfield,MichaelCollins,AnkurParikh,ChrisAlberti, DanielleEpstein,IlliaPolosukhin,JacobDevlin,KentonLee,etal. Naturalquestions: abenchmark forquestionansweringresearch. TransactionsoftheAssociationforComputationalLinguistics,7:453‚Äì466, 2019. 12 --- Page 13 --- KuanLi,ZhongwangZhang,HuifengYin,RuiYe,YidaZhao,LiwenZhang,LituOu,DingchuZhang, XixiWu,JialongWu,XinyuWang,ZileQiao,ZhenZhang,YongJiang,PengjunXie,FeiHuang,and JingrenZhou. Websailor-v2: Bridgingthechasmtoproprietaryagentsviasyntheticdataandscalable reinforcementlearning,2025a. URL KuanLi,ZhongwangZhang,HuifengYin,LiwenZhang,LituOu,JialongWu,WenbiaoYin,BaixuanLi, ZhengweiTao,XinyuWang,WeizhouShen,JunkaiZhang,DingchuZhang,XixiWu,YongJiang,Ming Yan,PengjunXie,FeiHuang,andJingrenZhou. Websailor: Navigatingsuper-humanreasoningfor webagent. arXivpreprintarXiv:2507.02592,2025b. XiaoxiLi,GuantingDong,JiajieJin,YuyaoZhang,YujiaZhou,YutaoZhu,PeitianZhang,andZhicheng Dou. Search-o1: Agenticsearch-enhancedlargereasoningmodels. arXivpreprintarXiv:2501.05366, 2025c. XiaoxiLi,JiajieJin,GuantingDong,HongjinQian,YutaoZhu,YongkangWu,Ji-RongWen,andZhicheng Dou. Webthinker: Empoweringlargereasoningmodelswithdeepresearchcapability. arXivpreprint arXiv:2504.21776,2025d. AixinLiu,BeiFeng,BingXue,BingxuanWang,BochaoWu,ChengdaLu,ChenggangZhao,Chengqi Deng,ChenyuZhang,ChongRuan,etal. Deepseek-v3technicalreport. arXivpreprintarXiv:2412.19437, 2024. AlexMallen,AkariAsai,VictorZhong,RajarshiDas,DanielKhashabi,andHannanehHajishirzi. When nottotrustlanguagemodels: Investigatingeffectivenessofparametricandnon-parametricmemories. arXivpreprintarXiv:2212.10511,2022. Gr√©goire Mialon, Cl√©mentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmarkforgeneralaiassistants. InTheTwelfthInternationalConferenceonLearningRepresentations, 2023. MoonshotAI. Kimi-researcher: End-to-endrltrainingforemergingagenticcapabilities.  shotai.github.io/Kimi-Researcher/,2025. URL her/. OpenAI. Openaideepresearch.  OfirPress,MuruZhang,SewonMin,LudwigSchmidt,NoahASmith,andMikeLewis. Measuringand narrowingthecompositionalitygapinlanguagemodels. arXivpreprintarXiv:2210.03350,2022. ChengQian,EmreCanAcikgoz,QiHe,HongruWang,XiusiChen,DilekHakkani-T√ºr,GokhanTur,and HengJi. Toolrl: Rewardisalltoollearningneeds. arXivpreprintarXiv:2504.13958,2025. ZhihongShao,PeiyiWang,QihaoZhu,RunxinXu,JunxiaoSong,XiaoBi,HaoweiZhang,Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open languagemodels. arXivpreprintarXiv:2402.03300,2024. Joykirat Singh, Raghav Magazine, Yash Pandya, and Akshay Nambi. Agentic reasoning and tool integrationforllmsviareinforcementlearning. arXivpreprintarXiv:2505.01441,2025. HuatongSong,JinhaoJiang,YingqianMin,JieChen,ZhipengChen,WayneXinZhao,LeiFang,and Ji-RongWen. R1-searcher: Incentivizingthesearchcapabilityinllmsviareinforcementlearning. arXiv preprintarXiv:2503.05592,2025. Zhenpeng Su, Leiyu Pan, Xue Bai, Dening Liu, Guanting Dong, Jiaming Huang, Wenping Hu, and GuoruiZhou. Klear-reasoner: Advancingreasoningcapabilityviagradient-preservingclippingpolicy optimization. arXivpreprintarXiv:2508.07629,2025. 13 --- Page 14 --- ShuangSun,HuatongSong,YuhaoWang,RuiyangRen,JinhaoJiang,JunjieZhang,FeiBai,JiaDeng, WayneXinZhao,ZhengLiu,etal. Simpledeepsearcher: Deepinformationseekingviaweb-powered reasoningtrajectorysynthesis. arXivpreprintarXiv:2505.16834,2025. Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang,XinyuWang,YongJiang,PengjunXie,FeiHuang,andJingrenZhou. Webshaper: Agentically datasynthesizingviainformation-seekingformalization. arXivpreprintarXiv:2507.15061,2025. KimiTeam,YifanBai,YipingBao,GuanduoChen,JiahaoChen,NingxinChen,RuijueChen,YanruChen, YuankunChen,YutianChen,etal. Kimik2: Openagenticintelligence. arXivpreprintarXiv:2507.20534, 2025. QwenTeam. Qwen3technicalreport,2025a. URL TongyiDeepResearchTeam. Tongyi-deepresearch.  2025b. HarshTrivedi,NiranjanBalasubramanian,TusharKhot,andAshishSabharwal. Musique:Multihopques- tionsviasingle-hopquestioncomposition. TransactionsoftheAssociationforComputationalLinguistics, 10:539‚Äì554,2022. LeiWang,ChenMa,XueyangFeng,ZeyuZhang,HaoYang,JingsenZhang,ZhiyuanChen,JiakaiTang, XuChen,YankaiLin,etal. Asurveyonlargelanguagemodelbasedautonomousagents. Frontiersof ComputerScience,18(6):186345,2024. JasonWei,ZhiqingSun,SpencerPapay,ScottMcKinney,JeffreyHan,IsaFulford,HyungWonChung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: A simple yet challenging benchmarkforbrowsingagents. arXivpreprintarXiv:2504.12516,2025. MuningWen,ZiyuWan,JunWang,WeinanZhang,andYingWen. Reinforcingllmagentsviapolicy optimizationwithactiondecomposition. AdvancesinNeuralInformationProcessingSystems,37:103774‚Äì 103805,2024. Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, ZekunXi,YongJiang,PengjunXie,FeiHuang,andJingrenZhou. Webdancer: Towardsautonomous informationseekingagency. arXivpreprintarXiv:2505.22648,2025a. JialongWu,WenbiaoYin,YongJiang,ZhenglinWang,ZekunXi,RunnanFang,DeyuZhou,Pengjun Xie,andFeiHuang. Webwalker: Benchmarkingllmsinwebtraversal. arXivpreprintarXiv:2501.07572, 2025b. Xixi Wu, Kuan Li, Yida Zhao, Liwen Zhang, Litu Ou, Huifeng Yin, Zhongwang Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Minhao Cheng, Shuai Wang, Hong Cheng, and Jingren Zhou. Resum: Unlockinglong-horizonsearchintelligenceviacontextsummarization,2025c. URL g/abs/2509.13313. xAITeam. Grokagents: Combiningreasoningandtooluse. #grok-agent s-combining-reasoning-and-tool-use,2025. Xbench-Team. Xbench-deepsearch,2025. URL ZhihengXi,WenxiangChen,XinGuo,WeiHe,YiwenDing,BoyangHong,MingZhang,JunzheWang, SenjieJin,EnyuZhou,etal. Theriseandpotentialoflargelanguagemodelbasedagents: Asurvey. ScienceChinaInformationSciences,68(2):121101,2025. ZhengXu,XuDai,ShaojunWei,ShouyiYin,andYangHu.Gspo:Agraphsubstitutionandparallelization jointoptimizationframeworkfordnninference. InProceedingsofthe61stACM/IEEEDesignAutomation Conference,pp.1‚Äì6,2024. 14 --- Page 15 --- ZhenghaiXue,LongtaoZheng,QianLiu,YingruLi,XiaosenZheng,ZejunMa,andBoAn.Simpletir:End- to-endreinforcementlearningformulti-turntool-integratedreasoning. arXivpreprintarXiv:2509.02479, 2025. AnYang,BaosongYang,BeichenZhang,BinyuanHui,BoZheng,BowenYu,ChengyuanLi,Dayiheng Liu,FeiHuang,HaoranWei,etal. Qwen2.5technicalreport. arXivpreprintarXiv:2412.15115,2024. ZhichengYang,ZhijiangGuo,YinyaHuang,XiaodanLiang,YiweiWang,andJingTang. Treerpo: Tree relativepolicyoptimization,2025. URL ZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,WilliamWCohen,RuslanSalakhutdinov,and ChristopherDManning. Hotpotqa: Adatasetfordiverse,explainablemulti-hopquestionanswering. arXivpreprintarXiv:1809.09600,2018. ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikNarasimhan,andYuanCao. Re- act: Synergizing reasoning and acting in language models. In International Conference on Learning Representations(ICLR),2023. QiyingYu,ZhengZhang,RuofeiZhu,YufengYuan,XiaochenZuo,YuYue,TiantianFan,GaohongLiu, LingjunLiu,XinLiu,etal. Dapo: Anopen-sourcellmreinforcementlearningsystematscale. arXiv preprintarXiv:2503.14476,2025. DingchuZhang,YidaZhao,JialongWu,BaixuanLi,WenbiaoYin,LiwenZhang,YongJiang,Yufeng Li,KeweiTu,PengjunXie,etal. Evolvesearch: Aniterativeself-evolvingsearchagent. arXivpreprint arXiv:2505.22501,2025a. Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learningtoreasonwithmultimodallargelanguagemodelsviastep-wisegrouprelativepolicy optimization. arXivpreprintarXiv:2503.12937,2025b. YuzhongZhao,YueLiu,JunpengLiu,JingyeChen,XunWu,YaruHao,TengchaoLv,ShaohanHuang, LeiCui,QixiangYe,etal. Geometric-meanpolicyoptimization. arXivpreprintarXiv:2507.20673,2025. YuxiangZheng,DayuanFu,XiangkunHu,XiaojieCai,LyumanshanYe,PengruiLu,andPengfeiLiu. Deepresearcher: Scalingdeepresearchviareinforcementlearninginreal-worldenvironments. arXiv preprintarXiv:2504.03160,2025. PeilinZhou,BruceLeon,XiangYing,CanZhang,YifanShao,QichenYe,DadingChong,ZhilingJin, ChenxuanXie,MengCao,etal.Browsecomp-zh:Benchmarkingwebbrowsingabilityoflargelanguage modelsinchinese. arXivpreprintarXiv:2504.19314,2025. 15 --- Page 16 --- A Format OurReActframeworkfollowsLietal.(2025b). Acompleterolloutfollowstheformatbelow: Format <think> thinkingprocesshere </think> <tool_call> {‚Äúname‚Äù:‚Äútoolnamehere‚Äù,‚Äúarguments‚Äù:{‚Äúparameternamehere‚Äù:parametervaluehere,‚Äúanother parameternamehere‚Äù: anotherparametervaluehere,...}} </tool_call> <tool_response> tool_responsehere </tool_response> (morethinkingprocesses,toolcallsandtoolresponseshere) <think> thinkingprocesshere </think> <answer> answerhere </answer> Anyresponsethatdoesnotstrictlyfollowtheformatwillbeconsideredacasewithformaterrors. B Discussion about Synthetic-data Entities B.1 Entity Construction First,weexplaintheconstructionoftheground-truthentitysetsfortwodifferentdatasynthesismethods usedinourexperiments. ASearcher. AsillustratedinFigure1andSection2.2,thequestionisiterativelyconstructedbyselecting an entity and replacing it with descriptive facts or fuzzing it. Consequently, we use all selected and modifiedentitiesforaquestionasitsground-truthentityset. SailorFog-QA. As shown in Section 2.2, data generation begins by sampling an entity subgraph, followedbypromptinganLLMtogenerateaquestioncenteredaroundtheseentitynodes. Therefore,the nodesetofthesampledsubgraphisregardedastheground-truthentityset. EntityQualityControl. Sincethequestiongenerationprocessensuresquestionquality,e.g.,injected factsstrictlyadheretotheselectedentity,andgeneratedquestionsareconsistentlycenteredaroundthe sampled subgraph, the resulting entity sets are highly precise. Even if there are unexpectedly noisy entities,ourrewardmechanismisrobusttothem. Sinceanyirrelevantentityislikelytobemissedby allrolloutswithinagroup,itdoesnotchangetheirrelativeperformanceandthusdoesnotaffectthe normalizedrewardsignal. B.2 Entity Matching Then,weconsidertwoquestionsrelatedtotheentitymatchingmechanism: (1)Whydoweusetheexact stringmatchratherthanusinganLLMformatching? (2)Whydoweonlycounttheentitiesmatchedin thoughts,excludingthosematchedinobservation? RationaleforExactStringMatching. OurdecisiontouseexactstringmatchinginsteadofanLLM- based judger is primarily motivated by the nature of our ground-truth entities, which are definite, short-formed strings with little ambiguity. This characteristic makes exact matching a natural and 16 --- Page 17 --- 3.5 3.0 2.5 2.0 1.5 1.0 0.5 0.0 0.0 0.2 0.4 0.6 0.8 1.0 Normalized (Thought) Entity Match Rate ytisneD elpmaS Correct Wrong 4.0 3.5 3.0 2.5 2.0 1.5 1.0 0.5 0.0 0.0 0.2 0.4 0.6 0.8 1.0 Normalized (Trajectory) Entity Match Rate ytisneD elpmaS Correct Wrong Figure5: ComparisonofNormalizedentitymatchrateinthoughtsandentiretrajectories. sufficientmethod,whichinturnaddressestwopracticalconcerns: trainingefficiencyandrobustness againstrewardhacking. First,employinganLLMtosemanticallyparseandmatchentitieswithinlongreasoningtraceswould introducesignificantcomputationallatency,impedingthethroughputoftheRLtrainingloop. Incontrast, exactstringmatchingiscomputationallytrivialandaddsnegligibleoverhead. Second,whileadvancedLLMscanperformsemanticmatching,theyarealsomoresusceptibletoexploita- tionbythepolicymodel. Inpreliminaryexperiments,weobservedadistinctreward-hackingbehavior: the agent learned to extend its thoughts with verbose, superficially relevant phrases that, while not containingthecorrectentities,wouldmisleadtheLLMjudgeintoerroneouslyassigningpartialcredit. Exactstringmatching,beinglessflexible,providesamorereliablerewardsignal,ensuringtheagentis rewardedforfactualcorrectnessratherthanplausible-soundingtext. RationaleforThought-OnlyMatching. Tojustifywhywematchentitiesexclusivelywithintheagent‚Äôs thoughts(<think> </think>blocks),weanalyzethedifferencebetweenthisapproachandmatching acrosstheentiretrajectory(includingobservations). AsshowninFigure5,thetwomethodsyieldnotablydifferentdistributionsforincorrecttrajectories. Whilethought-basedmatching(left)showsaclearseparationwithmostfailureshavingalowmatchrate, trajectory-basedmatching(right)producessignificantlymore‚Äúfalsepositives‚Äù: incorrectrolloutsthat stillachieveahighentitymatchrate. Wedoseveralcasestudiesandfindthecauseofthisdiscrepancy. Often,akeyentityispresentinthe observationreturnedbyatool(e.g.,asearchsnippet),buttheagentfailstoextractandincorporatethis informationintoitsreasoningprocess. Rewardingtheagentbasedontheentiretrajectorywouldgrant unearnedcreditformerelyencounteringinformation,notforunderstandingandactinguponit. This createsanoisyrewardsignalthatfailstopenalizeatruereasoningfailure. Therefore,byconfiningentity matching to the agent‚Äôs thoughts, we ensure the reward is directly coupled to the model‚Äôs ability to identifyandinternalizekeyinformation,providingacleanerandmoretargetedlearningsignal. C Hyperparameters SFT. Weapplyatrainingbatchsizeof32,acosinedecaylearningrateof5e-6forabout4.8epochs,with alinearwarmupstartingfrom1e-10for0.2epochs. 17 --- Page 18 --- RL. Weapplyacontextlengthof32k,atoolcallbudgetof40,atrainingbatchsizeof64,appobatch sizeof64(exactlyonpolicyoptimization),agroupsizeof8,andalearningrateof2e-6forabout5epochs. ForE-GRPO,wesettheentitymatchingweightŒ±as0.3bydefault. Inference. Weapplyatemperatureof0.6,atop_pof0.95,andatoolcallbudgetof40forinference. D Case Study Toillustratethequalitativedifferencesinreasoning,wepresentacomparativecasestudyusingtrajectories generatedbyWeb-30B-GRPOandWeb-30B-E-GRPOforthesamequery.TheE-GRPOagentdemonstrates amoredirectandefficientreasoningpath,requiringfewertoolcallstosuccessfullyidentifyallkeyentities andculminatinginthecorrectanswer.Instarkcontrast,theGRPOagentembarksonalonger,lessfruitful trajectory. Despiteusingmoretoolcalls,itfailstopinpointthecriticalentityinthethoughts,theFirst InternationalPolarYear(describedasthe‚Äúinternationalscientificinitiativefocusedonthepolarregions‚Äù), whichultimatelyleadstoanincorrectanswer. CaseTrajectoryonSailorFog-QA:SolvedbyWeb-30B-E-GRPO Question: A European naval officer, known for leading a significant polar expedition in the late 19th century aboard a vessel named after an Austrian admiral, later proposed a major internationalscientificinitiativefocusedonthepolarregions. Forhiscontributionstoexploration, he received one of the highest honors from a major British geographical society. Who is this explorer? Ground-truth Entities: [Tegetthoff, International Polar Year, Royal Geographical Society‚Äôs Founder‚ÄôsMedal] Answer: KarlWeyprecht Round1 <think> Toidentifytheexplorerinquestion,Ineedtoconnectseveralspecificclues: aEuropeannaval officerwholedanotablepolarexpeditioninthelate19thcenturyaboardashipnamedafteran Austrianadmiral, laterproposedamajorinternationalpolarscientificinitiative, andreceived atophonorfromamajorBritishgeographicalsociety. Thesearehighlydistinctivedetails,but the identity is not immediately clear. By structuring my search queries to include all these elements‚Äîexpeditiontiming,vesselnameorigin,scientificinitiative,andBritishrecognition‚ÄîI maximizethechanceofsurfacingsourcesthatdirectlylinkthesefactstoasingleindividual. This comprehensiveapproachshouldyieldeithertheexplorer‚Äôsnameorauthoritativereferencesthat willallowmetodeducetheanswer. Ineedtorunthesetargetedsearchesnowtosynthesizethe relevantinformation. </think> <tool_call> {"name": "search", "arguments": {"query": ["European naval officer polar expedition late 19th century vessel named after Austrian admiral", "European naval officer proposed major internationalscientificinitiativepolarregions","Europeannavalofficerreceivedhighesthonors majorBritishgeographicalsociety","late19thcenturyvesselnamedafterAustrianadmiralpolar expedition"]}} </tool_call> <tool_response> AGooglesearchfor‚ÄôEuropeannavalofficerpolarexpeditionlate19thcenturyvesselnamedafter 18 --- Page 19 --- Austrianadmiral‚Äôfound10results: ##WebResults1. [Austro-HungarianNorthPoleexpedition-Wikipedia] ( theTegetthoff,namedfortheAustrianAdmiralWilhelmvonTegetthoff,underwhomWeyprecht hadserved. ItwasbuiltforthisexpeditionbyTeklenborg&BeurmanninBremerhaven. Itwasa three-mastedbarkentineof220tons,38.34mlong,witha100horsepower(75kW)steamengine. 2. [WilhelmvonTegetthoff-Wikipedia] ( ... </tool_response> Round2 <think> TheWikipediaarticleontheAustro-HungarianNorthPoleexpeditionislikelytoprovideau- thoritativedetailsabouttheexpedition‚Äôsleader, theship‚Äôsname, anditsnamesake. Sincethe questionhingesonconnectingtheidentityofthenavalofficer,thevesselnamedafteranAustrian admiral,andsubsequentachievements,reviewingthispageshouldclarifywholedtheexpedition andconfirmthelinkbetweentheshipandAdmiralWilhelmvonTegetthoff. Additionally,the articlemaymentiontheleader‚Äôslaterscientificinitiativesandhonors,whicharecrucialforfully answeringthequestion. Ineedtovisitthispagetoextractthesespecificdetailsandestablishthe explorer‚Äôsidentity. </think> <tool_call> {"name": "visit", "arguments": {"url": [" Hungarian_North_Pole_expedition"], "goal": "Identify the commander (led by) and the vessel‚Äôsnamesakename,AdmiralWilhelmvonTegetthoff"}} </tool_call> <tool_response> Theusefulinformationin Hungarian_North_Pole_expeditionforusergoalIdentifythecommander(ledby)andthevessel‚Äôs namesakename,AdmiralWilhelmvonTegetthoffasfollows: Evidenceinpage: TheAustro-HungarianNorthPoleexpeditionwasanArcticexpeditiontofind theNorth-EastPassagethatranfrom1872to1874undertheleadershipofJuliusvonPayerand KarlWeyprecht. TheexpeditiondiscoveredandpartiallyexploredFranzJosefLand. ... </tool_response> Round3 <think> TodeterminewhetherKarlWeyprechtreceivedoneofthehighesthonorsfromamajorBritish geographicalsociety,Ineedtodirectlyassociatehisnamewithsuchrecognition. Theprevious informationconfirmedhisleadershipoftheexpeditionandthevessel‚Äôsnamesake,butthequestion also requires evidence of prestigious British honors. By searching for combinations of "Karl Weyprecht"withtermslike"highesthonors"and"Britishgeographicalsociety,"Icanefficiently uncoverwhetherhewasawardedsomethingliketheFounder‚ÄôsMedalorasimilardistinction from the Royal Geographical Society. This targeted search should clarify if Weyprecht fits all aspects of the question. I need to run these specific queries now to confirm the final piece of information. 19 --- Page 20 --- </think> <tool_call> {"name": "search","arguments": {"query": ["KarlWeyprechthighesthonorsBritishgeographical society","KarlWeyprechtBritishgeographicalsociety","KarlWeyprechtFounder‚ÄôsMedal","Karl WeyprechthonorsRoyalGeographicalSociety"]}} </tool_call> <tool_response> AGooglesearchfor‚ÄôKarlWeyprechthighesthonorsBritishgeographicalsociety‚Äôfound10results: ##WebResults1. [KarlWeyprecht-Wikipedia] ( highesthonors ... </tool_response> Round4 <think> The search results strongly suggest that Karl Weyprecht is the central figure in question, with multiple sources mentioning his leadership of the 1872‚Äì74 polar expedition aboard the Tegetthoff and his later scientific initiatives. However, to fully confirm his identity and gather all the necessary details‚Äîsuch as the ship‚Äôs name, the Austrian admiral it was named after, his role in proposing the InternationalPolarYear, and whether he received the RoyalGeographicalSociety‚ÄôsFounder‚ÄôsMedal‚ÄîI need a comprehensive, authoritative sum- mary. VisitingtheWikipediapagededicatedtoKarlWeyprechtshouldprovideaconsolidated accountofhisachievements,honors,andscientificcontributions. Ineedtovisitthispagenowto extractthesespecificdetailsanddefinitivelyanswerthequestion. </think> <tool_call> {"name": "visit","arguments": {"url": [""],"goal": "Extractdetails: vesselname,theAustrianadmiral,internationalscientificinitiative,Founder‚Äôs MedalofRoyalGeographicalSociety"}} </tool_call> <tool_response> Theusefulinformationin details: vesselname,theAustrianadmiral,internationalscientificinitiative,Founder‚ÄôsMedalof RoyalGeographicalSocietyasfollows: Evidence in page: He co-led, with Julius von Payer, the 1872-1874 [[Austro-Hungarian North PoleExpedition]]whichdiscoveredthearchipelago[[FranzJosefLand]]intheArcticOcean. The expedition‚Äôsship‚ÄúAdmiralTegetthoff‚Äù... </tool_response> Round5 <think> To arrive at the answer "Karl Weyprecht," I pieced together several key clues from the ques- tionandverifiedthemthroughtargetedsearchesandauthoritativesources. First,Iconfirmed thatWeyprechtledalate19th-centurypolarexpeditionaboardtheship"AdmiralTegetthoff," whichwasnamedafterAustrianAdmiralWilhelmvonTegetthoff‚Äîthisdirectlymatchedthe ship and admiral mentioned in the question. Next, I established that Weyprecht was instru- mentalinproposingtheInternationalPolarYear, amajorcollaborativescientificinitiativefor 20 --- Page 21 --- the polar regions. Finally, I verified through multiple sources that he was awarded the 1875 RoyalGeographicalSociety‚ÄôsFounder‚ÄôsMedal,oneofthehighesthonorsinBritishgeography. Theconvergenceofthesefacts‚ÄîleadershipoftheTegetthoffexpedition,advocacyfortheInterna- tionalPolarYear,andreceiptoftheRGSFounder‚ÄôsMedal‚ÄîconfirmsthatKarlWeyprechtisthe explorerdescribed. </think> <answer> KarlWeyprecht exemplifies the characteristics in your query: as commander of a significant polarexpedition(aboardtheship"AdmiralTegetthoff")afteraprominentAustrianadmiral,asa proposaltargetofamajorinternationalscientificinitiative(theInternationalPolarYear),andas arecipientofoneofthemostdistinguishedhonorsfromamajorBritishgeographicalsociety(the RoyalGeographicalSociety‚ÄôsFounder‚ÄôsMedal). </answer>‚úì CaseTrajectoryonSailorFog-QA:FailedbyWeb-30B-GRPO Question: A European naval officer, known for leading a significant polar expedition in the late 19th century aboard a vessel named after an Austrian admiral, later proposed a major internationalscientificinitiativefocusedonthepolarregions. Forhiscontributionstoexploration, he received one of the highest honors from a major British geographical society. Who is this explorer? Ground-truth Entities: [Tegetthoff, International Polar Year, Royal Geographical Society‚Äôs Founder‚ÄôsMedal] Answer: KarlWeyprecht Round1 <think> ThequestioninvolvesaEuropeannavalofficerwholedasignificantpolarexpeditioninthelate 19thcentury,hadavesselnamedafteranAustrianadmiral,laterproposedamajorinternational scientificinitiativeforthepolarregions,andwashonoredbyaprominentBritishgeographical society. Topinpointtheindividual,Ineedtoconnectthedotsbetweentheexpedition,thevessel‚Äôs name, the Austrian admiral reference, and the scientific initiative. Using search queries that combinetheseelements‚Äîsuchasthevessel‚Äôsnaming,theAustrianadmiral,thepolarexpedition, and the proposal of an international scientific initiative‚Äîshould surface historical records or articlesthatdirectlymentiontheexplorerandclarifyhiscontributions. Bysearchingwiththese targetedterms,Iaimtoidentifytheofficerandgatherthenecessarycontextfortherestofthe question. Ineedtoconductthissearchnowtofindtheexplorer‚Äôsidentity. </think> <tool_call> {"name": "search","arguments": {"query": ["vesselnamedafterAustrianadmiralpolarexpedition late19thcentury","proposalmajorinternationalscientificinitiativepolarregions","laterproposed majorinternationalscientificinitiativepolarregions"]}} </tool_call> <tool_response> A Google search for ‚Äôvessel named after Austrian admiral polar expedition late 19th century‚Äô found10results: 21 --- Page 22 --- ##WebResults 1. [Austro-HungarianNorthPoleexpedition-Wikipedia] ( theTegetthoff,namedfortheAustrianAdmiralWilhelmvonTegetthoff,underwhomWeyprecht hadserved. ItwasbuiltforthisexpeditionbyTeklenborg&BeurmanninBremerhaven. ... </tool_response> Round2 <think> TheWikipediapagefortheAustro-HungarianNorthPoleexpeditionislikelytocontainauthor- itativedetailsabouttheexpedition‚Äôsleadership,theroleofthevesselnamedafteranAustrian admiral,andkeyfiguresinvolved. Sincethequestionrequiresidentifyingthenavalofficerwho ledthisexpedition,aswellashislatercontributionsandhonors,thispageshouldprovidenot onlyhisnamebutalsocontextualinformationabouthiscareerandachievements. Byvisitingthis URL,Icandirectlyextractthenameoftheexpeditionleaderandseeiftherearereferencestohis subsequentactions,suchasproposingmajorscientificinitiativesandreceivingrecognitionfrom Britishgeographicalsocieties. Ineedtovisitthispagenowtogatherthesecrucialdetails. </think> <tool_call> {"name": "visit", "arguments": {"url": [" Hungarian_North_Pole_expedition"], "goal": "Identify the leader of the expedition and aircraftdetailsincludinghisname,andwhetherheproposedaPolarYearorsimilarinitiative later,andawards"}} </tool_call> <tool_response> Theusefulinformationin Hungarian_North_Pole_expeditionforusergoalIdentifytheleaderoftheexpeditionandaircraft detailsincludinghisname,andwhetherheproposedaPolarYearorsimilarinitiativelater,and awardsasfollows: Evidenceinpage: TheAustro-HungarianNorthPoleexpeditionwasanArcticexpeditiontofind theNorth-EastPassagethatranfrom1872to1874undertheleadershipofJuliusvonPayerand KarlWeyprecht. ... </tool_response> Round3 <think> AlthoughIhaveidentifiedJuliusvonPayerasaleaderoftheAustro-HungarianNorthPoleexpe- ditionaboardtheTegetthoffandfoundthattheexpeditioncontributedtotheideaofinternational polarscientificcollaboration,IhavenotyetpinpointedtheexacthonorfromaBritishgeographical society. Thisisacrucialdetailneededtofullyanswerthequestionanddistinguishhisachieve- mentsfromthoseofhiscontemporaries. Bysearchingwithfocusedqueriessuchas‚ÄôJuliusvon PayerBritishGeographicalSocietyhonors‚Äô,and‚ÄôWeyprechtBritishGeographicalSocietyhonors‚Äô, Icandirectlytargetsourcesthatclarifytheconnectionbetweentheexpeditionleadersandthe specifichonorsreceived. Ineedtoconductthissearchnowtofillthesekeyinformationgaps. </think> 22 --- Page 23 --- <tool_call> {"name": "search","arguments": {"query": ["JuliusvonPayerBritishGeographicalSocietyhonors", "WeyprechtBritishGeographicalSocietyhonors"]}} </tool_call> <tool_response> AGooglesearchfor‚ÄôJuliusvonPayerBritishGeographicalSocietyhonors‚Äôfound10results: ##WebResults1. [JuliusvonPayer-Wikipedia] ( NorthPoleexpeditionin1872‚Äì74andthediscoveryofFranzJosefLand... </tool_response> Round4 <think> To fully confirm whether Julius von Payer received one of the highest honors from a major British geographical society, and to directly tie him to any significant international scientific initiativesforthepolarregions,Ineedauthoritativebiographicaldetails. TheWikipediapage dedicatedtoJuliusvonPayerislikelytoprovideaconcisesummaryofhismajorachievements, awards, andanynotableproposalsorinitiativeshewasresponsiblefor. Byvisitingthispage, I can efficiently extract explicit references to British geographical society honors‚Äîsuch as the RoyalGeographicalSociety‚ÄôsFounder‚ÄôsMedalorPatron‚ÄôsMedal‚Äîandclarifyhisroleinshap- ing international polar scientific collaboration. I need to visit this page now to gather these definitivedetails. </think> <tool_call> {"name": "visit","arguments": {"url": [""],"goal": "ExtractawardsofJuliusvonPayer,particularlyanyfromBritishGeographicalSociety,andclarify hisroleinshapinginternationalpolarscientificcollaboration"}} </tool_call> <tool_response> Theusefulinformationin awardsofJuliusvonPayer,particularlyanyfromBritishGeographicalSociety,andclarifyhisrole inshapinginternationalpolarscientificcollaborationasfollows: ... </tool_response> Round5 <think> Todeterminewhichexpeditionleader‚ÄîJuliusvonPayerorKarlWeyprecht‚ÄîreceivedtheRoyal GeographicalSociety‚Äôshighesthonors,IneedtoclarifywhetherPayerwasactuallyawardedthe goldmedalorthePatron‚ÄôsMedal,andinwhichyear. ThesummaryonthemainJuliusvonPayer page referenced the Patron‚Äôs Medal in 1971, but individual award listings on the Gold Medal (RGS)Wikipediapageshouldprovideadefinitive, year-by-yearrosterofrecipients, ensuring accuracy regarding both the medal and its recipients. By visiting this page, I can confirm the exactyearandwhetherJuliusvonPayerisindeedlistedamongthehonorees,whichiscrucialfor answeringthequestionabouthisrecognitionbytheBritishgeographicalsociety. Ineedtocheck thisofficiallistnowtoconfirmthedetails. </think> 23 --- Page 24 --- <tool_call> {"name": "visit", "arguments": {"url": [""], "goal": "Check if Julius von Payer is listed as a recipient of the Royal Geographical Society‚Äôs highesthonorandyear"}} </tool_call> <tool_response> Theusefulinformationin ifJuliusvonPayerislistedasarecipientoftheRoyalGeographicalSociety‚Äôshighesthonorand yearasfollows: Evidenceinpage: 1875|[[JuliusPayer]]|ForexplorationsanddiscoveriesintheArcticregions... </tool_response> Round6 <think> Toarriveattheanswer,IfirstnoticedthatthequestiondescribedaEuropeannavalofficerwho ledasignificantpolarexpeditioninthelate19thcenturyaboardavesselnamedafteranAustrian admiral,laterproposedamajorinternationalscientificinitiative,andreceivedatophonorfroma Britishgeographicalsociety. MyinitialsearchesconfirmedthattheAustro-HungarianNorthPole expedition(1872‚Äì1874),ledbyJuliusvonPayerandCarlWeyprecht,usedtheshipTegetthoff, namedafterAdmiralWilhelmvonTegetthoff. Furtherinvestigationrevealedthatwhileboth menwerekeyfigures,itwasJuliusvonPayerwhowasawardedtheRoyalGeographicalSociety‚Äôs Patron‚Äôs Medal (the highest British geographical honor) for his achievements. Therefore, by piecingtogethertheexpeditiondetails,thenamingofthevessel,andthespecificrecognitionfrom theRGS,IconcludedthattheexplorerinquestionisJuliusvonPayer. </think> <answer> JuliusvonPayer stands out as the notable European naval officer who fits all aspects of your query,havingledalandmarkpolarexpedition,playedakeyroleinthefoundationofinternational polarscienceinitiatives,andbeenhonoredbytheforemostBritishgeographicalsocietyforhis achievements. </answer>‚úó 24