--- Page 1 --- Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers? YihaoLi1 SaeedSalehi2 LyleUngar1 KonradP.Kording1 1UniversityofPennsylvania 2MachineLearningGroup,TechnicalUniversityofBerlin     Abstract Object binding, the brain’s ability to bind the many features that collectively representanobjectintoacoherentwhole,iscentraltohumancognition. Itgroups low-levelperceptualfeaturesintohigh-levelobjectrepresentations,storesthose objectsefficientlyandcompositionallyinmemory,andsupportshumanreasoning aboutindividualobjectinstances. Whilepriorworkoftenimposesobject-centric attention(e.g.,SlotAttention)explicitlytoprobethesebenefits,itremainsunclear whetherthisabilitynaturallyemergesinpre-trainedVisionTransformers(ViTs). Intuitively,theycould:recognizingwhichpatchesbelongtothesameobjectshould beusefulfordownstreampredictionandthusguideattention. Motivatedbythe quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term IsSameObject. We decodeIsSameObjectfrompatchembeddingsacrossViTlayersusingasimilarity probe,whichreachesover90%accuracy. Crucially,thisobject-bindingcapability emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker in ImageNet-supervised models, suggesting that binding is not a trivial architecturalartifact,butanabilityacquiredthroughspecificpretrainingobjectives. WefurtherdiscoverthatIsSameObjectisencodedinalow-dimensionalsubspace ontopofobjectfeatures,andthatthissignalactivelyguidesattention. Ablating IsSameObjectfrommodelactivationsdegradesdownstreamperformanceandworks against the learning objective, implying that emergent object binding naturally servesthepretrainingobjective. OurfindingschallengetheviewthatViTslack object binding and highlight how symbolic knowledge of “which parts belong together”emergesnaturallyinaconnectionistsystem. 1 1 Introduction Humansnaturallyparsescenesintocoherentobjects[1](e.g.,groupingfeaturessuchasrounded shape,smoothsurface,andmutedcolorintothemug)andfurthergroundtheiridentitiesincontext (e.g., recognizing my coffee mug on the desk rather than just a mug). This is assumed to be made possible by what cognitive scientists call object binding [2], the brain’s ability to group an object’s low-level features (color, shape, motion, etc.) into a unified representation. This in turnenablesobjectstobestoredefficientlyandcompositionallyinmemoryandusedashigh-level symbolsforreasoning. Thebindingproblemisagenuinecomputationalchallenge,asevidenced byhumans’limitedcompetenceinconjunction-searchtasks[3]andclinicaldissociationssuchas Balint’s syndrome, where feature perception remains intact but binding breaks down [4]. If AI systemscouldreplicatethehumanabilityforobjectbinding,thatmayhelpthemgroundsymbolsfor 1Codeavailableat: 39thConferenceonNeuralInformationProcessingSystems(NeurIPS2025). 5202 tcO 82 ]VC.sc[ 1v90742.0152:viXra --- Page 2 --- perceptionandexploitingcompositionality[5]. Thekeyquestionis: docurrentAIsystemssolvethe bindingproblem? Figure 1: Assessing object binding in ViTs with IsSameObject. (a) We use a probe to decode IsSameObject, with scores near 1 for same-object pairs and near 0 for different-object pairs. (b) Downstreamtasksthatbenefitfromstrongobjectbindingincludeinstancesegmentationandvisual reasoning(e.g., locatingandcountingobjectswithspecificfeatures), wherepatchestriggeredby certainfeaturesareboundtotherestoftheirobjecttoallowextractionoftheentireobject. ObjectbindinghasreceivedlittleattentioninmainstreamAIresearch. Cognition-inspiredmodels[6, 7]buildinhuman-likeobject-basedattention. Bycontrast,mainstreamvisionmodelsareassumedto implicitlylearntohandlemultipleobjectsfromthetrainingset,yetempiricalstudiesshowtheyoften "attend"onlytothemostsalientregionsandoverlooktherest[8]. ViTscomputeattentionscores fromfeature-levelsimilarities,whichinprincipleshouldhelpwithobjectbinding. Butexperiments showthatself-attentionbehavesmorelikeasimilarity-basedgroupingmechanismthanselective attention,whichgroupsimagepatchesbylow-levelfeaturesimilarity(e.g. colorortexture),rather thanisolatingdistinctobjects[9].Object-centricmethodslikeSlotAttention[10]fixthisbyallocating asmallsetoflearnableslotsthatcompetefortokenfeatures,enforcingbindingbydesign. However, whetherAIvisionmodels,especiallyleadingViTs,canachieverobustobjectbindingwithoutexplicit mechanismsremainsanopenquestion. CognitivescientistshavequestionedwhetherViTscanbindobjectsatall: arguingthattheylack mechanismsfordynamicallyandflexiblygroupingfeatures[5];theylackrecurrencenecessaryfor iterative refinement of object representations in the cognitive science field [11, 7]; and as purely connectionist models, they appear incapable of true symbolic processing [8]. However, these architecturallimitationsdonotprecludebindingfromemergingthroughlearning. Ifamodelencodes whether two patches belong to the same object (IsSameObject), this signal can guide attention andimproveprediction[12,13]. Human-labeleddataalsoreflectsobject-levelstructure,soViTs canacquirebindingbyimitation. ThissuggeststhatViTsmaylearntobindobjectsdirectlyfrom large-scaletrainingdata,withoutrequiringexplicitarchitecturalinductivebiases. Here, weaskwhetherobjectbindingnaturallyemergesinlarge, pretrainedVisionTransformers, which is a question that matters for both cognitive science and AI. We propose IsSameObject (whethertwopatchesbelongtothesameobject)andshowthatitisreliablydecodable(with90.20% accuracy)usingaquadraticsimilarityprobestartingfrommid-layersofthetransformerlayers. This effectisrobustacrossself-supervisedmodelssuchasDINO,MAE,andCLIP,butlargelyabsent in ImageNet-supervised ViTs, suggesting that binding is an acquired ability rather than a trivial architecturalartifact. AcrosstheViT’slayerhierarchy,itprogressivelyencodesIsSameObjectina low-dimensionalprojection-spaceontopofthefeaturesoftheobject,anditguidesself-attention. AblatingIsSameObjectfrommodelactivationshurtsdownstreamperformanceandworksagainstthe pretrainingobjective. 2 --- Page 3 --- Ourmaincontributionsareasfollows: (i)Wedemonstratethatobjectbindingnaturallyemerges inlarge, pretrainedVisionTransformers, challengingthecognitive-scienceassumptionthatsuch bindingisn’tpossiblegiventheirarchitecture. (ii)WeshowthatViTsencodealow-dimensional signatureofIsSameObject(whethertwopatchesbelongtothesameobject)ontopoftheirfeature representations. (iii)Wesuggestthatlearning-objective–basedinductivebiasescanenableobject binding,pointingfutureworktowardimplicitlylearnedobject-basedrepresentations. 2 RelatedWork ObjectBindinginCognitiveScienceandNeuroscience. Theobjectbindingproblemaskshow the brain integrates features that are processed across many distinct cortical areas into coherent object representations [14]. The concept of binding2 rests on three key hypotheses: First, visual processingiswidelyunderstoodtobehierarchical,parallel,anddistributedacrossthecortex[16–20]. Second,weperceivetheworldprimarilyintermsofobjects,ratherthanasacollectionofscattered features[11,1]. Thisabstractionisfundamentaltobothperceptionandinteractionwiththeworld, allowingustorecognize,reasonabout,andmanipulateourenvironmenteffectively[21–23]. Third, featurebindingrequiresamechanismthatcorrectlyassignsfeatures,representedinspatiallydistinct corticalareas,totheircorrespondingobject[15,24,2]. Thisthirdhypothesisiswherethecoreof thebindingproblemlies,andithasbeenalongstandingpointofdebateamongneuroscientistsand cognitivescientists[25–27]. Despite their substantial difference, vision transformers (ViTs) share several key computational parallelswiththemammalianvisualsystem: theybothrelyonparallel,distributedandhierarchical processes. Moreimportantly,ViTsdohavetwoofthethreearchitecturalandcomputationalelements hypothesized to enable binding in the brain. The explicit position embeddings in ViTs resemble spatial tagging and the spatiotopic organization observed in the ventral stream [28, 26]; and the self-attentionmechanismisakintodynamictuningandattentionalmodulation,whicharethought tobeprimarymechanismsforobjectbinding[26,29,30](althoughattentionisbelievedtobeof recurrent nature in the brain [31, 32]). These parallels position ViTs as potential computational modelsforexploringobjectbindinginbothartificialandbiologicalsystems. Object-CentricLearning. Motivatedbyhowhumansnaturallyreasonaboutindividualobjects, Object-CentricLearning(OCL[10])aimstorepresentasceneasacompositionofdisentangledobject representations. Whilesegmentationonlypartitionsanimageintoobjectmasks,OCLgoesfurtherby encodingeachobjectintoitsownrepresentation[33]. UnsupervisedapproachessuchasMONet[34], IODINE[33],andespeciallySlotAttention[10]encodescenesintoasmall,permutation-invariantset of“slots”thatareiterativelyrefined,producingrobustobjectrepresentationsonbothsynthetic[10,35] and real-world data [36, 37] and enabling compositional generation and manipulation [38–40]. However,slot-basedmodelsimposeafixedslotbudgetandrequiremultiplerefinementiterations thatslowsinference,andbecauseSlotAttentionisboltedontoratherthanbuiltintothetransformer, scalingandtrainingbecomeharder[41]. Otherexplicitobject-centricapproachesincludeTensor ProductRepresentations[42]andCapsuleNetworks[43]. Instead of object-centric approaches that explicitly enforce object-level attention, we propose an alternativeviewthatViTsmayalreadyencodeimplicitobject-levelstructure. Priorworkhasassumed thisandattemptedtogrouppatchesintoobjectsdirectlyfromactivationsorattentionmapsViTs, usingmethodslikeclustering[44]orGraphCut [45]. Otherstudiesdesignself-supervisedobjectives (e.g.,object“movability”[46]orpatch-levelcontrastivelearning[47])andtrainmodelstostrengthen object-levelgrouping. Incontrast,ourstudydirectlyvalidatesthisassumptionbyshowingthatViT patchembeddingsintrinsicallyencodewhetheranytwopatchesbelongtothesameobject. BindinginTransformers. Bindinghasreceivedgrowingrecognitionintransformer-basedmachine learningresearchandbindingfailuresareseenasexamplesofperformancebreakdownsinmodern applications [48–51]. Diffusion models rely on binding attributes to entities, and failures cause attributeleakage(e.g.,bothadogandacatendupwearingsunglassesandasun-hat)[49,48]. Vision- languagemodelsfacesimilarbindingchallenges,strugglingwithdifferentiatingmultipleobjectswith featureconjunctions[51]. Despitethesebindingfailures,transformersstilldemonstratesomebinding 2ThetermbindingwasintroducedtoneurosciencebyChristophvonderMalsburgin1981,inspiredbythe notionofvariablebindingincomputerscience[15]. 3 --- Page 4 --- capability,yettheunderlyingmechanismisnotwellunderstood. FengandSteinhardt[52],Daietal. [53]studybindinginlanguagemodels,showingthatattributes(e.g.,“livesinShanghai”)arelinked totheirsubjects(e.g.,"Alice")viaalow-dimensionalbinding-IDcodethatisaddedtotheactivation andcanbeeditedtoswaporredirectrelations. Bindingmechanismsinvisiontransformersremain unexplored,andourstudyaimstofillthisgap. 3 AssessingObjectBindinginViTsthroughIsSameObject 3.1 ProbingIsSameObjectrepresentations VisionTransformers(ViTs)tokenizeimagesbydividingthemintoagridoffixed-sizepatches[54]. Becausethetokenistheminimalrepresentationalunit,anygroupingoffeaturesintoobjectsmust arisethroughrelationsbetweentokens,notwithinthem. TheonlymechanismViTshaveforsuch cross-token interaction is scaled dot-product attention, where attention scores can be viewed as dynamic edge weights in a graph that route information between tokens [5]. Therefore, if ViTs performanyformofobjectbinding,weexpecttoobserveapairwisetoken-levelrepresentationthat indicateswhethertwopatchesbelongtothesameobject,whichwetermIsSameObject. Sinceobjectbindingistheabilitytogroupanobject’sfeaturestogether, decodingIsSameObject reliably from ViT patch embeddings would provide direct evidence of object binding (and its representation)inthemodel. Weadoptprobing,whichtakesmeasurementsofViTactivationswith lightweight classifiers [55], to determine whether IsSameObject is encoded or unrecoverable by simpleoperations. s(ℓ) =LayerNorm(cid:0) h(ℓ)+MultiHeadAttention(h(ℓ))(cid:1) (1) h(ℓ+1) =LayerNorm(cid:0) s(ℓ)+FFN(s(ℓ))(cid:1) (2) Transformerspropagateinformationacrosslayersaccordingtotheaboveequations12,whereh(ℓ)is theresidual-streamoutput(whichwe’llcallthepatchembedding)atlayerℓ. Formally,wedefinetheIsSameObjectpredicateonapairoftokenembeddings(x(ℓ),x(ℓ))atlayerℓ i j by IsSameObject(cid:0) x(ℓ),x(ℓ)(cid:1) =ϕ(cid:0) x(ℓ),x(ℓ)(cid:1) , ϕ:Rd×Rd →[0,1], i j i j whereϕscorestheprobabilitythattokensiandj belongtothesameobject. We consider the following hypotheses about how IsSameObject may be encoded in the model’s activations: • Itmaybelinear(recoverablebyaweightedsumoffeatures)orfundamentallyquadratic (recoverableonlythroughpairwisefeatureinteractions). • Itisapairwiserelationshipversusapointwisemapping(i.e.themodelfirstmapseach patchtoanobjectidentityorclass,thencompares). • The model tells objects apart using only broad class labels or object identities–i.e., it may solve binding by recognizing “this is a dog vs. this is a chair” without actually representingwhichspecificpixelsbelongtoeachobject. Suchasolutionispossiblebecause classlabelsalreadyprovideacoarsenotionofobjectidentitywithoutrequiringpixel-level grounding. • The signal is stored in a few specialized dimensions versus distributed across many dimensions. Intheformercase,bindinginformationwouldbeisolatedtoasmallsubsetof channels,whileinthelatteritwouldbeencodeddiffusely(e.g.,asrotatedcombinationsof features)suchthatnosingledimensioncarriesthesignalonitsown. Totestthesehypotheses,wedecodeIsSameObjectusingseveralprobearchitectures,eachdefined byalearnableparametermatrixW: 1. Linearprobe: ϕ (x,y) = Wx + Wy,whereW ∈R1×d. lin 2. Diagonalquadraticprobe(specializeddims)ϕ (x,y) = x⊤diag(w)y,withw ∈Rd. diag 3. Quadraticprobe(distributed)ϕ (x,y)= x⊤W⊤W y, W ∈Rk×d, k ≪d. quad 4 --- Page 5 --- 4. Object-class/identityprobes(pointwise)Wefirstmapeachembeddingx,ytoclassdistributions p=softmax(W x),q =softmax(W y)andtrainW withmulticlasscross-entropyonobject-class c c c labels(andsimilarlyW onobject-identitylabels). ThepointwiseIsSameObjectscoreisthen N (cid:88)Nc ϕ (x,y)=p⊤q = p(c)q(c)=softmax(W x)⊤softmax(W y). class/identity N N c=1 5. Object-classprobes(pairwise)Alternatively,wecantreatIsSameObjectasabinarylabeland optimizep⊤qdirectlywithbinarycross-entropyloss.Thisversionispairwisebecausethesupervision isapplieddirectlyto(x,y)asapair. Themodelisthusoptimizedtoencodetherelationshipbetween twopatches,nottheirindividualclassmemberships. 3.2 IsSameObjectisbestdecodableinquadraticform Figure2: Quadraticprobesexcelatdecodingthebindingsignal. Layer-wiseaccuracyofthe IsSameObject probe on DINOv2-Large. The quadratic probe consistently outperforms all other probesfrommiddlelayersonward. ResultsforadditionalmodelsareshowninAppendixA.2. We extract DINOv2-Large [13] activations at each layer and train the probes on the ADE20K dataset[56]usingcross-entropylossforallpairwiseprobestoclassifysame-objectvs.different-object patch pairs. Figure 2 shows probe accuracy across layers. To test our hypotheses about how IsSameObjectisrepresented,wecompare: • Linearvs. quadraticprobes: Quadraticprobes(evenwiththesameparametercountas indiagonalquadratic)significantlyoutperformlinearones,suggestingthatIsSameObject isaquadraticrepresentation,consistentwiththequadraticformusedbytheself-attention mechanism. • Quadraticvs.object-classprobes: Trainedobjectclassprobesfallshortofthequadratic probe directly optimized for distinguishing same and different object instances. This indicatesthatthemodelencodesnotjustwhethertwopatchesshareanobjectclassbutalso subtleridentitycues—e.g.,distinguishingtwoidenticalcarsofthesamemakeandmodel. • Fullvs. diagonalquadraticprobes: Thefullquadraticprobeoutperformsitsdiagonal variant, implying that IsSameObject information is more distributed across dimensions ratherthanrestrictedtospecificchannels. • Pointwiseobjectclassprobevs. pairwiseobjectclassprobe: Mappingeachpatchtoan objectclassviasoftmaxandthencomparingclasses(pointwise)underperformsdirectly comparingtheirembeddings(pairwise),asthepointwiseapproachlosesinformationwhen itcollapsesembeddingsintodiscreteclasses. 3.3 Objectbindingemergesbroadlyacrossself-supervisedViTs WeextendouranalysisbeyondDINOv2toabroadersetofpretrainedVisionTransformers,including CLIP,MAE,andfullysupervisedViTs. Toenabledirectcomparison,westandardizeinputpatch coveragebyresizingallinputssothateachmodelprocessesthesamespatialpatchdivisions. Under 5 --- Page 6 --- thissetup,everyprobestartsfromthesametrivialbaselineof72.6%accuracy,whichcorrespondsto alwayspredicting“different”,reflectingtheclassimbalancethatmostpatchpairsdonotbelongto thesameobjectinthedataset. Table1reportsIsSameObjectdecodingaccuracyacrossmodels. DINOmodelsshowthestrongest bindingsignal,withlargeandgiantvariantsexceeding+16percentagepointsoverbaseline. CLIP andMAEalsoexhibitclearobject-bindingability,thoughtoalesserdegree. Incontrast,ImageNet- supervisedViTyieldspoorobject-bindingperformance,suggestingthatbindingisanacquiredability underspecificpretrainingobjectivesratherthanbeingauniversalpropertyofallvisionmodels. Table1: Bindingisconsistentlyrepresentedinself-supervisedViTs,butlesssoinsupervised ViTs. ProbeaccuracyonIsSameObjectacrosspretrainedViTs. ∆isreportedinpercentagepoints (pp),andthepeaklayerindexisnormalizedto[0,1]withineachmodel. Model HighestAccuracy(%) ∆overBaseline(pp) PeakLayer(0–1) DINOv2-Small 86.7 +14.1 1.00 DINOv2-Base 87.5 +14.9 0.82 DINOv2-Large 90.2 +17.6 0.78 DINOv2-Giant 88.8 +16.2 0.77 CLIP(ViT-L) 84.2 +11.6 0.39 MAE(ViT-L) 82.9 +10.3 0.65 Supervised(ViT-L) 76.3 +3.7 0.13 Our findings thus produce a much wider coverage of ViTs and we provide an understanding of potentialreasonswhybindingemerges: • DINO.Thecontrastiveteacher–studentlossenforcesconsistencyacrossaugmentedviews containing the same objects. This objective encourages the model to learn object-level featuresthatpersistunderaugmentedviews[12]. • CLIP. By aligning images with text captions, CLIP effectively assigns each object a symbolic label (e.g., “the red car”), which can act like a pointer that pulls together all patchesofthatobject. Thissupervisionlikelyencouragespatchesfromthesameobjectto clusterinfeaturespace. • MAE.Themaskedautoencoderobjectiverequiresthemodeltoreconstructamissingpatch from its surroundings. When the masked patch sits between multiple objects, correctly predictingitscontentforcesthemodeltoinferwhichobjectitbelongsto,thuspromoting thegroupingofpatchesfromthesameobject. • SupervisedImageNettraining. BecauseImageNetsupervisionlabelsonlythedominant objectineachimage[57],itrewardsclass-levelcategorizationbutnotgroupingofpatches intoobjectinstances,leadingtoaweakerbindingsignal. 4 ExtractingtheBindingSubspaceofViTRepresentations 4.1 DecomposingIsSameObjectfromfeatures Followingthelinearfeaturehypothesis[58],andsimilarto[52],weassumethatatlayerℓeachtoken embeddingdecomposesintoa“feature”partanda“binding”part: h(ℓ)(x )=f(ℓ)(x ,c) + b(ℓ)(x ), t t t wheref(ℓ)(x ,c) ∈ Rd encodesallattributesoftokenx (texture,shape,etc.) givencontextc = t t {x ,...,x },excludinganyinformationaboutwhichothertokensitbindswith,andb(ℓ)(x )∈Rd 1 T t encodesthebindinginformationthatdetermineswhichothertokensbelongtothesameobject(i.e., theIsSameObjectrelation). Considertwoidenticalpatchesx andx atcorrespondingpositionsofidenticalobjectsAandBin Ai Bi thesameimage,andlettheirresidualbe∆ . Itmaybetemptingtocancelthefeaturetermdirectly. ABi Indeed,withoutpositionalencoding(seeproofinAppendixA.4.1),wehavef(ℓ)(x )=f(ℓ)(x ), Ai Bi 6 --- Page 7 --- Figure3:ThegeometryofIsSameObjectrepresentation. Patchembeddingsh andh areprojected i j ontotheIsSameObjectsubspacebyW ,producingbindingvectorsb andb ,whosesimilarityis probe i j computedbyadotproduct. because foridentical tokensthe positionalencoding is theonly signalthat candifferentiate their cross-tokeninteractions. Wecanapproximatef(ℓ)(x )≈f(ℓ)(x ),sincethetwopatchesarevisuallyidentical,appearin Ai Bi nearlythesamecontext,andanypositionaldifferencecanbeoffloadedintothebindingcomponent. Thisyields: (cid:2) (cid:3) (cid:2) (cid:3) ∆ =h(x )−h(x )= f(x )−f(x ) + b(x )−b(x ) ≈b(x )−b(x ). ABi Ai Bi Ai Bi Ai Bi Ai Bi If∆ remainsroughlyconsistentacrosspatchpairswiththesameindexi,thenb(x )andb(x ) ABi Ai Bi canformlinearlyseparableclusters,whichcanthusserveasobjectidentityrepresentations. However, thisbecomesproblematicinnaturalimages,whereidenticalpatchesarerare. Instead,wetakeasupervised approachtodecodingthebindingcomponent. Ourquadraticprobe servesasatoolforseparatingbindingfromfeatureinformationwithineachtoken(Fig.3). Con- ceptually, thequadraticprobecanbeviewedasprojectinganactivationhintotheIsSameObject subspace,yieldingb(ℓ)(x)=h(ℓ)(x)⊤W,andthenmeasuresthedot-productsimilaritybetweentwo projectedvectors. Giventhatnaturalimagedatasetscontainnumerousobjectswherebistheprimary distinguishingfactor,theprobeshouldbeoptimizedtodiscoveradirectionthatisolatesb. Withthis strategywecanseparatethebindingsignalfromtherestoftherepresentation. Theobservationthatbindingvectorsremainmeaningfulunderlinearcombination,andbecomehard to discriminate when they are close together, is consistent with this interpretation [52]. In later ablationstudies,weuseourtrainedquadraticprobeviab(ℓ)(x)=h(ℓ)(x)⊤W. 4.2 AToyExperiment: distinguishingidenticalobjectsandsimilarlookingobjects ToprobethelimitsofobjectbindinginViTs,weconstructatestimagewithtwoidenticalredcars,a thirdredcarofadifferentbrand,andaredboat. ThissetupletsustrackIsSameObjectrepresentations acrosslayersbyevaluatingthreedistinctions:differentobject-classbutsimilarappearance,sameclass withsubtledifferences,andexactduplicates. Asexpected,thesedistinctionsbecomeprogressively harder. WechosenaturalobjectsratherthanabstractshapesbecauseboththeViTandourprobeare trainedonreal-worldimages,whichallowsustoanalyzebindinginanontrivialsetting. Toanalyzewherebindingemerges,weplottheIsSameObjectscorespredictedbyourtrainedquadratic probe(Figure4). Sincetheprobeperformsreliably(seeSection3.2),itsoutputsapproximatethe internal IsSameObject the model encodes. Weobservethat, from early tomid-layers, the model increasingly discerns the local object (the one to which each patch belongs). Surprisingly, from mid-layerstolaterlayers,themodelshiftstowardclass-basedgrouping,increasinglytreatingallred carsasthesame. Bindingemergesinthemiddleofthenetworkandisthenprogressivelylosttowards thetop. 7 --- Page 8 --- Figure4: Layer-wisevisualizationofIsSameObjectpredictionsonthetestimage. Weusedthree redcarsandoneredboattomakebindingdeliberatelydifficult. Earlylayersattendtosimilarsurface features(e.g.,theredcarorboathull),mid-layersfocusonlocalobjects,andhigherlayersshiftto groupingpatchesbyobjectclass. TheIsSameObjectrepresentationislow-dimensional. Weusefouridenticalred-carimagesand spliteachoneintopatchesusingexactlythesamegridalignment. Weperformprincipalcomponent analysis(PCA)ontheresidualssets{∆ ,∆ ,∆ },where∆ =h −h ≈b −b BA CA DA BA Bi Ai Bi Ai and visualize the first three components (see Figure 5). ∆ ,∆ ,∆ fall into three linearly BA CA DA separableclustersinthefirstthreeprincipalcomponentspace,showingthatb ,b ,b formobject- B C D levelrepresentations. Theseparationoftheseclustersinaverysmallnumberofprincipaldirections demonstratesthatIsSameObjectliesinalow-dimensionalsubspace: patchesfromthesameobject instancemaptocloselyalignedbindingvectors,anddifferentinstancesarelinearlyseparablewith largemargins. Mid-layerscapturelocalobjects,andhigherlayersshifttowardsgroupingpatchesbyobject class. Asurprisingobservationisthesuddenincreaseinthecross-objectIsSameObjectscore(fig.4) inthemid-layersoftheDINOV2modelforinstancesofthesameclass(Fig.4). Thisisconsistent withpriorworkshowingthatViTsrepresentdifferenttypesofinformationatdifferentlayers[59]. At thesametime,token-positiondecodabilitydropsindeeperlayers(seeAppendixA.4.3),suggesting thatthemodelisdeliberatelydiscardingpositionalinformation. Ourinterpretationisthatthenetwork initially relies on positional cues to support binding, since location is necessary to disambiguate tokensthatsharesimilarfeaturecontent. However,theDINOtrainingobjectiveenforcesposition invarianceattheoutputlayer[12],whichimplicitlyencouragesthenetworktoremovepositional signalonceitisnolongerusefulandtorepurposecapacityforsemanticallyrelevantobjectstructure. Ourfindingsareconsistentwithexperimentalevidencefromtheventralstreaminthebrain,showing thatwhiletheretinotopicorganizationofearlyventralareasisnecessaryforperceptionandbinding, globalspatialinformationisinsteadprocessedandmaintainedbythedorsalstream[60–62]. 4.3 Attentionweights(query-keysimilarity)correlatewithIsSameObject InSection3.2weshowedthatIsSameObjectisbestdecodedquadratically. Sinceself-attentionis alsoaquadraticinteraction,bindinginformationintheresidualstreamatlayerℓcaninprinciple guidewhereattentionisallocatedatlayerℓ+1,allowingthemodeltoselectivelyrouteattention withinthesameobjecttobuildacoherentobject-levelrepresentation. Totestthis,wecomputethePearsoncorrelationbetweenattentionweightsandtheIsSameObject scores(seeFig.6andAppendixA.5).Inmid-levellayers,weobserveapositivebutmodestcorrelation, indicatingthatthemodeldoesmakeuseoftheIsSameObjectsignalwhenallocatingattention. The modeststrengthoftheeffectisexpected,becauseattentionservesmanyrolesbeyondbinding. 8 --- Page 9 --- Figure 5: Identical objects form distinct object-level representations. The first 3 principal componentsofthefouridenticalcars,withthethreeclustersdenoting∆ ,∆ ,∆DA. Thethree BA CA linearlyseparableclusterssuggestthatidenticalobjects’bindingvectorsformdistinctobject-level representations. Thepercentageinparenthesesindicatesthevarianceexplainedbythatprincipal component. Figure6: AttentionweightsarecorrelatedwithIsSameObject. Dotsizeisproportionaltothe Euclideandistancebetweenpatches. AttentionweightscorrelatewithIsSameObjectinmiddlelayers: (a)Pearsonr =0.163,(b)Pearsonr =0.201. 4.4 AblationofIsSameObjecthurtsdownstreamperformanceandworksagainstthe minimizationofthepretrainingloss Weconductablationstudiesandevaluatetheimpactondownstreamsegmentationperformanceand pretrainingloss. InsteadofdirectlysubtractingtheIsSameObjectrepresentationb(x )fromh(x ), i i weuselessaggressiveapproaches: • UninformedAblation: Randomlyshuffleb(x )acrosspatchesintheimageataspecified i ratio 9 --- Page 10 --- Table 2: Ablations demonstrate the functional role of IsSameObject. Segmentation accuracy, instance accuracy, and DINO loss on layer 18 under uninformed (random shuffle) and informed (ground-truthinjection)ablations. Higheruninformedratiosmeanmoreaggressiveshuffling;lower informedαvaluesmeanstrongerinjectionoftrueobjectlabels. Uninformed/ratio Informed/α 0 0.5 1 1 0.5 0 SegmentationAcc.(%) 44.14 41.03 39.20 44.14 44.91 43.59 InstanceAcc.(%) 35.14 31.39 28.19 35.14 36.37 37.02 DINOLoss 0.6182 0.6591 0.6749 0.6182 — — • InformedAblation/Injection: Usingground-truthinstancemasks,weablateorinjectthe trueIsSameObjectsignalbylinearlycombiningthemeanobjectdirectionwitheachpatch’s bindingvectorb :˜b =(1−α) 1 (cid:80) b +αb . i i |I| j∈I object,j object,i Weevaluatethesemanticandinstancesegmentationperformancewithretrainedsegmentationheads onasubsetofADE20Kunderthesevariations. Wealsoevaluatetheteacher–studentself-distillation lossasemployedinDINO(seeAppendixA.6fordetails). Resultsshowthatuninformedablation,whichrandomlyshufflesthebindingvector,reducesseg- mentationperformance,whereasinjectingthemeanobjectdirectionimprovesaccuracy. Ablating IsSameObjectwithrandomshufflingleadstoanoticeablegradualincreaseintheDINOloss,suggest- ingthatablationofIsSameObjectworksagainstthispretrainingloss. 5 Limitations We assume the trained probe cleanly splits each patch embedding into “feature” and “binding” components,asimplificationthatwouldbenefitfromfurtherempiricalexploration. Additionally, whilerandomizingtheIsSameObjectsubspaceclearlyincreasesDINO’spretrainingloss,thisindirect evidencefallsshortofdemonstratingacausallinktodownstreamtaskperformanceorisolatingits precisecontribution. Finally,ourdownstreamevaluationsfocusonlyonsegmentation,leavingopen whethertheseemergentbindingsignalsalsobenefitothervisiontaskssuchasvisualreasoning. More broadly,thispaperstudiesobject-identitybinding,i.e.,whethertwotokensbelongtothesameobject; compositionalbinding,suchasreusingobjectstructureacrosscontexts,isnotexploredhereandis leftforfuturework. 6 Conclusion Inthispaper,weshowthatobjectbindingnaturallyemergesinlarge,pretrainedvisiontransformers, especiallyinDINOv2,andthiseffectisconsistentacrossmultiplemodels. Wealsoshowthatitis anacquiredratherthaninnateabilitythroughcomparisonswithsupervisedmodels. IsSameObject, whethertwopatchesbelongtothesameobject,isreliablydecodableandliesinalow-dimensional latentspace. Ablatingthissignalbothdegradesdownstreamsegmentationandopposesthemodel’s pretrainingloss,suggestingthatemergentobjectbindingisanaturalsolutiontotheself-supervised objective. Ourstudybridgeswhatpsychologistsidentifyasobjectbindingwithemergentbehaviorin ViTs,challengesthebeliefthatViTslackthisabilityanddemonstrateshowsymbolicprocessingcan ariseinconnectionistmodels. Moving forward, we propose that AI researchers seeking to address binding failures adopt an alternativetoexplicitbindingmodules(suchasSlotAttention[10])byenhancingViTs’inherent object-bindingthroughtailoredtrainingobjectivesorminimalarchitecturaltweaks.Futureworkcould extendourapproachtovideomodels,inwhichobjectpersistenceacrosstimeisessential[11,63];such modelsshouldexhibitsimilarspatialbindingseeninViTsandadditionallydeveloptemporalbinding tomaintainobjectidentityacrossframes. Alsoofinterestwillbehowtheseobjectrepresentations formed by binding interact with one another, with one possibility being that they interact with low-dimensional“objectfiles”[64]. Together,theseeffortswilldeepenourunderstandingofhow symbolicprocessingofobjectscanemergeinconnectionistmodels. 10 --- Page 11 --- References [1] BrianJScholl. Objectsandattention: Thestateoftheart. Cognition,80(1-2):1–46,2001. [2] AnneTreisman. Thebindingproblem. Currentopinioninneurobiology,6(2):171–178,1996. [3] AnneTreismanandHilarySchmidt.Illusoryconjunctionsintheperceptionofobjects.Cognitive psychology,14(1):107–141,1982. [4] LynnRobertson,AnneTreisman,StaciaFriedman-Hill,andMarciaGrabowecky. Theinterac- tionofspatialandobjectpathways: Evidencefrombalint’ssyndrome. JournalofCognitive Neuroscience,9(3):295–317,1997. [5] Klaus Greff, Sjoerd Van Steenkiste, and Jürgen Schmidhuber. On the binding problem in artificialneuralnetworks. arXivpreprintarXiv:2012.05208,2020. [6] DrewLinsley,DanShiebler,SvenEberhardt,andThomasSerre. Learningwhatandwhereto attend. arXivpreprintarXiv:1805.08819,2018. [7] Saeed Salehi, Jordan Lei, Ari S Benjamin, Klaus-Robert Müller, and Konrad P Kording. Modelingattentionandbindinginthebrainthroughbidirectionalrecurrentgating. bioRxiv, pages2024–09,2024. [8] TarunKhajuria, BraianOlmiroDias, andJaanAru. Howstructuredaretherepresentations intransformer-basedvisionencoders? ananalysisofmulti-objectrepresentationsinvision- languagemodels. arXivpreprintarXiv:2406.09067,2024. [9] PariaMehraniandJohnKTsotsos. Self-attentioninvisiontransformersperformsperceptual grouping,notattention. FrontiersinComputerScience,5:1178450,2023. [10] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,JakobUszkoreit,AlexeyDosovitskiy,andThomasKipf. Object-centriclearningwith slotattention. Advancesinneuralinformationprocessingsystems,33:11525–11538,2020. [11] Benjamin Peters and Nikolaus Kriegeskorte. Capturing the objects of vision with neural networks. Naturehumanbehaviour,5(9):1127–1144,2021. [12] MathildeCaron,HugoTouvron,IshanMisra,HervéJégou,JulienMairal,PiotrBojanowski, andArmandJoulin. Emergingpropertiesinself-supervisedvisiontransformers. InProceedings oftheIEEE/CVFinternationalconferenceoncomputervision,pages9650–9660,2021. [13] MaximeOquab,TimothéeDarcet,ThéoMoutakanni,HuyVo,MarcSzafraniec,VasilKhalidov, PierreFernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal.Dinov2:Learning robustvisualfeatureswithoutsupervision. arXivpreprintarXiv:2304.07193,2023. [14] ChristophVonderMalsburg. Thewhatandwhyofbinding: themodeler’sperspective. Neuron, 24(1):95–104,1999. [15] JeromeFeldman. Theneuralbindingproblem(s). Cognitiveneurodynamics,7:1–11,2013. [16] SemirMZeki. Functionalspecialisationinthevisualcortexoftherhesusmonkey. Nature,274 (5670):423–428,1978. [17] Margaret Livingstone and David Hubel. Segregation of form, color, movement, and depth: anatomy,physiology,andperception. Science,240(4853):740–749,1988. [18] MortimerMishkin,LeslieGUngerleider,andKathleenAMacko. Objectvisionandspatial vision: twocorticalpathways. Trendsinneurosciences,6:414–417,1983. [19] DanielJFellemanandDavidCVanEssen. Distributedhierarchicalprocessingintheprimate cerebralcortex. Cerebralcortex(NewYork,NY:1991),1(1):1–47,1991. [20] KalanitGrill-SpectorandKevinSWeiner. Thefunctionalarchitectureoftheventraltemporal cortexanditsroleincategorization. NatureReviewsNeuroscience,15(8):536–548,2014. 11 --- Page 12 --- [21] GaetanoKanizsa,PaoloLegrenzi,andPaoloBozzi. Organizationinvision: Essaysongestalt perception. (NoTitle),1979. [22] StephenEPalmer. Hierarchicalstructureinperceptualrepresentation. Cognitivepsychology,9 (4):441–474,1977. [23] Irving Biederman. Recognition-by-components: a theory of human image understanding. Psychologicalreview,94(2):115,1987. [24] ChristophVonDerMalsburg. Thecorrelationtheoryofbrainfunction. InModelsofneural networks: Temporalaspectsofcodingandinformationprocessinginbiologicalsystems,pages 95–119.Springer,1994. [25] HStevenScholteandEdwardHFdeHaan. Beyondbinding: frommodulartonaturalvision. TrendsinCognitiveSciences,2025. [26] Lynn C Robertson. Binding, spatial attention and perceptual awareness. Nature Reviews Neuroscience,4(2):93–102,2003. [27] AdinaLRoskies. Thebindingproblem. Neuron,24(1):7–9,1999. [28] AnneTreisman. Focusedattentionintheperceptionandretrievalofmultidimensionalstimuli. Perception&Psychophysics,22:1–11,1977. [29] JohnHReynoldsandRobertDesimone. Theroleofneuralmechanismsofattentioninsolving thebindingproblem. Neuron,24(1):19–29,1999. [30] PieterRRoelfsema. Solvingthebindingproblem: Assembliesformwhenneuronsenhance theirfiringrate—theydon’tneedtooscillateorsynchronize. Neuron,111(7):1003–1019,2023. [31] RubenSvanBergenandNikolausKriegeskorte. Goingincirclesisthewayforward: therole ofrecurrenceinvisualinference. CurrentOpinioninNeurobiology,65:176–193,2020. [32] Kohitij Kar, Jonas Kubilius, Kailyn Schmidt, Elias B Issa, and James J DiCarlo. Evidence thatrecurrentcircuitsarecriticaltotheventralstream’sexecutionofcoreobjectrecognition behavior. Natureneuroscience,22(6):974–983,2019. [33] Klaus Greff, Raphaël Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, DanielZoran,LoicMatthey,MatthewBotvinick,andAlexanderLerchner. Multi-objectrepre- sentationlearningwithiterativevariationalinference. InInternationalconferenceonmachine learning,pages2424–2433.PMLR,2019. [34] ChristopherPBurgess,LoicMatthey,NicholasWatters,RishabhKabra,IrinaHiggins,Matt Botvinick,andAlexanderLerchner. Monet: Unsupervisedscenedecompositionandrepresenta- tion. arXivpreprintarXiv:1901.11390,2019. URL [35] ThomasKipf,GamaleldinFElsayed,AravindhMahendran,AustinStone,SaraSabour,Georg Heigold,RicoJonschkowski,AlexeyDosovitskiy,andKlausGreff. Conditionalobject-centric learningfromvideo. arXivpreprintarXiv:2111.12594,2021. [36] Maximilian Seitzer, Max Horn, Andrii Zadaianchuk, Dominik Zietlow, Tianjun Xiao, Carl- Johann Simon-Gabriel, Tong He, Zheng Zhang, Bernhard Schölkopf, Thomas Brox, et al. Bridgingthegaptoreal-worldobject-centriclearning. arXivpreprintarXiv:2209.14860,2022. [37] GautamSingh,Yi-FuWu,andSungjinAhn. Simpleunsupervisedobject-centriclearningfor complex and naturalistic videos. Advances in Neural Information Processing Systems, 35: 18181–18196,2022. [38] JindongJiang,FeiDeng,GautamSingh,andSungjinAhn. Object-centricslotdiffusion. arXiv preprintarXiv:2303.10834,2023. [39] WhieJung,JaehoonYoo,SungjinAhn,andSeunghoonHong. Learningtocompose:Improving objectcentriclearningbyinjectingcompositionality. arXivpreprintarXiv:2405.00646,2024. 12 --- Page 13 --- [40] JinwooKim,JanghyukChoi,JaehyunKang,ChangyeonLee,Ho-JinChoi,andSeonJooKim. Leveragingimageaugmentationforobjectmanipulation: Towardsinterpretablecontrollability inobject-centriclearning. arXivpreprintarXiv:2310.08929,2023. [41] AlexanderRubinstein,AmeyaPrabhu,MatthiasBethge,andSeongJoonOh. Arewedonewith object-centriclearning? arXivpreprintarXiv:2504.07092,2025. [42] WeiYuenTeh,ChernHongLim,MeiKuanLim,andIanKTTan. Towardsdiscreteobject representationsinvisiontransformerswithtensorproducts. InProceedingsofthe20237th International Conference on Computer Science and Artificial Intelligence, pages 190–194, 2023. [43] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. Advancesinneuralinformationprocessingsystems,30,2017. [44] JianingQian,AnastasiosPanagopoulos,andDineshJayaraman. Recastinggenericpretrained visiontransformersasobject-centricsceneencodersformanipulationpolicies. In2024IEEE International Conference on Robotics and Automation (ICRA), pages 17544–17552. IEEE, 2024. [45] YangtaoWang,XiShen,YuanYuan,YumingDu,MaomaoLi,ShellXuHu,JamesLCrowley, andDominiqueVaufreydaz. Tokencut: Segmentingobjectsinimagesandvideoswithself- supervisedtransformerandnormalizedcut. IEEEtransactionsonpatternanalysisandmachine intelligence,45(12):15790–15801,2023. [46] Adam Bielski and Paolo Favaro. Move: Unsupervised movable object segmentation and detection. AdvancesinNeuralInformationProcessingSystems,35:33371–33386,2022. [47] Jian Ding, Enze Xie, Hang Xu, Chenhan Jiang, Zhenguo Li, Ping Luo, and Gui-Song Xia. Deeplyunsupervisedpatchre-identificationforpre-trainingobjectdetectors.IEEETransactions onPatternAnalysisandMachineIntelligence,46(3):1348–1361,2022. [48] MariaMihaelaTrusca,WolfNuyts,JonathanThomm,RobertHonig,ThomasHofmann,Tinne Tuytelaars,andMarie-FrancineMoens. Object-attributebindingintext-to-imagegeneration: Evaluationandcontrol. arXivpreprintarXiv:2404.13766,2024. [49] TaihangHu,LinxuanLi,JoostvandeWeijer,HongchengGao,FahadShahbazKhan,JianYang, Ming-MingCheng,KaiWang,andYaxingWang. Tokenmergingfortraining-freesemantic bindingintext-to-imagesynthesis. AdvancesinNeuralInformationProcessingSystems,37: 137646–137672,2024. [50] BoshiWangandHuanSun. Isthereversalcurseabindingproblem? uncoveringlimitationsof transformersfromabasicgeneralizationfailure. arXivpreprintarXiv:2504.01928,2025. [51] DeclanCampbell,SunayanaRane,TylerGiallanza,CamilloNicolòDeSabbata,KiaGhods, AmoghJoshi,AlexanderKu,StevenFrankland,TomGriffiths,JonathanDCohen,etal. Under- standingthelimitsofvisionlanguagemodelsthroughthelensofthebindingproblem.Advances inNeuralInformationProcessingSystems,37:113436–113460,2024. [52] JiahaiFengandJacobSteinhardt. Howdolanguagemodelsbindentitiesincontext? arXiv preprintarXiv:2310.17191,2023. [53] QinDai,BenjaminHeinzerling,andKentaroInui. Representationalanalysisofbindinginlarge languagemodels. arXive-prints,pagesarXiv–2409,2024. [54] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai, ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,etal. Animageisworth16x16words: Transformersforimagerecognitionatscale. arXivpreprint arXiv:2010.11929,2020. [55] GuillaumeAlainandYoshuaBengio. Understandingintermediatelayersusinglinearclassifier probes. arXivpreprintarXiv:1610.01644,2016. 13 --- Page 14 --- [56] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Sceneparsingthroughade20kdataset. InProceedingsoftheIEEEconferenceoncomputer visionandpatternrecognition,pages633–641,2017. [57] OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,SanjeevSatheesh,SeanMa,Zhiheng Huang,AndrejKarpathy,AdityaKhosla,MichaelBernstein,etal. Imagenetlargescalevisual recognitionchallenge. Internationaljournalofcomputervision,115(3):211–252,2015. [58] KihoPark,YoJoongChoe,andVictorVeitch. Thelinearrepresentationhypothesisandthe geometryoflargelanguagemodels. arXivpreprintarXiv:2311.03658,2023. [59] ShirAmir,YossiGandelsman,ShaiBagon,andTaliDekel. Deepvitfeaturesasdensevisual descriptors. arXivpreprintarXiv:2112.05814,2(3):4,2021. [60] MichaelJArcaro,StephanieAMcMains,BenjaminDSinger,andSabineKastner. Retinotopic organizationofhumanventralvisualcortex. Journalofneuroscience,29(34):10638–10652, 2009. [61] L.G.UngerleiderandL.Pessoa. Whatandwherepathways. Scholarpedia,3(11):5342,2008. doi: 10.4249/scholarpedia.5342. revision#91940. [62] VladislavAyzenbergandMarleneBehrmann. Thedorsalvisualpathwayrepresentsobject- centeredspatialrelationsforobjectrecognition. JournalofNeuroscience,42(23):4693–4710, 2022. [63] NikhilaRavi,ValentinGabeur,Yuan-TingHu,RonghangHu,ChaitanyaRyali,TengyuMa, HaithamKhedr,RomanRädle,ChloeRolland,LauraGustafson,etal.Sam2:Segmentanything inimagesandvideos. arXivpreprintarXiv:2408.00714,2024. [64] DanielKahneman,AnneTreisman,andBrianJGibbs. Thereviewingofobjectfiles: Object- specificintegrationofinformation. Cognitivepsychology,24(2):175–219,1992. [65] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez, ŁukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformation processingsystems,30,2017. 14 --- Page 15 --- A Appendix A.1 ExperimentalSetup DatasetandPreprocessing. FollowingtheDINOv2standardsetup,weusetheADE20Kdataset withimagesresizedandcroppedto512×512pixels,thenpaddedto518×518pixels. Weemploya patchsizeof14×14,resultinginatotalof1,369patchesperimage. Allcomputationsareperformed usingfloat32precisiononaNVIDIARTX4090GPU. Training Configuration. We use the Adam optimizer with a learning rate of 0.001 and a step learningrateschedulerwithstepsizeof8epochsandgammadecayfactorof0.2. Allprobesaretrainedfor16epochswithabatchsizeof256. Foreachsampleimage,werandomly select64patchesandtrainwithsupervision. Forpairwiseprobes,weapplysupervisionovertheuppertriangularportionofthe64×64pairwise matrix. Themodelisoptimizedusingbinarycross-entropyloss. Forpointwiseprobes,weconsidertwotasks: • Semantic Segmentation: We use standard cross-entropy loss for pixel-level object class classification. • InstanceSegmentation: FollowingtheDETRframework,weemployHungarianmatching forobjectassignment. Thetotallossiscomputedas: L =L +λ L +λ L (3) total cls mask mask dice dice whereL isreplacedwithaweightedbinarycross-entropylossdistinguishingbetween cls objectandno-objectclasses: L =−w log(p )−w log(1−p ) (4) cls obj obj no-obj obj The hyperparameters follow the DETR configuration: no_object_weight = 0.1, mask_weight (λ ) = 5.0, dice_weight (λ ) = 5.0, and num_object_queries = mask dice 100. A.2 ProbePerformance We train and evaluate quadratic probes on various ViTs including the complete DINOv2 family (small,base,large,giant)andCLIP-L14. Allmodelsutilizeapatchsizeof14×14pixels. CLIP-L/14 processes224×224inputimages,resultingin256patchesperimage(16×16grid). Overall,weobservethatDINOv2modelsachieveconsistenthighclassificationaccuracyat88%, andprogressalonglayers,whileCLIPmodelachievesalower84.8%anddisplaysnosuchgrowing trend. DINOmodelsseemtohaveobjectbindingbetterthanCLIP,althoughitispossiblethatless fine-grainedgridleadstoworseIsSameObject,sinceforeachpatch,thesemanticsareentangled. A.3 Cross-LayerBinding Cross-layerbindingmayactivelyhappenundertheseconditions: • Layer-specific information processing: ViTs represent different types of information at differentlayers. Althoughtheself-attentionimplementsa"fullrange"interaction(alltokens toalltokensinteraction), themodellearnstolimitthe"interaction"betweenthecorrect tokensandintegratefeaturesoftheobjecttheybelongtoaswegodeeper • Informationretrievalfromearlierlayers: ViTsmayaccessinformationfromearlierlayers whencurrentlayerslacknecessarybindingcues. AsshowninAppendixA.4.3positional informationisgraduallyremovedasthenetworkdeepens,andotherlow-levelfeaturesmay alsobediscarded. Thisinformationlossmaytriggercross-layerretrievalasacomputational strategy, which rather than maintaining all binding-relevant information in every layer, themodelefficientlyaccesseswhatitneedsfromearlierrepresentations. Thishypothesis requiresdeeperanalysisinfutureresearch. 15 --- Page 16 --- Figure7: Layer-wiseIsSameObjectclassificationaccuracyacrossvisiontransformerarchitec- tures. DINOv2modelsconsistentlyachieve 88%accuracy,demonstratingrobustdevelopmentof IsSameObjectrepresentationsacrossallmodelsizes. CLIP-L/14reaches 85%accuracyandperforms closertothebaselinelevel. Cross-layerinteractionsinViTscanoccurviatheresidualskipconnections,sowealsotrainquadratic probes between non-adjacent layers. Specifically, we compute phi(x,y) = xTWTW2y, where 1 W and W are the learned projection matrices from layer 15 and layer 18, respectively. The 1 2 probeaccuraciesare: layer15: 89.0%,layer18: 90.1%,layer15-layer18: 83.3%,showingdecent cross-layerobjectbinding,andacertaindegreeofobjectbindingacrossthesystem. A.4 PositionalInformation A.4.1 PositionalEncodingDistinguishesIdenticalObjects Asingletransformerencoderlayercanbeviewedascomprisingtwocomplementarytypesofcompu- tations: token-wise(i.e.,position-wise)operations,whichactlocallyonindividualtokensandcan beexecutedinparalleltoextractfeaturesandshort-rangeinteractions;andcross-tokenoperations, implementedthroughscaled-dot-productattention,whichenableslong-rangeinteractionsbyinte- gratingcontextualinformationacrossalltokens. Wheninspectingthemathematicalformulation, wecanalsoshowthatforidenticaltokens(i.e.,identicalpatches),positionalencodingistheonly informationthatcanguidethecross-tokeninteractions. Herewereviewtheoperationsforatrans- formerencoderwithasingleheadfrom[65]inorder. Forsimplicity,wealsoassumethatquery-key vectorshavethesamedimensionasthemodel(i.e.,d =d). Weusebluecolorforthetoken-wise k operationsandredcolorforcross-tokenoperations. Forasequenceofinputtokenst ∈Rk where i k =n-channels×patch-height×patchwidth: 16 --- Page 17 --- pre-processing: embedding:e =t W i i E addingpositionembedding:x =e +p i i i encoderlayer: Query-Key-Value:q =x W , k =x W , v =x W i i Q i i K i i V (cid:18) Q(K)⊺(cid:19) self-attention:U=softmax √ V d projectionMLP:y =u W i i O residualconnection:y =x +y i i i normalization:z =LayerNorm(y ) i i feed-forwardnetwork:z =ReLU(z W +b )W +b2 i i 1 1 2 residualconnection:z =z +y i i i where W ∈ Rk×d is the embedding layer, W ∈ Rd×d, W ∈ Rd×d, and WV ∈ Rd×d are E Q K the Query, Key, and Value layers, W ∈ Rd×d is the linear projection layer, and W ∈ Rd×m, O 1 W ∈Rm×darethefeed-forwardweights. 2 Our goal is to show that for two identical tokens (i.e., two patches with identical features), the transformerhastousethepositiontaggingascueforbinding. Sincemostoperationsaretoken-wise (positionagnostic),weonlyneedtoshowtheresultsfortheself-attentionoperation.Wewillshowthat iftwoinputtokensareidenticalwithnopositionalembedding(orwithequalpositionalembedding), thenduetothesymmetryoftheattentionmechanism,theiroutputvectorsafterself-attentionwillbe identical. Formally,ift =t i̸=j andp =p wewanttoshowthatu =u . i j i j i j Assumingt =t andp =p : i j i j x =t W +p =t W +p =x i i E i j E j j ifx =x then: i j q =q , k =k , v =v i j i j i j Thustheattentionscorecomputedbyq andq againstallkeyswouldbethesame: i j ⊺ ⊺ q k =q k ∀n i n j n Sotheattentionweights(aftersoftmax)forrowsiandj arethesame: (cid:18) q K⊤(cid:19) a =a ∀n where:a =softmax √i i,n j,n i d k AndsincethevaluesVarethesameacrossallinputsforthesamex ,theweightedsumofvalues n willalsobeidentical: N N (cid:88) (cid:88) u = a v = a v =u i i,n n j,n n j n=1 n=1 A.4.2 QuantifyingtheDegreeofDistinguishingIdenticalObjects Here,weuseasimplifiedformofourproposedtoyexamplecontainingtwoidenticalcarsandone redboat. We quantify the model’s ability to distinguish identical objects by examining the kernel density estimationofIsSameObjectscoresbetweenpatchpairsfromthesameobject(CarA,CarB,Boat,or Background)acrosslayersinDINOv2-LargeinFigure8. Ideally,patchpairsfromthesameobjectshouldachieveIsSameObjectscoresapproaching1.0. In earlylayers,thedistributionsclusteraround0.5,indicatingthemodelcannotreliablydistinguish 17 --- Page 18 --- Figure8: Layer-wisevisualizationofIsSameObjectpredictionsonthetestimage(twoidentical redcarsandoneredboat.) same-objectfromdifferent-objectpatchpairs. Asprocessingprogressesthroughlaterlayers,these distributionsshifttoward1.0. However,somesame-objectpatchpairscontinuetoscorenear0.0even indeeperlayers,representingindistinguishabletokenpairs. WealsoanalyzedpatchpairsfromdifferentobjectsinFigure10,whereweexpectIsSameObject scorestoapproach0.0. Inlayersbefore12,thedistributionscorrectlyclusternear0.0,showingthe modelcandistinguishdifferentobjects. However,asthemodellearnstogrouppatcheswithinthe sameobject(asshowninthepreviousanalysis), itsimultaneouslylosesitsabilitytotellthetwo identicalcarsapart. Thistrade-offisvisibleintheCar1-Car2distribution,whichgraduallyshifts upwardthroughthelayersanddevelopsastrongpeakat1.0bythefinallayer. A.4.3 PositionInformationDecay Wehypothesizethatthetransitionfrommiddlelayers’capacitytodistinguishidenticalobjectstolater layers’failurestemsfromthegradualdiffusionofprecisepositionalinformationintomoreglobal, semantically-focusedrepresentations. Totestthishypothesis,wetrainedlinearprobestodecodethe (x,y)coordinatesofeachpatchfromthemodel’sinternalrepresentations(Figure11). Weobservea markedincreaseinprobeRMSEatlayer21,whichsupportsourhypothesis. A.5 Attentionweights(query-keysimilarity)vs. IsSameObject Weinvestigatetherelationshipbetweenattentionmechanismsandobjectidentityrepresentationsby comparingattentionweightswithIsSameObjectscores. Attentionweightsarecomputedas: (cid:32) (cid:33) Q KT Attention =softmax √i j (5) ij d k whereQ andK representthequeryandkeyvectorsforpatchesiandj,respectively,andd isthe i j k keydimension. WethencomputethePearsoncorrelationbetweenattentionweightsatlayerℓ+1andIsSameObject scoresderivedfromthequadraticprobeatlayerℓ: ρ=corr(cid:0) Attention(ℓ+1),IsSameObject(ℓ)(cid:1) . (6) ij ij Usingthesimplifiedtwo-carscenariofromFigure8, weexaminehowattentionweightsatlayer ℓ+1correlatewithIsSameObjectscoresatlayerl. Inearlylayers,weobserveminimalcorrelation 18 --- Page 19 --- Figure9: Bindingstrengthenswithdepthforsame-objectpatchpairs. Kerneldensityestimation ofIsSameObjectscoresforpatchpairswithinthesameobjectacrossdifferentlayers. betweenthesetwomeasures,whichislikelybecauseIsSameObjectrepresentationhasnotyetfully developed. Indeeperlayers,certainpatchpairsreceivehighattentionweightsdespitehavinglowIsSameObject scores(indicatingthemodelbelievestheybelongtodifferentobjects). Thisphenomenonmaybe explainedbybackgroundpatchesbeingrepurposedforinternalcomputationalprocesses,asidentified inpriorworkonDINOregistertokens.Futureresearchcouldfurtherinvestigatehowthesespecialized backgroundpatchescontributetoobjectrepresentationandtheirroleinmaintainingdistinct“object files”. ThePearsoncorrelationsinFig.6arestatisticallysignificant(p<0.001underpermutationtest). A.6 ImplementationofAblationStudies. Weconductablationexperimentsatlayer18ofDINOv2-Large,whereIsSameObjectrepresentation achievesthebestdecodability.Weapplybothuninformedandinformedablationmethodsasdescribed inSection3.3. SegmentationEvaluation. Forbothsemanticandinstancesegmentationtasks,weretrainlinear segmentation heads with ablated representations. The uninformed ablation randomly permutes 19 --- Page 20 --- Figure10: Identicalobjectscollapseinrepresentationatdeeperlayers. Kerneldensityestimation ofIsSameObjectscoresforpatchpairsfromdifferentobjectsacrosslayers. bindingvectorsacrosspatches,whiletheinformedablationinjectsobject-averagedbindingvectors usingground-truthmasks. Theselinearheadsuseidenticalconfigurationstothepointwiseprobes describedinSectionA.1,whichareeffectivelypointwiseprobesappliedtothefinaltransformerlayer. DINOLossEvaluation. Toassesstheimpactonthepretrainingobjective,weevaluateDINOloss using the pretrained model as both student and teacher networks. For computational simplicity, weexcludetheiBOTandKoLeolosscomponentsfromthisanalysis. Notethatinformedablation cannotbeevaluatedunderDINOloss,astheuseoflocalcropsaltersthepatchdivisions,making object-averagedbindingvectorsundefinedforthecroppedregions. 20 --- Page 21 --- Figure11: Positionalinformationdecaysinlaterlayers. Layer-wisedecodingperformancefor patch(x,y)coordinates,comparedwiththesuccessrateofdistinguishingpatchesfromCarAandCar B. Figure12: Attentionvs.IsSameObjectinearlylayers. Scatterplotscomparingattentionweights toIsSameObjectscoresforpatchpairs;correlationisstillweak,indicatingthatbindinghasnotyet developedsufficientlytoinfluenceattention. Figure13: Attentionvs.IsSameObjectinlaterlayers. Attentionissometimesallocatedtolow- IsSameObjectbackgroundtokens,suggestingthesetokensmightberepurposedforinternalcomputa- tion. 21