--- Page 1 --- Learning to Drive Safely with Hybrid Options BramDeCooman((cid:66)),JohanSuykens Abstract. Out of the many deep reinforcement learning ap- for autonomous driving applications, in which a virtual proaches for autonomous driving, only few make use of the op- driver needs access to both fast acceleration manoeuvres tions (or skills) framework. That is surprising, as this frame- and slower lane change manoeuvres. work is naturally suited for hierarchical control applications in A drawback of purely discrete action spaces is that they general, and autonomous driving tasks in specific. Therefore, lack flexibility and expressiveness, as, for example, the ve- in this work the options framework is applied and tailored to locity of the ego vehicle can only be changed in fixed in- autonomous driving tasks on highways. More specifically, we crements. This can be resolved using continuous action define dedicated options for longitudinal and lateral manoeuvres spaces. However, the extra flexibility of continuous actions with embedded safety and comfort constraints. This way, prior domain knowledge can be incorporated into the learning process also comes at a cost. Typically, the training time signifi- andthelearneddrivingbehaviourcanbeconstrainedmoreeasily. cantly increases and it becomes much harder to constrain We propose several setups for hierarchical control with options the learned policies. Extra penalties could be introduced and derive practical algorithms following state-of-the-art rein- in the reward signal to try and constrain the vehicle’s driv- forcement learning techniques. By separately selecting actions ing behaviour. However, this approach becomes increas- for longitudinal and lateral control, the introduced policies over ingly more complex as the number of constraints grows. combinedandhybridoptionsobtainthesameexpressivenessand Moreover, determining good penalties for manoeuvres that flexibility that human drivers have, while being easier to inter- take multiple timesteps to complete, is not a straightfor- pretthanclassicalpoliciesovercontinuousactions. Ofallthein- wardtaskeither. Incontrast,undertheintroducedoptions vestigated approaches, these flexible policies over hybrid options framework, we have the ability to constrain vehicle move- perform the best under varying traffic conditions, outperforming mentsmorereliablyusingdedicatedoptionswithembedded the baseline policies over actions. safety and comfort constraints for certain manoeuvres. Keywords. Reinforcement Learning, Autonomous Driving, Theproposedframeworkforhierarchicalcontrolwithop- Options, Skills, Hierarchical Control, Hybrid Actions tions has several other benefits. It has the ability to flexi- bly combine dedicated options for longitudinal and lateral 1 Introduction control, for which already existing driving assistance sys- tems (such as cruise control or lane assist) can be reused. Duetoitsgreatpotentialtosolvecomplexdecision-making Thiscanacceleratethelearningprocess,asindividualdriv- problems, Reinforcement Learning (RL) is a promising ing manoeuvres no longer have to be learned from scratch. technique to train virtual drivers for autonomous driving Additionally, it can improve the trust of the general pub- applications[3,13,18,21,26,42,45]. Animportantdesign lic in the learned driving policies. In particular, the vehi- decision is the definition of the action space, defining how cle will never perform any other (weird) manoeuvres than the virtual driver (agent) interacts with the environment. the predefined ones, which can be based on automated fea- Most attempts so far have restricted their action spaces to tures that are already widely used and trusted by human be either fully discrete (with actions such as ‘lane change drivers. Finally, the obtained models are also more inter- left’ and ‘brake’) [2, 25] or fully continuous (with actions pretable than their continuous counterparts, as it is imme- such as ‘turn the steering wheel by x°’ or ‘accelerate by diatelyclearwhatkindofmanoeuvretheegovehicleisper- ym/s2’) [14, 34]. In this paper, we investigate some alter- forming by looking at which option is active. Ultimately, native approaches with options. The resulting hierarchi- the proposed hybrid policies with options share the same cal control architectures using combined or hybrid options expressiveness and flexibility as human drivers, effectively have several benefits over the fully discrete or continuous combining the best of discrete and continuous approaches. approaches. A classical RL setup with a discrete action space re- Contributions quires each action to have the same duration, correspond- ing to exactly one timestep in the underlying Markov De- There are two key contributions in this work. First, we cision Process (MDP). An MDP with options, on the other proposeasetofoptionsandassociatedmanoeuvrestailored hand, inherently supports different durations for each op- toautonomousdrivingtasks. Weshowhowsafetymeasures tion. This makes the options approach especially useful appliedtopoliciesovercontinuousactionscanbereapplied B. De Cooman  and J. to the hierarchical control setup with options, resulting in  safe and comfortable driving behaviour by design. Second, terforDynamicalSystems,SignalProcessingandDataAnalytics,De- we introduce two novel hierarchical control architectures partmentofElectricalEngineering(ESAT),KULeuven,3001Leuven, Belgium. with combined options and hybrid options. Both support 1 5202 tcO 82 ]GL.sc[ 1v47642.0152:viXra --- Page 2 --- theseparateselectionofactionsforlongitudinalandlateral The expectation is taken over s ∼ ι(·), a ∼ π(·|s ) and 0 t t control, leading to more flexible driving policies. s ∼ τ(·|s ,a ) for all timesteps. The discount factor t+1 t t γ ∈ [0,1) ensures the discounted return objective remains Organization finite for bounded reward functions and trades off the im- portanceoffutureversusimmediaterewards. Fordetermin- This paper is further organized as follows. Section 2 istic policies, which we primarily work with in this paper, gives an introduction to reinforcement learning, the op- the notation a =π(s) is used. tions framework, and the methods used throughout this work. The autonomous driving setup and simulation envi- Value Function ronment are briefly reviewed in Section 3. Afterwards, in Section 4 the practical algorithms for hierarchical control It is often helpful to consider the value func- with options tailored to the autonomous driving task are tions of a certain policy π, defined as vπ(s) = introduced. The proposed methodology is evaluated and E [(cid:80)∞ γkr(s ,a ,s ) | s =s] and π,τ k=0 t+k t+k t+k+1 t compared with other approaches in Section 5. Finally, the qπ(s,a) = E [(cid:80)∞ γkr(s ,a ,s ) | π,τ k=0 t+k t+k t+k+1 relatedworkisdiscussedinSection6beforetheconclusion. s =s, a =a]. These value functions aid in evalu- t t ating the respective policy, as they both provide the expected discounted return when following policy π 2 Preliminaries starting from an arbitrary state s (and action a for qπ). Furthermore, the value functions for the opti- The basic concepts of reinforcement learning and the op- mal policy π∗ satisfy the Bellman optimality equations tions framework are briefly introduced here. For a more v∗(s)=max E [r(s ,a ,s )+γv∗(s )|s =s, a =a] detailed overview, the reader is referred to Sutton et al. a τ t t t+1 t+1 t t and q∗(s,a) = E [r(s ,a ,s ) + γmax q∗(s ,a′) | [35, 36]. τ t t t+1 a′ t+1 s =s, a =a]. t t 2.1 Reinforcement Learning Parameterized Actions Reinforcement Learning (RL) is a machine learning tech- AlthoughtheactionspaceofanMDPisusuallyeithercon- nique that can be applied to solve sequential decision- tinuous or discrete, more complex environments with hy- making problems, in which an agent interacts with an en- brid action spaces can also be considered. In a Parameter- vironment. ized Action MDP (PAMDP) [23], the hybrid action space has a particular structure, consisting of a discrete set of Markov Decision Process actions A = [D] = {1,...,D}, each parameterized by d Formally, the environment can be described as a Markov a continuous set of parameters A c,ad ⊆ Rm. More pre- Decision Process (MDP) [32] M = (S,A,ι,τ,r,γ) consist- cisely, the parameterized action space can be defined as ing of state space S, action space A, initial state distri- A = {(a d,a c)|a d ∈ A d,a c ∈ A c,ad}. For such PAMDPs, bution ι, state transition distribution τ, reward function r the policy can be decomposed into a discrete and contin- and discount factor γ. At every timestep t, the agent is uous part as π(a |s) = π d(a d |s)π c,ad(a c |s). Sampling able to observe the environment’s current state s t ∈S and an action a =(a d,a c) can then be done in two steps: first perform a certain action a t ∈ A. This brings the environ- the discrete action a d is sampled from π d, and afterwards ment to a new state s t+1 in the following timestep, accord- the continuous parameters a c are sampled from the corre- ing to the stochastic environment dynamics τ(s t+1|s t,a t). sponding π c,ad. Simultaneously, the agent receives a scalar reward signal r t = r(s t,a t,s t+1), indicating how favorable the taken ac- Model-Free RL and Exploration tion and resulting state transition are. This process of ob- servingastate(andreward)andexecutinganactioniscon- In the model-free RL setting used throughout this work, tinuouslyrepeatedthroughoutanepisode,andisstartedby the environment dynamics and reward function (ι, τ, r) sampling an initial state from ι(s ). remain unknown to the agent. To find an optimal pol- 0 icy, the agent is thus required to explore the state-action space during training. For this purpose, a behavioural pol- Policy and Objective icy β(a|s) is deployed, for which the encountered experi- Theagent’spolicyπ(a |s )describestheprobability(den- ence tuples (s ,a ,r ,s ) are collected in a replay buffer t t t t t t+1 sity) of taking action a after perceiving s . This policy B. Batches of experience tuples sampled from this replay t t canbeimprovedbytakingfeedbackfromtherewardsignal buffercanthenbeusedforoptimizingthepolicyandother into account. More specifically, the goal in reinforcement models. To sufficiently explore the state-action space when learning is to find the optimal policy π∗, maximizing the working with deterministic policies, ϵ-greedy behavioural expected discounted return policies are typically used for discrete action spaces, and Gaussian behavioural policies for continuous action spaces. (cid:34) ∞ (cid:35) π∗ =argmax E (cid:88) γtr(s ,a ,s ) . (1) Such an ϵ-greedy policy takes a random action with prob- π ι,π,τ t=0 t t t+1 ability ϵ and the greedy action following the policy π(s) 2 --- Page 3 --- cial case under this options framework. In particular, the option (S,I ,1) is effectively equivalent to the primitive {a} action a, as it can be selected for all states I = S, has deterministic policy π(s ) = a for all states s , and termi- t t nates after one timestep as χ(s ) = 1 for all states s t+1 t+1 (T =S). Hierarchical Control ProvidingtheagentwithasetofoptionsO tochoosefrom, Figure1: Schematicoverviewofanoptionactinginstatespace. ratherthanthesetofactionsA,naturallyleadstoahierar- Once in control, the option policy π can bring the agent from o chical control setup, as shown in Figure 3 and explained in statesintheinitiationsetI allthewaytostateswithnonzero o terminationprobabilitiesχ . Foreachintermediatestate(yellow Subsection 4.2. At the lowest level, the policy of the active o dots), the option policy provides the environment with suitable optiono ∈O isnowresponsiblefortakingtheappropriate t actions at every timestep to realize this transition. actions at every timestep t, similar to the policy over ac- tionsinaclassicalsetup. Atthehighestlevel,anovelpolicy over options or master policy π (o |s ) is introduced, de- m t t otherwise, i.e. β(a|s) = ϵ|A|−1+(1−ϵ)I {π(s)}(a) where scribing the probability of activating option o t in state s t. I X(x) is the indicator function — equal to 1 when x ∈ X This master policy can only select from the set of available and 0 otherwise. The Gaussian policy, on the other hand, optionsforaparticularstate,denotedbyOav(s)={o ∈O| has the action taken by π as mean and σ2 as variance, i.e. s ∈ I }, which is fully determined by each of the option’s o a ∼ N(π(s),σ2I). The ϵ and σ parameters trade off the initiation sets. amount of exploration versus exploitation of the learned Remarkthatthepoliciesateachleveloperateondifferent policy, and are typically attenuated throughout the train- timescales. The active option’s policy π operates on the o ing process. finest timescale, selecting a new action at t every timestep; whereas the policy over options π operates on a coarser m timescale, only selecting a new option on termination of 2.2 Options the previously active option. More precisely, o ∼ π (·| t+1 m Instead of working directly with primitive actions a, one s ) when o terminates in s , and o =o otherwise. t+1 t t+1 t+1 t can also work with temporally extended actions, commonly Fromthehigherlevel’sperspective,wearethusoperatingin referredtoasskills,optionsortemplates[27,36,39]. While aSemi-MDP [32],withtheoptionsprovidingthenecessary a primitive action is only ‘active’ for a single timestep, link back and forth to the underlying MDP. bringing the agent from one state to the next; options can last multiple timesteps, bringing the agent from one region State-Option Value Function in state space to another. Once an option is activated, and until it is terminated, the option policy takes over control Analogous to the state-action value functions for poli- and provides the environment with the necessary primitive cies over actions, we can define the state-option actions at every timestep to complete this transition. Fig- value function for policies over options qπm(s,o) = ure 1 visualizes this process. E O,πm,τ[(cid:80)∞ k=0γkr(s t+k,a t+k,s t+k+1)|s t = s,o t = o]. In this case, the expectation is taken over the probability distributions describing the active options o ∼ (1 − Definition t+1 χ (s ))I (·)+χ (s )π (·|s ),theselectedactions o t t+1 {o t} o t t+1 m t+1 More formally an option o can be defined by the triplet a t ∼π o t(·|s t), and the state transitions s t+1 ∼τ(·|s t,a t) (I ,π ,χ )withinitiationsetI ⊂S,optionpolicyπ (a | for all timesteps. o o o o o t s ) and termination condition χ (s ). The initiation set Sutton et al. [36] describe various strategies for find- t o t+1 determines for which regions in state space the option can ing the optimal master policy π m∗. The simplest ap- be activated. Once the option is active, the agent sam- proachisoption-to-option valuelearning,whichisbasedon ples actions from the option policy a ∼ π (·|s ) at every the optimal Bellman equation for the superimposed Semi- timestep until its termination. In gent eral, o the opt tion’s ter- MDP, q∗(s,o) = E πo,τ(cid:2)(cid:80)d k− =1 0γkr(s t+k,a t+k,s t+k+1) + mination conditions χ o are probabilistic. More precisely, γdmax o t+d∈Oav(s t+d)q∗(s t+d,o t+d)(cid:12) (cid:12)s t = s,o t = o(cid:3) , with for every state s the option terminates with probability d the duration of o from s. In other words, this method t+1 χ (s ) or remains active with probability 1−χ (s ). considers the execution of an option, from activation un- o t+1 o t+1 In this work we specifically focus on deterministic options, til termination, as an indivisible operation. In this work whichhavebothadeterministicpolicya =π (s )andde- we specifically focus on the alternative intra-option value t o t terministic termination conditions χ (s ) ∈ {0,1}. Such learning technique, which ‘breaks up’ the options in the o t+1 deterministic termination conditions effectively partition finer timescale of the underlying MDP and also provides the state space, allowing us to define the termination set valueupdatesfortheintermediatelyvisitedstatesthrough- T ={s ∈S|χ (s )=1}. out an option’s execution. When working with determin- o t+1 o t+1 Remark that primitive actions a can be seen as a spe- istic options, the optimal value function can then be writ- 3 --- Page 4 --- ten down as q∗(s,o) = E [r(s ,a ,s )+γu∗(s ,o )| architecture of the critic model, as illustrated in Figure 2. τ t t t+1 t+1 t s =s,o =o,a =π (s )] with The continuous actions in the actor-critic setup are passed t t t o t as an input to the critic model, which has a single output  u∗(s t+1,o t)= oq t∗ +( 1s ∈t m O+ a a1 vx, (o s tt +) 1)q∗(s t+1,o t+1) s st t+ +1 1 ∈ ∈/ T To ot t . n p ste a au i trr e.o O in snc po atr h sr se e es dop to ahn sed r ain nhg a in nt do p, ut i th ne tt ov ha tel hu v ee a clf u ro ier t- it b ch a me seg odi dv s ee e ln ,tu wst p ha , it coe hn-a l hyct ati so hn ae dedicated output neuron for each discrete action to denote In Section 4 we show how a practical estimate of this op- the value for the given state and respective action. When timal value function and the corresponding optimal master working with hybrid action spaces, a mixed critic architec- policycanbefoundusingdeepconstrainedQ-learning[16]. ture is used, which takes the continuous part of the action Following the intra-option technique instead of the option- asaninput(togetherwiththestate),andhasseparateout- to-option approach will turn out to be pivotal for support- put neurons for the discrete part of the action. Further ing combined and hybrid options. detailsabouteachoftheusedmethodsarediscussedinthe remainder of this section. 2.3 Methods There are many ways to find an approximate optimal pol- Continuous Actions icy for (1). In this paper, we specifically limit ourselves to For continuous action spaces, we work with actor-critic some recent model-free methods for learning deterministic methods, such as ‘Deep Deterministic Policy Gradient’ policies. In particular, due to the discrete nature of op- (DDPG) [22] and ‘Twin Delayed DDPG’ (TD3) [12]. To tion selection, we primarily focus on methods that support ensure sufficient exploration of the state-action space, the discrete (A = A d ⊆ O) or hybrid (A = A c ×A d) action Gaussian behavioural policy N(π(s),σ2I) is deployed to spaces. Policies over continuous action spaces (A = A ) c populate the replay buffer. The critic network is trained are only considered as a baseline to compare against in the by minimizing the critic loss (2) with target value y = t conducted experiments. Figure 2 provides an overview of r +γq(s ,µ(s ;θ′);θ′) for the DDPG method. The the different setups used throughout this work. t t+1 t+1 µ q actornetworkisupdatedsimultaneouslybyminimizingthe actor loss L (θ )=−E [q(s ,µ(s ;θ );θ )]. Actors and Critics µ µ s t∼B t t µ q Two categories of methods can be immediately distin- TD3 The performance and training stability of the orig- guished from this figure. On the one hand, actor-critic inal DDPG method can be further improved according methods jointly train two deep neural networks: an ac- to Fujimoto et al. [12]. In their proposed TD3 algo- tor µ(s;θ ) and a critic q(s,a;θ ). The actor network rithm, extra twin networks are introduced, actor and tar- µ q represents a deterministic policy over continuous actions, get networks are updated less frequently than critic net- a = π (s) = µ(s;θ ), approximating the optimal pol- works, and smoothed (regularized) target values are used. c c µ icy; whereas the critic network approximates the optimal More precisely, two separate critic networks (the twins), value function. On the other hand, value-based meth- denoted by parameters θ q,1 and θ q,2, are jointly trained ods only train one deep neural network, the Q-Network using the same critic loss (2) but with altered target val- (critic) q(s,a;θ q), which approximates the optimal value ues y t =r t+γmin i∈{1,2}q(s t+1,µ(s t+1;θ µ′)+ϵ c;θ q′ ,i) with function. The optimal policy over discrete actions is not ϵ c a noise sample from the clipped Gaussian distribution explicitly learned, but rather derived as the greedy pol- N(0,σ c2I)| [−c,c]. icy for this learned value function, i.e. a = π (s) = d d argmax q(s,a;θ ). Discrete Actions a q The critic network is typically trained by minimizing a For discrete action spaces, we resort to value-based squaredtemporaldifferenceerror,resultinginthecriticloss approaches based on the popular ‘Deep Q-Network’ L (θ )= E (cid:2) (q(s ,a ;θ )−y )2(cid:3) , (2) (DQN) [24]. To sufficiently explore the state-action space q q (s t,a t,rt,s t+1)∼B t t q t andfillthereplaybuffer, the ϵ-greedybehaviouralpolicyis wherethetargetvaluey canbecalculatedinvariousways, deployed. The model parameters can then be updated us- t depending on the specific method. To improve the training ing samples from the replay buffer by minimizing the same stability, these target values are kept steady throughout criticloss(2)asbefore,althoughwithadifferentcalculation training through the use of extra target networks. The ofthetargetvaluesy t. FortheDQNmethod,thetargetval- parameters of these target networks are denoted with a ues are calculated as y t =r t+γmax a t+1q(s t+1,a t+1;θ q′). prime,andareupdatedmoreslowlyusingPolyakaveraging θ′ ←τθ +(1−τ)θ′. Theactornetworkistrainedbymax- Clipped Double DQN The performance of the orig- i i i imizinganexpecteddiscountedreturnestimate,usuallyin- inal DQN method can be further improved by reducing volving an evaluation of the critic network. The specific overestimation bias [12, 41]. To this end, extra twin net- definition of the actor loss L (θ ) depends on the method. works are introduced and the calculation of the target val- µ µ Although there are quite some similarities between the ues is altered. More precisely, two separate Q-Networks various approaches, there is an important difference in the (the twins), denoted by the parameters θ and θ , are q,1 q,2 4 --- Page 5 --- Figure 2: Overview of the model architectures and derived policies for the methods discussed in this paper. Top: actor-critic methods for policies over continuous actions ac ∈ Ac. Middle: value-based methods for policies over discrete actions ad ∈ Ad. Bottom: actor-critic methods for policies over hybrid actions (ac,ad)∈Ac×Ad. jointlytrained. Thepolicyisderivedfromthefirsttwin,i.e. model π (s) = µ(s;θ ), whereas the policy over dis- c µ π(s)=argmax q(s,a;θ ). Thesamelossfunction(2)is crete actions is implicitly derived from the learned critic a q,1 used to train each of the twins, but the target value is now model as π (s) = argmax q(s,(π (s),a );θ ). To suf- d ad c d q calculated as y =r +γmin q(s ,π(s );θ′ ). ficiently explore the state-action space, an ϵ-greedy be- t t i∈{1,2} t+1 t+1 q,i havioural policy is used for the discrete actions, and a Gaussian behavioural policy for the continuous parame- Constrained DQN Another DQN extension, the ‘Con- ters. Once again, the same loss (2) can be used for op- strained DQN’ method, is proposed by Kalweit et al. [16] timizing the critic model, with target values calculated and aims to properly handle constrained (state-dependent) as y = r +γmax q(s ,(a ,a );θ′) and action spaces A(s) ⊂ A. This is done by deriv- t t ad,t+1∈Ad t+1 c,t+1 d,t+1 q ing the policy as the pruned greedy policy π(s) = a c,t+1 = µ(s t+1;θ µ′). The actor model is trained by argmax a∈A(s)q(s,a;θ q). Kalweitetal.[16]illustratethat minimizingL µ(θ µ)=−E s t∼B[(cid:80) ad∈Adq(s t,(µ(s t;θ µ),a d); this pruning is also necessary in the calculation of the tar- θ )]. q get values, y =r +γmax q(s ,a ;θ′), as t t a ∈A(s ) t+1 t+1 q t+1 t+1 otherwise suboptimal policies are retrieved. 3 Autonomous Driving Setup Hybrid Actions The hybrid options framework introduced in the next sec- There are various approaches to deal with hybrid action tion is applied to an autonomous driving problem on high- spaces, depending on the specific application [11, 23, 28]. ways. Aproprietaryhighwaysimulatorisusedforthispur- In this work we exclusively train agents in PAMDPs which pose. In this section the most relevant components of this have the same continuous parameters for each discrete ac- simulator and environment setup are briefly introduced. tion, i.e. A = A ×A . The ‘Parameterized DQN’ (P- c d DQN) [43] method was chosen to deal with such hybrid action spaces, as its architecture and methodology resem- 3.1 Objective and Reward ble the previously discussed TD3 and DQN methods the most. The objective in this environment is to efficiently navigate highway traffic, while adhering to safety constraints (avoid Parameterized DQN In the P-DQN method, the hy- crashes), driving regulations (keep right) and comfort con- bridpolicycanbesplitintoadiscreteandcontinuouspart, straints (smooth movements). A three lane highway with a = π(s) = (π (s),π (s)) = (a ,a ). Similar to the both straight and curved segments is considered for all ex- c d c d previously discussed actor-critic methods, two deep neu- periments. Variousvehiclesarerandomlydistributedalong ral networks are jointly trained: a critic q(s,(a ,a );θ ) thedifferentavailablelanesandareassigneddifferenttarget c d q for approximating the optimal value function and an ac- velocities at initialization. The virtual driver (agent) is in tor µ(s;θ ) for approximating the optimal policy over control of a single vehicle, assigned to drive the maximum µ the continuous parameters. The policy over continuous allowed speed. parameters is thus explicitly learned through the actor The objective and soft constraints are encoded in the 5 --- Page 6 --- reward signal as a weighted sum of different penalties 3.3 Safety and Comfort constraints w r +w r +w r +w r The safety and comfort requirements are not encoded in f f v v c c r r r = . w +w +w +w the reward signal as soft constraints, but dealt with sep- f v c r arately as hard constraints. For policies over continuous In order of appearance, these components give a penalty actions (directly providing setpoints for the motion con- for insufficient following distance (r ), inappropriate speed trollers), safety is ensured through state-dependent action f (r ), drifting from lane center (r ) and not keeping right bounds as in De Cooman et al. [10]. These action bounds v c (r ). An extra penalty is given when the virtual driver a (s)≤a ≤a (s) are derived from the braking criterion r l u collides with other vehicles or the highway boundaries. (cid:18) v2 v2(cid:19) min ∆x,∆x+ L − F >∆x , (3) 2b 2b SAFE 3.2 Vehicle Dynamics and Policies where ∆x is the following distance between 2 vehicles, v The kinematic bicycle model (KBM) [33] is used to update L andv arethevelocitiesoftheleadingandfollowingvehicle each vehicle’s state, consisting of a global x and y position, F respectively,bisthemaximumdecelerationofbothvehicles, heading angle ψ and speed v, and ∆x is a minimum safe distance to keep between SAFE  x˙ vcos(ψ+ζ) both vehicles. Complying with this braking criterion en-   ψy v˙˙ ˙  =  vsi ln v r( s aψ in+ ζ ζ)   ζ =arctan(cid:18) l f l +r l r tanδ(cid:19) . s w au si sr t ue hs mtt h ph e te iolf e no al sl do [i 1w n 0gi ]n .vg Tev h oe ich inli ec d,l ie cu ai ns td ea el trw hs aa oy tms ae nab r al e ce a tst iooo nna av abo ili sed mwa o ac r po s pl tl e-i cs daio tsn oe cosζ thesafeintervaldefinedbytheactionbounds,thefollowing (clipping) notation is used a| =min[max(a,a ),a ]. Theinputsofthisdynamicalsystemarethesteeringangleδ [al,au] l u In this work, we employ the same braking criterion to andthelongitudinalaccelerationa. Inthiswork,wedonot enforce safety on policies using (hybrid) options. More- further consider such global state vectors and low-level in- over,underthisoptionsframework,additionalcomfortcon- puts; instead projected state vectors (into the road’s frame straints (smooth lane changes) can be readily embedded in of reference) and high-level actions are used. Henceforth, dedicated manoeuvre policies, as discussed in the next sec- we denote the ego vehicle’s projected longitudinal veloc- tion. ity (along the road) by v, and its projected lateral offset (with respect to the road edge) by d. Dedicated vehicle motion controllers aid in stabilizing the vehicle on the road 4 Methodology and facilitate the driving task of the virtual driver. These controllers track the references (actions) a, consisting of a To restrict the driving behaviour of the virtual driver, we desiredrelativelongitudinalvelocity∆vandrelativelateral specifically predefine a set of options for longitudinal and position ∆d, set by the driving policies. lateral control of autonomous vehicles. Each option’s pol- To take correct high level steering decisions, the virtual icyautomaticallyprovidesthenecessaryactionstoperform driverneedssomeextrainformationaboutothertrafficpar- certain manoeuvres over the course of multiple timesteps, ticipantsinitsneighbourhood. Thisinformationisgathered whileadheringtothegivenhardconstraintsbydesign. For in the agent’s observation vector s, containing local infor- example, safety of the learned driving policies can be en- mation such as the vehicle’s velocity components and its sured at the finest (MDP) timescale, by reusing the state- offset with respect to different lane centers, and relative dependent action bounds for policies over continuous ac- information such as relative distances and velocities with tions. The virtual driver can thus choose from a com- respect to neighbouring traffic. The relative offsets with prehensive collection of safe and comfortable manoeuvres respect to the lane center of the current lane (∆L=0) and (options) based on the current traffic situation, instead of nearest lanes on the left (∆L = 1) and right (∆L = −1) selecting individual low-level actions. This makes the op- side of the ego vehicle are referred to as c . In case there tions framework especially suitable for autonomous driving ∆L is no subsequent lane on a particular side, c is set equal applications, and naturally leads to a hierarchical control ±1 to c . architecture, as visualized in Figure 3. 0 Every vehicle is controlled by a policy, mapping obser- vations s to suitable actions a. The policy of the au- 4.1 Safe Options for Autonomous Driving tonomous vehicle is learned using any of the described re- inforcementlearningmethodsinthispaper. Thepoliciesof We start with a concise description of the various prede- the other vehicles in the simulation environment are fixed fined options tailored to autonomous driving tasks. A vir- beforehand. A mixture of vehicles equipped with a custom tualdrivertypicallyneedstoconsidertwohigh-levelcontrol rule-basedpolicyandapolicyimplementingthe‘Intelligent decisions at every moment in time: longitudinal (velocity) DriverModel’(IDM)[40]and‘MinimizingOverallBraking control and lateral (position) control. For both of these Induced by Lane change’ (MOBIL) [17] is used. Both poli- controltasks,dedicatedoptionsareintroducedandsumma- cies try to mimic rudimentary human driving behaviour, rized in Table 1. The set of options acting on the longitu- although being fully deterministic. dinalvelocityisdenotedbyO ={o ,o ,o ,o },whereas v e m vd vi 6 --- Page 7 --- Table 1: Overview of the introduced options for autonomous driving. Manoeuvre Option Target Velocity v Target Offset d Initiation Set I Termination Set T T T Emergency oe 0| [v+∆vl,v+∆vu] d| [d+∆dl,d+∆du] S S Maintain om v d {s∈S|Safe[s;v T,d T]} S V Ve el lo oc ci it ty yC Ch ha an ng ge e( (− +) )o ov vd i (cid:6) (cid:4)δ δv vv v − +11(cid:7) (cid:5)δ δv v d {s∈S|Safe[s;v T,d T]} (cid:40) s∈S(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)|v T− ∨v ¬| S< afϵ ev [s;v T,d T](cid:41) (cid:40) (cid:12) (cid:41) (cid:40) (cid:12) (cid:41) LaneChange(left) oll v d+c′ 1 s∈S(cid:12) (cid:12)|d T−d|≥ϵ d∧v≥v m s∈S(cid:12) (cid:12)|d T−d|<ϵ d∨v<v m LaneChange(right) olr d+c′ −1 (cid:12) (cid:12) ∧Safe[s;v T,d T] (cid:12) (cid:12) ∨¬Safe[s;v T,d T] thesetofoptionsactingonthelateralpositionisdenotedby to obtain more accurate state predictions, for which the O = {o ,o ,o ,o }. Once activated, the option’s policy braking criterion can be checked (or from which action d e m ll lr provides the vehicle motion controllers with the necessary bounds can be derived). actions to perform a certain predefined manoeuvre, bring- ingtheegovehiclefromoneregioninstatespacetoanother Option Safety overthecourseofmultipletimesteps(e.g. alanechangema- Safety of the various options is then ensured by extend- noeuvre). The virtual driver only needs to select another ing the termination sets and restricting the initiation sets. option after the active option terminates, either after suc- More specifically, if the planned manoeuvre is considered cessfully completing its associated manoeuvre, or after an unsafe in a certain state, that state is added to the termi- abortion caused by other termination conditions (such as nationsetandremovedfromtheinitiationset. Asaresult, safety constraint violations). Although this manoeuvre is an option can only be activated if it will not bring the vir- limited to either a longitudinal or lateral movement for the tual driver to an unsafe state during the option’s execution optionsintroducedinthiswork, ourmethodologyallowsto (according to the used prediction model); and it is termi- readily combine multiple options (and their manoeuvres), nated as soon as new predictions, obtained throughout the as outlined in Subsection 4.2. option’s execution, become unsafe. Remark that the ini- tiation and termination sets do not need to be fully con- Option Targets structed beforehand, rather the initiation and termination conditions can be evaluated for each individual state en- The effect of each option on the longitudinal velocity or countered by the virtual driver. By reevaluating the safety lateral position is indicated by the target velocity v and T conditions at every timestep, the latest available state in- target offset d . The associated option policy provides T formation is always used in deciding whether to activate or the necessary actions to achieve these targets from any terminate an option. initial state, while adhering to hard safety and comfort constraints during the manoeuvre’s execution by design. Option Interruption For example, the following option policy could be used for (cid:2) (cid:3) this purpose, π o(s) = v T−v;d T−d | [a l(s),a u(s)], as it Most of the introduced options run until their targets are takessafetyconstraintsintoaccountbyenforcingthestate- reached or their safety conditions are violated. To prevent dependentactionbounds(butdisregardsanyothercomfort unlimited execution and to ensure other options can be se- constraints). RemarkthatfortheMarkovpropertytohold, lected at any time, certain primitive options are also de- it is essential to have fixed targets for all states, regardless fined, which are automatically terminated after every time ofwhethertheoptionisalreadyactiveornotinaparticular step (T = S). An alternative implementation could make state. the options interruptable instead [36], such an extension is The option targets are also used to assess the safety of however not further investigated in this work. theoption’sassociatedmanoeuvre. Moreprecisely,thema- noeuvre is considered to be safe for a certain state if the Overview target velocity and position are within the state-dependent action bounds for that state, i.e. Safe[s;v ,d ] = ∆v ≤ The various options from Table 1 are succinctly described T T l v − v ≤ ∆v ∧ ∆d ≤ d − d ≤ ∆d . In our (model- below. Safety and comfort constraints are embedded in T u l T u free) setup, this effectively corresponds to verifying that each option’s policy, as well as through its termination and the braking criterion (3) holds for all interpolated states initiation sets. between the current state and an estimated target state at the end of the manoeuvre. This estimated target state is Emergency Option To ensure that at least one option simply derived from the current state by instantaneously is always available, the emergency option o is introduced. e moving the ego vehicle to the target position and immedi- Thisoptiontriestoslowdownthevehicleasfastaspossible ately adopting the target velocity, while leaving all other (v →0) and tries to maintain the current lateral position T state components unchanged. For model-based methods, (d → d), while adhering to the safety constraints. For T the learned environment model could be leveraged instead anyencounteredunsafestates,thevehicleismovedtowards 7 --- Page 8 --- the nearest safe region, by clipping the target values using Baseline the state-dependent action bounds. Although it is the only Thedifferentsetupsforhierarchicalcontrolwithoptionsare option that is always available (I = S), it should be con- compared with other approaches using the baseline setup sidered as a fallback for emergency situations; as generally, (Figure 3.a). In this baseline setup, a ‘classical’ policy over other,moresuitableoptionsshouldbeavailable. Therefore, primitive actions directly interacts with the vehicle motion it is defined as a primitive option (T = S), ensuring this (cid:2) (cid:3) controllersthroughthecontinuoussetpointsa = ∆v;∆d . emergency manoeuvre can be interrupted at any time. This policy can either be learned using standard reinforce- mentlearningmethods(suchasTD3),orbepredefined(e.g. Maintain Option Additionally, wespecifytheprimitive following IDM and MOBIL). maintain option o , which maintains the current longitudi- m nal velocity (v = v) and lateral position (d = d). This T T Options option is only available when it is safe to keep driving at the current velocity and position (Safe[s;v ,d ]); and it is With the introduction of dedicated options, the virtual T T terminated after every time step (T = S), allowing other driver obtains the ability to select appropriate manoeuvres options to be selected at any moment. to efficiently navigate traffic. In other words, the problem becomes a decision-making task over a discrete set of op- Longitudinal Options For longitudinal control, we de- tions (manoeuvres) instead of over a continuous range of fine two velocity change options, which decrease (o ) or actions. We distinguish the higher level actions selected vd increase (o vi) the current velocity with fixed increments1 by the master policy, a m = π m(s), from the lower level δ =2m/s. These options provide the necessary actions to actions that are passed to the vehicle motion controllers, v (cid:2) (cid:3) attainthetargetvelocityv T inasafeandcomfortableman- a =π(s,a m)= ∆v;∆d . ner. They can only be initiated when the target velocity is In the most basic setup (Figure 3.b), only one option is considered to be safe (Safe[s;v ,d ]); and they terminate activated at any time, selected from the complete set of T T oncethetargetvelocityisreached(|v T−v|<ϵ v =0.01m/s) options, a m = o ∈ O (= O v ∪O d). The selected option or when it is no longer safe to drive at the target velocity takes over control until termination and supplies the ve- (¬Safe[s;v ,d ]). hicle motion controllers with both longitudinal and lateral T T (cid:2) (cid:3) setpoints, ∆v;∆d =π (s). The master policy π and its o m Lateral Options For lateral control, we similarly define behavioural counterpart β m only select a new option upon twolane change options,providingthenecessaryactionsto termination of the previously active one (or at the start of steer the ego vehicle towards the nearest lane center to its the episode). left (o ) or right (o ) in approximately 5s (for a complete ll lr lane change at maximum velocity). The relative offsets to Combined Options the nearest lane center on the left and right of the ego ve- A drawback of the basic options approach is that only a hicle are denoted by c′ and c′ . Both of these are often 1 −1 single manoeuvre can be performed at any given time. For equal to their non-primed counterparts, unless the vehicle example, the learned policies lack the ability to combine a isnotproperlycenteredwithinitscurrentlaneandthecur- lateral lane change manoeuvre with a longitudinal acceler- rent lane’s center is the nearest in the respective direction ation manoeuvre, which can be performed by the contin- (in which case c′ = c )1. These options can only be ac- ±1 0 uous baseline policies. To address this limitation, we also tivated when the car is not yet centered in the target lane propose an extended hierarchical control architecture with (|d −d| ≥ ϵ = 0.05m), and the predicted target state T d support for combined options for longitudinal and lateral is safe (Safe[s;v ,d ]). Additionally, a minimum veloc- T T control(Figure3.c). Suchasetupcansignificantlyincrease ity condition (v ≥ v = 3m/s) is enforced to ensure that m theflexibilityofthevirtualdriver,providingitwithsimilar the lane change manoeuvre can be completed in a reason- capabilities as the baseline models. In particular, two op- able timeframe, as otherwise the option could remain ac- tions are activated at any time, a = (o ,o ) ∈ O ×O , tive indefinitely. The option terminates once aligned with m v d v d one for longitudinal control o and one for lateral control the target lane’s center (|d −d| < ϵ ), or once the mini- v T d o . Theselectedoptionstakeovercontroluntiltermination, mum velocity is no longer attained (v < v ), or when the d m and each of them supplies a longitudinal or lateral setpoint predicted target state is no longer considered to be safe (cid:2) (cid:3) to the vehicle motion controllers, ∆v = 1 0 π (s) and (¬Safe[s;v T,d T]). ∆d = (cid:2) 0 1(cid:3) π (s). As soon as either option teo rv minates, od the (behavioural) master policy π (β ) selects a new op- m m 4.2 Hierarchical Control with Options tion for the respective direction of control, while the other non-terminating option remains active. The previously described options provide the virtual driver with several safe and comfortable manoeuvres for the con- trol of autonomous vehicles. These options can be applied Hybrid Options invarioushierarchicalcontrolarchitectures. Figure3shows Finally, a hybrid setup (Figure 3.d) is proposed with the anoverviewofthedifferentsetupsinvestigatedinthiswork. aim of combining the best of the continuous (baseline) and 1ThisisnecessaryfortheMarkovpropertytohold. discrete (options) approaches. The resulting setup closely 8 --- Page 9 --- Figure3: Schematicoverviewofthedifferentautonomousdrivingpoliciesusedinthiswork. Ontheleft(a)thebaselinepolicyover primitiveactions(∆v,∆d),directlyinterfacingwiththevehiclemotioncontrollers. Ontheright(b+c+d)differentconfigurations ofpoliciesoveroptions,whichselectone(ortwo)newoption(s)toexecuteafterapreviousoneterminated. Onceanoptionpolicy is in control, it provides the vehicle motion controllers with suitable actions to perform its associated manoeuvre and reach the targets v and d (see Figure 1 and Table 1). T T aligns with the decision process of human drivers. More Algorithm 1 Hierarchical RL with Options (Generic) specifically, the hybrid master policy can select a continu- Initialize critic (and actor) models θ ,θ′ (θ ,θ′) ouslongitudinalvelocityparameter,whilepickingadiscrete q,i q,i µ µ B ←∅ lateral manoeuvre (option), a = (∆v,o ) ∈ R×O . The m d d for each episode do velocitysetpoint∆vispasseddirectlytothevehiclemotion for each environment step t do controllers,whiletheselectedoptionsuppliesthelateralpo- a ∼β (·|s ,a ) ▷ Select high level action sition setpoint ∆d = (cid:2) 0 1(cid:3) π (s). Remark that the con- m,t m t m,t−1 tinuous part of the (behaviouro ad l) master policy π (β ) a t =π(s,a m,t) ▷ Obtain low level action m,c m,c s ∼τ(·|s ,a ) selects a new velocity setpoint at every timestep, whereas t+1 t t r =r(s ,a ,s ) the discrete part of the (behavioural) master policy π t t t t+1 m,d B ←B∪{(s ,a ,r ,s )} (β ) only selects a new option upon termination of the t m,t t t+1 m,d end for previously active one. for each gradient step do Sample batch B from B 4.3 Implementation for all (s t,a m,t,r t,s t+1)∈B do a =π (s ,a ) ▷ Calculate targets m,t+1 m t+1 m,t We apply a combination of existing RL techniques to de- y =r +γmin q(s ,a ;θ′ ) t t i∈{1,2} t+1 m,t+1 q,i rive intra-option value learning and actor-critic algorithms end for forthevarioushierarchicalcontrolsetupswithoptions. The L(cid:98)q(θ q,i;B)= |B1 |(cid:80) B(q(s t,a m,t;θ q,i)−y t)2 g foe ln loe wri sc ta hlg eo tr rit ah inm ini gss pu rm inm cipa lr eiz se od fi sn taA tl eg -o or f-i tt hh em -a1 rta Rnd Lc mlo es te hly - L(cid:98)µ(θ µ;B)= |− B1 |(cid:80) (cid:80) q(s t,(µ(s t;θ µ),o d);θ q,1) ods. Moredetailedimplementationsforeachsetuparepro- B od∈Odav(s t) vided in Appendix A. θ q,i ←θ q,i−η q∇ θq,iL(cid:98)q(θ q,i;B) ▷ Update critics θ µ ←θ µ−η µ∇ θµL(cid:98)µ(θ µ;B) ▷ Update actor Remark. Although in Algorithm 1 a sample from the θ m′ ←τθ m+(1−τ)θ m′ ▷ Update target models semi-Markov (behavioural) master policy is taken for every end for timestep, this is often not needed in practice. In particu- end for lar, the discrete component of the policy should only select a new option upon termination of the active option. Option Availability At several points during training and deployment of the uninterruptible, the only available option is often the cur- master policy over options, we need to know which options rently active one. Therefore, we also define the closely re- are available to the agent. The set Oav(s) provides this in- lated set of available options for a particular state s′, given formation at the time of selecting a new option, but that is that option o is active upon reaching this state. This ex- insufficient. Since the options investigated in this work are tendedavailabilitysetadditionallydependsontheoption’s 9 --- Page 10 --- termination sets, and is defined as argmax q(s′,a′ ;θ ). This policy a′ m∈Ovav(s′,ov)×Odav(s′,od) m q,1 only selects a new option upon termination of any previ- (cid:40) Oav (s′,o)= Oav(s′) if s′ ∈T o . ously active one, which becomes more clear when rewriting {o} otherwise it as  avaS ii lm abil ia lir tl yy sa es tsfo dr ot nh oe ti nn eit ei dat ti oon bean fud llt yer cm onin sta rt uio ctn edse bts e, fot rh ee - o′ v∈Ovava (r sg ′)m ,o′ da ∈x Odav(s′)q(s,(o′ v,o′ d);θ q,1) s′∈T ov∩T od hand, but can be evaluated ‘on the go’ for the encountered a′ m= (argmax o′ v∈Ovav(s′)q(s′,(o′ v,o d);θ q,1),o d) s′∈T ov\T od . s nt aa tt ie os na sn etd co op nt dio itn ios. nsF ,o thr eve or py tioco nm ap vale ix labin ili it ti ia et sio fon ro sr tat te er sm si- ( (o ov, ,a org )max o′ d∈Odav(s′)q(s′,(o v,o′ d);θ q,1)) os t′ h∈ eT ro wd i\ sT eov t v d ands canalsobestoredinthereplaybuffertospeedup t+1 the computations (at the cost of increased memory usage). The behavioural master policy β m is the ϵ-greedy policy derived from π , taking into account the option avail- m abilities for the random selection, i.e. β (a′ | s′,a ) = Intra-Option Value Learning m m m ϵ|O vav(s′,o v)×O dav(s′,o d)|−1I Ovav(s′,ov)×Odav(s′,od)(a′ m)+(1− The critic networks are trained using a variant of the ϵ)I (a′ ). {πm(s′,am)} m clipped DDQN method [12], adapted for use with op- tions by following the intra-option value learning tech- Hybrid Options nique. Additionally, the methodology of the constrained DQN method [16] is applied to enforce the constrained Theimplementationforhybridoptions(Figure3.d)isaddi- initiation and termination of the options. Ultimately, tionallybasedontheP-DQNmethod[43]andhasaspecial both critic networks (twins) can be trained by minimiz- (hybrid) structure for the critic networks (see Figure 2). ing the critic loss (2) with target values y = r + Similar to classical TD3 (used in the baseline setup), the t t γmin i∈{1,2}q(s t+1,a m,t+1;θ q′ ,i) and a m,t+1 obtained from calculation of target values y t is slightly altered to per- the (constrained) master policy π . form target policy smoothing [12]. Furthermore, the state- m Bybreakinguptheoptionsintotheirconstituentactions dependentactionboundsmethodology[10]isappliedtoen- for every timestep, the critic models can be updated from sure the selected continuous (velocity) parameters are safe. every individual interaction with the environment stored In this setup, there is an extra actor model for select- in the replay buffer (similar to classical RL approaches). ingthecontinuousparameters,explicitlymodelingthecon- Moreover, this allows to flexibly combine an action from tinuous part of the master policy. More precisely, the theoption’spolicywithanactionfromotherpolicies(either actor network provides the normalized parameter ∆v˜ = over options or primitive actions) in every timestep, which µ(s;θ µ) ∈ [−1, 1], which is rescaled to ∆v = π m,c(s) = is not possible in the option-to-option approach. σ pwl(∆v˜;s) using the piecewise linear rescaling function withstate-dependentactionbounds[∆v (s), ∆v (s)][10]. l u Remark. Even though the option policies have to satisfy This actor model is trained by minimizing the actor loss the Markov property in order to be able to use the intra- L (θ ) = −E [(cid:80) q(s ,(µ(s ;θ ),o );θ )]. option learning rule, experiments suggested it also works Tµ he µ discrete pas rt t∼B of thod e∈O mdav a( ss tt) er pt olicy it s iµ mplid citlyq,1 de- wellforoptionpoliciesthatdonotalwayssatisfytheMarkov rived from the first critic model as o′ = π (s′,o ) = d m,d d property. argmax q(s′,(µ(s′;θ ),o′);θ ),orequivalently o′ d∈Odav(s′,od) µ d q,1  Options argmax q(s ,(∆v˜ ,o′);θ ) s ∈T The master policy over single options (Figure 3.b) is de- o d,t+1= o′ d∈Odav(s t+1) t+1 t+1 d q,1 t+1 od,t . o otherwise rived from the first critic twin as a′ = π (s′,a ) = d,t m m m argmax q(s′,o′;θ ), or equivalently o′∈Oav(s′,o) q,1 Thebehaviouralmasterpolicyβ mcansimilarlybesplitin acontinuousanddiscretepart. Forthecontinuouspart,the (cid:40) o = argmax o′∈Oav(s t+1)q(s t+1,o′;θ q,1) s t+1 ∈T o t . truncatedGaussianbehaviouralpolicyN −1 1(µ(s;θ µ),σ ϵ2)is t+1 o otherwise used. For the discrete part, the ϵ-greedy policy derived t from π is used, taking into account each of the option m,d Thebehaviouralmasterpolicyβ m istheϵ-greedypolicyde- availabilitiesfortherandomselection,i.e. β m,d(o′ d|s′,o d)= r iniv te od af cr co om unπ tm t. hR ee om pta ir ok nt ah va at it lah be ilr ia tn ied so ,m i.es .e βlec (t aio ′n |m su ′,s att )ak =e ϵ|O dav(s′,o d)|−1I Odav(s′,od)(o′ d)+(1−ϵ)I {πm,d(s′,od)}(o′ d). m m m ϵ|Oav(s′,o)|−1I Oav(s′,o)(a′ m)+(1−ϵ)I {πm(s′,am)}(a′ m). 5 Experiments Combined Options All presented methods for hierarchical control with options When working with combined options (Figure 3.c), the are compared with baseline policies in the simulated au- high level action a is a two-dimensional discrete deci- tonomousdrivingenvironmentdescribedinSection3. More m sion variable. In this case, the master policy can be specifically,4differentRLpoliciesaretrained: onebaseline derived from the first critic twin as a′ =π (s′,a ) = policy over continuous actions, and three master policies m m m 10 --- Page 11 --- over single, combined, and hybrid options (see Figure 3). Thebaselinepolicyovercontinuousactionsistrainedusing STD3 [9] (for improved smoothness) with enforced state- dependent action bounds [10] (for safety). To make the comparisons fair, both the smoothness regularization and actionboundsarealsoappliedtothecontinuouscomponent of the hybrid master policy, as illustrated in Algorithm 4. An overview of the used hyperparameters can be found in Appendix C. Foreveryconfiguration,R=10policiesaretrained,each using a different seed for initialization of the training pro- cess. Throughout training, E =10 independent evaluation episodes are executed to obtain an estimate of the learned policy’saverageperformance. OutofallRrepeats,thebest performing policy is selected. This selection is based on Figure 4: Summarized evaluation performance for each policy the average evaluation reward and only considers policies throughout training. Solid lines represent the mean value over obtained after 2/3 of the training process has completed, 10repeatedexperiments,theshadedareasdenotetheminimum whichguaranteesagoodperformance-smoothnesstrade-off and maximum values. for the policies over continuous actions. This best policy is separately evaluated on some additional benchmark sce- nariosforautonomousdrivingandextensivelytestedunder gradual transitions and without any overshoot. Although varying traffic conditions. the smoothness regularization for the baseline policy over continuous actions already improves passenger comfort to some extent — by avoiding abrupt and oscillatory lateral 5.1 Training and Evaluation movements — it is still insufficient to obtain the same kind TheaveragerewardsforeachoftheRE evaluationepisodes of comfortable and consistent lane change manoeuvres as andforeveryconfigurationareaggregatedandplotted(us- for the option approaches. ing exponential average smoothing with α = 0.1) in Fig- Looking carefully at the velocity plot, some interesting ure 4. It is clear the hybrid options approach outperforms properties of the various option setups are revealed. The the other policies, as it has both a higher average reward velocity profiles for the policies over single and combined and lower variance between the different repeated exper- options are less smooth and contain some ‘discrete jumps’, iments. Both the continuous and hybrid policies quickly reducing passenger comfort. This phenomenon is caused converge, after approximately 2 · 105 training timesteps, by the particular definition of the options for longitudinal whereas the single options approach converges much more control, which modify the velocity in fixed increments δ v slowly and might not have fully converged yet at the end to adhere to the Markov property. Some additional exper- of training. Looking at the wall clock time (using the same iments with non-Markov options for longitudinal control hardwarefortraining),weobservethefastesttrainingtime (with more flexible velocity adaptation to avoid this inher- for the continuous policies. Our implementations of the ent drawback), yielded promising initial results. Slightly hierarchical approaches seem to be roughly 10% (single), harder to spot is the different operation of policies over 20% (combined) and 30% (hybrid) slower on average, al- single options compared to policies over combined options. though further optimizations could potentially accelerate The former policies can only perform one longitudinal or training. AppendixBadditionallyvisualizestheactivityof lateral manoeuvre at once, resulting in a constant velocity options during evaluation, which underlines the increased profileduringthelanechanges. Thelatterpoliciescancom- interpretability of the option methods. Finally, it should binebothalongitudinaland lateralmanoeuvreatthesame be stressed that all trained policies are completely safe and time, resulting in slightly faster and more efficient move- causenocrashes(intheusedsimulationenvironment)both ments on the road. Overall the policy over hybrid options during training and evaluation. scores the best, with the smoothest velocity and position profiles on this overtaking benchmark. 5.2 Benchmark Results 5.3 Test Results Thebestperformingpoliciesforeachconfigurationaresep- aratelyevaluatedonsometypicaldrivingtasksandscenar- Finally, the best performing policies for each configuration ios. Figure 5 shows the observed driving behaviour for the are also extensively tested under various traffic conditions, simple scenario of overtaking one slower vehicle in front of rangingfromverycalmtodensetrafficscenarios, andcom- the ego vehicle. From the lateral position plot, we can im- paredwithaconservativebaselinepolicyfollowingIDMand mediately see the benefit of the hierarchical policies over MOBIL.ThesetestresultsaresummarizedinFigure6,with options. By using constrained option policies for lateral additional plots provided in Appendix B. control, we obtain better alignment within the lane and Overall, all policies have very comparable performances much more desirable lane change manoeuvres, with more and outperform the IDM/MOBIL policy in all circum- 11 --- Page 12 --- Figure 6: Evaluation of the selected policies in various traffic conditions and comparison with a conservative IDM/MOBIL policy. Solid lines represent the mean value over 10 episodes, Figure 5: Lateral position and longitudinal velocity plots for the shaded areas denote the minimum and maximum values. each of the selected policies applied to the overtaking bench- mark. PAMDPs [11, 23]. This is also the setting considered in stances. The baseline policy over continuous actions and thispaperforthehybridoptions. Additionally,manyworks policy over hybrid options seem to outperform the other onhierarchicalreinforcementlearningcanbeinterpretedas policies at higher traffic densities (more congestion), not finding policies over hybrid action spaces, with discrete ac- seen during training. As expected, all hierarchical ap- tion selections at the higher levels and continuous action proaches with constrained options perform more reliable selections at the lower levels (see for example Figure 3). and consistent lane change manoeuvres. In particular, the Anoverviewofthebroaderfieldofhierarchicalreinforce- lane change time is almost constant and takes around 5s, mentlearningisgivenbyPateriaetal.[29]andHutsebaut- complying with the intended comfort constraints. In con- Buysseetal.[15]. Theoptions(orskills)frameworkconsid- trast, the baseline policy over continuous actions exhibits ered in this work is only one specific subfield. The focus in highly variable lane change times, with lane change ma- most recent works lies on learning a suitable set of options noeuvres that are often too abrupt. from scratch [4, 44], or learning a policy that can achieve various subgoals [19, 20], or discovering useful skills from 6 Related Work large datasets of interaction data [31, 37]. In contrast, in this work we specifically predefine a small set of options, tailored to the considered autonomous driving tasks, and Hierarchical and Hybrid Reinforcement apply them in various hierarchical control setups. From Learning thisperspective,ourmethodologyissimilartotheworksof Variousreinforcementlearningmethodstodealwithhierar- Neumannetal.[27]andDalaletal.[7],whichlearnamaster chicalandhybridactionspaceshavebeenproposed,mainly policyforselectingtheparametersofpredefinedparameter- differing in the specific structure of the action space or the ized skills. Although we apply more recent deep RL tech- consideredapplication. Neunertetal.[28]provideageneral niquesandapplyintra-optionlearning,whereastheseworks algorithm for finding optimal hybrid policies with mixed andothers[19,38]typicallyperformSemi-MDP(option-to- continuous and discrete action spaces. In other works, the option) updates. focus lies often on hybrid action spaces with a more par- Barretoetal.[5]andPengetal.[30]introducealgorithms ticular structure, such as parameterized action spaces in that allow the agent to execute a combination of learned 12 --- Page 13 --- options. Both approaches are more generic than our pro- change manoeuvres; hence, such an approach should add posed method for hierarchical control with combined op- appropriate deliberation costs on option interruption. An- tions. More precisely, these methods compose new options other promising direction for future research would be a acting on the whole (lower level) action space, whereas our more complete implementation of intra-option learning. In approach specifically combines options acting on indepen- our current implementation, the intra-option updates are dent subsets of the action space (dedicated options for lon- only applied for active options. Truly updating the value gitudinal and lateral control). estimatesforalloptionsthatwouldtakethesameactionas theactiveoption,couldthusfurtheracceleratethelearning process. Finally, it would be interesting to investigate pa- Reinforcement Learning for Autonomous rameterizable option policies for generic manoeuvres, such Driving as a lane change manoeuvre with a variable duration pa- Our methodology is most comparable with the work of rameter, similar to the parameterized trajectories of Wang Naveed et al. [26] and Wang et al. [42], which apply deep et al. [42]. Concurrently, additional (more complex) value hierarchical RL to learn parameters for dedicated parame- constraints [1] or dual policy constraints [8] could be im- terized trajectory planners. A similar strategy is also fol- posed on the driving policies to further improve their per- lowed by Chen et al. [6], who first train options for ded- formance and reliability. icated manoeuvres, and afterwards apply a policy gradi- ent method to learn the master policy over options. In- Acknowledgments stead of working with dedicated manoeuvre options, many This research was partially supported by Ford, under Ford Al- other works try to learn options with a fixed time horizon lianceProjectKUL0076;andTAILOR,aprojectfundedbyEU from large datasets or from interactions with the environ- Horizon2020researchandinnovationprogrammeunderGANo ment, which are afterwards used to train a master policy 952215. This research received funding from the Flemish Gov- in the Semi-MDP setting [13, 21, 45]. An important dif- ernment (AI Research Program). Johan Suykens is also affili- ference with our work is that most of these methods use ated to Leuven.AI - KU Leuven institute for AI, B-3000, Leu- skills with a fixed time horizon and perform Semi-MDP ven,Belgium. Theresearchleadingtotheseresultshasreceived (option-to-option) updates. In contrast, our options are funding from the European Research Council under the Euro- predefined and have variable length, while the master poli- pean Union’s Horizon 2020 research and innovation program / cies are trained using intra-option updates. This is cru- ERC Advanced Grant E-DUALITY (787960). This paper re- cial to support combined or hybrid options, and allows to flects only the authors’ views and the Union is not liable for seamlessly combine slow lane change manoeuvres with fast any use that may be made of the contained information. The acceleration manoeuvres. resources and services used in this work were provided by the VSC (Flemish Supercomputer Center), funded by the Research 7 Conclusion Foundation - Flanders (FWO) and the Flemish Government. This paper introduced several options tailored to the au- References tonomous driving problem, and showed how they can be applied in various hierarchical control architectures. The [1] J. Achiam, D. Held, A. Tamar, and P. Abbeel. resulting framework leads to a very natural representa- Constrained Policy Optimization. In Proceedings of tionoftheproblem,withdedicatedoptionsforlongitudinal the 34th International Conference on Machine Learn- and lateral manoeuvres of varying durations, and separate ing, ICML 2017, volume 70, pages 22–31. PMLR, decision-making modules operating at different timescales July 2017. URL  and levels in the control hierarchy. In such a setup the achiam17a.html. learneddrivingbehaviourcanalsobeconstrainedmoreeas- ily, as safety and comfort constraints can be individually [2] A. Alizadeh, M. Moghadam, Y. Bicer, N. K. Ure, imposed on the options implementing the manoeuvres. As U. Yavas, and C. Kurtulus. Automated Lane Change a result, even though option selection happens at coarser Decision Making using Deep Reinforcement Learning (Semi-MDP)timescales,wecanstillmaintainstrongsafety in Dynamic and Uncertain Highway Environment. In guarantees at the finest (MDP) timescale. Furthermore, in 2019IEEEIntelligentTransportationSystemsConfer- the proposed framework, actions for longitudinal and lat- ence, ITSC 2019, pages 1399–1404, 2019. ISBN 978- eral control can be selected separately, either by options or 1-5386-7024-8. doi:10.1109/ITSC.2019.8917192. classicalpolicies,asexemplifiedbythemoreflexiblepolicies [3] S. Aradi. Survey of Deep Reinforcement Learn- over combined or hybrid options. Ultimately, our contribu- ing for Motion Planning of Autonomous Vehicles. tionscanleadtomoreinterpretable,reliable,andtrustwor- IEEE Transactions on Intelligent Transportation Sys- thy policies for autonomous driving. tems, 23(2):740–759, Feb. 2022. ISSN 1558-0016. Nevertheless, there are still many interesting open direc- doi:10.1109/TITS.2020.3024655. tions for future research. For example, proper support for interrupting options could be added. Some smaller experi- [4] P.-L. Bacon, J. Harb, and D. Precup. The Option- ments with interruption showed frequent abortions of lane Critic Architecture. In Proceedings of the 31st 13 --- Page 14 --- AAAI Conference on Artificial Intelligence, AAAI [12] S. Fujimoto, H. Van Hoof, and D. Meger. Addressing 2017, volume 31, pages 1726–1734, Feb. 2017. Function Approximation Error in Actor-Critic Meth- doi:10.1609/aaai.v31i1.10916. ods. In Proceedings of the 35th International Confer- ence on Machine Learning, ICML 2018, volume 80, [5] A.Barreto,D.Borsa,S.Hou,G.Comanici,E.Aygu¨n, pages 1587–1596. PMLR, July 2018. URL https:// P. Hamel, D. Toyama, J. hunt, S. Mourad, D. Silver, proceedings.mlr.press/v80/fujimoto18a.html. and D. Precup. The Option Keyboard: Combin- ing Skills in Reinforcement Learning. In Advances [13] Y. Gurses, K. Buyukdemirci, and Y. Yildiz. Devel- in Neural Information Processing Systems 32, oping Driving Strategies Efficiently: A Skill-Based Hi- NeurIPS 2019, volume 32, pages 13052–13062. erarchical Reinforcement Learning Approach. IEEE Curran Associates, Inc., 2019. URL https:// Control Systems Letters, 8:121–126, 2024. ISSN 2475- proceedings.neurips.cc/paper files/paper/2019/hash/ 1456. doi:10.1109/LCSYS.2024.3349511. 251c5ffd6b62cc21c446c963c76cf214-Abstract.html. [14] Z. Hou, W. Liu, and A. Knoll. Safe Reinforce- [6] J. Chen, Z. Wang, and M. Tomizuka. Deep Hierarchi- ment Learning for Autonomous Driving by Using cal Reinforcement Learning for Autonomous Driving Disturbance-Observer-Based Control Barrier Func- with Distinct Behaviors. In 2018 IEEE Intelligent Ve- tions. IEEE Transactions on Intelligent Vehi- hicles Symposium (IV), pages 1239–1244, June 2018. cles, 10(6):3782–3791, June 2025. ISSN 2379-8904. doi:10.1109/IVS.2018.8500368. doi:10.1109/TIV.2024.3463468. [7] M. Dalal, D. Pathak, and R. R. Salakhutdinov. Accel- [15] M. Hutsebaut-Buysse, K. Mets, and S. Latr´e. Hier- erating Robotic Reinforcement Learning via Parame- archical Reinforcement Learning: A Survey and Open terizedActionPrimitives.InAdvancesinNeuralInfor- Research Challenges. Machine Learning and Knowl- mation Processing Systems, volume 34, pages 21847– edge Extraction, 4(1):172–221, Mar. 2022. ISSN 2504- 21859. Curran Associates, Inc., 2021. URL https:// 4990. doi:10.3390/make4010009. proceedings.neurips.cc/paper files/paper/2021/hash/ [16] G. Kalweit, M. Huegle, M. Werling, and J. Boedecker. b6846b0186a035fcc76b1b1d26fd42fa-Abstract.html. Deep Constrained Q-learning, Sept. 2020. [8] B. De Cooman and J. Suykens. A Dual Perspec- [17] A.Kesting,M.Treiber,andD.Helbing. GeneralLane- tive of Reinforcement Learning for Imposing Policy Changing Model MOBIL for Car-Following Models. Constraints. IEEE Transactions on Artificial Intel- Transportation Research Record, 1999(1):86–94, Jan. ligence, pages 1–14, Apr. 2024. ISSN 2691-4581. 2007. ISSN 0361-1981. doi:10.3141/1999-10. doi:10.1109/TAI.2025.3564898. [18] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. [9] B. De Cooman, J. Suykens, and A. Ortseifen. Improv- Sallab, S. Yogamani, and P. Perez. Deep Reinforce- ing temporal smoothness of deterministic reinforce- ment Learning for Autonomous Driving: A Survey. mentlearningpolicieswithcontinuousactions. In33rd IEEE Transactions on Intelligent Transportation Sys- Benelux Conference on Artificial Intelligence, BNAIC tems, 2021. doi:10.1109/TITS.2021.3054625. 2021, pages 217–240, Esch-sur-Alzette, Luxembourg, Nov. 2021. URL  [19] T. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum. Hierarchical Deep Re- [10] B. De Cooman, J. Suykens, and A. Ortseifen. En- inforcement Learning: Integrating Temporal forcingHardState-DependentActionBoundsonDeep Abstraction and Intrinsic Motivation. In Ad- Reinforcement Learning Policies. In G. Nicosia, vances in Neural Information Processing Systems V. Ojha, E. La Malfa, G. La Malfa, P. Pardalos, 29, NIPS 2016, volume 29, pages 3675–3683. G. Di Fatta, G. Giuffrida, and R. Umeton, edi- Curran Associates, Inc., 2016. URL https:// tors, Proceedings of the 8th International Conference proceedings.neurips.cc/paper files/paper/2016/hash/ on Machine Learning, Optimization, and Data Sci- f442d33fa06832082290ad8544a8da27-Abstract.html. ence, LOD 2022, volume 13811 of Lecture Notes in Computer Science, pages 193–218, Cham, Mar. 2023. [20] A. Levy, G. Konidaris, R. Platt, and K. Saenko. SpringerNatureSwitzerland.ISBN978-3-031-25891-6. Learning Multi-Level Hierarchies with Hind- doi:10.1007/978-3-031-25891-6 16. sight. In 7th International Conference on Learn- ing Representations, ICLR 2019, Sept. 2019. [11] Z. Fan, R. Su, W. Zhang, and Y. Yu. Hybrid doi:10.48550/arXiv.1712.00948. Actor-Critic Reinforcement Learning in Parameter- ized Action Space. In Proceedings of the 28th Inter- [21] Z. Li, F. Nie, Q. Sun, F. Da, and H. Zhao. Boost- national Joint Conference on Artificial Intelligence, ing Offline Reinforcement Learning for Autonomous IJCAI-19, pages 2279–2285, Macao, China, Aug. Driving with Hierarchical Latent Skills. In 2024 2019. International Joint Conferences on Artificial IEEE International Conference on Robotics and Au- Intelligence Organization. ISBN 978-0-9992411-4-1. tomation (ICRA), pages 18362–18369, May 2024. doi:10.24963/ijcai.2019/316. doi:10.1109/ICRA57147.2024.10611197. 14 --- Page 15 --- [22] T.P.Lillicrap,J.J.Hunt,A.Pritzel,N.Heess,T.Erez, Curran Associates, Inc., 2019. URL https:// Y. Tassa, D. Silver, and D. Wierstra. Continuous con- proceedings.neurips.cc/paper files/paper/2019/hash/ trol with deep reinforcement learning. In 4th Interna- 95192c98732387165bf8e396c0f2dad2-Abstract.html. tional Conference on Learning Representations, ICLR [31] K. Pertsch, Y. Lee, and J. Lim. Accelerating Rein- 2016, 2016. doi:10.48550/arXiv.1509.02971. forcement Learning with Learned Skill Priors. In Pro- [23] W. Masson, P. Ranchod, and G. Konidaris. Rein- ceedings of the 2020 Conference on Robot Learning, forcement Learning with Parameterized Actions. In volume 155, pages 188–204. PMLR, Oct. 2021. URL Proceedings of the 30th AAAI Conference on Artifi-  cial Intelligence, AAAI 2016, volume 30, Feb. 2016. [32] M.L.Puterman. Markov Decision Processes: Discrete doi:10.1609/aaai.v30i1.10226. StochasticDynamicProgramming.JohnWiley&Sons, Jan. 1994. ISBN 978-1-118-62587-3. URL https:// [24] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, books.google.com/books?id=VvBjBAAAQBAJ. J. Veness, M. G. Bellemare, A. Graves, M. Ried- miller, A. K. Fidjeland, G. Ostrovski, S. Petersen, [33] R. Rajamani. Vehicle Dynamics and Control. Me- C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Ku- chanical Engineering Series. Springer US, Boston, maran,D.Wierstra,S.Legg,andD.Hassabis.Human- MA, 2nd edition, 2012. ISBN 978-1-4614-1432-2. levelcontrolthroughdeepreinforcementlearning. Na- doi:10.1007/978-1-4614-1433-9. ture, 518(7540):529–533, Feb. 2015. ISSN 1476-4687. doi:10.1038/nature14236. [34] D. M. Saxena, S. Bae, A. Nakhaei, K. Fujimura, and M. Likhachev. Driving in Dense Traffic with [25] S.Nageshrao,H.E.Tseng,andD.Filev. Autonomous Model-FreeReinforcementLearning. InProceedings of Highway Driving using Deep Reinforcement Learning. the 2020 IEEE International Conference on Robotics InProceedingsofthe2019IEEEInternationalConfer- and Automation (ICRA),pages5385–5392, May2020. ence on Systems, Man and Cybernetics (SMC), pages doi:10.1109/ICRA40945.2020.9197132. 2326–2331,Oct.2019.doi:10.1109/SMC.2019.8914621. [35] R. S. Sutton and A. G. Barto. Reinforcement Learn- [26] K. B. Naveed, Z. Qiao, and J. M. Dolan. Tra- ing: An Introduction, volume 258 of A Bradford Book. jectory Planning for Autonomous Vehicles Using MIT Press, 2nd edition, Nov. 2018. ISBN 978-0- Hierarchical Reinforcement Learning. In 2021 262-35270-3.URL IEEE International Intelligent Transportation Sys- uWV0DwAAQBAJ. tems Conference (ITSC), pages 601–606, Sept. 2021. [36] R. S. Sutton, D. Precup, and S. Singh. Between doi:10.1109/ITSC48978.2021.9564634. MDPsandsemi-MDPs: Aframeworkfortemporalab- [27] G.Neumann,W.Maass,andJ.Peters. Learningcom- straction in reinforcement learning. Artificial Intelli- plex motions by sequencing simpler motion templates. gence, 112(1):181–211, Aug. 1999. ISSN 0004-3702. In Proceedings of the 26th Annual International Con- doi:10.1016/S0004-3702(99)00052-1. ference on Machine Learning, ICML 2009, ICML ’09, [37] D. Tanneberg, K. Ploeger, E. Rueckert, and J. Pe- pages753–760,NewYork,NY,USA,June2009.Asso- ters. SKID RAW: Skill Discovery From Raw Tra- ciation for Computing Machinery. ISBN 978-1-60558- jectories. IEEE Robotics and Automation Let- 516-1. doi:10.1145/1553374.1553471. ters, 6(3):4696–4703, July 2021. ISSN 2377-3766. doi:10.1109/LRA.2021.3068891. [28] M. Neunert, A. Abdolmaleki, M. Wulfmeier, T. Lampe, T. Springenberg, R. Hafner, F. Ro- [38] C. Tessler, S. Givony, T. Zahavy, D. Mankowitz, mano, J. Buchli, N. Heess, and M. Riedmiller. and S. Mannor. A Deep Hierarchical Approach to Continuous-Discrete Reinforcement Learning for Lifelong Learning in Minecraft. In Proceedings of Hybrid Control in Robotics. In Proceedings the 31st AAAI Conference on Artificial Intelligence, of the Conference on Robot Learning, volume AAAI 2017, volume 31, pages 1553–1561, Feb. 2017. 100, pages 735–751. PMLR, May 2020. URL doi:10.1609/aaai.v31i1.10744.  [39] S. Thrun and A. Schwartz. Finding Structure in [29] S. Pateria, B. Subagdja, A.-h. Tan, and C. Quek. Hi- Reinforcement Learning. In Advances in Neural erarchical Reinforcement Learning: A Comprehensive Information Processing Systems 7, NIPS 1994, Survey.ACMComput.Surv.,54(5):109:1–109:35,June volume 7, pages 385–392. MIT Press, 1994. URL 2021. ISSN 0360-0300. doi:10.1145/3453160.  7ce3284b743aefde80ffd9aec500e085-Abstract.html. [30] X. B. Peng, M. Chang, G. Zhang, P. Abbeel, and S. Levine. MCP: Learning Composable [40] M. Treiber, A. Hennecke, and D. Helbing. Congested Hierarchical Control with Multiplicative Com- trafficstatesinempiricalobservationsandmicroscopic positional Policies. In Advances in Neural simulations. PhysicalReviewE,62(2):1805–1824,Aug. Information Processing Systems, volume 32. 2000. doi:10.1103/PhysRevE.62.1805. 15 --- Page 16 --- [41] H. Van Hasselt, A. Guez, and D. Silver. Deep Rein- forcement Learning with Double Q-Learning. In Pro- ceedings of the 30th AAAI Conference on Artificial In- telligence, AAAI 2016, volume 30, pages 2094–2100, Mar. 2016. doi:10.1609/aaai.v30i1.10295. [42] L. Wang, J. Liu, H. Shao, W. Wang, R. Chen, Y. Liu, andS.L.Waslander. EfficientReinforcementLearning forAutonomousDrivingwithParameterizedSkillsand Priors, May 2023. [43] J.Xiong,Q.Wang,Z.Yang,P.Sun,L.Han,Y.Zheng, H. Fu, T. Zhang, J. Liu, and H. Liu. Parametrized Deep Q-Networks Learning: Reinforcement Learning with Discrete-Continuous Hybrid Action Space, Oct. 2018. [44] S. Zhang and S. Whiteson. DAC: The Double Actor-Critic Architecture for Learning Options. In Advances in Neural Information Processing Systems 32, NeurIPS 2019, volume 32, pages 2012–2022. Curran Associates, Inc., 2019. URL  4f284803bd0966cc24fa8683a34afc6e-Abstract.html. [45] T. Zhou, L. Wang, R. Chen, W. Wang, and Y. Liu. Accelerating Reinforcement Learning for Autonomous Driving Using Task-Agnostic and Ego- Centric Motion Skills. In 2023 IEEE/RSJ In- ternational Conference on Intelligent Robots and Systems (IROS), pages 11289–11296, Oct. 2023. doi:10.1109/IROS55552.2023.10341449. 16 --- Page 17 --- Appendices Algorithm 3 Hierarchical RL with Combined Options Initialize critic models θ ,θ′ q,i q,i A Algorithms B ←∅ for each episode do Below we provide the concrete implementations of the for each environment step t do generic Algorithm 1 for hierarchical reinforcement learning if t=0 or s ∈T ∩T then ▷ ϵ-greedy β withsingleoptions(Algorithm2),combinedoptions(Algo- with probt abilito yv,t ϵ−1 do od,t−1 m rithm 3) and hybrid options (Algorithm 4). (o ,o )∼U[Oav(s )×Oav(s )] v,t d,t v t d t else Algorithm 2 Hierarchical RL with Options (o ,o )= argmax q(s ,(o ,o );θ ) v,t d,t t v d q,1 Initialize critic models θ ,θ′ ov∈Ovav(s t),od∈Odav(s t) q,i q,i end B ←∅ else if s ∈T then for each episode do t ov,t−1 o =o for each environment step t do d,t d,t−1 with probability ϵ do if wt i= th0 po rr os bt ab∈ ilT ito yt− ϵ1 dth oen ▷ Activat ▷e ϵn -ge rw eeo dp yti βon o v,t ∼U[O vav(s t)] m else o ∼U[Oav(s )] t t o =argmax q(s ,(o ,o );θ ) else v,t ov∈Ovav(s t) t v d,t q,1 end o =argmax q(s ,o;θ ) endt o∈Oav(s t) t q,1 else if s t ∈T od,t−1 then o =o else v,t v,t−1 with probability ϵ do o t =o t−1 o ∼U[Oav(s )] end if d,t d t else a =π (s) s tt +1 ∼o τt (·|s t,a t) eno dd,t =argmax od∈Odav(s t)q(s t,(o v,t,o d);θ q,1) r =r(s ,a ,s ) t t t t+1 else B ←B∪{(s ,o ,r ,s )} t t t t+1 (o ,o )=(o ,o ) end for v,t d,t v,t−1 d,t−1 end if for each gradient step do (cid:20) 1 0(cid:21) (cid:20) 0 0(cid:21) Sample batch B from B a t = 0 0 π ov,t(s)+ 0 1 π od,t(s) for all (s ,o ,r ,s )∈B do t t t t+1 s ∼τ(·|s ,a ) t+1 t t if s ∈T then ▷ Determine next active option ot+1 =aro gt max q(s ,o′;θ ) r t =r(s t,a t,s t+1) t+1 o′∈Oav(s t+1) t+1 q,1 B ←B∪{(s t,(o v,t,o d,t),r t,s t+1)} else end for o =o for each gradient step do t+1 t end if Sample batch B from B y t =r t+γmin i∈{1,2}q(s t+1,o t+1;θ q′ ,i) for all (s t,(o v,t,o d,t),r t,s t+1)∈B do end for if s ∈T ∩T then ▷ π L(cid:98)q(θ q,i;B)= |B1 |(cid:80) B(cid:0) q(s t,o t;θ q,i)−y t(cid:1)2 (ot v+ ,t1 +1,o dov ,t,t +1)=od,t m θ θq m′,i ←← τθ θq m,i− +η (q 1∇ −θq τ,i )L θ(cid:98) m′q(θ q,i;B) o′ v∈Ovav(sa t+rg 1)m ,o′ da ∈x Odav(s t+1)q(s t+1,(o′ v,o′ d);θ q,1) end for else if s ∈T then t+1 ov,t end for o =o d,t+1 d,t o =argmax q(s ,(o′,o );θ ) v,t+1 o′ v∈Ovav(s t+1) t+1 v d,t+1 q,1 else if s ∈T then t+1 od,t o =o v,t+1 v,t o =argmax q(s ,(o ,o′);θ ) d,t+1 o′ d∈Odav(s t+1) t+1 v,t+1 d q,1 else (o ,o )=(o ,o ) v,t+1 d,t+1 v,t d,t end if y =r +γmin q(s ,(o ,o );θ′ ) t t i∈{1,2} t+1 v,t+1 d,t+1 q,i end for L(cid:98)q(θ q,i;B)= |B1 |(cid:80) B(cid:0) q(s t,(o v,t,o d,t);θ q,i)−y t(cid:1)2 θ q,i ←θ q,i−η q∇ θq,iL(cid:98)q(θ q,i;B) θ′ ←τθ +(1−τ)θ′ m m m end for end for 17 --- Page 18 --- Algorithm 4 Hierarchical RL with Hybrid Options Initialize actor and critic models θ ,θ′,θ ,θ′ µ µ q,i q,i B ←∅ for each episode do for each environment step t do ∆v˜ ∼N1 (µ(s ;θ );σ2) ▷ Truncated Gaussian t −1 t µ ϵ β m,c if t=0 or s ∈T then t od,t−1 with probability ϵ do ▷ ϵ-greedy β m,d o ∼U[Oav(s )] d,t d t else o =argmax q(s ,(∆v˜,o );θ ) d,t od∈Odav(s t) t t d q,1 end else o =o d,t d,t−1 end if ∆v =σ (∆v˜;s ) ▷ State-dependent bounds [10] t pwl t t (cid:2) (cid:3) ∆d = 0 1 π (s ) t (cid:2) (cid:3)od,t t a = ∆v ;∆d t t t s ∼τ(·|s ,a ) t+1 t t r =r(s ,a ,s ) t t t t+1 B ←B∪{(s ,(∆v˜,o ),r ,s )} t t d,t t t+1 end for for each gradient step do Sample batch B from B for all (s ,(∆v˜,o ),r ,s )∈B do t t d,t t t+1 ∆v˜ =µ(s ;θ′) ▷ π′ t+1 t+1 µ m,c ϵ ∼N(0,σ2)| ▷ Target policy c c [−c,c] smoothing [12] ∆v˜ ←∆v˜ +ϵ t+1 t+1 c if s ∈T then ▷ π t+1 od,t m,d o = argmax q(s ,(∆v˜ ,o′);θ ) d,t+1 t+1 t+1 d q,1 o′ d∈Odav(s t+1) else o =o d,t+1 d,t end if y =r +γmin q(s ,(∆v˜ ,o );θ′ ) t t i∈{1,2} t+1 t+1 d,t+1 q,i end for L(cid:98)q(θ q,i;B)= |B1 |(cid:80) B(cid:0) q(s t,(∆v˜ t,o d,t);θ q,i)−y t(cid:1)2 L(cid:98)µ(θ µ;B)=− |B1 |(cid:80) (cid:80) q(s t,(µ(s t;θ µ),o d);θ q,1) B od∈Odav(s t) R(cid:98)s(θ µ;B)= |B1 |(cid:80) B(cid:0) µ(s t+1;θ µ)−∆v˜ t(cid:1)2 ▷ Smoothness regularization [9] θ q,i ←θ q,i−η q∇ θq,iL(cid:98)q(θ q,i;B) (cid:0) (cid:1) θ µ ←θ µ−η µ∇ θµ L(cid:98)µ(θ µ;B)+λ sR(cid:98)s(θ µ;B) θ′ ←τθ +(1−τ)θ′ Figure 7: Evaluation of the selected policies in various traffic m m m end for conditions and comparison with a conservative IDM/MOBIL end for policy. Solid lines represent the mean value over 10 episodes, the shaded areas denote the minimum and maximum values. 18 --- Page 19 --- Figure 8: Option activity for the selected policies over options during E = 10 evaluation episodes. The percentages denote the fraction of time spent with a certain option active. B Experimental Results ward signal for activating this emergency option. Results for experiments with such a reward modification are how- Some additional test metrics for the selected policies are ever not shown in this work to make a fair comparison be- shown in Figure 7, which further supports the observations tweenthedifferentapproacheseasier(usingtheexactsame madeinSection5. Inparticular, therewardsandfollowing reward for all experiments). distances of all policies are quite similar and roughly fol- low the same trends. In more dense traffic conditions, the C Hyperparameters rightmost (slower) lane is used less frequently by the base- line policy over continuous actions and master policy over Table2belowshowstheusedhyperparametersfortheper- hybrid options. This confirms their slight outperformance formed experiments. comparedtootherapproachesinmorecongestedtrafficsit- uations. Finally, due to the use of constrained options for Table 2: Overview of the used hyperparameters. lateral control, the hierarchical setups achieve better align- ment within the lane with almost no deviation from lane Hyperparameters Values center. Maximumepisodelength T 5000 Figure8showstheactivityofoptionsduringevaluationof (truncation) Totaltrainingepisodes 300 the selected (best performing) policies. These plots can be Warmuptimesteps 6400 usedtoanalyzethelearneddrivingbehaviourinsomemore Replaybuffersize |B| 1000000 detail, and thus increase the interpretability of the learned Batchsize |B| 64 modelsascomparedtothecontinuousbaselinemodels. For Discountfactor γ 0.99 example, we can observe that the policy over combined op- Learningrates(strides) ηq 5·10−4(1) ηµ 5·10−4(2) tions effectively uses the flexibility of combining longitudi- Targetnetworks nal and lateral manoeuvres, as it actively chooses to alter averagingconstant τ 1·10−3(2) the vehicle’s velocity during lane changes ∼ 50% of the (stride) Networkarchitecture– (critic) 64×32 time. Performing such an analysis or determining which hiddendimensions (actor) 32×16×8 manoeuvres are being performed, is much harder for poli- cies over continuous actions. In fact, only the immediate reference signals selected by a policy over continuous ac- tions are known at any moment in time. In contrast, for the policies over options the exact manoeuvre selected by the master policy is known at every moment in time (and can be determined for any possible traffic situation on the road). Remark that for lateral control, the maintain and emer- gencyoptionsbothprovidethesameactiontomaintainthe lateral position (unless this would be unsafe). It seems the selected master policies have learned a slight preference for the (lateral) emergency option, which is unintended. This can be easily resolved by adding a small penalty to the re- 19