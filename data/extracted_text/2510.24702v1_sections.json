{
  "abstract": "ABSTRACT Public research",
  "results": "results across benchmarks. ShadedrowsareourADP-tunedmodels. 6.1 ADPDATARESULTSINHIGHLYEFFECTIVEAGENTSACROSSDIVERSETASKS ADP fine-tuning consistently improves performance across models, benchmarks, and agent harnesses. As shown in Table 3, Table 4, and Table 5, training on standard- ized ADP data yields substantial gains across 7B, 14B, and 32B models on several popu- lar evaluation benchmarks. On SWE-Bench (Verified), ADP training delivers remarkable im- provements: Qwen-2.5-7B-Coder-Instruct improves from 0.4% to 20.2% (+19.8%) with SWE-Agent and from 2.8% to 20.4% (+17.6%) with OpenHands. At 14B scale, Qwen-2.5-14B-Coder-Instruct achieves 34.4% (+32.4%) with SWE-Agent and 30.6% (+24.8%)withOpenHands. The32Bmodelreaches40.3%(+38.1%)withSWE-Agentand36.8% (+26.2%) with OpenHands, matching or exceeding Claude 3.5 Sonnet with SWE-Agent\u2019s 33.6% performance. OnWebArena,ADPtrainingshowsconsistentgainsacrossmodelsizes: 7Bachieves 21.0%(+16.5%),14Breaches22.2%(+16.7%),and32Battains22.9%(+12.0%). OnAgentBench OS,theimprovementsaresubstantial: the7Bmodelimprovesfrom3.5%to27.1%(+23.6%), the 14Bmodelimprovesfrom2.8%to20.8%(+18.0%),and32Bmodelsfrom27.8%to34.7%(+6.9%). Finally,onGAIA,the7Bmodelimprovesfrom7.3%to9.1%(+1.8%). 45 40 35 30 25 20 15 10 5 0 7B 14B 32B Model Size (Billions of Parameters) )%( ecnamrofreP 45 ADP-Trained Models SWE-Agent ADP (SWE-Bench) OpenHands ADP (SWE-Bench) 40.3% 40 AgentLab ADP (WebArena) 36.8% 35 34.4% 30.6% 30 22.2% 22.9% 25 20.4%21.0% 20.2% 20 Base Models 15 SWE-Agent Base (SWE-Bench) 10.9% OpenHands Base (SWE-Bench) AgentLab Base (WebArena) 10.6% 10 5.5% 4.5% 5.8% 5 2.0% 2.2% 2.08.%4% 0 7B 14B 32B Model Size (Billions of Parameters) Figure 3: Performance Scaling Across Agents andBenchmarks(BasevsADPTrained) )%( secnamrofreP deniarT-PDA dna esaB neewteB atleD Models SWE-Agent ADP (SWE-Bench) OpenHands ADP (SWE-Bench) AgentLab ADP (WebArena) 38.1% 32.4% 26.2% 24.8% 17.6%1 19 6. .8 5% % 16.7% 12.0% Figure 4: Performance Gains Across Agents andBenchmarks. These gains, spanning both coding and browsing settings, show that a unified, cross-domain ADP training corpus can deliver SOTA or near-SOTA performance without domain-specific tuning and is effective across models, action spaces, and agent harnesses. Figure 3 and Figure 4 show clear monotonicgainswithmodelsizeandconsistentboostsfromADPtrainingacrossagentsandtasks, withADP-trainedmodelsoutperformingtheirbasecounterpartsateveryscale. 9 --- Page 10 --- arXivSubmission 6.2 DIVERSEDATARESULTSINCROSS-TASKTRANSFER Agent Model TrainingData Accuracy SWE-Bench(Verified)(Jimenezetal.,2024;Chowdhuryetal.,2024) OpenHandsCodeActAgent Qwen-2.5-7B-Instruct SWE-smithOnly 1.0% (Wangetal.,2025) Qwen-2.5-7B-Instruct ADPData 10.4% Qwen-3-8B CodeActInstruct+Code-Feedback 0.2% Qwen-3-8B SWE-smithOnly 11.0% Qwen-3-8B ADPData 16.6% WebArena(Zhouetal.,2024) AgentLab(Drouinetal.,2024) Qwen-2.5-7B-Instruct Go-BrowseOnly 16.0% (deChezellesetal.,2025) Qwen-2.5-7B-Instruct ADPData 20.1% AgentBenchOS(Liuetal.,2024b) OpenHandsCodeActAgent Qwen-3-8B AgentInstructOnly 21.5% (Wangetal.,2025) Qwen-3-8B ADPData 25.7% GAIA(Mialonetal.,2023) OpenHandsCodeActAgent Qwen-2.5-7B-Instruct AgentInstructOnly 0.6% (Wangetal.,2025) Qwen-2.5-7B-Instruct ADPData 9.1% Table 6: Cross-task transfer with diverse vs. task-specific data. For each benchmark, we compare thesameharness+modelundertask-specific\u201conly\u201dtuningandtrainingonADPcorpus. Westudywhetherdatadiversityhelpsagentsgeneralizeacrosstasks. Holdingtheagentsetupand evaluationfixed, wecomparetrainingwithdifferentdatamixtures: (i)Base(notuning), (ii)Task- specific only fine-tuning (e.g., SWE-smith Only, etc.), and (iii) ADP Data (as detailed in \u00a7 5), a mixed, cross-domain corpus. As shown in Table 6, ADP consistently outperforms task-specific tuningonthetargettaskand,critically,avoidsthenegativetransferthatsingle-domaintuning ofteninducesonothertasks(Muelleretal.,2024;Kothaetal.,2024;Lietal.,2024). Concretely,onSWE-Bench,trainingQwen-2.5-7B-InstructwithADPdataachieves10.4%, versus 1.0% with SWE-smith Only; for Qwen-3-8B, ADP reaches 16.6% versus 0.2% with CodeActInstruct + Code-Feedback and 11.0% with SWE-smith Only. On WebArena, ADP trained Qwen-2.5-7B-Instructattains20.1%comparedto16.0%withGo-BrowseOnly. OnAgent- Bench OS, ADP lifts Qwen-3-8B to 25.7% versus 21.5% with AgentInstruct Only. Finally, on GAIA,AgentInstructOnlyresultsin0.6%accuracy,whileADPimprovesitto9.1%.Overall,mixed ADP training yields better in-domain accuracy and stronger cross-task generalization than single- domaintuning. 6.3 ADPEASESADAPTATIONTONEWAGENTHARNESSES Table7: LOCforconvertingdatasetstoADP. Table7demonstratesthelinesofcode(LOC)3 the authors and community contributors used Dataset TotalLOC to convert 13 datasets from distinct sources AgentInstruct \u223c1500 to the ADP schema. A single Raw\u2192ADP Code-Feedback 134 converter per dataset performs the same nor- CodeActInstruct 269 malization work (schema mapping, tool/action Go-Browse 335 alignment, conversation formatting) that a tra- Mind2Web 476 NebiusSWE-AgentTrajectories 260 ditional Raw\u2192SFT converter would do for a NNetNav(live+wa) 290 specific agent harness. Therefore, LOC statis- openhands-feedback 879 tics in Table 7 are a reasonable proxy for the OrcaAgentInstruct 155 per-agentharnesseffortwithoutADP. SWE-Gym 221 SWE-smith 228 Without ADP. Using this proxy, the cost of Synatra 145 converting D-many datasets to A-many har- nesses without ADP is Cost (A,D) \u2248 Total 4892 no-ADP A\u00b7(cid:80)D LOC . Thus the total con- i=0 i,Raw\u2192ADP versioncostacrossthecommunityisquadratic(O(D\u00d7A)effort),asdepictedinFigure2. Inour 3AllLOCexcludeprompttext(e.g.,systemprompts);onlyconvertercodeiscounted. 10 --- Page 11 --- arXivSubmission data,(cid:80)D LOC =4892LOCacross13datasets,soforA=100harnessesthetotalcost i=0 i,Raw\u2192ADP isCost \u2248100\u00d74892=489,200LOC. no-ADP With ADP. The total cost becomes Cost (A,D) \u2248 ADP Table8:LOCforADP\u2192SFTconverters. (cid:80)D LOC + (cid:80)A LOC with AgentHarness TotalLOC i=0 i,Raw\u2192ADP j=0 ADP\u2192SFT,j ADP. Thus, as shown in Figure 2, the total conver- OpenHandsCodeActAgent \u223c150 sion cost across the community now becomes linear SWE-Agent \u223c50 with ADP (O(D + A) effort). Table 8 demonstrates AgentLab \u223c30 thatconvertingADPstandardizeddatatoagentharness Average \u223c77 format takes an average of 77 LOC. For A = 100, Cost (A,D) \u2248 4892+77\u00d7100 = 12,592across ADP the 13 datasets we used, greatly less than the no-ADP setting. Additionally, adding a new harness only require writing one script converting ADP stan- dardizeddatatoSFT,greatlyeasingadaptationtonewagentharnesses. Hence,ADPsubstantially reducesthecommunity\u2019scollectiveeffortrequiredtodevelopscalable,reproducibleagents. 7",
  "introduction": "1 INTRODUCTION Pre-traininglargelanguagemodels(LLMs)benefitsfromabundant,readilyavailableInternet-scale data. In contrast, post-training presents a much harder challenge: high-quality task-specific data must be carefully curated. While creative strategies have emerged for collecting data in relatively simple settings, such as single-turn user interactions like code generation (Nijkamp et al., 2023), questionanswering(Rajpurkaretal.,2016),andsentimentanalysis(Maasetal.,2011),manyreal- worldtasksarefarmorecomplex. A particularly difficult case is agent applications, where models must take sequential actions and interactwiththeworlditeratively. Buildingdatasetsforsuchscenariosrequiresrecordingandstruc- turingtrajectoriesofagentbehavior,muchmorechallengingthancollectingstaticinput-outputpairs. Despite these difficulties, a growing body of work has explored different approaches for creating agentdatasets. Theseeffortsvaryinmethodology, frommanualcuration(Rawlesetal.,2023;Xu etal.,2024a),tosyntheticdatageneration(Ouetal.,2024;Zhengetal.,2024a),torecordedagent rollouts (Pan et al., 2025; Yang et al., 2025b). The resulting datasets span a wide range of tasks, including web navigation (Deng et al., 2023; Lu` et al., 2024), software development (Yang et al., 2025b; Pan et al., 2025), visual interface control (Rawles et al., 2023; Kapoor et al., 2024), and generaltooluse(Zengetal.,2023;Liuetal.,2024a)(anoverviewofthesedatasetsin\u00a72.1). 1 5202 tcO 82 ]LC.sc[ 1v20742.0152:viXra --- Page 2 --- arXivSubmission Raw Data Agent Data Protocol OpenHands SFT \u2756 AgentInstruct Action Observation \u2756 CodeActInstruct SWE Agent SFT \u2756 SWE-Gym \u27a2 API Action \u27a2 Text Observation \u2756 Mind2Web \u27a2 Code Action \u27a2 Web Observation AgentLab SFT \u2756 \u2026\u2026 \u27a2 Message Action APIAction ( goto(url=google.com) function=goto, WebObservation ( kwargs={url: google.com} url=google.com, <!doctype html> ) html=<html>..., <html itemscope axtree=RootWebArena \u2026 <title> Google </title> \u2026 </html> 'Google', focused \u2026 Trajectory ( CodeAction ( ) id=example_id, ```python language=python, content=[...] print(\"Hello World\") content=print(\"Hello World\") TextObservation ( ) ) ``` content=Hello World, Execution",
  "experiments": "experiments focus on each framework\u2019s specialized domain to demonstrate targeted effec- tiveness. Each agent has unique architectures, tool interfaces, and interaction environments. This diversity allows us to validate that ADP-standardized data can be readily and easily converted to differentagentformats,demonstratingtheprotocol\u2019sutilityacrossvariousagentimplementations. OpenHands(Wangetal.,2025)isanopenplatformforbuildinggeneralistAIagentsthatoperate like software developers: writing code, using command lines, and browsing the web. It provides sandboxedexecutionenvironments,toolcoordination,andbenchmarkevaluation. AgentLab (Drouin et al., 2024; de Chezelles et al., 2025) is an open-source framework for de- veloping, testing, and benchmarking web agents across diverse tasks, emphasizing scalability and reproducibility. ItsupportsasuiteofevaluationbenchmarkslikeWebArenaandWorkArena. SWE-Agent(Yangetal.,2024)introducesacustomAgent-ComputerInterface(ACI)thatenables language model agents to autonomously perform software engineering tasks by navigating code- bases,editingandrunningcode,viewingfiles,andexecutingtests. 5.2 EVALUATIONBENCHMARKS We evaluated these agents across 4 benchmarks (based on the availability of benchmark evalua- tioncodeandspecializationofagents)thatspandifferentdomains. Thiscomprehensiveevaluation demonstratesADP\u2019sexpressivenessinpreservingcriticalinformationacrossdiversetasks. SWE-Bench(Jimenezetal.,2024)evaluatesagentsonreal-worldsoftwareengineeringtasks.Given aGithubcodebaseandabugreport,agentsmustgeneratepatchesthatsatisfyexistingunittests. We usedtheSWE-BenchVerifiedsubsetforevaluation(Chowdhuryetal.,2024). WebArena(Zhouetal.,2024)providesarealistic,self-hostedwebenvironmentcomposedoffully functional websites in domains like e-commerce, forums, and map navigation, requiring agents to interprethigh-levelnaturallanguagecommandsandperformconcretewebinteractions. 7 --- Page 8 --- arXivSubmission AgentBench (Liu et al., 2024b) evaluates agents across different environments, such as operating systems, databases, and web browsing. It emphasizes multi-turn reasoning, decision making, and adaptabilityacrossdomains. GAIA (Mialon et al., 2023) is a benchmark for general AI assistants featuring human-annotated tasksthatcombinereasoning,tooluse,andmulti-stepproblemsolving,oftenwithmultimodalinput. Tasksvaryindifficultybynumberofstepsandrequiredtools. 6 EXPERIMENTAL",
  "related_work": "RELATED WORK The development of effective LLM-based agents critically depends on high-quality training data thatcapturesthecomplexityofmulti-stepreasoning,toolusage,andenvironmentalinteraction(Yao etal.,2022b;Schicketal.,2023;Dengetal.,2023;Mastermanetal.,2024). Thissectionreviews existingmethodsforagentdatacollectionandthechallengesthatmotivateouragentdataprotocol. 2.1 AGENTDATACOLLECTIONMETHODS Existingapproachesspanmanualcreation(humanexpertscreatingstep-by-stepdemonstrationsof desired agent behaviors) (Nakano et al., 2021; Yao et al., 2022a), synthetic generation (leverages existingLLMstocreateagenttrajectoriesthroughpromptingorstructuredgeneration)(Luoetal., 2023;Xuetal.,2024b),andrecordedagentrollouts(capturestrajectoriesfromexistingagentsys- temsduringtaskexecution)(Wangetal.,2024a;Panetal.,2025),etc,resultinginabundantagent trainingdata,arepresentativesetofwhichlistedinTable1. Table 1: Overview of Existing Agent Training Datasets. C=Coding, S=Software Engineering, T=API/ToolUse,W=WebBrowsing. Dataset Variety Count Source Note AgentInstruct(Zengetal.,2023) CTW 1.9K synthetic MixtureofBrowsing,Database,OS,etc. Code-Feedback(Zhengetal.,2024a) C 66.4K manual Codegenerationwithruntimefeedbackloops CodeActInstruct(Wangetal.,2024b) C 7.1K synthetic Codegenerationandtoolusewithexecution Go-Browse(Gandhi&Neubig,2025) W 9.5K rollout Structuredexplorationwebrollouts Mind2Web(Dengetal.,2023) W 1.0K manual Humanwebdemosonrealwebsites NebiusSWETrajectories S 13.4K rollout SWE-agent trajectories from Nebius relying (Golubevetal.,2024) solelyonopen-weightmodels NNetNav-live(Murtyetal.,2024) W 5.0K rollout Retroactivelylabeledlivewebexploration NNetNav-wa(Murtyetal.,2024) W 4.2K rollout RetroactivelylabeledWebArenaexploration openhands-feedback CTW 0.2K rollout RecordedOpenHandsagenttrajectorieswithhu- (AllHandsAI,2024) manfeedback OrcaAgentinstruct(Mitraetal.,2024) T 1046.1K synthetic Large-scalesynthetictool-useinstructionsdata SWE-Gym(Panetal.,2025) S 0.5K rollout AgenttrajectoriessolvingrealGitHubrepotasks SWE-smith(Yangetal.,2025b) S 5.0K manual Trajectoriesofagentsonsynthesizedbug-fixtasks Synatra(Ouetal.,2024) W 99.9K rollout Syntheticallycreatedwebdemosoftutorials Wealsogroupeachdatasetintoacoarsetaskcategory. \u2022 Coding: generallyincludesfundamentalprogrammingtasks,suchascommandlinecodegenera- tion,algorithmimplementation,codecompletion,codetranslation,andcoderepair,etc. \u2022 SoftwareEngineering: oftenconsistsofrepository-levelsoftwareengineeringtasks,suchasbug fixing,featureimplementation,coderefactoring,anddependencymanagement,etc. \u2022 API/ToolUse: usuallyrequiresagentstouseexternalAPIs/toolseffectivelytosolvetasks. Com- montoolsincludefilemanipulation,databasequeries,andcustomizedAPIs,etc. \u2022 Web Browsing: commonly encompasses tasks including web navigation, online shopping, and socialmediainteractions,etc,requiringagentstounderstandGUIs. 2.2 CHALLENGESANDLIMITATIONS Despiteabundantexistingagenttrainingdatasets,severalfundamentalchallengespreventeffective large-scaleutilizationoftheseresources: \u2022 ComplexityofDataCuration: Creationofhigh-qualityagenttrainingdatarequiressignificant resources and expertise (Paullada et al., 2021; Bhardwaj et al., 2024; Zha et al., 2025). Manual curation is expensive and requires domain knowledge; synthetic generation faces challenges in verifying data quality; recorded agent rollouts are fundamentally constrained by the capabilities of existing baseline agents, limiting the diversity and complexity of trajectories. Each approach requires significant time and investment. While recent efforts have scaled trajectory collection 3 --- Page 4 --- arXivSubmission (Songetal.,2024;Mitraetal.,2024), thefundamentalchallengeofbalancingquality, diversity, andscaleacrossdifferentcurationapproachesremains. \u2022 HeterogeneityofDatasetformat:Existingagenttrainingdatasetseachemployitsownrepresen- tationformat,actionspaces,andobservationstructures(Ningetal.,2025;Luoetal.,2025). For example,somewebdatasetsuseHTMLwhilesomeuseaccessibilitytreestructures(deChezelles etal.,2025). Existingeffortshavenotedandbegunaddressingdatastandardization(Zhangetal., 2024; Chen et al., 2024; Mohammadi et al., 2025; Xi et al., 2025; Zhang et al., 2025), but they mostly focused on proposing task-specific or agent-specific unification rather than community- widestandardizationofdatarepresentation,limitingplug-and-playwithotherdatasetsoragents, wheresignificantengineeringeffortisstillrequiredtoutilizemultipledatasetstogether,hindering integrationacrossdifferentdatasources. \u2022 DifficultyofAnalysisandComparison: Thediversestructuresofexistingdatasetsalsomakesit difficulttoperformsystematiccomparisonsorquantitativeanalysisacrossdifferentdatasources (Putrama & Martinek, 2024), limiting researchers\u2019 ability to understand the relative usefulness, coverage,andqualityofdifferentdatasets,hinderingdata-drivenselectionorimprovements. 3 THE AGENT DATA PROTOCOL To overcome these challenges and limitations, and to make good use of existing data resources, weproposetheagentdataprotocol(ADP).ADPestablishesaunifiedschemathatbridgesthegap betweenexistingheterogeneousagenttrainingdatasetsandlarge-scalesupervisedagentfine-tuning. 3.1 DESIGNPRINCIPLES WedesignADParoundthefollowingcoreprinciples: \u2022 Simplicity:ADPmaintainsasimpleandintuitivestructure.Thisdirectlyaddressesthecomplexity of data curation challenge by providing a straightforward framework that eliminates the need for specialized per-dataset engineering, making large-scale agent data utilization accessible to researcherswithoutextensiveadaptationeffort. \u2022 Standardization: ADPisdesignedtoprovideaunifiedrepresentationthatunifiesexistingagent trainingdatasetsofvariousdifferentformatstoastandardizedformat,addressingthechallengeof heterogeneousdatasetformats. \u2022 Expressiveness: ADPisdesignedtoensurethatcomplexagentictrajectoriescouldbeaccurately expressedwithnolossofcriticalinformation.Thisdirectlyaddressesthedifficultyofanalysisand comparison challenge because ADP is expressive enough to cover the broad variety of existing agent datasets across different domains, enabling researchers to put these diverse datasets under thesameconditionsandcontext. Byaddressingthefundamentalchallengesinutilizationagentdata,ADPaimstopushtheprogress inagenttraining,makinglarge-scaleagentSFTmoreaccessibletothebroaderresearchcommunity. 3.2 ARCHITECTURE TheADPschemaisimplementedasPydanticschemas,andissimpleyetexpressiveindesign. Each ADPstandardizedagenttrajectoryisrepresentedasaTrajectoryobject. Trajectory consists of (1) id: trajectory id, (2) content: an alternating sequence of actions andobservationsrepresentingtheagent\u2019sinteractionwiththeuser/environment, (3)details: A flexiblemetadatadictionaryfordataset-specificinformation(e.g.,datasetsourceURLs). Actionsrepresentagents\u2019decisionsandbehaviors. Wecategorizeactionsintothreetypes: \u2022 APIActions: Functioncallswithstructuredparametersandoutputscapturingtooluse. EachAPI actionincludes: (1)function: nameoftoolcall, (2)kwargs: adictionaryoffunctionargu- ments, and (3) description: optional reasoning or explanation for the action. For example, withADP,awebnavigationcallgoto(url= APIAction(function=goto, kwargs=url: 4 --- Page 5 --- arXivSubmission \u2022 CodeActions: Codegenerationandexecutionacrossprogramminglanguages. Eachcodeaction specifies: (1)language: theprogramminglanguage(e.g.,python),(2)content: thecodeto execute,and(3)description: optionalreasoningorexplanationfortheaction. Forexample, theADPrepresentationofapythoncodeblock\u2018\u2018\u2018python print(\"Hello World\")\u2018\u2018\u2018 isCodeAction(language=python,content=print(\"Hello World\"). \u2022 MessageActions: Naturallanguagecommunicationsbetweenagentsandusers,eachcontaining acontentfield,documentingagents\u2019explanations,clarifications,andresponses. Forexample, MessageAction(content=How can I help you?). Observationsrepresentagents\u2019perceptionsfromtheenvironment,categorizedintotwotypes: \u2022 Text Observations: Captures the text information from various sources, including user instruc- tions and environmental feedback. Each text observation includes: (1) source: the origin of theobservation(\u201cuser\u201dor\u201cenvironment\u201d),and(2)content: theobservedtext. Forexample,a pythonexecutionoutputExecution",
  "conclusion": "CONCLUSION AND FUTURE WORK ADP provides a practical, lightweight \u201cinterlingua\u201d that unifies heterogeneous agent datasets into a single schema consumable by many agent harnesses, turning today\u2019s fragmented data landscape into a scalable training pipeline. Looking ahead, we see three immediate directions. (i) Multi- modality:extendingADPbeyondtexttoimages,screenrecordings,andothermodalitiestocapture richeragent\u2013environmentinteractions. (ii)Standardizedevaluationartifacts: applyingthesame standardized \u201cprotocol\u201d idea to evaluation and environment settings so that datasets, agents, and evaluationscomposecleanly. (iii)Communitygrowthanddataquality: continuingopen-source releases,strongerautomatedvalidationorevenautomateddatasetconversion,tosustainscalewhile preservingquality. Webelievethat,byloweringintegrationcostsandenablingsystematicandscal- abletrainingandanalysisacrosssources,ADPcancatalyzethenextwaveofagent-trainingresearch andpractice. REPRODUCIBILITY STATEMENT. Weprovideclearpointerstoenableindependentreproductionofallresults. WedescribetheADP schema and conversion pipeline (\u00a7 3), allowing others to regenerate the training corpus from raw sources. We list the datasets and their characteristics in \u00a7 2.1. The exact training and evaluation setup-including base models, agent harnesses, our SFT pipeline, the evaluation benchmarks and protocol-is specified in \u00a7 5. Finally, we will release all code and data open source, including the ADPschemas,converters,andscriptsreferencedabove.",
  "references": "REFERENCES All Hands AI. Openhands feedback dataset.  all-hands/openhands-feedback,2024. Anthropic Team. The claude 3 model family: Opus, sonnet, haiku. URL  semanticscholar.org/CorpusID:268232499. Eshta Bhardwaj, Harshit Gujral, Siyi Wu, Ciara Zogheib, Tegan Maharaj, and Christoph Becker. Machinelearningdatapracticesthroughadatacurationlens: Anevaluationframework. FAccT \u201924, pp.1055\u20131067, NewYork, NY,USA,2024.AssociationforComputingMachinery. ISBN 9798400704505. doi: 10.1145/3630106.3658955. URL  3630106.3658955. Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. Agent-FLAN: Designing data and",
  "methods": "methods of effective agent tuning for large languagemodels. InLun-WeiKu,AndreMartins,andVivekSrikumar(eds.),FindingsoftheAs- sociationforComputationalLinguistics: ACL2024,pp.9354\u20139366,Bangkok,Thailand,August 2024.AssociationforComputationalLinguistics. doi: 10.18653/v1/2024.findings-acl.557. URL  11 --- Page 12 --- arXivSubmission NeilChowdhury,JamesAung,ChanJunShern,OliverJaffe,DaneSherburn,GiulioStarace,Evan Mays,RachelDias,MarwanAljubeh,MiaGlaese,etal.Introducingswe-benchverified.https: //openai.com/index/introducing-swe-bench-verified,2024. Thibault Le Sellier de Chezelles, Maxime Gasse, Alexandre Lacoste, Massimo Caccia, Alexan- dre Drouin, Le\u00b4o Boisvert, Megh Thakkar, Tom Marty, Rim Assouel, Sahar Omidi Shayegan, Lawrence Keunho Jang, Xing Han Lu`, Ori Yoran, Dehan Kong, Frank F. Xu, Siva Reddy, Gra- ham Neubig, Quentin Cappart, Russ Salakhutdinov, and Nicolas Chapados. The browsergym ecosystem for web agent research. Transactions on Machine Learning Research, 2025. ISSN 2835-8856. URL ExpertCertifi- cation. XiangDeng,YuGu,BoyuanZheng,ShijieChen,SamStevens,BoshiWang,HuanSun,andYuSu. Mind2web: Towardsageneralistagentfortheweb. AdvancesinNeuralInformationProcessing Systems,36:28091\u201328114,2023. Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty, David Vazquez, Nicolas Chapados, and Alexandre Lacoste. WorkArena: How capa- ble are web agents at solving common knowledge work tasks? In Ruslan Salakhutdinov, Zico Kolter, KatherineHeller, AdrianWeller, NuriaOliver, JonathanScarlett, andFelixBerkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 11642\u201311662. PMLR, 21\u201327 Jul 2024. URL  ApurvaGandhiandGrahamNeubig. Go-browse: Trainingwebagentswithstructuredexploration. arXivpreprintarXiv:2506.03533,2025. Alexander Golubev, Sergey Polezhaev, Karina Zainullina, Maria Trofimova, Ibragim Badert- dinov, Yury Anapolskiy, Daria Litvintseva, Simon Karasik, Filipp Fisin, Sergey Skvortsov, Maxim Nekrashevich, Anton Shevtsov, Sergey Abramov, and Boris Yangel. Leverag- ing training and search for better software engineering agents. Nebius blog, 2024.  MengkangHu,YuhangZhou,WendongFan,YuzhouNie,BoweiXia,TaoSun,ZiyuYe,Zhaoxuan Jin,YingruLi,QiguangChen,ZeyuZhang,YifengWang,QianshuoYe,BernardGhanem,Ping Luo, and Guohao Li. Owl: Optimized workforce learning for general multi-agent assistance in real-worldtaskautomation,2025. URL CarlosEJimenez,JohnYang,AlexanderWettig,ShunyuYao,KexinPei,OfirPress,andKarthikR Narasimhan. SWE-bench:Canlanguagemodelsresolvereal-worldgithubissues? InTheTwelfth InternationalConferenceonLearningRepresentations,2024. URL net/forum?id=VTF8yNQM66. RaghavKapoor,YashParagButala,MelisaRussak,JingYuKoh,KiranKamble,WaseemAlShikh, andRuslanSalakhutdinov.Omniact:Adatasetandbenchmarkforenablingmultimodalgeneralist autonomousagentsfordesktopandweb. InEuropeanConferenceonComputerVision,pp.161\u2013 178.Springer,2024. SuhasKotha,JacobMitchellSpringer,andAditiRaghunathan. Understandingcatastrophicforget- tinginlanguagemodelsviaimplicitinference.InTheTwelfthInternationalConferenceonLearn- ingRepresentations,2024. URL HongyuLi,LiangDing,MengFang,andDachengTao. Revisitingcatastrophicforgettinginlarge languagemodeltuning.InYaserAl-Onaizan,MohitBansal,andYun-NungChen(eds.),Findings oftheAssociationforComputationalLinguistics:EMNLP2024,pp.4297\u20134308,Miami,Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. findings-emnlp.249. URL  249/. ShilongLiu,HaoCheng,HaotianLiu,HaoZhang,FengLi,TianheRen,XueyanZou,JianweiYang, Hang Su, Jun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. In Europeanconferenceoncomputervision,pp.126\u2013142.Springer,2024a. 12 --- Page 13 --- arXivSubmission Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, KaiwenMen, KejuanYang, ShudanZhang, XiangDeng, AohanZeng, ZhengxiaoDu, Chenhui Zhang,ShengShen,TianjunZhang,YuSu,HuanSun,MinlieHuang,YuxiaoDong,andJieTang. Agentbench: EvaluatingLLMsasagents. InTheTwelfthInternationalConferenceonLearning Representations,2024b. URL XingHanLu`,Zdene\u02c7kKasner,andSivaReddy. Weblinx:Real-worldwebsitenavigationwithmulti- turndialogue. arXivpreprintarXiv:2402.05930,2024. Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen,ZiyueQiao,QingqingLong,etal. Largelanguagemodelagent:Asurveyonmethodology, applicationsandchallenges. arXivpreprintarXiv:2503.21460,2025. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma,QingweiLin,andDaxinJiang. Wizardcoder: Empoweringcodelargelanguagemodelswith evol-instruct. arXivpreprintarXiv:2306.08568,2023. Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learningwordvectorsforsentimentanalysis. InProceedingsofthe49thAnnualMeeting oftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,pp.142\u2013150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http: //www.aclweb.org/anthology/P11-1015. Tula Masterman, Sandi Besen, Mason Sawtell, and Alex Chao. The landscape of emerging ai agent architectures for reasoning, planning, and tool calling: A survey. arXiv preprint arXiv:2404.11584,2024. Gre\u00b4goire Mialon, Cle\u00b4mentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations,2023. ArindamMitra,LucianoDelCorro,GuoqingZheng,ShwetiMahajan,DanyRouhana,AndresCo- das,YadongLu,Wei-geChen,OlgaVrousgos,CorbyRosset,etal. Agentinstruct: Towardgen- erativeteachingwithagenticflows. arXivpreprintarXiv:2407.03502,2024. MahmoudMohammadi,YipengLi,JaneLo,andWendyYip. Evaluationandbenchmarkingofllm agents:Asurvey. InProceedingsofthe31stACMSIGKDDConferenceonKnowledgeDiscovery andDataMiningV.2,pp.6129\u20136139,2025. DavidMueller,MarkDredze,andNicholasAndrews.Multi-tasktransfermattersduringinstruction- tuning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Associa- tion for Computational Linguistics: ACL 2024, pp. 14880\u201314891, Bangkok, Thailand, August 2024.AssociationforComputationalLinguistics. doi: 10.18653/v1/2024.findings-acl.883. URL  Shikhar Murty, Hao Zhu, Dzmitry Bahdanau, and Christopher D Manning. Nnetnav: Unsuper- vised learning of browser agents through environment interaction in the wild. arXiv preprint arXiv:2410.02907,2024. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo- pherHesse,ShantanuJain,VineetKosaraju,WilliamSaunders,etal. Webgpt: Browser-assisted question-answeringwithhumanfeedback. arXivpreprintarXiv:2112.09332,2021. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, andCaimingXiong. Codegen: Anopenlargelanguagemodelforcodewithmulti-turnprogram synthesis. ICLR,2023. Liangbo Ning, Ziran Liang, Zhuohang Jiang, Haohao Qu, Yujuan Ding, Wenqi Fan, Xiao-yong Wei, Shanru Lin, Hui Liu, Philip S Yu, et al. A survey of webagents: Towards next-generation ai agents for web automation with large foundation models. In Proceedings of the 31st ACM SIGKDDConferenceonKnowledgeDiscoveryandDataMiningV.2,pp.6140\u20136150,2025. 13 --- Page 14 --- arXivSubmission Tianyue Ou, Frank F. Xu, Aman Madaan, Jiarui Liu, Robert Lo, Abishek Sridhar, Sudipta Sen- gupta,DanRoth,GrahamNeubig,andShuyanZhou. Synatra: Turningindirectknowledgeinto direct demonstrations for computer agents at scale. In Conference on Neural Information Pro- cessing Systems (NeurIPS), Vancouver, BC, December 2024. URL  abs/2409.15637. JiayiPan,XingyaoWang,GrahamNeubig,NavdeepJaitly,HengJi,AlaneSuhr,andYizheZhang. TrainingsoftwareengineeringagentsandverifierswithSWE-gym. InForty-secondInternational Conference on Machine Learning, 2025. URL  Cq1BNvHx74. AmandalynnePaullada,InioluwaDeborahRaji,EmilyMBender,EmilyDenton,andAlexHanna. Dataandits(dis)contents:Asurveyofdatasetdevelopmentanduseinmachinelearningresearch. Patterns,2(11),2021. I Made Putrama and Pe\u00b4ter Martinek. Heterogeneous data integration: Challenges and opportu- nities. Data in Brief, 56:110853, 2024. ISSN 2352-3409. doi:  2024.110853. URL  S2352340924008175. QwenTeam.Qwen2.5:Apartyoffoundationmodels,September2024.URL github.io/blog/qwen2.5/. PranavRajpurkar,JianZhang,KonstantinLopyrev,andPercyLiang. SQuAD:100,000+questions formachinecomprehensionoftext.InProceedingsofthe2016ConferenceonEmpiricalMethods inNaturalLanguageProcessing,pp.2383\u20132392,Austin,Texas,November2016.Associationfor Computational Linguistics. doi: 10.18653/v1/D16-1264. URL  org/D16-1264. Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. An- droidinthewild: Alarge-scaledatasetforandroiddevicecontrol. AdvancesinNeuralInformation ProcessingSystems,36:59708\u201359728,2023. Timo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:68539\u2013 68551,2023. YifanSong,WeiminXiong,XiutianZhao,DaweiZhu,WenhaoWu,KeWang,ChengLi,WeiPeng, and Sujian Li. AgentBank: Towards generalized LLM agents via fine-tuning on 50000+ inter- actiontrajectories. InYaserAl-Onaizan,MohitBansal,andYun-NungChen(eds.),Findingsof the Association for Computational Linguistics: EMNLP 2024, pp. 2124\u20132141, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. findings-emnlp.116. URL  116/. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language mod- els. Transactions on Machine Learning Research, 2024a. ISSN 2835-8856. URL https: //openreview.net/forum?id=ehfRiF0R3a. XingyaoWang,YangyiChen,LifanYuan,YizheZhang,YunzhuLi,HaoPeng,andHengJi. Exe- cutablecodeactionselicitbetterllmagents. InForty-firstInternationalConferenceonMachine Learning,2024b. XingyaoWang, BoxuanLi, YufanSong, Frank F.Xu, Xiangru Tang, MingchenZhuge, Jiayi Pan, YueqiSong,BowenLi,JaskiratSingh,HoangH.Tran,FuqiangLi,RenMa,MingzhangZheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. Openhands: An open platform for AI soft- ware developers as generalist agents. In The Thirteenth International Conference on Learning Representations,2025. URL 14 --- Page 15 --- arXivSubmission Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Xin Guo, Dingwen Yang, Chenyang Liao, Wei He, Songyang Gao, Lu Chen, Rui Zheng, Yicheng Zou, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, and Yu-Gang Jiang. Agent- Gym: Evaluating and training large language model-based agents across diverse environments. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedingsofthe63rdAnnualMeetingoftheAssociationforComputationalLinguistics(Vol- ume 1: Long Papers), pp. 27914\u201327961, Vienna, Austria, July 2025. Association for Compu- tational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.1355. URL  Kevin Xu, Yeganeh Kordi, Tanay Nayak, Adi Asija, Yizhong Wang, Kate Sanders, Adam Byerly, JingyuZhang,BenjaminVanDurme,andDanielKhashabi.Tur[k]ingbench:Achallengebench- markforwebagents. arXivpreprintarXiv:2403.11905,2024a. Yiheng Xu, Dunjie Lu, Zhennan Shen, Junli Wang, Zekun Wang, Yuchen Mao, Caiming Xiong, andTaoYu. Agenttrek: Agenttrajectorysynthesisviaguidingreplaywithwebtutorials. arXiv preprintarXiv:2412.09605,2024b. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388,2025a. JohnYang, CarlosEJimenez, AlexanderWettig, KilianLieret, ShunyuYao, KarthikNarasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. AdvancesinNeuralInformationProcessingSystems,37:50528\u201350652,2024. John Yang, Kilian Leret, Carlos E Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, BinyuanHui,OfirPress,LudwigSchmidt,andDiyiYang. Swe-smith: Scalingdataforsoftware engineeringagents. arXivpreprintarXiv:2504.21798,2025b. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-worldwebinteractionwithgroundedlanguageagents. AdvancesinNeuralInformationPro- cessingSystems,35:20744\u201320757,2022a. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizingreasoningandactinginlanguagemodels. InTheeleventhinternational conferenceonlearningrepresentations,2022b. AohanZeng,MingdaoLiu,RuiLu,BowenWang,XiaoLiu,YuxiaoDong,andJieTang. Agenttun- ing: Enablinggeneralizedagentabilitiesforllms. arXivpreprintarXiv:2310.12823,2023. Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang, Shaochen Zhong, andXiaHu. Data-centricartificialintelligence: Asurvey. ACMComputingSurveys,57(5):1\u201342, 2025. Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Ming Zhu, Juntao Tan, Thai Hoang,ZuxinLiu,LiangweiYang,etal. Agentohana: Designunifieddataandtrainingpipeline foreffectiveagentlearning. arXivpreprintarXiv:2402.15506,2024. Jianguo Zhang, Tian Lan, Ming Zhu, Zuxin Liu, Thai Quoc Hoang, Shirley Kokane, Weiran Yao, Juntao Tan, Akshara Prabhakar, Haolin Chen, Zhiwei Liu, Yihao Feng, Tulika Manoj Awal- gaonkar, Rithesh R N, Zeyuan Chen, Ran Xu, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Silvio Savarese, and Caiming Xiong. xLAM: A family of large action models to em- power AI agent systems. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computa- tionalLinguistics: HumanLanguageTechnologies(Volume1: LongPapers),pp.11583\u201311597, Albuquerque, NewMexico, April2025.AssociationforComputationalLinguistics. ISBN979- 8-89176-189-6. doi: 10.18653/v1/2025.naacl-long.578. URL  org/2025.naacl-long.578/. 15 --- Page 16 --- arXivSubmission Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. OpenCodeInterpreter: Integrating code generation with execution and refine- ment. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Associa- tion for Computational Linguistics: ACL 2024, pp. 12834\u201312859, Bangkok, Thailand, August 2024a.AssociationforComputationalLinguistics.doi:10.18653/v1/2024.findings-acl.762.URL  Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Pro- ceedingsofthe62ndAnnualMeetingoftheAssociationforComputationalLinguistics(Volume 3: SystemDemonstrations),Bangkok,Thailand,2024b.AssociationforComputationalLinguis- tics. URL Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A real- istic web environment for building autonomous agents. In The Twelfth International Confer- ence on Learning Representations, 2024. URL  oKn9c6ytLx. A USE OF LLMS WeusedLLMstoaidandpolishwritingforstyleandpresentation. Specifically,LLMswereemployedto: \u2022 polishwording,tightenparagraphs,andimproveclarity/flow; \u2022 improvelatexpresentation(e.g.,table/figurecaptions) B DATA SAMPLING FOR BALANCED TRAINING To balance domains and reduce over-represented sources, we resample each dataset with a per- datasetmultiplierw . Fordatasetdwithn rawtrajectories,wedrawm =\u2308w n \u2309examplesper d d d d d epoch; if w < 1 we sample without replacement (downsample), and if w > 1 we sample with d d replacement (upsample). This yields an effective mixture proportional to w across datasets (and d thereforeacrossdomains),whilekeepingtheoverallepochsizestable. Table9: Per-datasetsamplingmultipliersw . w < 1indicatesdownsampling; w > 1indicates d d d upsampling. Dataset w Direction d agenttuning alfworld 2 up agenttuning db 2 up agenttuning kg 2 up agenttuning mind2web 2 up agenttuning os 2 up agenttuning webshop 2 up code feedback 0.1 down codeactinstruct 1 neutral go-browse-wa 1 neutral mind2web 1 neutral nebius SWE-agent-trajectories 1 neutral nnetnav-live 1 neutral nnetnav-wa 1 neutral openhands 1 neutral orca agentinstruct 0.001 down swe-gym openhands sampled trajectories 3 up swe-smith 1 neutral synatra 0.01 down 16 --- Page 17 --- arXivSubmission In practice, we fix a random seed for reproducibility and shuffle the union of sampled examples across datasets each epoch. This scheme targets a more balanced distribution across coding, SWE, tool-use, and web-browsing sources by attenuating very large corpora (e.g., orca agentinstruct at w =0.001) and amplifying under-represented ones (e.g., d swe-gym openhands sampled trajectoriesatw =3). d B.1 DOMAIN-SPECIFICDATAFILTERING Beyondbalancedsampling,weapplydomain-specificfilteringtooptimizetrainingeffectivenessfor eachagentframeworkbasedontheirevaluationfocusandcapabilities. OpenHands and SWE-Agent Training Data. For OpenHands CodeActAgent and SWE-Agent, whichareprimarilyevaluatedoncodingandsoftwareengineeringtasks(SWE-Bench,AgentBench OS,andGAIA),weuseonlythenon-webportionoftheADPtrainingcorpus.Thisincludesdatasets focusedoncodegeneration,softwareengineering,generalagentinstructionfollowing,andAPI/tool usage. Specifically, we exclude web browsing datasets Mind2Web, Go-Browse, NNetNav, and Synatratoavoidpotentialinterferencefromweb-specificinteractionpatternsthatarenotapplicable tocommand-lineandcodingenvironments. AgentLabTrainingData. ForAgentLab,whichisdesignedforwebbrowsingtasksandweevalu- atedexclusivelyitonWebArena,weuseonlythewebportionoftheADPtrainingcorpus. Thisin- cludesdatasetsfocusedonwebnavigation,browser-basedtaskcompletion,andweb-specificagent instruction following (Mind2Web, Go-Browse, NNetNav, and Synatra). We exclude coding and software engineering datasets to ensure the model is optimized for web browsing patterns and UI elementinteractionwithoutdilutionfromlesscompatibledomains. 17"
}