{
  "abstract": "Abstract ReinforcementLearningfromHumanFeedback(RLHF)hasemergedasakeytech- niqueforpost-traininglargelanguagemodels. Despiteitsempiricalsuccess,the theoreticalunderstandingofRLHFisstilllimited,aslearningtheKL-regularized targetwithonlypreferencefeedbackposesadditionalchallengescomparedwith canonicalRL.Existingworksmostlystudythereward-basedBradley-Terry(BT) preferencemodel,andextendclassicaldesignsutilizingoptimismorpessimism. Thiswork,instead,considersthegeneralpreferencemodel(whosepracticalrele- vancehasbeenobservedrecently)andobtainsperformanceguaranteeswithmajor, order-wiseimprovementsoverexistingones. Surprisingly,theseresultsarederived fromalgorithmsthatdirectlyusetheempiricalestimates(i.e.,greedysampling), asopposedtoconstructingoptimisticorpessimisticestimatesinpreviousworks. Thisinsighthasadeeprootintheuniquestructuralpropertyoftheoptimalpolicy classundertheKL-regularizedtarget,andwefurtherspecializeittotheBTmodel, highlightingthesurprisingsufficiencyofgreedysamplinginRLHF.",
  "introduction": "introduction. MIT press Cambridge. Team,G.,Anil,R.,Borgeaud,S.,Alayrac,J.-B.,Yu,J.,Soricut,R.,Schalkwyk,J.,Dai,A.M.,Hauth, A., Millican, K., etal.(2023). Gemini: afamilyofhighlycapablemultimodalmodels. arXiv preprintarXiv:2312.11805. Touvron,H.,Martin,L.,Stone,K.,Albert,P.,Almahairi,A.,Babaei,Y.,Bashlykov,N.,Batra,S., Bhargava,P.,Bhosale,S.,etal.(2023). Llama2: Openfoundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288. Wang, Y., Liu, Q., and Jin, C. (2023). Is RLHF more difficult than standard RL? a theoretical perspective. AdvancesinNeuralInformationProcessingSystems,36:76006\u201376032. Xie,T.,Cheng,C.-A.,Jiang,N.,Mineiro,P.,andAgarwal,A.(2021a). Bellman-consistentpessimism forofflinereinforcementlearning. AdvancesinNeuralInformationProcessingSystems,34:6683\u2013 6694. Xie, T., Foster, D. J., Krishnamurthy, A., Rosset, C., Awadallah, A., and Rakhlin, A. (2024). Exploratorypreferenceoptimization: HarnessingimplicitQ*-approximationforsample-efficient RLHF. arXivpreprintarXiv:2405.21046. Xie,T.,Jiang,N.,Wang,H.,Xiong,C.,andBai,Y.(2021b). Policyfinetuning: Bridgingsample- efficientofflineandonlinereinforcementlearning. AdvancesinNeuralInformationProcessing Systems,34:27395\u201327407. Xiong,W.,Dong,H.,Ye,C.,Wang,Z.,Zhong,H.,Ji,H.,Jiang,N.,andZhang,T.(2023). Iterative preference learning from human feedback: Bridging theory and practice for RLHF under KL- constraint. arXivpreprintarXiv:2312.11456. Xiong,W.,Shi,C.,Shen,J.,Rosenberg,A.,Qin,Z.,Calandriello,D.,Khalman,M.,Joshi,R.,Piot, B.,Saleh,M.,etal.(2024). Buildingmathagentswithmulti-turniterativepreferencelearning. arXivpreprintarXiv:2409.02392. 12 --- Page 13 --- Xiong,W.,Zhong,H.,Shi,C.,Shen,C.,Wang,L.,andZhang,T.(2022). Nearlyminimaxoptimal offlinereinforcementlearningwithlinearfunctionapproximation: Single-agentMDPandmarkov game. arXivpreprintarXiv:2205.15512. Ye,C.,Xiong,W.,Gu,Q.,andZhang,T.(2024a). Corruption-robustalgorithmswithuncertainty weightingfornonlinearcontextualbanditsandMarkovdecisionprocesses. Ye, C., Xiong, W., Zhang, Y., Dong, H., Jiang, N., andZhang, T.(2024b). Onlineiterativerein- forcementlearningfromhumanfeedbackwithgeneralpreferencemodel. AdvancesinNeural InformationProcessingSystems,37:81773\u201381807. Yin, M. and Wang, Y.-X. (2021). Towards instance-optimal offline reinforcement learning with pessimism. AdvancesinNeuralInformationProcessingSystems,34:4065\u20134078. Yue,Y.andJoachims,T.(2009). Interactivelyoptimizinginformationretrievalsystemsasadueling banditsproblem.InProceedingsofthe26thAnnualInternationalConferenceonMachineLearning, pages1201\u20131208. Zhan,W.,Uehara,M.,Sun,W.,andLee,J.D.(2023). Provablereward-agnosticpreference-based reinforcementlearning. arXivpreprintarXiv:2305.18505. Zhang,T.(2023). Mathematicalanalysisofmachinelearningalgorithms. CambridgeUniversity Press. Zhang, Y., Yu, D., Ge, T., Song, L., Zeng, Z., Mi, H., Jiang, N., and Yu, D. (2025). Improv- ing LLM general preference alignment via optimistic online mirror descent. arXiv preprint arXiv:2502.16852. Zhang, Y., Yu, D., Peng, B., Song, L., Tian, Y., Huo, M., Jiang, N., Mi, H., and Yu, D. (2024). IterativeNashpolicyoptimization: AligningLLMswithgeneralpreferencesviano-regretlearning. arXivpreprintarXiv:2407.00617. Zhao,H.,Ye,C.,Gu,Q.,andZhang,T.(2024). SharpanalysisforKL-regularizedcontextualbandits andRLHF. arXivpreprintarXiv:2411.04625. Zhao, H., Ye, C., Xiong, W., Gu, Q., and Zhang, T. (2025a). Logarithmic regret for online KL- regularizedreinforcementlearning. arXivpreprintarXiv:2502.07460. Zhao, Q., Ji, K., Zhao, H., Zhang, T., and Gu, Q. (2025b). Nearly optimal sample complexity ofofflineKL-regularizedcontextualbanditsundersingle-policyconcentrability. arXivpreprint arXiv:2502.06051. Zhao, Y., Joshi, R., Liu, T., Khalman, M., Saleh, M., and Liu, P.J. (2023). SLiC-HF:Sequence likelihoodcalibrationwithhumanfeedback. arXivpreprintarXiv:2305.10425. Zhong,H.,Shan,Z.,Feng,G.,Xiong,W.,Cheng,X.,Zhao,L.,He,D.,Bian,J.,andWang,L.(2024). DPOmeetsPPO:ReinforcedtokenoptimizationforRLHF. arXivpreprintarXiv:2404.18922. Zhong,H.andZhang,T.(2023). Atheoreticalanalysisofoptimisticproximalpolicyoptimizationin linearMarkovdecisionprocesses. AdvancesinNeuralInformationProcessingSystems,36:73666\u2013 73690. Zhu,B.,Jordan,M.,andJiao,J.(2023). Principledreinforcementlearningwithhumanfeedback frompairwiseork-wisecomparisons. InInternationalConferenceonMachineLearning,pages 43037\u201343067.PMLR. Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. (2019). Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593. 13 --- Page 14 --- A Discussions A.1 BroadImpacts Thisworkfocusesonthetheoreticalstudyofreinforcementlearningfromhumanfeedback(RLHF) anddemonstratesthatitisprovablyefficienttousegreedysamplinginboththegeneralpreference modelandtheBradley-Terry(BT)model,underboththeonlineandofflinesettings. Whileacknowl- edgingtheneedforresponsibleusageoftheproposedmethod,wedonotforeseemajornegative societalimpactsduetothetheoreticalnatureofthiswork. A.2 LimitationsandFutureDirections Inthefollowing,somedirectionsworthfurtherinvestigationarelisted. \u2022 From the theoretical perspective, it would be interesting to further tighten the obtained performanceboundsofgreedysampling. Inparticular,asillustratedinthelaterproof,the boundscontainmultiplicativeconstantsofexp(\u03b7),whichmaybeworthfurtherstudieson itsnecessity. \u2022 Ontheempiricalside, asmentionedinthemainpaper, thetheoreticaldesignsbasedon optimismorpessimismareoftendifficulttoimplement,asthebonustermortheoptimization intheconfidencesetcanbecomputationallydemanding. Withthetheoreticalguarantees andsomeempiricalevidenceontheeffectivenessofgreedysamplinginthiswork,more empirical,large-scaleexperimentswouldbehelpfultocomparegreedysamplingwithother methodsfurther. \u2022 Thisworkfocusesonthesingle-stepsetting,i.e.,acontextualbanditproblem,asitisthe most widely adopted RLHF scenario. Still, with a growing interest in performing post- traininginmulti-turnscenarios(Xiongetal.,2024),itwouldbeapromisingdirectionto extendthetheoreticalresultsinthisworktomulti-stepRL. A.3 ComparisontoRelatedWorks Tables1and2compareourresultswithpriorworks,emphasizingthedependenceonthehorizonT (forregret)andthesub-optimalitygap\u03b5(forsamplecomplexity). Itcanbeobservedthatunderthe generalperformancemodel,thisworksubstantiallyimprovesthepreviousresultsestablishedinYe etal.(2024b). UndertheBTmodel,thisworkmatchesthepreviousperformanceboundsinZhao etal.(2025a,b)whilenotrequiringoptimismorpessimism. Itisnotedthatintheonlinesetting,the conversionfromregretstosamplecomplexitiesisperformedviaLemma14. Table1: ComparisonsintheonlinesettingunderthegeneralpreferencemodelandtheBTmodel. Setting Algorithm Regret SampleComplexity Optimism GP Yeetal.(2024b) \u2212 O(cid:101)(cid:0) 1/\u03f52(cid:1) \u2713 (cid:0) (cid:1) GreedySampling(Theorem1) O(cid:101)(log(T)) O(cid:101) 1/\u03f5 \u00d7 Xiongetal.(2023) \u2212 O(cid:101)(cid:0) 1/\u03f52(cid:1) \u2713 BT Zhaoetal.(2025a) O(cid:101)(log(T)) O(cid:101)(cid:0) 1/\u03f5(cid:1) \u2713 (cid:0) (cid:1) GreedySampling(Theorem2) O(cid:101)(log(T)) O(cid:101) 1/\u03f5 \u00d7 B OptimalPolicyClassandValueDecomposition TheoptimalpolicyundertheBTmodeliswell-knowntobeaGibbsdistributionwithrespecttothe rewardfunction,asstatedinthefollowingproposition,whoseproofcanbefoundinZhang(2023, Proposition7.16)andRafailovetal.(2023). Proposition2(OptimalPolicy,BTModel) ForanyrewardfunctionR\u2217,thecorrespondingoptimal policy\u03c0\u2217 satisfiesthat\u03c0\u2217 (a|x)\u221d\u03c0 (x|a)exp(\u03b7R\u2217(x,a)). BT BT 0 14 --- Page 15 --- Table2: ComparisonsintheofflinesettingunderthegeneralpreferencemodelandtheBTmodel. Setting Algorithm SampleComplexity Pessimism GP Yeetal.(2024b) O(cid:101)(cid:0) 1/\u03f52(cid:1) \u2713 (cid:0) (cid:1) GreedySampling(Theorem3) O(cid:101) 1/\u03f5 \u00d7 Xiongetal.(2023) O(cid:101)(cid:0) 1/\u03f52(cid:1) \u2713 BT Zhaoetal.(2025b) O(cid:101)(cid:0) 1/\u03f5(cid:1) \u2713 (cid:0) (cid:1) GreedySampling(Theorem4) O(cid:101) 1/\u03f5 \u00d7 Forthegeneralpreferencemodel,thefollowingpropositioncanbeestablished,demonstratingthat theequilibriumpolicystillfollowsasimilarstructure. Proposition3(EquilibriumPolicy,GeneralPreferenceModel) For any preference model P\u2217, thecorrespondingNEpolicy\u03c0\u2217 satisfiesthat\u03c0\u2217 (a|x)\u221d\u03c0 (a|x)exp(\u03b7P\u2217(x,a,\u03c0\u2217 )). GP GP 0 GP Proof: Recallthat (\u03c01,\u2217,\u03c02,\u2217)=argmax argminE P\u2217(x,\u03c01,\u03c02)\u2212\u03b7\u22121KL(\u03c01,\u03c0 |x)+\u03b7\u22121KL(\u03c02,\u03c0 |x), x\u223cd0 0 0 \u03c01\u2208\u03a0 \u03c02\u2208\u03a0 andLemma4inYeetal.(2024b)showsthat\u03c01,\u2217isequalto\u03c02,\u2217andwedenotethemas\u03c0\u2217 . Thus, GP wehave \u03c01,\u2217 =argmax E P\u2217(x,\u03c01,\u03c02,\u2217)\u2212\u03b7\u22121KL(\u03c01,\u03c0 |x)+\u03b7\u22121KL(\u03c02,\u2217,\u03c0 |x) x\u223cd0 0 0 \u03c01\u2208\u03a0 =argmax E P\u2217(x,\u03c0 ,\u03c02,\u2217)\u2212\u03b7\u22121KL(\u03c0 ,\u03c0 |x), x\u223cd0 1 1 0 \u03c01\u2208\u03a0 and \u03c02,\u2217 =argmin E P\u2217(x,\u03c01,\u2217,\u03c02)\u2212\u03b7\u22121KL(\u03c01,\u2217,\u03c0 |x)+\u03b7\u22121KL(\u03c02,\u03c0 |x) x\u223cd0 0 0 \u03c02\u2208\u03a0 =argmin E P\u2217(x,\u03c01,\u2217,\u03c02)+\u03b7\u22121KL(\u03c02,\u03c0 |x) x\u223cd0 0 \u03c02\u2208\u03a0 =argmax E [\u2212P\u2217(x,\u03c01,\u2217,\u03c02)\u2212\u03b7\u22121KL(\u03c02,\u03c0 |x)]. x\u223cd0 0 \u03c02\u2208\u03a0 TakingR(x,a)=P\u2217(x,a,\u03c02,\u2217)and\u2212P\u2217(x,\u03c01,\u2217,a)inProposition2leadstothedesiredresult. \u25a0 Proposition1canthenbeestablishedbyaggregatingtheabovetwopropositionstogether. To facilitate further discussions, the following notation is introduced: for the function f(x,a) : X \u00d7A\u2192R,itisdenotedthat (cid:88) Z (x):= \u03c0 (a|x)exp(\u03b7f(x,a)) f 0 a\u2208A \u03c0 (a|x)\u221d\u03c0 (a|x)exp(\u03b7f(x,a)) f 0 V(\u03c0,f):=E [f(x,a)\u2212\u03b7\u22121KL(\u03c0(\u00b7|x)||\u03c0 (\u00b7||x))]. x\u223cd0,a\u223c\u03c0 0 Also,theclosed-formsolutiontoaKL-regularizedobjective maxE [f(x,a)\u2212KL(\u03c0,\u03c0 |x)] \u03c0\u2208\u03a0 x\u223cd0,a\u223c\u03c0 0 isdenotedas\u03c0\u2217(a|x)\u221d\u03c0 (a|x)exp(\u03b7f(x,a)). Furthermore,whenthereisnoambiguity,wewill 0 omitx\u223cd intheexpectationandusethefollowingsimplifiednotation: 0 E [f(x,a)]:=E [f(x,a)]. \u03c0 x\u223cd0,a\u223c\u03c0(\u00b7|x) 15 --- Page 16 --- Lemma2(ValueDecomposition) Foranyfunctionf\u2217(x,a),f(x,a):X \u00d7A\u2192R,wehave V(\u03c0 ,f\u2217)\u2212V(\u03c0 ,f\u2217)\u2264\u03b7\u00b7E (cid:2)(cid:0) f(x,a)\u2212f\u2217(x,a)(cid:1)2(cid:3) f\u2217 f \u03c0 f\u2032 wheref\u2032(\u00b7,\u00b7)=\u03b3f(\u00b7,\u00b7)+(1\u2212\u03b3)f\u2217(\u00b7,\u00b7)forsome\u03b3 \u2208[0,1]. Proof: ThefollowingprooflargelyfollowsLemma3.9inZhaoetal.(2024),whichisincludedhere forcompleteness. Foranyfunctionf\u2217(x,a),f(x,a):X \u00d7A\u2192R,with (cid:88) J(f):=logZ (x)\u2212\u03b7 \u03c0 (a|x)\u00b7\u2206(x,a) f f a\u2208A itcanbeobservedthat V(\u03c0 ,f\u2217)\u2212V(\u03c0 ,f\u2217) f\u2217 f (cid:20) (cid:21) (cid:20) (cid:21) 1 \u03c0 (a|x) 1 \u03c0 (a|x) =E f\u2217(x,a)\u2212 log f\u2217 \u2212E f\u2217(x,a)\u2212 log f \u03c0f\u2217 \u03b7 \u03c0 (a|x) \u03c0f \u03b7 \u03c0 (a|x) 0 0 1 (cid:20) \u03c0 (a|x)\u00b7exp(cid:0) \u03b7f\u2217(x,a)(cid:1)(cid:21) 1 (cid:20) \u03c0 (a|x)\u00b7exp(cid:0) \u03b7f\u2217(x,a)(cid:1)(cid:21) = E log 0 \u2212 E log 0 \u03b7 \u03c0f\u2217 \u03c0 (a|x) \u03b7 \u03c0f \u03c0 (a|x) f\u2217 f (cid:20) (cid:21) = 1 E (cid:2) logZ (x)(cid:3) \u2212 1 E (cid:2) logZ (x)(cid:3) \u2212E (cid:88) \u03c0 (a|x)\u00b7(cid:0) f\u2217(x,a)\u2212f(x,a)(cid:1) \u03b7 x\u223cd0 f\u2217 \u03b7 x\u223cd0 f x\u223cd0 f a\u2208A 1 = E [J(f\u2217)\u2212J(f)] \u03b7 x\u223cd0 wherethefirstequalityfollowsfromthedefinitionoftheKL-divergence. Furthermore, the first derivative of J(f) with respect to \u2206(x,a) := f(x,a) \u2212 f\u2217(x,a) can be obtainedas (cid:20) (cid:21) \u2202J(f) \u2202 (cid:88) = logZ (x)\u2212\u03b7 \u03c0 (a\u2032|x)\u00b7\u2206(x,a\u2032) \u2202\u2206(x,a) \u2202\u2206(x,a) f f a\u2032\u2208A 1 (cid:0) (cid:1) = \u00b7\u03c0 (a|x)exp \u03b7\u00b7f(x,a) \u00b7\u03b7\u2212\u03b7\u00b7\u03c0 (a|x) Z (x) 0 f f (cid:0) (cid:1) (cid:2) (cid:0) (cid:1)(cid:3)2 \u03c0 (a|x)\u00b7exp \u03b7\u00b7f(x,a) \u03c0 (a|x)\u00b7exp \u03b7\u00b7f(x,a) 0 0 \u2212\u03b7\u00b7\u2206(x,a)\u00b7 \u00b7\u03b7+\u03b7\u00b7\u2206(x,a)\u00b7 \u00b7\u03b7 Z (x) [Z (x)]2 f f +\u03b7 (cid:88) \u03c0 0(a\u2032|x)\u00b7exp(cid:0) \u03b7\u00b7f(x,a\u2032)(cid:1) \u00b7\u03b7\u00b7\u2206(x,a\u2032)\u00b7 \u03c0 0(a|x)\u00b7exp(cid:0) \u03b7\u00b7f(x,a)(cid:1) Z (x) Z (x) f f a\u2032\u2208A\\{a} (cid:88) =\u2212\u03b72\u03c0 (a|x)\u2206 (x,a)+\u03b72 \u03c0 (a\u2032|x)\u03c0 (a|x)\u2206(x,a\u2032). f f f f a\u2032\u2208A Then, via the mean value theorem, there exists an f\u2032(\u00b7,\u00b7) = \u03b3f(\u00b7,\u00b7)+(1\u2212\u03b3)f\u2217(\u00b7,\u00b7) for some \u03b3 \u2208[0,1]suchthat 1 E [J(f\u2217)\u2212J(f)] \u03b7 x\u223cd0 (cid:20) (cid:21) = 1 E \u03b72 (cid:88) \u03c0 (a|x)\u00b7\u03b3\u00b7(cid:0) f(x,a)\u2212f\u2217(x,a)(cid:1)2 \u03b7 x\u223cd0 f\u2032 a\u2208A (cid:20) (cid:21) \u2212 1 E \u03b3\u03b72 (cid:88) (cid:88) \u03c0 (a |x)\u03c0 (a |x)(cid:0) f(x,a )\u2212f\u2217(x,a )(cid:1)(cid:0) f(x,a )\u2212f\u2217(x,a )(cid:1) \u03b7 x\u223cd0 f\u2032 1 f\u2032 2 1 1 2 2 a1\u2208Aa2\u2208A \u2264\u03b7\u00b7E (cid:2)(cid:0) f(x,a)\u2212f\u2217(x,a)(cid:1)2(cid:3) \u03c0 f\u2032 wherethelastinequalityholdssince (cid:88) (cid:88) \u03c0 (a |x)\u03c0 (a |x)(cid:0) f(x,a )\u2212f\u2217(x,a )(cid:1)(cid:0) f(x,a )\u2212f\u2217(x,a )(cid:1) f\u2032 1 f\u2032 2 1 1 2 2 a1\u2208Aa2\u2208A =(cid:2)E [f(x,a)\u2212f\u2217(x,a)](cid:3)2 \u22650. a\u223c\u03c0 f\u2032(\u00b7|x) Theproofisthenconcluded. \u25a0 16 --- Page 17 --- C ProofsforOfflineGreedyRLHF Whileweheavilyfocusontheonlinesettinginthemainpaper,wewillstartwiththeproofsforthe offlinesettingbeforethosefortheonlinesettinginordertofacilitatethepresentationoftheanalyses. C.1 GeneralPreferenceModel Wefirstintroducethefollowinglemmastoboundtheerrorofthemaximumlikelihoodestimates. Lemma3 GiventhetrainingdataD ={(x ,a1,a2,y )}n ,withprobability1\u2212\u03b4andtheMLE i i i i i=1 estimatorP\u02c6 satisfiesthat n (cid:88) (P\u02c6(x ,a1,a2)\u2212P\u2217(x ,a1,a2))2 \u2264logN P. i i i i i i \u03b4 i=1 Proof: For any fixed function P \u2208 P, we first upper bound its logarithmic moment generating functionas logEexp(cid:18) (cid:88)n log P(y i|x i,a1 i,a2 i) (cid:19) P\u2217(y |x ,a1,a2) i=1 i i i i (cid:115) =logEexp(cid:18)n (cid:88)\u22121 log P(y i|x i,a1 i,a2 i) (cid:19) +log2E P(y n|x n,a1 n,a2 n) i=1 P\u2217(y i|x i,a1 i,a2 i) yn|xn,a1 n,a2 n P\u2217(y n|x n,a1 n,a2 n) =logEexp(cid:18)n (cid:88)\u22121 log P(y i|x i,a1 i,a2 i) (cid:19) +log(cid:16) 1\u2212H(cid:0) P(y |x ,a1,a2)\u2225P\u2217(y |x ,a1,a2)(cid:1)2(cid:17) P\u2217(y |x ,a1,a2) n n n n n n n n i=1 i i i i \u2264logEexp(cid:18)n (cid:88)\u22121 log P(y i|x i,a1 i,a2 i) (cid:19) \u2212H(cid:16) P(y |x ,a1,a2)\u2225P\u2217(y |x ,a1,a2)(cid:1)(cid:17)2 P\u2217(y |x ,a1,a2) n n n n n n n n i=1 i i i i n \u2264...\u2264\u2212(cid:88) H(cid:16) P(y |x ,a1,a2)\u2225P\u2217(y |x ,a1,a2)(cid:1)(cid:17)2 , (2) i i i i i i i i i=1 whereH(P||Q)istheHellingerdistancedefinedby (cid:90) (cid:16)(cid:112) (cid:112) (cid:17)2 H(P||Q)2 := p(z)\u2212 q(z) d\u00b5(z). \u2126 Wecontinuetolower-boundtheHellingerdistanceby n (cid:88)(cid:16) H(P(y |x ,a1,a2)\u2225P\u2217(y |x ,a1,a2)(cid:1)(cid:17)2 i i i i i i i i i=1 n \u2265(cid:88)(cid:16) TV(P(y |x ,a1,a2)\u2225P\u2217(y |x ,a1,a2)(cid:1)(cid:17)2 i i i i i i i i i=1 n (cid:88) = (P(x ,a1,a2)\u2212P\u2217(x ,a1,a2))2, (3) i i i i i i i=1 wheretheinequalityusesthefactthatforanydistributionp,q,H(p,q) \u2265 TV(p,q)accordingto TheoremB.9ofZhang(2023). Then,byinvokingLemma10,weobtainforanyP \u2208P,withprobabilityatleast1\u2212\u03b4, (cid:88)n log P(y i|x i,a1 i,a2 i) \u2264log(N /\u03b4)+logEexp(cid:18) (cid:88)n log P(y i|x i,a1 i,a2 i) (cid:19) P\u2217(y |x ,a1,a2) P P\u2217(y |x ,a1,a2) i=1 i i i i i=1 i i i i n \u2264\u2212(cid:88) H(cid:16) P(y |x ,a1,a2)\u2225P\u2217(y |x ,a1,a2)(cid:1)(cid:17)2 +log(N /\u03b4) i i i i i i i i P i=1 n (cid:88) \u2264\u2212 (P(x ,a1,a2)\u2212P\u2217(x ,a1,a2))2+log(N /\u03b4), i i i i i i P i=1 17 --- Page 18 --- wherethesecondinequalityusesEqn.(2),andthelastinequalityusesEqn.(3). BytakingP asP\u02c6, sinceP\u02c6 istheMLE,weget (cid:88)n (P\u02c6(x ,a1,a2)\u2212P\u2217(x ,a1,a2))2 \u2264(cid:88)n logP\u2217(y i|x i,a1 i,a2 i) +log(N /\u03b4) i i i i i i P\u02c6(y |x ,a1,a2) P i=1 i=1 i i i i \u2264log(N /\u03b4), P whichconcludestheproof. \u25a0 Lemma4 Considertwoarbitrarypolicies\u03c0 ,\u03c0 ,andasetofdata{(x ,a1,a2,y )}n generated 1 2 i i i i i=1 i.i.d. fromthegeneralpreferencemodelP\u2217andpolicies\u03c0 ,\u03c0 . SupposethatP\u02c6 istheMLEestimate, 1 2 withprobabilityatleast1\u2212\u03b4,itholdsthat n N E [(P\u02c6(x,a1,a2)\u2212P\u2217(x,a1,a2))2]\u22642log P. 2 x\u223cd0,a1\u223c\u03c01,a2\u223c\u03c02 \u03b4 Proof: BythemultiplicativeChernoffbounds(refertoLemma11andRemark2),withprobabilityat least1\u2212\u03b4,foranyP \u2208P,wehave n E [(P(x,a1,a2)\u2212P\u2217(x,a1,a2))2] 2 x\u223cd0,a1\u223c\u03c01,a2\u223c\u03c02 n \u2264(cid:88) (P(x ,a1,a2)\u2212P\u2217(x ,a1,a2))2+log(N P). i i i i i i \u03b4 i=1 TakingP =P\u02c6 andusingLemma3,wecanget n E [(P\u02c6(x,a1,a2)\u2212P\u2217(x,a1,a2))2] 2 x\u223cd0,a1\u223c\u03c01,a2\u223c\u03c02 n \u2264(cid:88) (P\u02c6(x ,a1,a2)\u2212P\u2217(x ,a1,a2))2+log(N P) i i i i i i \u03b4 i=1 N \u22642log( P), \u03b4 whichconcludestheproof. \u25a0 WearenowreadytoproveTheorem3. Proof: [ProofofTheorem3]Thefollowingnotationsarefirstintroduced: (\u03c01,\u2217,\u03c02,\u2217)=argmax argminE P\u2217(x,\u03c01,\u03c02)\u2212\u03b7\u22121KL(\u03c01,\u03c0 |x)+\u03b7\u22121KL(\u03c02,\u03c0 |x), x\u223cd0 0 0 \u03c01\u2208\u03a0 \u03c02\u2208\u03a0 (\u03c0\u02c61,\u03c0\u02c62)=argmax argminE P\u02c6(x,\u03c01,\u03c02)\u2212\u03b7\u22121KL(\u03c01,\u03c0 |x)+\u03b7\u22121KL(\u03c02,\u03c0 |x), x\u223cd0 0 0 \u03c01\u2208\u03a0 \u03c02\u2208\u03a0 \u03c0\u02dc2 =argminE P\u2217(x,\u03c0\u02c61,\u03c0)+\u03b7\u22121KL(\u03c0,\u03c0 |x)=argminJ (\u03c0\u02c61,\u03c0), x\u223cd0 0 GP \u03c0\u2208\u03a0 \u03c0\u2208\u03a0 \u03c0\u02dc1 =argmaxE P\u2217(x,\u03c0,\u03c0\u02dc2)\u2212\u03b7\u22121KL(\u03c0,\u03c0 |x)=argmaxJ (\u03c0,\u03c0\u02dc2), x\u223cd0 0 GP \u03c0\u2208\u03a0 \u03c0\u2208\u03a0 anditcanbenoticedthatthesuboptimalgapcanbede-compositedas J (\u03c01,\u2217,\u03c02,\u2217)\u2212J (\u03c0\u02c61,\u03c0\u02dc2) GP GP =[J (\u03c01,\u2217,\u03c02,\u2217)\u2212J (\u03c01,\u2217,\u03c0\u02dc2)]+[J (\u03c01,\u2217,\u03c0\u02dc2)\u2212J (\u03c0\u02dc1,\u03c0\u02dc2)]+[J (\u03c0\u02dc1,\u03c0\u02dc2)\u2212J (\u03c0\u02c61,\u03c0\u02dc2)] GP GP GP GP GP GP \u2264J (\u03c0\u02dc1,\u03c0\u02dc2)\u2212J (\u03c0\u02c61,\u03c0\u02dc2), GP GP wheretheinequalityholdsasthefirsttwotermsarenegativeduetotheabovedefinitions. Recallthatwehave \u03c01,\u2217(a|x)\u221d\u03c0 (a|x)exp(\u03b7P\u2217(x,a,\u03c02,\u2217)), \u03c02,\u2217(a|x)\u221d\u03c0 (a|x)exp(\u2212\u03b7P\u2217(x,\u03c01,\u2217,a)). 0 0 18 --- Page 19 --- WithLemma2,wecanget J (\u03c0\u02dc1,\u03c0\u02dc2)\u2212J (\u03c0\u02c61,\u03c0\u02dc2) GP GP =E P\u2217(x,\u03c0\u02dc1,\u03c0\u02dc2)\u2212\u03b7\u22121KL(\u03c0\u02dc1,\u03c0 |x)+\u03b7\u22121KL(\u03c0\u02dc2,\u03c0 |x) x\u223cd0 0 0 \u2212(E P\u2217(x,\u03c0\u02c61,\u03c0\u02dc2)\u2212\u03b7\u22121KL(\u03c0\u02c61,\u03c0 |x)+\u03b7\u22121KL(\u03c0\u02dc2,\u03c0 |x)) x\u223cd0 0 0 =E P\u2217(x,a,\u03c0\u02dc2)\u2212\u03b7\u22121KL(\u03c0\u02dc1,\u03c0 |x) x\u223cd0,a\u223c\u03c0\u02dc1 0 \u2212(E P\u2217(x,a,\u03c0\u02dc2)\u2212\u03b7\u22121KL(\u03c0\u02c61,\u03c0 |x)) x\u223cd0,a\u223c\u03c0\u02c61 0 \u2264\u03b7E [(P\u2217(x,a,\u03c0\u02dc2)\u2212P\u02c6(x,a,\u03c0\u02c62))2] x\u223cd0,a\u223c\u03c0 f\u2032 \u22642\u03b7E (cid:2)(cid:0) P\u2217(x,a,\u03c0\u02dc2)\u2212P\u02c6(x,a,\u03c0\u02dc2)(cid:1)2(cid:3) +2\u03b7E (cid:2)(cid:0) P\u02c6(x,a,\u03c0\u02dc2)\u2212P\u02c6(x,a,\u03c0\u02c62)(cid:1)2(cid:3) . \u03c0 f\u2032 \u03c0 f\u2032 Then,itcanbeboundedthat E (cid:2)(cid:0) P\u2217(x,a,\u03c0\u02dc2)\u2212P\u02c6(x,a,\u03c0\u02dc2)(cid:1)2(cid:3) \u03c0 f\u2032 =E (cid:2)(cid:0)E [P\u2217(x,a1,a2)\u2212P\u02c6(x,a1,a2)](cid:1)2(cid:3) x\u223cd0,a1\u223c\u03c0 f\u2032 a2\u223c\u03c0\u02dc2 \u2264E (cid:2)(cid:0) P\u2217(x,a1,a2)\u2212P\u02c6(x,a1,a2)(cid:1)2(cid:3) x\u223cd0,a1\u223c\u03c0 f\u2032,a2\u223c\u03c0\u02dc2 \u2264exp(2\u03b7)E (cid:2)(cid:0) P\u2217(x,a1,a2)\u2212P\u02c6(x,a1,a2)(cid:1)2(cid:3) x\u223cd0,a1\u223c\u03c00,a2\u223c\u03c00 \u2264exp(2\u03b7)C(D ,(\u03c0 ,\u03c0 ))E (cid:2)(cid:0) P\u2217(x,a1,a2)\u2212P\u02c6(x,a1,a2)(cid:1)2(cid:3) 0 0 0 x\u223cd0,a1\u223c\u00b51,a2\u223c\u00b52 4exp(2\u03b7)C(D ,(\u03c0 ,\u03c0 )) \u2264 0 0 0 log(N /\u03b4) n P where the first inequality is putting the square into the expectation, the second inequality is by Lemma1,thethirdinequalityisbytheassumptionofdatacoverage,andthefinalinequalityisby Lemma4. Fortheotherterm2\u03b7E (cid:2)(cid:0) P\u02c6(x,a,\u03c0\u02dc2)\u2212P\u02c6(x,a,\u03c0\u02c62)(cid:1)2(cid:3) ,weboundthatbyseveralsteps. First,itis \u03c0 f\u2032 noticedthat \u03c0\u02c62(a|x)\u221d\u03c0 (a|x)exp(\u2212\u03b7P\u02c6(x,\u03c0\u02c61,a)), \u03c0\u02dc2(a|x)\u221d\u03c0 (a|x)exp(\u2212\u03b7P\u2217(x,\u03c0\u02c61,a)), 0 0 andwecorrespondinglydefine Z\u2032(x)=(cid:88) \u03c0 (a|x)exp(\u2212\u03b7P\u02c6(x,\u03c0\u02c61,a)), Z\u2032\u2032(x)=(cid:88) \u03c0 (a|x)exp(\u2212\u03b7P\u2217(x,\u03c0\u02c61,a)). 0 0 a a Itcanbeobservedthat |Z\u2032(x)\u2212Z\u2032\u2032(x)|=|(cid:88) \u03c0 (a|x)[exp(\u2212\u03b7P\u02c6(x,\u03c0\u02c61,a))\u2212exp(\u2212\u03b7P\u2217(x,\u03c0\u02c61,a))]| 0 a \u2264\u03b7|(cid:88) \u03c0 (a|x)(P\u02c6(x,\u03c0\u02c61,a)\u2212P\u2217(x,\u03c0\u02c61,a))| 0 a =\u03b7|E [P\u02c6(x,a1,a2)\u2212P\u2217(x,a1,a2)]| (4) a1\u223c\u03c0\u02c61,a2\u223c\u03c00 wheretheinequalityisfromthemeanvaluetheoremandthefactthattheboundofthederivativeof exp(\u2212\u03b7x)withrespecttoxisboundedin[\u2212\u03b7,\u2212\u03b7exp(\u2212\u03b7)]forx\u2208[0,1]. Withthefollowingrelationship (cid:88) (cid:88) 1= \u03c0 (a|x)\u2265Z\u2032(x),Z\u2032\u2032(x)\u2265 \u03c0 (a|x)\u00b7exp(\u2212\u03b7)=exp(\u2212\u03b7), (5) 0 0 a a 19 --- Page 20 --- itcanbeestablishedthat |P\u02c6(x,a\u2032,\u03c0\u02dc2)\u2212P\u02c6(x,a\u2032,\u03c0\u02c62)| =|(cid:88) (\u03c0\u02dc2(a|x)\u2212\u03c0\u02c62(a|x))P\u02c6(x,a\u2032,a)| a (cid:88) \u2264 |\u03c0\u02dc2(a|x)\u2212\u03c0\u02c62(a|x)| a =(cid:88) |\u03c0 0(a|x)exp(\u2212\u03b7P\u2217(x,\u03c0\u02c61,a)) \u2212 \u03c0 0(a|x)exp(\u2212\u03b7P\u02c6(x,\u03c0\u02c61,a)) | Z\u2032\u2032(x) Z\u2032(x) a \u2264exp(\u03b7)(cid:88) |\u03c0 (a|x)exp(\u2212\u03b7P\u2217(x,\u03c0\u02c61,a))\u2212 Z\u2032\u2032(x)\u03c0 0(a|x)exp(\u2212\u03b7P\u02c6(x,\u03c0\u02c61,a)) | 0 Z\u2032(x) a \u2264exp(\u03b7)(cid:88) |\u03c0 (a|x)(exp(\u2212\u03b7P\u2217(x,\u03c0\u02c61,a))\u2212exp(\u2212\u03b7P\u02c6(x,\u03c0\u02c61,a))| 0 a +exp(\u03b7)(cid:88) |Z\u2032(x)\u2212Z\u2032\u2032(x) \u03c0 (a|x)exp(\u2212\u03b7P\u02c6(x,\u03c0\u02c61,a))| Z\u2032(x) 0 a \u2264\u03b7exp(\u03b7)(cid:88) \u03c0 (a|x)|(P\u2217(x,\u03c0\u02c61,a)\u2212P\u02c6(x,\u03c0\u02c61,a))| 0 a +exp(\u03b7)(cid:88) \u03c0 (a|x)\u03b7|E a1\u223c\u03c0\u02c61,a2\u223c\u03c00[P\u02c6(x,a1,a2)\u2212P\u2217(x,a1,a2)]| 0 exp(\u2212\u03b7) a \u2264\u03b7exp(\u03b7)E |E [P\u02c6(x,a\u2032,a)\u2212P\u2217(x,a\u2032,a)]| a\u223c\u03c00 a\u2032\u223c\u03c0\u02c61 +\u03b7exp(2\u03b7)|E [P\u02c6(x,a1,a2)\u2212P\u2217(x,a1,a2)]| a1\u223c\u03c0\u02c61,a2\u223c\u03c00 \u2264\u03b7exp(3\u03b7)E |E [P\u02c6(x,a\u2032,a)\u2212P\u2217(x,a\u2032,a)]| a\u223c\u03c00 a\u2032\u223c\u03c00 +\u03b7exp(4\u03b7)|E [P\u02c6(x,a1,a2)\u2212P\u2217(x,a1,a2)]| a1\u223c\u03c00,a2\u223c\u03c00 \u22642\u03b7exp(4\u03b7)E E [|P\u02c6(x,a1,a2)\u2212P\u2217(x,a1,a2)|], a1\u223c\u03c00 a2\u223c\u03c00 wherethesecondinequalityisfromEquation(5),thefourthinequalityisfromEquations(5)and(4), andthelasttwoinequalitiesarebyLemma1. Thus,wehave (P\u02c6(x,a\u2032,\u03c0\u02dc2)\u2212P\u02c6(x,a,\u03c0\u02c62))2 \u22644\u03b72exp(8\u03b7)E E [(P\u02c6(x,a1,a2)\u2212P\u2217(x,a1,a2))2] (6) a1\u223c\u03c00 a2\u223c\u03c00 \u22644\u03b72exp(8\u03b7)C(D ,(\u03c0 ,\u03c0 ))E E [(P\u02c6(x,a1,a2)\u2212P\u2217(x,a1,a2))2]. 0 0 0 a1\u223c\u00b51 a2\u223c\u00b52 Combiningtheaboveresults,weget J (\u03c01,\u2217,\u03c02,\u2217)\u2212J (\u03c0\u02c61,\u03c0\u02dc2) GP GP \u22642\u03b7C(D ,(\u03c0 ,\u03c0 ))(exp(2\u03b7)+4\u03b72exp(8\u03b7))E [(P\u02c6(x,a1,a2)\u2212P\u2217(x,a1,a2))2] 0 0 0 x\u223cd0,a1,a2\u223c\u00b51,\u00b52 8\u03b7C(D ,(\u03c0 ,\u03c0 ))(exp(2\u03b7)+4\u03b72exp(8\u03b7)) (cid:18) N (cid:19) \u2264 0 0 0 log P , m \u03b4 andwhen 8\u03b7C(D ,(\u03c0 ,\u03c0 ))(exp(2\u03b7)+4\u03b72exp(8\u03b7)) (cid:18) N (cid:19) m\u2265 0 0 0 log P , \u03f5 \u03b4 thesuboptimalgapislessthan\u03f5. \u25a0 C.2 TheBradley-TerryModel Similarly,undertheBTmodel,westartwithboundingtheestimationerroroftheMLE. 20 --- Page 21 --- Lemma5 GiventhetrainingdataD ={(x ,a1,a2,y )}n ,withprobability1\u2212\u03b4andtheMLE i i i i i=1 estimatorR\u02c6satisfiesthefollowingestimation: (cid:88)n (cid:104) R\u02c6(x ,a1)\u2212R\u02c6(x ,a2)\u2212(cid:0) R\u2217(x ,a1)\u2212R\u2217(x ,a2)(cid:1)(cid:105)2 \u22642elog(cid:18) N R(cid:19) . i i i i i i i i \u03b4 i=1 Proof: SubstitutingP(x ,a1,a2)with\u03c3(R(x ,a1)\u2212R(x ,a2))inLemma3,weimmediatelyget i i i i i i i (cid:88)n (cid:104) \u03c3(cid:16) R\u02c6(x ,a1)\u2212R\u02c6(x ,a2)(cid:17) \u2212\u03c3(cid:0) R\u2217(x ,a1)\u2212R\u2217(x ,a2)(cid:1)(cid:105)2 \u2264log(cid:18) N R(cid:19) . i i i i i i i i \u03b4 i=1 Withthefactthat\u03c3\u2032(r)=\u03c3(r)(1\u2212\u03c3(r))\u2265 1 (astherewardisassumedtobeboundedin[0,1]),it 2e canbeestablishedthat n (cid:88)(cid:104) R\u02c6(x ,a1)\u2212R\u02c6(x ,a2)\u2212(cid:0) R\u2217(x ,a1)\u2212R\u2217(x ,a2)(cid:1)(cid:105)2 i i i i i i i i i=1 n \u22642e(cid:88)(cid:104) \u03c3(R\u02c6(x ,a1)\u2212R\u02c6(x ,a2))\u2212\u03c3(cid:0) R\u2217(x ,a1)\u2212R\u2217(x ,a2)(cid:1)(cid:105)2 i i i i i i i i i=1 (cid:18) (cid:19) N \u22642elog R , \u03b4 whichconcludestheproof. \u25a0 Lemma6 Considerarbitrarypolicies\u03c01,\u03c02,andasetofcontext-actionpairs{(x ,a1,a2,y )}n i i i i i=1 generatedi.i.d. fromtheBTmodelwherea1 \u223c\u03c01,a2 \u223c\u03c02. SupposethatR\u02c6istheMLEestimator. i i Wehavewithprobabilityatleast1\u2212\u03b4, (cid:18) (cid:19) E (cid:2)(cid:0) R\u02c6(x,a1)\u2212R\u02c6(x,a2)\u2212(R\u2217(x,a1)\u2212R\u2217(x,a2))(cid:1)2(cid:3) \u2264 6e log N R . x\u223cd0,a1\u223c\u03c01,a2\u223c\u03c02 n \u03b4 Proof: SimilartotheproofofLemma4,withprobabilityatleast1\u2212\u03b4,foranyR\u2208R,wehave n E (cid:2)(cid:0) R(x,a1)\u2212R(x,a2)\u2212(R\u2217(x,a1)\u2212R\u2217(x,a2))(cid:1)2(cid:3) 2 x\u223cd0,a1\u223c\u03c01,a2\u223c\u03c02 n (cid:18) (cid:19) \u2264(cid:88)(cid:0) R(x ,a1)\u2212R(x ,a2)\u2212(R\u2217(x ,a1)\u2212R\u2217(x ,a2))(cid:1)2 +log N R . i i i i i i i i \u03b4 i=1 BytakingR=R\u02c6andusingLemma5,wecanget n E (cid:2)(cid:0) R\u02c6(x,a )\u2212R\u02c6(x,a )\u2212(R\u2217(x,a )\u2212R\u2217(x,a ))(cid:1)2(cid:3) 2 x\u223cd0,a1\u223c\u03c01,a2\u223c\u03c02 1 2 1 2 n (cid:18) (cid:19) \u2264(cid:88)(cid:0) R\u02c6(x ,a1)\u2212R\u02c6(x ,a2)\u2212(R\u2217(x ,a1)\u2212R\u2217(x ,a2))(cid:1)2 +log N R i i i i i i i i \u03b4 i=1 (cid:18) (cid:19) (cid:18) (cid:19) N N \u2264(2e+1)log R \u22643elog R , \u03b4 \u03b4 whichprovesthelemma. \u25a0 Proof: [ProofofTheorem4]First,theoutputpolicy\u03c0\u02c6 satisfies \u03c0\u02c6 =argmaxE [R\u02c6(x,a)\u2212KL(\u03c0,\u03c0 |x)] \u03c0 0 \u03c0\u2208\u03a0 =argmaxE [R\u02c6(x,a)\u2212b(x)\u2212KL(\u03c0,\u03c0 |x)], \u03c0 0 \u03c0\u2208\u03a0 21 --- Page 22 --- foranyfunctionb(x),whichmeansthepoliciesinducedbyR\u02c6(x,a)andR\u02c6(x,a)\u2212b(x)arethesame. Thus,with b(x):=E [R\u02c6(x,a\u2032)\u2212R\u2217(x,a\u2032)] a\u2032\u223c\u03c00(\u00b7|x) andLemma2,wehave J (\u03c0\u2217)\u2212J (\u03c0\u02c6) BT BT \u2264\u03b7E [(R\u02c6(x,a)\u2212b(x)\u2212R\u2217(x,a))2] \u03c0 f\u2032 =\u03b7E [(R\u02c6(x,a1)\u2212R\u2217(x,a1)\u2212E (R\u02c6(x,a2)\u2212R\u2217(x,a2)))2] x\u223cd0,a1\u223c\u03c0 f\u2032 a2\u223c\u03c00 =\u03b7E [(E [R\u02c6(x,a1)\u2212R\u2217(x,a1)\u2212(R\u02c6(x,a2)\u2212R\u2217(x,a2))])2] x\u223cd0,a1\u223c\u03c0 f\u2032 a2\u223c\u03c00 =\u03b7E [(R\u02c6(x,a1)\u2212R\u2217(x,a1)\u2212(R\u02c6(x,a2)\u2212R\u2217(x,a2)))2] x\u223cd0,a1\u223c\u03c0 f\u2032,a2\u223c\u03c00 \u2264\u03b7exp(2\u03b7)E [(R\u02c6(x,a1)\u2212R\u2217(x,a1)\u2212(R\u02c6(x,a2)\u2212R\u2217(x,a2)))2] x\u223cd0,a1\u223c\u03c00,a2\u223c\u03c00 \u2264\u03b7exp(2\u03b7)C(D ,(\u03c0 ,\u03c0 ))E [(R\u02c6(x,a1)\u2212R\u2217(x,a1)\u2212(R\u02c6(x,a2)\u2212R\u2217(x,a2)))2] 0 0 0 \u00b51,\u00b52 (cid:18) (cid:19) N 1 \u22646\u03b7exp(2\u03b7+1)C(D ,(\u03c0 ,\u03c0 ))log R , (7) 0 0 0 \u03b4 m wherethelastthreeinequalitiesuseLemma1,Lemma6andDefinition5. Taking (cid:18) (cid:19) N m\u22656\u03b7exp(2\u03b7+1)C(D ,(\u03c0 ,\u03c0 ))log R /\u03f5, 0 0 0 \u03b4 wehaveJ (\u03c0\u2217)\u2212J (\u03c0\u02c6)\u2264\u03f5. \u25a0 BT BT D ProofsforOnlineRLHFwithGreedySampling D.1 GeneralPreferenceModel DefinebGP(x,a1,a2)=min{1,\u03b2 \u00b7U (\u03bb,x,a1,a2;P ,D )}and t T;GP GP t t (cid:40) t (cid:41) P = P \u2208P :(cid:88) (P(x,a1,a2)\u2212P\u02c6(x,a1,a2))2+\u03bb\u2264\u03b22 , t i i t i i T;GP i=1 where\u03b22 =2log(N T/\u03b4)and\u03bb\u2264\u03b22 /2. T;GP P T;GP Lemma7 UnderAlgorithm1,wehavewithprobabilityatleast1\u2212\u03b4forallt\u2208[T],theuniform optimismeventthatE = {P\u02c6(x,a1,a2)+bGP(x,a1,a2)\u2212P\u2217(x,a1,a2) > 0,\u2200(x,a1,a2) \u2208 X \u00d7 t t t A\u00d7A}holdstrue. Proof: ByLemma3,wehavethat,forallt\u2208[T]withprobabilityatleast1\u2212\u03b4 n (cid:18) (cid:19) (cid:88) (P\u02c6(x ,a1,a2)\u2212P\u2217(x ,a1,a2))2 \u2264log N PT = 1 \u03b22 . (8) i i i i i i \u03b4 2 T;GP i=1 Hence,wededucethatforany(x,a1,a2)\u2208X \u00d7A\u00d7A, |P (x,a1,a2)\u2212P (x,a1,a2)| |P\u02c6(x,a1,a2)\u2212P\u2217(x,a1,a2)|\u2264 sup 1 2 t (cid:113) P1,P2\u2208Pt \u03bb+(cid:80)t i=1(P 1(x,a1 i,a2 i)\u2212P 2(x,a1 i,a2 i))2 (cid:118) (cid:117) t \u00b7(cid:117) (cid:116)\u03bb+(cid:88) (P\u02c6(x,a1,a2)\u2212P\u2217(x,a1,a2))2 t i i i i i=1 (cid:114) 1 \u2264U (\u03bb,x,a1,a2;P ,D ) \u03bb+ \u03b22 GP t t 2 T;GP \u2264U (\u03bb,x,a1,a2;P ,D )\u03b2 , GP t t T;GP 22 --- Page 23 --- whichconcludestheproof. \u25a0 Proof: [ProofofTheorem1]Similartotheproofintheofflinesetting,wedefine (\u03c0\u02c61,\u03c0\u02c62)=argmax argminE [P\u02c6 (x,\u03c01,\u03c02)\u2212\u03b7\u22121KL(\u03c01,\u03c0 |x)+\u03b7\u22121KL(\u03c02,\u03c0 |x)], t t x\u223cd0 t\u22121 0 0 \u03c01\u2208\u03a0 \u03c02\u2208\u03a0 \u03c0\u02dc2 =argminE P\u2217(x,\u03c0\u02c61,\u03c0)+\u03b7\u22121KL(\u03c0,\u03c0 |x), t x\u223cd0 t 0 \u03c0\u2208\u03a0 \u03c0\u02dc1 =argmaxE P\u2217(x,\u03c0,\u03c0\u02dc2)\u2212\u03b7\u22121KL(\u03c0,\u03c0 |x). t x\u223cd0 t 0 \u03c0\u2208\u03a0 Conditioningontheevent\u222a E inLemma7,wecanboundthedesiredregretasfollows. For t\u2208[T] t eachstep,asshownintheproofoftheofflinesetting,wehave J (\u03c01,\u2217,\u03c02,\u2217)\u2212J (\u03c0\u02c61,\u03c0\u02dc2)\u2264\u03b7E (cid:2)(cid:0) P\u02c6 (x,a,\u03c0\u02c62)\u2212P\u2217(x,a,\u03c0\u02dc2)(cid:1)2(cid:3) . (9) GP GP t t \u03c0f t\u22121 t t t ByLemma7,wecanboundEquation(9)as: E (cid:2)(cid:0) P\u2217(x,\u00b7,\u03c0\u02dc2)\u2212P\u02c6 (x,\u00b7,\u03c0\u02c62)(cid:1)2(cid:3) \u03c0f t t\u22121 t t \u2264E (cid:2)(cid:0) P\u2217(x,\u00b7,\u03c0\u02dc2)\u2212P\u2217(x,\u00b7,\u03c0\u02c62)+P\u2217(x,\u00b7,\u03c0\u02c62)\u2212P\u02c6 (x,\u00b7,\u03c0\u02c62)(cid:1)2(cid:3) \u03c0f t t t t\u22121 t t \u22642E (cid:2)(cid:0) P\u2217(x,\u00b7,\u03c0\u02dc2)\u2212P\u2217(x,\u00b7,\u03c0\u02c62)(cid:1)2 +(cid:0) P\u2217(x,\u00b7,\u03c0\u02c62)\u2212P\u02c6 (x,\u00b7,\u03c0\u02c62)(cid:1)2(cid:3) \u03c0f t t t t\u22121 t t =2E (cid:2)(cid:0) P\u2217(x,\u00b7,\u03c0\u02dc2)\u2212P\u2217(x,\u00b7,\u03c0\u02c62)(cid:1)2(cid:3) +2E (cid:2)(cid:0) P\u2217(x,\u00b7,\u03c0\u02c62)\u2212P\u02c6 (x,\u00b7,\u03c0\u02c62)(cid:1)2(cid:3) \u03c0f t t \u03c0f t t\u22121 t t t \u22642E (cid:2)(cid:0) P\u2217(x,\u00b7,\u03c0\u02dc2)\u2212P\u2217(x,\u00b7,\u03c0\u02c62)(cid:1)2(cid:3) +2E (cid:2)(cid:0) bGP (x,\u00b7,\u03c0\u02c62)(cid:1)2(cid:3) \u03c0f t t \u03c0f t\u22121 t t t \u22648\u03b72exp(8\u03b7)E E [(P\u02c6(x,a1,a2)\u2212P\u2217(x,a1,a2))2]+2E (cid:2)(cid:0) bGP (x,\u00b7,\u03c0\u02c62)(cid:1)2(cid:3) a1\u223c\u03c00 a2\u223c\u03c00 \u03c0 tf t\u22121 t \u2264(8\u03b72exp(8\u03b7)+2exp(2\u03b7))E E (cid:2)(cid:0) bGP (x,a1,a2)(cid:1)2(cid:3) ) a1\u223c\u03c00 a2\u223c\u03c00 t\u22121 \u2264(8\u03b72exp(9\u03b7)+2exp(3\u03b7))E E (cid:2)(cid:0) bGP (x,a1,a2)(cid:1)2(cid:3) ), a1\u223c\u03c0 t1 a2\u223c\u03c00 t\u22121 wheretheboundofE (cid:2)(cid:0) P\u2217(x,\u00b7,\u03c0\u02dc2)\u2212P\u2217(x,\u00b7,\u03c0\u02c62)(cid:1)2(cid:3) followsthetechnicalanalysisintheoffline \u03c0f t t t setting(refertoEquation(6))andthelasttwoinequalitiesfollowLemma1. Thus,wehave (cid:88) Regret (T)= J (\u03c01,\u2217,\u03c02,\u2217)\u2212J (\u03c0\u02c61,\u03c0\u02dc2) GP GP GP t t t\u2208[T] (cid:88) \u2264(8\u03b73exp(9\u03b7)+2\u03b7exp(3\u03b7)) E [bGP (x,a1,a2)2] x\u223cd0,a1\u223c\u03c0 t1,a2\u223c\u03c00 t\u22121 t\u2208[T] (cid:88) \u2264(8\u03b73exp(9\u03b7)+2\u03b7exp(3\u03b7)) E [min{1,U (\u03bb,x,a1,a2;P ,D )}2]\u03b22 x\u223cd0,a1\u223c\u03c0 t1,a2\u223c\u03c00 GP t t T;GP t\u2208[T] \u22644\u03b7exp(3\u03b7)(4\u03b72exp(6\u03b7)+1)log(N T/\u03b4)d (P,\u03bb,T). P GP Therefore,wehavethefinalresult Regret (T)=O(log(N T/\u03b4)d (P,\u03bb,T)), GP P GP where T (cid:88) d (P,\u03bb,T):= sup min{1,[U (\u03bb,x ,a1,a2;P,D )]2}. GP GP t t t t\u22121 x1:T,a1 1:T,a2 1:T t=1 \u25a0 D.2 TheBradley-TerryModel Similar to the proof for the general preference model, we define bBT(x,a1,a2) = min{1,\u03b2 \u00b7 t T;BT U (\u03bb,x,a1,a2;R ,D )}and BT t t t R ={R\u2208R:(cid:88) (R\u02c6 (x ,a1)\u2212R\u02c6 (x ,a2)\u2212(R\u2217(x ,a1)\u2212R\u2217(x ,a2)))2+\u03bb\u2264\u03b22 }, t t i i t i i i i i i T;BT i=1 where\u03b22 =4elog(N T/\u03b4)and\u03bb\u2264\u03b22 /2. T;BT R T;BT 23 --- Page 24 --- Lemma8 UnderAlgorithm1,wehavewithprobabilityatleast1\u2212\u03b4forallt\u2208[T],theuniform optimismeventthat E ={R\u02c6 (x,a1)\u2212R\u02c6 (x,a2)+bBT(x,a1,a2)\u2212(R\u2217(x,a1)\u2212R\u2217(x,a2))>0, t t t t \u2200(x,a1,a2)\u2208X \u00d7A\u00d7A} holdstrue. Proof: ByLemma5,forallt\u2208[T],withprobabilityatleast1\u2212\u03b4, (cid:88)t (cid:104) R\u02c6(x ,a1)\u2212R\u02c6(x ,a2)\u2212(cid:0) R\u2217(x ,a1)\u2212R\u2217(x ,a2)(cid:1)(cid:105)2 \u22642elog(cid:18) N RT(cid:19) = 1 \u03b22 . i i i i i i i i \u03b4 2 T;BT i=1 Hence,wededucethatforany(x,a1,a2)\u2208X \u00d7A\u00d7A, |R\u02c6 (x,a1)\u2212R\u02c6 (x,a2)\u2212(R\u2217(x,a1)\u2212R\u2217(x,a2))| t t |R (x,a1)\u2212R (x,a2)\u2212R (x,a1)+R (x,a2)| \u2264 sup 1 1 2 2 (cid:113) R1,R2\u2208R \u03bb+(cid:80)t i=1(R 1(x i,a1 i)\u2212R 1(x i,a2 i)\u2212R 2(x i,a1 i)+R 2(x i,a2 i))2 (cid:118) (cid:117) t \u00b7(cid:117) (cid:116)\u03bb+(cid:88) (R\u02c6 (x ,a1)\u2212R\u02c6 (x ,a2)\u2212(R\u2217(x ,a1)\u2212R\u2217(x ,a2)))2 t i i t i i i i i i i=1 (cid:114) 1 \u2264U (\u03bb,x,a1,a2;R ,D ) \u03bb+ \u03b22 BT t t 2 T;BT \u2264U (\u03bb,x,a1,a2;R ,D )\u03b2 , BT t t T;BT whichconcludestheproof. \u25a0 Proof: [Proof of Theorem 2] Conditioning on the event \u222a E in Lemma 8, we can bound the t\u2208[T] desireregretasfollows. Forsingle-stepregret,fromtheanalysisfortheofflinesetting(Equation(7)), wehave J (\u03c0\u2217)\u2212J (\u03c0\u02c6 ) BT BT t \u2264\u03b7E [(R\u02c6 (x,a1)\u2212R\u2217(x,a1)\u2212(R\u02c6 (x,a2)\u2212R\u2217(x,a2)))2] x\u223cd0,a1\u223c\u03c0 f\u2032,a2\u223c\u03c00 t\u22121 t\u22121 \u2264\u03b7E [bBT (x,a1,a2)2] x\u223cd0,a1\u223c\u03c0 f\u2032,a2\u223c\u03c00 t\u22121 \u2264\u03b7exp(2\u03b7)E [bBT (x,a1,a2)2], x\u223cd0,a1\u223c\u03c0 t1,a2\u223c\u03c00 t\u22121 wherethelastinequalityisbyLemma1. Thus (cid:88) Regret (T)= J (\u03c0\u2217)\u2212J (\u03c0\u02c6 ) BT BT BT t t\u2208[T] (cid:88) \u2264\u03b7exp(2\u03b7) E [bBT (x,a1,a2)2] x\u223cd0,a1\u223c\u03c0 t1,a2\u223c\u03c00 t\u22121 t\u2208[T] (cid:88) \u2264\u03b7exp(2\u03b7) E [min{1,U (\u03bb,x,a1,a2;R ,D )}2]\u03b22 x\u223cd0,a1\u223c\u03c0 t1,a2\u223c\u03c00 BT t t T;BT t\u2208[T] T (cid:88) \u22644\u03b7exp(2\u03b7+1)log(N T/\u03b4) sup min{1,[U (\u03bb,x,a1,a2;R,D )]2} R BT t\u22121 x1:T,a1 1:T,a2 1:T t=1 \u22644\u03b7exp(2\u03b7+1)log(N T/\u03b4)d (R,\u03bb,T), R BT whichconcludestheproof. \u25a0 Remark1 Theabovetheoreticalresultsindicatethatthegreedyalgorithmssufferfromsomeextra constantof\u03b7(e.g.exp(\u03b7))comparedtousingoptimism.Atthesametime,aswehavediscussedinthe mainpaper,thebonustermsinoptimismareusuallycomputationallyintensive,whereasthegreedy policyenjoyslesscomputationalcomplexity. Additionally, itisunclearwhethertheseadditional dependencieson\u03b7arefundamental\u2013wewillexplorehowtotightentheseboundsinfuturework. 24 --- Page 25 --- E ExperimentDetails E.1 ImplementationConsiderationsoftheGeneralPreferenceModel Nashequilibriumoracleapproximationviaaniterationmethod. GivenafunctionP \u2208P,the Nashequilibriumis (\u03c01,\u2217,\u03c02,\u2217)=(\u03c0 ,\u03c0 )=argmax min J (\u03c01,\u03c02). (10) P P P P GP \u03c01\u2208\u03a0 \u03c02\u2208\u03a0 However,Equation(10)doesnothaveaclosedform,whichposesachallengeintheexperiment. We notethatitcanbetransformedtoafixed-pointproblembyProposition3: \u03c0 (a|x)exp(\u03b7(cid:80) \u03c0(a\u2032|x)P(x,a,a\u2032)) \u03c0 =f(\u03c0 ), f(\u03c0)= 0 a\u2032 . P P (cid:80) \u03c0 (a|x)exp(\u03b7(cid:80) \u03c0(a\u2032|x)P(x,a,a\u2032)) a 0 a\u2032 Inthisspirit,weusetheiterationmethodtosolvetheabovefixed-pointproblem,andtheiterationhas beenshowntoconvergetothefixed-point(i.e.,theNashpoint)inourexperiment. Achieve optimism by enhancer and rejection sampling. One way to achieve optimism is by choosing\u03c02 astheenhancerthatmaximizestheuncertaintyto\u03c01 (Yeetal.,2024b;Xiongetal., 2023). Our",
  "results": "results further corroborated the effectiveness of greedy samplingacrossbothpreferencemodels. Acknowledgments TheauthorsthankWeiXiongandChenluYefortheirhelpfulfeedbackontheinitialdraftofthispaper. TheworkofDiWuandCongShenwaspartiallysupportedbytheU.S.NationalScienceFoundation (NSF)undergrants2143559and2313110,andtheUniversityofVirginiaGrandChallengeResearch Investments\u2013DigitalTechnologySmartInfrastructure(StrategicInvestmentFundAward#200). The workofJingYangwaspartiallysupportedbytheU.S.NSFundergrants2531023and2531789.",
  "experiments": "experiment also adopts this method to simulate the online RLHF with optimism. In thegeneralcase,\u03c02 doesnotadmitaclosedform. Thus,weuseatournament-styleprocedureto getthebestresponse(andrejectallotherresponses),andtakethebestresponsesat\u03c02 (Yeetal., 2024b;Dongetal.,2023). Thenumberoftournamentcompetitorscanbeusedtocontrolthelevelof optimism. E.2 ExperimentSetup InboththegeneralpreferencemodelandtheBTmodel,welimitthescopetothelinearcaseand assumetheactionandcontextarevectorsinRk. Inthegeneralpreferencemodel,weparameterize thepreferenceoracleP\u2217 \u2208P byatensorM (withsizek\u00d7k\u00d7k),andintheBradley-Terrymodel, weparameterizetherewardfunctionR\u2217 \u2208RbyamatrixW (withsizek\u00d7k). Wegivetheexact formbelow. Generalpreferencemodel. WeparameterizethepreferenceoracleP \u2208P byatensorM (with sizek\u00d7k\u00d7k)as (a1)T(xM)a2 P :X \u00d7A\u00d7A\u2192[0,1], (x,a1,a2)(cid:55)\u2192 \u2208[0,1]. (a1)T(xM)a2+(a2)T(xM)a1 Bradley-Terrymodel. WeparameterizetherewardfunctionR \u2208 RbyamatrixW (withsize k\u00d7k)as R:X \u00d7A\u2192[0,1], (x,a)(cid:55)\u2192xTWa. Forimplementation,wechoosek =5andfirstuniformlyrandomlysamplefrom[0,1]toconstructthe ground-truthpreferencemodelparametersM\u2217andW\u2217. Wesimilarlysample6vectorsfrom[0,1]5 astheactionsetA. Ineachiteration,werandomlysampleavectorfromtheuniformdistribution in[0,1]5 asthecontextvector,andthenwesampleactionpairs(a1,a2)basedonthepolicies. We runthetrajectoryforT iterationsandrepeattheexperiments5times,computingtheaveragesand standarddeviations. E.3 AdditionalExperiments Tostudytheinfluenceoftheregularizationcoefficient\u03b7,wefurtherconductexperimentsevaluating theperformanceofthealgorithmunderdifferent\u03b7values. Theexperimentsettingremainsthesame as Section E.2. We choose \u03b7 = 1,2,3 in the greedy sampling algorithm under both the general preferencemodelandtheBTmodel. TheresultsarepresentedinFigure3. Wecanseethatunderall \u03b7values,thegreedysamplingconvergesasourtheoremhassuggested. 25 --- Page 26 --- (a)Stepregret,GP (b)Cumulativeregret,GP Figure3: Thecomparisonofdifferentregularizationcoefficients\u03b7underthegeneralpreferencemodel. (a)Stepregret,BT (b)Cumulativeregret,BT Figure4: Comparisonofdifferentregularizationcoefficients\u03b7undertheBradley-Terrymodel. F AuxiliaryLemmas Lemma9(Freedman\u2019sInequality) LetM,v >0befixedconstants. Let{X }n beastochastic i i=1 process,{G } beasequenceof\u03c3-fields,andX beG -measurable,whilealmostsurely i i i i n (cid:88) E[X |G ]=0,|X |\u2264M, and E[X2|G ]\u2264v. i i i i i\u22121 i=1 Thenforany\u03b4 >0,withprobabilityatleast1\u2212\u03b4,itholdsthat n (cid:88) (cid:112) 2 X \u2264 2vlog(1/\u03b4)+ Mlog(1/\u03b4). i 3 i=1 Lemma10(MartingaleExponentialInequalities) Consider a sequence of random functions \u03be (Z ),...,\u03be (Z ),...withrespecttofiltration{F }. Wehaveforany\u03b4 \u2208(0,1)and\u03bb>0: 1 1 t t t (cid:34) n n (cid:35) (cid:88) log(1/\u03b4) 1 (cid:88) P \u2203n>0:\u2212 \u03be \u2265 + logE exp(\u2212\u03bb\u03be ) \u2264\u03b4, i \u03bb \u03bb Z(y) i i i=1 i=1 whereZ =(Z(x),Z(y))andZ =(Z ,...,Z ). t t t t 1 t 26 --- Page 27 --- Lemma11(MultiplicativeChernoffBounds) AssumethatX \u2208[0,1]withEX =\u00b5. Thenforall \u03f5>0, (cid:16) (cid:17) (cid:104)\u22122n\u00b5\u03f52(cid:105) P X\u00af \u2265(1+\u03f5)\u00b5 \u2264exp n 2+\u03f5 (cid:16) (cid:17) (cid:104)\u22122n\u00b5\u03f52(cid:105) P X\u00af \u2264(1\u2212\u03f5)\u00b5 \u2264exp . n 2 Moreover,fort>0,wehave (cid:114) (cid:16) 2\u00b5t t (cid:17) P X\u00af \u2265\u00b5+ + \u2264exp(\u2212t). n n 3n Proof: RefertotheproofofCorollary2.18inZhang(2023). Remark2 The multiplicative Chernoff bounds (Lemma 11) can be expressed as follows. With probabilityatleast1\u2212\u03b4: (cid:114) 2\u00b5ln(1/\u03b4) \u00b5\u2264X\u00af + . n n Itimpliesthatforany\u03b3 \u2208(0,1): ln(1/\u03b4) X\u00af \u2265(1\u2212\u03b3)\u00b5\u2212 . n 2\u03b3n \u25a0 Lemma12 Supposea,b\u22650. Ifx2 \u2264a+b\u00b7x,thenx2 \u22642b2+2a. Proof: Bysolvingtherootofquadraticpolynomialq(x):=x2\u2212b\u00b7x\u2212a,weobtainmax{x ,x }= \u221a \u221a 1 2 (b+ b2+4a)/2. Hence,wehavex\u2264(b+ b2+4a)/2providedthatq(x)\u22640. Thenwefurther have x2 \u2264 1(cid:16) b+(cid:112) b2+4a(cid:17)2 \u2264 1 \u00b72(cid:0) b2+b2+4a(cid:1) \u22642b2+2a. (11) 4 4 \u25a0 Lemma13 LetX bearandomvariableand0\u2264X \u2264M,wehave Var(X)\u2264ME(X). Proof: FromtheBhatia-Davisinequality,wehave Var(X)\u2264(M \u2212E(X))E(X)\u2264ME(X). \u25a0 Lemma14(Online-to-batchconversion) Ifanalgorithmhasasublinearregretofc\u2020\u00b7logT,then thealgorithmfindsan\u03f5-optimalpolicywithatmost\u0398(cid:101)(cid:0) c\u2020/\u03f5(cid:1) samples,where\u0398(cid:101) omitslogarithmic termsofc\u2020/\u03f5. Herec\u2020isaproblem-dependentconstant. Proof: Wedenotethepolicysequenceas{\u03c01,\u00b7\u00b7\u00b7 ,\u03c0T}. Then,bydefinitionofregret,weknow T Regret(T)=TV\u2217(x )\u2212(cid:88) V\u03c0t (x ) 1 1 1 1 t=1 \u2264c\u2020logT. 27 --- Page 28 --- Weconsidertheuniformpolicy\u03c0\u02dc :=Uniform(\u03c01,\u00b7\u00b7\u00b7 ,\u03c0T). Itfollowsthat T V\u2217(x )\u2212V\u03c0\u02dc(x )=V\u2217(x )\u2212 1 (cid:88) V\u03c0t (x )\u2264c\u2020logT . 1 1 1 1 1 1 T 1 1 T t=1 Itsufficestoprovethat logT c\u2020 \u2264\u03f5, T whichisequivalenttosolving T \u2264exp(T\u03f5/c\u2020). ByusingtheLambertW function,wecanprovethat W(1)c\u2020 T \u2265 , \u03f5 whereW(1)\u2265log(1/\u03f5)\u2212loglog(1/\u03f5). \u25a0 28",
  "conclusion": "Conclusion This work investigated reinforcement learning from human feedback (RLHF) under the KL- regularizedcontextualbanditsframework. UnderboththegeneralpreferencemodelandtheBradley- Terry(BT)model,itwasdemonstratedthattheseeminglysimplegreedysamplingwithrespectto empiricalestimatesisprovablyefficient. Inparticular,itachievesregretsofO(log(T))intheonline settingandsamplecomplexityofO(\u03b5\u22121)withthesingle-policycoverageintheofflinesetting. These resultsarereportedforthefirsttimeunderthegeneralpreferencemodel,andalsomatchprevious performanceboundsundertheBTmodelwhileeliminatingtheneedforconstructingconfidence bounds(thusresultinginmuchlowercomputationaloverhead). Thekeytechnicalinsightisthat KLregularizationconfineseverycandidateoptimalpolicywithinaboundedlikelihood-ratioregime around the",
  "references": "References Abbasi-Yadkori,Y.,P\u00e1l,D.,andSzepesv\u00e1ri,C.(2011). Improvedalgorithmsforlinearstochastic bandits. AdvancesinNeuralInformationProcessingSystems,24. Achiam,J.,Adler,S.,Agarwal,S.,Ahmad,L.,Akkaya,I.,Aleman,F.L.,Almeida,D.,Altenschmidt, J.,Altman,S.,Anadkat,S.,etal.(2023). GPT-4technicalreport. arXivpreprintarXiv:2303.08774. 10 --- Page 11 --- Agarwal,A.,Jiang,N.,Kakade,S.M.,andSun,W.(2019). Reinforcementlearning: Theoryand algorithms. CSDept.,UWSeattle,Seattle,WA,USA,Tech.Rep,32:96. Auer,P.,Cesa-Bianchi,N.,andFischer,P.(2002). Finite-timeanalysisofthemultiarmedbandit problem. Machinelearning,47:235\u2013256. Auer,P.,Jaksch,T.,andOrtner,R.(2008). Near-optimalregretboundsforreinforcementlearning. AdvancesinNeuralInformationProcessingSystems,21. Azar,M.G.,Guo,Z.D.,Piot,B.,Munos,R.,Rowland,M.,Valko,M.,andCalandriello,D.(2024). Ageneraltheoreticalparadigmtounderstandlearningfromhumanpreferences. InInternational ConferenceonArtificialIntelligenceandStatistics,pages4447\u20134455.PMLR. Azar,M.G.,Osband,I.,andMunos,R.(2017). Minimaxregretboundsforreinforcementlearning. InInternationalconferenceonmachinelearning,pages263\u2013272.PMLR. Bai,Y.,Jones,A.,Ndousse,K.,Askell,A.,Chen,A.,DasSarma,N.,Drain,D.,Fort,S.,Ganguli,D., Henighan,T.,etal.(2022). Trainingahelpfulandharmlessassistantwithreinforcementlearning fromhumanfeedback. arXivpreprintarXiv:2204.05862. Bradley,R.A.andTerry,M.E.(1952). Rankanalysisofincompleteblockdesigns: I.themethodof pairedcomparisons. Biometrika,39(3/4):324\u2013345. Cai,Q.,Yang,Z.,Jin,C.,andWang,Z.(2020). Provablyefficientexplorationinpolicyoptimization. InInternationalConferenceonMachineLearning,pages1283\u20131294.PMLR. Cen,S.,Mei,J.,Goshvadi,K.,Dai,H.,Yang,T.,Yang,S.,Schuurmans,D.,Chi,Y.,andDai,B. (2024). Value-incentivized preference optimization: A unified approach to online and offline RLHF. arXivpreprintarXiv:2405.19320. Dong,H.,Xiong,W.,Goyal,D.,Zhang,Y.,Chow,W.,Pan,R.,Diao,S.,Zhang,J.,Shum,K.,and Zhang,T.(2023). RAFT:Rewardrankedfinetuningforgenerativefoundationmodelalignment. arXivpreprintarXiv:2304.06767. Dud\u00edk,M.,Hofmann,K.,Schapire,R.E.,Slivkins,A.,andZoghi,M.(2015). Contextualdueling bandits. InConferenceonLearningTheory,pages563\u2013587.PMLR. Guo,S.,Zhang,B.,Liu,T.,Liu,T.,Khalman,M.,Llinares,F.,Rame,A.,Mesnard,T.,Zhao,Y., Piot,B.,etal.(2024). DirectlanguagemodelalignmentfromonlineAIfeedback. arXivpreprint arXiv:2402.04792. Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M. I. (2018). Is Q-learning provably efficient? AdvancesinNeuralInformationProcessingSystems,31. Jin,C.,Yang,Z.,Wang,Z.,andJordan,M.I.(2020). Provablyefficientreinforcementlearningwith linearfunctionapproximation. InConferenceonlearningtheory,pages2137\u20132143.PMLR. Jin,Y.,Yang,Z.,andWang,Z.(2021).IspessimismprovablyefficientforofflineRL?InInternational ConferenceonMachineLearning,pages5084\u20135096.PMLR. Lattimore,T.andSzepesv\u00e1ri,C.(2020). BanditAlgorithms. CambridgeUniversityPress. Li,G.,Shi,L.,Chen,Y.,Chi,Y.,andWei,Y.(2024). Settlingthesamplecomplexityofmodel-based offlinereinforcementlearning. TheAnnalsofStatistics,52(1):233\u2013260. Munos, R., Valko, M., Calandriello, D., Azar, M. G., Rowland, M., Guo, Z. D., Tang, Y., Geist, M.,Mesnard,T.,Michi,A.,etal.(2023). Nashlearningfromhumanfeedback. arXivpreprint arXiv:2312.00886,18. Osband,I.andVanRoy,B.(2016). Onlowerboundsforregretinreinforcementlearning. arXiv preprintarXiv:1608.02732. Ouyang,L.,Wu,J.,Jiang,X.,Almeida,D.,Wainwright,C.,Mishkin,P.,Zhang,C.,Agarwal,S., Slama,K.,Ray,A.,etal.(2022). Traininglanguagemodelstofollowinstructionswithhuman feedback. AdvancesinNeuralInformationProcessingSystems,35:27730\u201327744. 11 --- Page 12 --- Pacchiano, A., Saha, A., andLee, J.(2021). DuelingRL:reinforcementlearningwithtrajectory preferences. arXivpreprintarXiv:2111.04850. Rafailov,R.,Hejna,J.,Park,R.,andFinn,C.(2024). FromrtoQ*: Yourlanguagemodelissecretly aQ-function. arXivpreprintarXiv:2404.12358. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. (2023). Direct preferenceoptimization: Yourlanguagemodelissecretlyarewardmodel. AdvancesinNeural InformationProcessingSystems,36:53728\u201353741. Rashidinejad,P.,Zhu,B.,Ma,C.,Jiao,J.,andRussell,S.(2021). Bridgingofflinereinforcement learningandimitationlearning: Ataleofpessimism. AdvancesinNeuralInformationProcessing Systems,34:11702\u201311716. Russo, D. and Van Roy, B. (2013). Eluder dimension and the sample complexity of optimistic exploration. AdvancesinNeuralInformationProcessingSystems,26. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy optimizationalgorithms. arXivpreprintarXiv:1707.06347. Sherman,U.,Cohen,A.,Koren,T.,andMansour,Y.(2023). Rate-optimalpolicyoptimizationfor linearMarkovdecisionprocesses. arXivpreprintarXiv:2308.14642. Song,Y.,Swamy,G.,Singh,A.,Bagnell,J.A.,andSun,W.(2024). Theimportanceofonlinedata: Understandingpreferencefine-tuningviacoverage. Stiennon,N., Ouyang,L., Wu,J., Ziegler,D., Lowe, R.,Voss,C.,Radford, A.,Amodei, D.,and Christiano, P. F. (2020). Learning to summarize with human feedback. Advances in Neural InformationProcessingSystems,33:3008\u20133021. Sutton, R. S., Barto, A. G., et al. (2018). Reinforcement learning: An"
}