--- Page 1 --- Causal Ordering for Structure Learning From Time Series Pedro P. Sanchez∗  School of Engineering, University of Edinburgh, UK Damian Machlanski∗  School of Engineering, University of Edinburgh, UK Causality in Healthcare AI Hub (CHAI), UK Steven McDonagh  School of Engineering, University of Edinburgh, UK Causality in Healthcare AI Hub (CHAI), UK Sotirios A. Tsaftaris  School of Engineering, University of Edinburgh, UK Causality in Healthcare AI Hub (CHAI), UK Abstract Predicting causal structure from time series data is crucial for understanding complex phe- nomena in physiology, brain connectivity, climate dynamics, and socio-economic behaviour. Causal discovery in time series is hindered by the combinatorial complexity of identifying true causal relationships, especially as the number of variables and time points grow. A common approach to simplify the task is the so-called ordering-based methods. Traditional ordering methods inherently limit the representational capacity of the resulting model. In this work, we fix this issue by leveraging multiple valid causal orderings, instead of a single one as standard practice. We propose DOTS (Diffusion Ordered Temporal Structure), us- ing diffusion-based causal discovery for temporal data. By integrating multiple orderings, DOTSeffectivelyrecoversthetransitiveclosureoftheunderlyingdirectedacyclicgraph,mit- igating spurious artifacts inherent in single-ordering approaches. We formalise the problem understandardassumptionssuchasstationarityandtheadditivenoisemodel,andleverage score matching with diffusion processes to enable efficient Hessian estimation. Extensive experimentsvalidatetheapproach. Empiricalevaluationsonsyntheticandreal-worlddata- sets demonstrate that DOTS outperforms state-of-the-art baselines, offering a scalable and robust approach to temporal causal discovery. On synthetic benchmarks (d=3−6 variables, T=200−5,000 samples), DOTS improves mean window-graph F1 from 0.63 (best baseline) to 0.81. On the CausalTime real-world benchmark (d=20−36), while baselines remain the best on individual datasets, DOTS attains the highest average summary-graph F1 while halving runtime relative to graph-optimisation methods. These results establish DOTS as a scalable and accurate solution for temporal causal discovery. 1 Introduction Understandingcause-effectrelationshipsfromtimeseriesdataisessentialinfieldslikebiology(Marbachetal., 2009), neuroscience (Friston et al., 2003), climate science (Runge et al., 2019), and economics (Pamfil et al., 2020),whereuncoveringhowoneeventinfluencesanothercanleadtovaluableinsightsandbetterpredictions. A key challenge in temporal causal discovery (Granger, 1969; Peters et al., 2013; Nauta et al., 2019; Runge, 2020)isthecombinatorialcomplexity—therearemanypossiblewaysthatvariablescaninfluenceeachother, making it difficult to identify the true causal structure. The goal is to discover a temporal Directed Acyclic Graph (DAG), G, representing these relationships. The core problem is illustrated in Figure 1. ∗Equalcontribution. 1 5202 tcO 82 ]GL.sc[ 1v93642.0152:viXra --- Page 2 --- Input xT×d Target G xt xt+1 xt+2 1 1 1 Learnstructure xt xt+1 xt+2 2 2 2 xt xt+1 xt+2 3 3 3 Time(t) Figure 1: Temporal causal discovery estimates, from raw time series data (left), the underlying temporal causal DAG G (right). Causalorderingapproaches(Verma&Pearl,1990;Friedman&Koller,2003;Bühlmannetal.,2014;Rolland et al., 2022; Sanchez et al., 2023) offer a scalable alternative to direct graph estimation by constraining the searchspace,reducingitfromafulladjacencymatrixtoasetoforderedpermutations. Whilethisreduction scalesefficientlywithrespecttothenumberofvariablesandsamples,itcompromisesrepresentationalpower: a single causal ordering can imply extra edges, spurious artifacts, that are absent in the original DAG. In other words, committing to a single arbitrary ordering has an inherent downside: every node is deemed a potential ancestor of all subsequent nodes, creating spurious edges that must be pruned heuristically. To mitigate this, existing methods employ a two-step process, wherein a feature selection post-processing step prunes spurious artifacts introduced by the ordering. Crucially, a DAG generally admits multiple valid orderings consistent with edge directions. Each ordering contributescomplementaryinformationabouttheunderlyingcausalstructure. Bysystematicallygenerating and aggregating information from diverse orderings, we can filter out the spurious artifacts specific to any single,arbitraryorderingchoice. Asweaggregatemoreorderings,weconvergetowardsthetransitiveclosure (G+)oftheunderlyingtemporalDAG.G+ containsalltruedirectedgespresentinG. WhileG+ alsoincludes edges representing indirect causal pathways, it represents the stable, necessary ancestral relationships to recover the true graph. Therefore, harnessing several orderings increases both precision and recall and does so without incurring the combinatorial cost of full graph search. Our central insight is to embrace this multiplicity rather than fight it. Thebenefitofmultipleorderingshingesonaccesstoadiverse collectionofvalidorderings. Naïveresampling or greedy heuristics tend to revisit near-duplicate permutations. We address this limitation by drawing on denoising diffusion models (Song & Ermon, 2019; Ho et al., 2020; Sanchez et al., 2023). Once trained to approximate the data score, a single diffusion network delivers Hessian estimates at many noise scales; applyingaleaf-detectionruleateachscaleyieldsafreshordering. Becausedifferentscalesemphasisedifferent frequency bands of the data, the resulting orderings cover the search space more uniformly. Moreover, diffusiontrainingamortisescomputation—afteronenetworkfitwecansamplehundredsoforderingswithout retraining—making the approach viable for datasets that are large w.r.t. variables and samples. Finally, causal ordering approaches have not been extensively explored for temporal causal discovery, but their principles naturally extend to time-dependent data. Our focus on the temporal setting is motivated by two key advantages: (i) temporal constraints provide additional structural information that enhances ordering reliability through the temporal priority principle, and (ii) the temporal priority principle offers a natural filteringmechanismforspuriousedges. Thetemporaldimensionprovidesaninherentsetofcausalorderings, each contributing incremental insights. Building on these ideas, we introduce DOTS (Diffusion-Ordered Temporal Structure), a theoretically motivated ensemble-based approach for temporal causal discovery that systematically aggregates multiple diffusion-generated orderings. Our contributions are three-fold: 2 --- Page 3 --- (i) Weestablishtheoreticallyandempiricallythataggregatingmultiplecausalorderingstoestimatethe transitive closure (G+) provides a more robust foundation for temporal causal discovery than tradi- tional single-ordering approaches, effectively filtering spurious edges arising from arbitrary ordering choices. (ii) WeintroduceDOTS,anovelalgorithmleveragingmulti-scalediffusionmodelstoefficientlygenerate the diverse set of causal orderings required for robust aggregation in the temporal domain. (iii) We provide extensive benchmarks, adapting several static ordering methods for temporal data and demonstrating the superior performance of the multi-ordering strategy implemented by DOTS against both these adapted methods and state-of-the-art temporal causal discovery baselines on synthetic and real-world datasets. The rest of the paper is organised as follows. Section 2 reviews notation and identifiability assumptions. Section 3 establishes the theoretical foundations of multi-ordering aggregation, and Section 4 presents the DOTSalgorithmindetail. Section5showsexperimentalresults. RelevantliteratureisdiscussedinSection6. Section 7 concludes the paper. 2 Preliminaries 2.1 Notation We present in Table 1 a description of all the symbols in the manuscript. 2.2 Problem Definition We aim to discover the causal structure among d variables arranged in a temporal setting. Let xt ∈ Rd denoteavector-valuedrandomvariableattimet,withcomponents(cid:0)xt, ..., xt(cid:1). Atemporaldirectedacyclic 1 d graph (temporal DAG) Gt then describes the causal relationships among these variables, where each node corresponds to a component xt, and the directed edges represent causal links between variables at the same i time or across time steps (Eichler, 2012). We now introduce standard assumptions for causal discovery from time series: Assumption 1(TemporalDAG). ThetruecausalstructureofthedatacanberepresentedbyatemporalDAG Gt. Thisgraphincludeslaggededgesoftheformxt−τ →xt (forτ >0)andcontemporaneousedgesxt →xt. i j i j This assumption effectively captures the temporal priority principle (Hume, 1904; Rankin & McCormack, 2013; Assaad et al., 2022) where causes precede effects in time. For any pair (cid:0)xt−τ, xt(cid:1), an edge xt−τ →xt i j i j indicatesthatthevalueofx attime(t−τ)influencesx attimet. Thisforbidsbackward-in-timecausation i j and helps simplify structure learning since the search space is smaller. Assumption 2 (Stationarity). The causal relationships in Gt remain invariant over time; that is, both the causal links and their strengths remain unchanged for all values of t. Assumption 3 (TimeSeriesModelswithIndependentNoise(TiMINo)). Thestructuralcausalmodelfollows a temporal additive noise formulation: (cid:16) (cid:17) xt = f Pa (xt) + ϵt, (1) j j Gt j j where Pa (xt) denotes the set of parent variables of xt in Gt, f is a nonlinear function, and ϵt is an Gt j j j j independent noise term. TiMINo (Peters et al., 2013) essentially extends the additive noise model (ANM) framework to time series. Assumption 4 (Causal Sufficiency). All common causes of observed variables are measured; that is, there are no unobserved confounders that influence multiple components simultaneously. Assumption 5 (Noise Distribution Regularity). The exogenous noise terms ϵt in Equation 1 have densities j pu such that ∂2logpu is constant. This includes Gaussian noise as a special case but also encompasses ∂x2 other distributions with quadratic log-densities. This regularity condition is required for the score-matching approach to satisfy the leaf-detection criterion in Equation 3 and extends the assumptions of Sanchez et al. (2023) to the temporal domain using the TiMINo identifiability framework (Peters et al., 2013). 3 --- Page 4 --- Table 1: Summary of notation used throughout the paper. Symbol Meaning / role in the paper x∈Rd Random vector of d variables; component i is x . i xt, xt Value / vector at time index t. i d Number of variables (dimensionality). T Number of observed time steps. τ Time lag; xt−τ→xt. i j τ Maximum lag included in A. max k∈{0,...,k } Diffusion (noise-scale) timestep. max k Final diffusion step (fully noised). max π Causal ordering (topological permutation). π Variable at position i in ordering π. i G (Temporal) causal DAG. G+ Transitive closure of G. A∈{0,1}d(τmax+1)×d(τmax+1) Temporal adjacency matrix to be learned. p(x) Data distribution (density). ϵ (x,k) Neural network estimating ∇ logp(x) at scale k. θ x x˜k Noisy version of x at diffusion step k. q(x˜k|x,k) Forward noising distribution. f Structural function generating x from its parents. j j ϵt Independent noise term for xt. j j Pa (xt) Parent set of xt in the temporal DAG. Gt j j Ch(x ) Children of node x . j j (cid:0) (cid:1) H logp(x) (i,j)-entry of Hessian of logp(x) (score Jacobian). i,j E(π) Edge set implied by ordering π. E(m) Intersection of edge sets from m sampled orderings. agg T Set of temporally valid edges (causes precede effects). E(m,T) Aggregated edge set restricted to T. agg W Fraction of sampled orderings where edge i→j appears (vote matrix). ij A˜ Soft transitive-closure entry after thresholding W. ij θ Threshold for vote-matrix aggregation (0<θ≤1). α Variance-preserving coefficient in the forward diffusion process. k 2.3 Objective Given an observational multivariate timeseries dataset D ∈ RT×d containing T timesteps of d variables, our goal is to learn the temporal adjacency structure A∈{0,1}d(τmax+1)×d(τmax+1), where τ max denotes the maximum lag that is being captured in the adjacency matrix. Each entry of A encodes whether there is a directed edge from xt−τ to xt for 0≤τ ≤τ . Note that τ =0 indicates a contemporaneous link xt →xt. i j max i j 2.4 Key Identifiability Results Under stationarity, the additive noise assumption, and causal sufficiency, Peters et al. (2013) show that temporal causal relationships become identifiable if the data follow a restricted structural equation model in which each noise term is statistically independent and no directed cycles exist within a single time slice. Specifically, their Time Series Models with Independent Noise (TiMINo) framework demonstrates that both lagged and instantaneous effects can be recovered uniquely, provided the functional form and noise distribu- tions meet certain identifiability criteria (e.g., linear non-Gaussian or nonlinear Gaussian). 4 --- Page 5 --- 2.5 Causal ordering Causal search over the space of DAGs is an NP-hard problem (Chickering, 1996). Traditional approaches leverage heuristic search strategies to navigate the combinatorial space of potential DAG structures. Order- based search offers a simpler and more effective alternative. By shifting the search from graph structures to node orderings, the strategy exploits the fact that, for a given ordering, identifying the highest-scoring network is not NP-hard. Such causal ordering approaches reduce the search space and inherently satisfy the acyclicity constraints. This bypasses the need for explicit acyclicity checks during the search. These methods find a particular causal ordering of the nodes, i.e., a list of nodes such that a node in the ordering can be a parent only of the nodes appearing after it in the exact ordering. Causal ordering is also known as topological ordering or a causal list in the causal discovery literature (Peters et al., 2017). Formally, causal ordering of a DAG G is defined as a non-unique permutation π of d nodes. Hence, a given node in π always appears before its descendants in the list. Or more formally, π <π ⇐⇒ j ∈De(x ) where De(x ) are the i j i i descendants of the ith node in G (Appendix B in Peters et al. (2017)). 2.6 Causal ordering via score matching Rolland et al. (2022) propose that the score of an ANM with distribution p(x) can be used to estimate the causal ordering by finding leaves. Leaves are nodes of DAG G that do not possess children. Rolland et al. (2022) propose a method to find leaves based on the derivative of the ANM log density (also called score). An analytical expression for the score of an ANM from Equation 1 is ∇ xjlogp(x)= ∂logpu ∂( xx j −f j) − X ∂∂ xf i ∂logpu ∂( xx i−f i) , (2) j j i∈Ch(xj) whereCh(x )denotesthechildrenofx . Usingthisanalyticalequationof∇ logp(x),Rollandetal.(2022) j j xj derive the following condition used to find leaf nodes. Given a nonlinear ANM with a noise distribution pu and a leaf node j, assuming that ∂2logpu =a, where a is a constant, then ∂x2 Var [H (logp(x))]=0. (3) D j,j This rule is based on the score’s Jacobian (or Hessian of the log distribution). H (logp(x)) is used in j,j Rolland et al. (2022) to propose a causal ordering algorithm that iteratively finds and removes leaf nodes from the dataset. Rolland et al. (2022) re-compute the score’s Jacobian with a kernel-based estimation method at each iteration. 2.7 Approximating the score’s Jacobian via diffusion training Instead of computing logp(x) via a kernel-based estimation (Li & Turner, 2018; Rolland et al., 2022), we follow Sanchez et al. (2023) and estimate the score’s Jacobian with diffusion models (Song & Ermon, 2019; Ho et al., 2020). This estimation is based on a diffusion process that progressively corrupts x with Gaussian noiseovertimestepsk ∈{0,...,k }. Letx˜k bethenoisyversionofxatdiffusionstepk. Aneuralnetwork max ϵ (x˜k,k) is trained to denoise x˜k back to x, thereby approximating the true score ∇ logp(x). Formally, θ x this can be written as: E (cid:13) (cid:13)ϵ (cid:0) x˜k,k(cid:1) − ∇ logp(cid:0) x˜k |k(cid:1)(cid:13) (cid:13)2 , x∼p(x), x˜k∼q(x˜k|x,k)(cid:13) θ x˜k (cid:13) where q(x˜k | x,k) defines the forward noising process. Once trained, ϵ effectively yields ∇ logp(x) at θ x various noise scales k, which can be used to estimate the Hessian for causal discovery. Noise at multiple scales explores regions of low data density (Song & Ermon, 2019). The score’s Jacobian can be approximated by learning the score ϵ with denoising diffusion training of θ neuralnetworksandback-propagating(Rumelhartetal.,1986)1 fromtheoutputtotheinputvariables. The quantity can be written, for an input data point x∈Rd, as H logp(x)≈∇ ϵ (x,k), (4) i,j i,j θ 1The Jacobian of a neural network can be efficiently computed with auto-differentiation libraries such as functorch (Hor- aceHe,2021). 5 --- Page 6 --- x 1 DAG to ordered list x x x x x x 2 3 2 4 3 1 x 4 Figure2: ADAGcanberepresentedasanorderedlistfollowingthecausaldirection. Anodeintheordering can cause any subsequent node. Searching over the space of permutations is more efficient than searching over the 2D space of matrices. However, topologically sorting nodes reduces the amount of information in the representation of causal relationships. where ∇ ϵ (x,k) means the ith output of ϵ is backpropagated to the jth input. The diagonal of the i,j θ θ Hessian in Equation 4 can be used for finding leaf nodes as in Equation 3. We use masking (Sanchez et al., 2023) to iteratively find and remove leaf nodes, without retraining the score. 3 Theoretical motivation: temporal structure from multiple causal orderings Causal ordering methods, illustrated in Figure 2, traditionally rely on a single ordering for inferring the full DAG in causal discovery. A single causal graph does not contain sufficient information to reliably infer the DAG, since each node is considered a cause for all subsequent nodes. Therefore, pruning methods are used toremovethespuriousedges(Bühlmannetal.,2014). Withinfinitedataaperfectconditional-independence oracle could delete the spurious edges and retain the true ones. In practice we face finite samples, noisy tests and high dimensionality. Starting from an over-dense candidate set inflates both kinds of statistical error: (i) false positives remain whenever a test fails to reject a truly absent edge (type-II error), and (ii) false negatives appear when a test mistakenly deletes a true edge (type-I error) because that edge co-varies with many irrelevant ancestors in the initial ordering. Hence a single ordering often yields a fragile estimate whosequalityvarieswildlywithsamplesizeandnoiselevel. Incontrast,leveragingmultiplecausalorderings provides a richer representation of the underlying structure. Exploiting multiple valid causal orderings from data naturally follows from the fact that a given DAG typically admits more than one linear ordering, consistent with its structure. Rather than committing to a single topological sort, estimating multiple valid orderings can offer a more complete representationof ancestor–descendant relationships. Indoing so, wecan aggregate local adjacency constraints from each ordering and thereby recover, asymptotically, the transitive closure of the DAG—the minimal set of edges that preserves causal reachability. This perspective avoids over-specifying the order of variablesthatarenotcausallylinkedandreducestheriskofintroducingextraedgesthatdonotexistinthe underlying temporal DAG. 3.1 Recovering DAG structure from a complete set of causal orderings We now investigate how the topological orderings of a DAG relate to its underlying structure. The central takeawayisthatthecollectionofalltopologicalorderingsofaDAGuniquelydeterminesthetransitiveclosure G+, which is a robust representation of the graph’s reachability structure. Topological orderings capture the reachability relation of the graph—a partial order—but multiple DAGs can share the same reachability relation and, consequently, the same set of topological orderings. InaDAGG,everytopologicalorderingisalinearextensionofthereachabilityrelationR. Akeyinsightfrom order theory is formalised in Hiraguchi’s theorem (Hiraguchi, 1955), which establishes that any finite partial ordercanbeexactlyrecoveredastheintersectionofallitslinearextensions(i.e.,totalordersconsistentwith it). In our setting, this implies that aggregating multiple topological orderings of a DAG asymptotically recovers its transitive closure. This idea is formalized in the following proposition: 6 --- Page 7 --- Proposition 1 (Reconstruction of the Transitive Closure). Let G=(V,E) be a DAG, and let L be the set of all its topological orderings. Define a binary relation ≺ on V by: x≺y ⇐⇒ x appears before y in every π ∈L. Then: (i) ≺ is a strict partial order on V (irreflexive and transitive). (ii) For all x,y ∈V, x≺y ⇐⇒ there exists a directed path from x to y in G with x̸=y. Thus, ≺ matches the edges of the transitive closure G+. (iii) Consequently, aggregating all topological orderings in L recovers G+, but not necessarily the original DAG G. Justification. We show part (2) by establishing the equivalence: • (⇒): If there is a directed path from x to y in G with x̸=y, then every topological ordering π ∈L must place x before y to respect the direction of edges. Hence, x≺y. • (⇐): Suppose x ≺ y but no directed path from x to y exists in G with x ̸= y. Since G is a DAG and no path exists from x to y, adding the edge (y,x) to G does not introduce a cycle (otherwise, a path from x to y would exist, contradicting the assumption). In this modified DAG, there exists a topological ordering with y before x, contradicting x ≺ y. Thus, a directed path from x to y must exist in G. This shows that ≺ corresponds to the strict reachability relation, i.e., the edges of G+. Since distinct DAGs can share the same G+, the original G cannot be uniquely recovered from L. Example 1. Let G = (V,E ) with V = {a,b,c} and E = {(a,b),(b,c)}, and let G = (V,E ) with 1 1 1 2 2 E = {(a,b),(b,c),(a,c)}. Both DAGs share the same set of topological orderings: {a,b,c}. In G , the 2 1 ordering respects a → b and b → c; in G , the additional edge (a,c) is consistent with the order. The 2 transitive closure for both is G+ = G+ = (V,{(a,b),(b,c),(a,c)}). Thus, from the common ordering alone, 1 2 we recover G+ (or G+) but cannot distinguish between G and G . 1 2 1 2 Aggregating all topological orderings of a DAG G yields its transitive closure G+ because the relation ≺ captures all pairs of nodes connected by a directed path. However, since distinct DAGs can share the same transitive closure, the original edge set E remains ambiguous. This highlights a fundamental limit: while topological orderings reveal the reachability structure of the graph, they do not specify the precise topology of the DAG. It is crucial to understand that the transitive closure G+ includes both direct edges from the original DAG G and indirect relationships (edges representing multi-step causal pathways). While this means that G+ is not identical to the true causal DAG G, it serves as a robust intermediate representation that captures all ancestral relationships. The subsequent pruning step (Section 4.4) is specifically designed to distinguish between direct and indirect relationships, refining G+ to recover the sparse structure of G. Nevertheless,recoveringthetransitiveclosureG+ isastrongerclaimthanwhathasbeenpreviouslyachieved bysingleorderingmethods(CAM,SCORE,DAS,NoGAM,DiffAN).Asingleorderingrecoversonlyaweak approximationofthecausalstructurebecause,foranygivenpositionintheordering,allsubsequentpositions are considered as potential descendants, introducing spurious edges in addition to possible indirect edges. 7 --- Page 8 --- 3.2 Ordering aggregation and recovery of temporal structure Incausaldiscovery, wedonotknowapriorithetotalnumberofvalidcausalorderingsthataresultingDAG willadmit. Thenumberofcausalorderingsisnotonlyrelatedtothenumberofvariablesbutalsototheedge topology and density. Therefore, we next explore how aggregating a subset of the total orderings improves structure estimation. Empirical evidence, shown in Figure 3, suggests that enumerating all topological sorts is unnecessary to achieve strong performance, as a relatively small subset of randomly sampled orderings can suffice for accurate structure recovery. To formalize the benefit of multiple orderings, assume that for each valid causal ordering π of a DAG G, we obtain an edge set E(π) that corresponds to the directed edges implied by that ordering. Define the aggregated edge set over the intersection of m orderings as m \ E(m) = E(π ). agg i i=1 Then, under the assumption that each valid ordering provides complementary information about the true ancestral relations, following Proposition 1, we have lim E(m) =G+, agg m→M where G+ denotes the transitive closure of the true edge set E of G and where M is the number of possible orderings for G. To validate this idea, we generate random temporal DAGs and use Kahn’s algorithm (Kahn, 1962) to list all possible orderings with topological sorting. Then, we estimate the transitive closure for each DAG and compare it with the estimated E(m) as we increase m for each DAG. We compare the estimated and true agg transitive closure via F1 score. In Figure 3, we show the percentage of orderings m ∗100 used with respect M to the F1 score. For our data, we observe that ∼40% of all orderings is generally enough to recover the transitive closure with high F1 score. Figure3: ImpactofmultiplecausalorderingsonDAGrecovery. Asingle(orfew)ordering(left)mayinclude extra or spurious edges, whereas aggregating multiple orderings (right) more accurately recovers the full transitive closure of the underlying DAG. 8 --- Page 9 --- 3.3 Incorporating Temporal Constraints Temporal constraints are critical when recovering causal structure from time series data. Following the temporalpriorityprinciple;causesprecedeeffects,intime. Thisprincipleforbidsbackward-in-timecausation and simplifies structure learning. To incorporate this constraint into edge aggregation, we restrict the aggregated edge set to include only temporal edges satisfying t−τ <t. Formally, let T ={(xt−τ, xt)|τ ≥0, t−τ <t} i j denote the set of all temporally valid edges. Then, define the temporally constrained aggregated edge set as E(m,T) =E(m)∩T. agg agg This filtering ensures that only edges adhering to the temporal priority principle are retained, thereby excluding any spurious backward-in-time connections and further enhancing the reliability of the recovered DAG. Empirically, as illustrated in Figure 3, adding the temporal constraint decreases the number of causal orderings required to estimate the causal structure for a given F1 score. In summary, integrating temporal constraints into the aggregation of multiple causal orderings is both a natural extension of, and an efficient strategy for, structure learning in time series data. By leveraging the inherent temporal priority principle—where causes always precede effects—we effectively filter and refine the aggregated edge set, ensuring that only temporally valid connections are retained. This dual approach not only enhances the robustness of the recovered causal structure by capturing complementary ancestral informationacrossorderingsbutalsostreamlinesthelearningprocess,asitreducestheeffectivesearchspace and mitigates spurious dependencies. Whileourempiricalvalidationdemonstratesthetheoreticalpotentialofaggregatingmultiplecausalorderings for recovering the transitive closure of a DAG, it is important to note that this evaluation was performed on known graphs where all valid causal orderings were available and the total number of orderings was predetermined. Orderings were selected uniformly at random from this complete set. We further note that for practical applications, an algorithm may generate very similar orderings, thereby limiting the diversity necessary for robust structure recovery. In the following section, we explore methods to induce sufficient variabilityinthegeneratedorderings, enablingtheaggregationprocesstoremainbotheffectiveandefficient in recovering the true causal structure. 4 A Diffusion-Based Approach for Temporal Discovery We introduce DOTS, a method that utilises diffusion processes to recover multiple valid causal orderings in temporal data. Our approach, illustrated in Figure 4, integrates both a frequency domain perspective and multi-scale causal ordering to capture the complex structure of temporal relationships. We refer to our notation (Section 2.1), distinguishing between time lags (τ), diffusion timesteps (k), and the indices for causal orderings (π). 4.1 Why do diffusion steps capture different frequency components? Different diffusion steps k capture distinct aspects of the data, which can be understood from a frequency perspective. Consider a forward diffusion process (e.g., DDPM (Ho et al., 2020)), which decomposes each observation at step k as √ √ x = α x + 1−α ϵ, ϵ∼N(0,I). k k 0 k Since x and ϵ are independent, the characteristic function of x becomes: 0 k √ √ ϕ (ω)=ϕ√ (ω)·ϕ√ (ω)=ϕ ( α ω)·ϕ ( 1−α ω). xk αkx0 1−αkϵ x0 k ϵ k 9 --- Page 10 --- Lag-Embedded Time-Series Data x0 x0 x0 x1 x1 x1 x2 x2 x2 1 2 3 1 2 3 1 2 3 Diffusion training + causal ordering Denoising training Hessian Computation Causal ordering per timestep (k) Multiple Orderings Single Ordering per Noise Scales σk σ 1 x1 x0 x2 x2 x0 x2 x0 x1 x1 3 1 2 3 2 1 3 2 1 σ 2 x2 x2 x1 x0 x1 x2 x1 x0 x0 1 3 3 1 2 2 1 3 2 σ 3 x0 x1 x2 x1 x2 x0 x1 x0 x2 2 3 1 2 3 3 1 1 2 Aggregation Combine Orderings CAM Pruning Single Adjacency Final Temporal DAG x0 1 x1 1 x2 1 x0 2 x1 2 x2 2 x0 3 x1 3 x2 3 Figure4: DOTSpipelinefor temporal causaldiscovery. We startwith lag-embedded time-seriesdata, apply diffusion-based single-order discovery, then extend to multiple orderings and aggregate them. The final temporal DAG below shows an example with three variables over three timesteps. For Gaussian ϵ, we have ϕ ϵ(ω) = exp(−1 2|ω|2), so the nois √e component contributes exp(−1− 2αk|ω|2), which actsasalow-passfilterwithbandwidthinverselyrelatedto 1−α . Askincreases,α decreases,makingthe k k filter narrower and emphasizing lower frequencies. This creates a natural multi-scale decomposition where large k values capture coarse, low-frequency structure while small k values preserve fine, high-frequency details. 10 --- Page 11 --- k→0 k→k max |ϕ xk(ω)| |ϕ xk(ω)| ϕ x0(√ α kω) exp(cid:0) −1− 2αk|ω|2(cid:1) ϕ x0(√ α kω) exp(cid:0) −1− 2αk|ω|2(cid:1) ω ω √ Figure 5: Frequency emphasis of diffusion steps. A forward diffusion step decomposes x = α x + √ k k 0 1−α ϵ. AscanbeseenintheFourierdomain, highvaluesofk emphasizelearningoflow-frequencywhile k low values of k force the network to focus on high-frequency components. Figure 5 schematically illustrates how each k emphasizes different frequency components; large k reveals coarse causal links and small k highlights finer edges. This multi-scale view enables the network to focus on spectral components most impacted by noise at each scale. Similar conclusions were drawn based on observations in the imaging domain (Kascenas et al., 2023). 4.2 Lag-Embedded Representation of Time Series The diffusion model is trained on the lag-embedded representation of the time series. Lag-embedding is a common technique from dynamical systems theory to capture temporal dependencies (Takens, 1981). Let X = [x1,...,xT]⊤ ∈ RT×d denote the raw sequence with d variables and T time steps. We construct a lag-embedded matrix D = (cid:2) xt, xt−1,...,xt−τmax(cid:3)T−1 ∈R(T−τmax)×d(τmax+1). t=τmax Each column of D now refers to a specific variable–lag pair xt−τ.This representation feeds the diffusion i network; temporal precedence is enforced later when we discard any edge that points backwards in time. Incorporating temporal information For temporal data with lags τ, each variable is indexed as xt, i andpotentialedgesincludebothlagged(xt−τ →xt)andcontemporaneous(xt →xt)relationships. Asingle i j i j diffusionmodelistrainedonthelag-embeddeddatasetD∈R(T−τmax)×(d×τmax),andtheleaf-findingprocess is applied in the same manner, ensuring that each node is treated as a time-indexed variable. This strategy enforces a temporal DAG that captures both lagged and instantaneous dependencies. 4.3 Multi-Scale Causal Orderings Eachdiffusionstepk correspondstoadistinctnoiseregime,andconsequently,theHessianH logp(x)com- x putedateachkrevealsdifferentadjacencyconstraints. Largekvaluestendtohighlightbroad,low-frequency cause–effect relationships, while small k values accentuate fine-grained, high-frequency interactions. Multipleorderingsatdifferentk. Insteadofrelyingonasinglenoisescale,weexecuteacausalordering algorithm at several discrete diffusion steps {k ,...,k }. Each execution yields a causal ordering, denoted 1 S byπ, whichreflectsthepartialorderimpliedbythatparticulark. Thisprocessgeneratesmultipleorderings π ,...,π , thereby capturing the multi-scale structure inherent in the data. We then identify leaf nodes via 1 S the diagonal of the Hessian, following the approach of Rolland et al. (2022) and (Sanchez et al., 2023). 11 --- Page 12 --- Algorithm 1 Estimating Multi-Scale Causal Orderings. Require: D∈RT×d ▷ observational time-series τ ▷ largest lag to consider max ϵ (·,k) ▷ A trained diffusion model approximating ∇logp(x) at k ∈[0,k ] θ max K={k ,...,k } ▷ selected noise scales 1 S Ensure: orders ▷ List of valid causal orderings 1: function DOTS(D,K,τ max) 2: orders←∅ 3: for all k ∈K do 4: V ←{xτ i |i=1...d, τ =0...τ max} ▷ lag-embedded nodes 5: π ←[] ▷ ordering for this scale 6: while V ̸=∅ do 7: H diag←HessianDiagVar(cid:0) ϵ θ,D[:,V],k(cid:1) 8: L←argmin v∈V H diag[v] ▷ leaf(s) 9: π ←[L|π] 10: V ←V \L 11: end while 12: orders←orders∪{π} 13: end for 14: return orders 15: end function Causal ordering with a Hessian from diffusion training. After training a diffusion model ϵ (x,k), θ we approximate the partial derivatives as ∂ h i H logp(x) ≈ ϵ (x,k) , i,j ∂x j θ i for each k. The diagonal entries H exhibit lower variance for leaf nodes than for non-leaf nodes (Rolland i,i et al., 2022; Sanchez et al., 2023). To identify a leaf node, we: (i) Estimate H(x,k) on a mini-batch of data D. (ii) Identify the variable x with the lowest diagonal variance Var(cid:2) H (cid:3). (iii) Remove x from the ℓ ℓ,ℓ ℓ distribution by masking out the variables in the input as done with DiffAN masking (Sanchez et al., 2023). This procedure is repeated until all variables are assigned an order, yielding a complete causal ordering π. Repeating this process for each chosen k produces the set {π ,...,π }. 1 S After this procedure, the set {π ,...,π } can be aggregated as described in Section 4.4. In essence, each 1 S π represents a valid causal ordering that reflects the partial order constraints emphasized at its respective s diffusion timestep k . By uniting these multi-scale perspectives, the DOTS algorithm produces a final s temporal DAG that captures both coarse (low-frequency) and fine (high-frequency) causal interactions. 4.4 Aggregating Multiple Orderings Section3showedthattakingthe intersection of all topological sortsofaDAGG yields itstransitiveclosure G+, which is in general a superset of G. Because enumerating every ordering is infeasible, we combine a finite sample of orderings in two simple steps. Soft voting. From S orderings {π ,...,π } obtained at diffusion steps k we form a vote matrix W = 1 S 1:S ij S−1P 1{(i→j) ∈ π }. Thresholding at θ ∈ (0,1] produces the soft transitive closure A˜ = 1{W ≥ θ}. s s ij ij Theextremesθ =0andθ =1reducetotheplainunionandthehardintersection,respectively;intermediate values let us balance recall against precision. 12 --- Page 13 --- CAM pruning. The matrix A˜ can still contain indirect or spurious edges. We refine it with the likeli- hood–based pruning routine of Bühlmann et al. (2014), removing edges that do not improve the predictive loss of the child variable given its other parents. The result is our final estimate Aˆ. This procedure is commonly used across most causal ordering approaches from Section 6.1 and constitutes an integral part of our method as it allows us to recover DAG G from G+ by eliminating indirect links. Practical reliability. Soft voting has the appealing property that any edge appearing in every sampled ordering is retained, whereas edges that never appear are discarded automatically. When (i) the sampler generates a diverse set of valid orderings and (ii) the sample size is large enough for CAM’s tests to be informative, A˜ approximates G+ increasingly well and the pruning step tends to eliminate the remaining indirect links, often recovering G exactly in practice. 5 Experiments 5.1 Setup Our experimental framework prioritizes replication of results, modularity, and ease of extension. These are features found in the Snakemake (Mölder et al., 2021) workflow management system, that forms a base for our experimental setup. Snakemake has previously been used for benchmarking purposes in (non-temporal) causal discovery (Rios et al., 2021), from which we draw inspiration. Our codebase is accessible online2. 5.2 Data Simulations. Our synthetic Data Generating Process (DGP) is based on the work of Beaumont et al. (2021) and Lawrence et al. (2020). Our experimental setup involves DGPs with the following properties: samplesize(observedtimesteps)T ∈{200,1000,2000,5000},numberofgraphnodesd∈{3,4,5,6},lagsize τ ∈ {1,2,3}. In addition, all setups use non-linear causal mechanisms (piecewise linear and trigonometric) and incorporate the same noise distribution ϵ ∼ N(0,[0.01,0.05]). Each setting has been repeated 10 times to obtain robust results. Causal mechanisms and relationships are invariant across time (i.e. they are stationary). Borrowing the notation from Lawrence et al. (2020), a set of temporal causal links T generated in the DGP is defined as follows: T :={X (t−τ)→X (t)|i,j ∈{1,...,d}}, (5) t,τ i j with t denoting time index. Note that we do not consider instantaneous links in our experiments (τ > 0), butdoallowforautoregressiverelationships(i=j). Wealsofixthelagsizeτ acrossallrelationshipswithin any single dataset to isolate and study the influence of τ on algorithmic performance. Real datasets. We also perform experiments on datasets closer to real-life complexities. To achieve this, we incorporate CausalTime (Cheng et al., 2024), a realistic benchmark for time series causal discovery. CausalTimeprovidesthreedatasets: AirQualityIndex(AQI),Traffic,andMedical. TheAQIdataconsistof 36variables,whereasTrafficandMedicalhave20. Intermsofsamplesize,alldatasetsconsistof480samples, with time length of T=40. Since DOTS requires a large sample size to perform well, we combine all samples (all480samplesoflengthT=40stackedvertically),plusonerowofzeros3 toavoidcross-contamination,into a single dataset of length T=480×(40+1)−1=19679. We apply the same pre-processing procedure to all three datasets. • AQI: hourly PM readings from N=36 monitoring stations across China (T=8760). A geographical 2.5 distance kernel supplies a sparse prior graph. • Traffic: average speed measured every 5 min at N=20 loop detectors in the San-Francisco Bay Area (T=52116); the prior graph again follows pairwise distance. 2 3Separatingzerosarenotneededattheendofthedata,henceminusoneintheformula. 13 --- Page 14 --- • Medical: N=20 vital-sign and chart-event channels extracted from 1000 MIMIC-IV ICU stays, re- sampled to 2-h resolution (T=600 on average). 5.3 Algorithms We compare our proposed method to the following baselines: • Temporal: – Dummy: Returns a fully-connected temporal DAG as a naive estimation. – PCMCI(Runge,2020): Employslaggedconditional-independencetestsforconstraint-baseddiscov- ery in autocorrelated time series. – PCMCI+ (Runge, 2020): Extends PCMCI with additional conditioning to control false positives. – VARLiNGAM (Hyvärinen et al., 2010): Combines linear VAR modeling with non-Gaussian ICA to recover a unique causal order. – DYNOTEARS (Pamfil et al., 2020): Casts temporal DAG learning as a single continuous optim- ization with an acyclicity constraint. – TCDF(Nautaetal.,2019): Trainstemporalconvolutionalnetworksandvalidatesedgesviain-silico interventions. – TiMINo(Petersetal.,2013): Appliesadditive-noiseregressionswithindependencetestsonresiduals for both lagged and instant effects. • Non-temporal: – CAM (Bühlmann et al., 2014): Fits additive noise models with restricted maximum likelihood and sparsity-based pruning. – SCORE (Rolland et al., 2022): Uses score-matching to estimate the Hessian variance for iterative leaf removal. – DAS (Montagna et al., 2023c): Scalable ANM ordering via efficient Hessian diagonal estimation. – NoGAM(Montagnaetal.,2023b): GeneralizesANMorderingwithoutGaussiannoiseassumptions, leveraging kernelized score estimates. – DiffAN (Sanchez et al., 2023): Uses denoising diffusion models to approximate the score Jacobian for fast, retraining-free causal ordering. While the ordering-based algorithms (CAM, SCORE, DAS, NoGAM, DiffAN) were not developed for tem- poraltasks,westillincludetheminourexperimentsduetoourproposedmethod’sstrongrootsintopological ordering. Tomaketheuseoftheordering-basedmethodsmoreappropriateinthistemporalsetting,wepost- process their predicted graphs by removing the edges that defy the arrow of time. This mild addition is indicated by the ‘-C’ suffix added to the name of the methods in question (e.g. CAM-C) when reporting the results in Section 5.6. Comparing to temporal methods, however, remains the main validation target for us. 5.4 Graph Representation Temporal causal graphs can be represented at different granularities (Assaad et al., 2022): • Window Causal Graph: Restricts the full time causal graph to a finite lag window τ , rep- max resenting only edges from xt−τ to xt for τ ≤ τ . This trades off completeness for computational i j max feasibility. • Summary Causal Graph: Aggregates causal relationships across time without specifying exact lagindices,creatingamorecompactbutlessdetailedrepresentationaslaggedandcontemporaneous edges are represented the same way. Autocorrelated variables are represented with self-loops. Window graphs are naturally more suitable to our method’s use case. However, we also include summary graphs in our study as we build upon TiMINo that outputs only this type of graphs. Including summary graphs allows us to directly compare to TiMINo. 14 --- Page 15 --- 5.5 Evaluation TheassumptionthatSCMsareinvariantacrosstime(stationarity)resultsinrepeatedcausallinks. Therefore, we focus on the correctness of the predicted edges that terminate at (non-lagged) time t, that is T . We t,τ then compare predicted edges to the ground truth and calculate True Positives (TP), False Positives (FP) and False Negatives (FN), from which we obtain Recall, Precision and F1 metrics as follows: TP TP 2×Recall×Precision Recall= , Precision= , F1= , (6) TP +FN TP +FP Recall+Precision as per (Assaad et al., 2022). We report F1 on window and summary graphs (F1 and F1 , respectively) W S as the main metric of interest, but we also supplement our results with Recall and Precision. 5.6 Results 5.6.1 Simulations Figures 6 and 7 show the main results (averages and 95% confidence intervals). In both cases (window and summary graphs), DOTS shows very strong and robust performance across different sample sizes, numbers of features and lag sizes. Apart from a clear separation from the competition, DOTS is also one of the few methods that keep improving in larger sample sizes (T = 5000). Ordering-based methods and PCMCI family perform comparatively, with another diffusion-based method (DiffAN-C) coming out on top among these, showing the advantage of diffusion models in strongly nonlinear tasks. VARLiNGAM, DYNOTEARS and TCDF struggle to outperform the naive baseline that predicts fully-connected DAGs, suggesting their leniency towards linear settings. 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 200 1000 2000 5000 sample size W1F dummy CAM-C SCORE-C DAS-C NoGAM-C PCMCI PCMCI+ VARLiNGAM DYNOTEARS TCDF DiffAN-C DOTS (ours) 3 4 5 6 1 2 3 number of features number of lags Figure 6: F1 scores on simulated window graphs (F1 ). TiMINo does not provide F1 . W W 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 200 1000 2000 5000 sample size S1F dummy CAM-C SCORE-C DAS-C NoGAM-C PCMCI PCMCI+ VARLiNGAM DYNOTEARS TCDF TiMINo DiffAN-C DOTS (ours) 3 4 5 6 1 2 3 number of features number of lags Figure 7: F1 scores on simulated summary graphs (F1 ). S 15 --- Page 16 --- Figure 8 shows average running times of all methods across the simulations. DOTS places in the middle amongthecompetitors,providingstrongpredictionperformanceatnoextracomputationalcostsascompared to baselines. The runtime of DOTS also scales well that shows its promise in high-dimensional tasks. 104 103 102 101 100 101 200 1000 2000 5000 sample size ]s[ emitnur 104 104 103 103 CAM-C SCORE-C DAS-C 102 102 NoGAM-C PCMCI 101 101 PCMCI+ VARLiNGAM 100 100 D TCY DN FOTEARS TiMINo 101 101 D Di OffA TSN - (C ours) 3 4 5 6 1 2 3 number of features number of lags Figure 8: Average runtime of each algorithm in seconds obtained on synthetic data. Note the logarithmic scale on the y axis. 5.6.2 Real Datasets Table 2 summarises the main findings. Two broad trends emerge. 1. Overall difficulty. AverageF1 rarelyexceeds0.45, farbelowsyntheticbaselines, underscoringthegap S between toy DGPs and real dynamics. Constraint-based PCMCI loses precision on the noisier Medical subset,whilescore-basedDYNOTEARS collapsesonAQIpotentiallyduetoitslinearityassumption. On average, however, many temporal methods are competitive except for TCDF and DYNOTEARS. 2. Best-in-class methods. Among approaches with functional assumption, TiMINo, which leverages the same function assumptions as DOTS, excels on the Medical data. VARLiNGAM dominates the spatial datasets,hintingthatweaknon-GaussianitiessufficeforidentifiabilitywhenthelinearVARfitisadequate, though both TiMINo and PCMCI are not far behind. Our method (DOTS), despite not achieving the top rank on individual datasets, is within one standard error of the leader on every subset and achieves the highest aggregate mean, validating the multi-ordering strategy. Qualitatively, failures tend to cluster around long-range edges (large spatial distance in AQI/Traffic or cross-systeminteractionsinMedical). Futureworkshouldexploreexplicitdistance-awareregularisationand non-stationary mechanisms to close this performance gap. Table 2: F1 (higher is better) on CausalTime. Best and second best per column are highlighted. SCORE, S DAS and NoGAM (non-temporal) have been excluded due to exceeded memory allocation limits. Type Method Medical AQI Traffic Avg. ↑ non-temporal CAM-C 0.413 0.351 0.304 0.356 DiffAN-C 0.313 0.356 0.282 0.317 temporal PCMCI/PCMCI+ 0.427 0.382 0.372 0.393 VARLiNGAM 0.457 0.464 0.391 0.437 DYNOTEARS 0.107 0.010 0.315 0.144 TCDF 0.286 0.218 0.000 0.168 TiMINo 0.553 0.429 0.340 0.441 DOTS (ours) 0.548 0.457 0.349 0.451 16 --- Page 17 --- 0.925 0.900 0.875 0.850 0.825 0.800 0.775 0.750 1 2 3 4 5 6 7 8 910 15 20 25 30 Number of orderings W1F Figure 9: The relationship between prediction performance (F1 ) and the number of explored causal W orderings in DOTS. 5.6.3 Ablation on the number of orderings We now study how the number of causal orderings influence the predictive performance of DOTS. Our theoretical results in Section 3 consider the case where all valid causal orderings are present. In Figure 3, we study the impact of the number of orderings in an ideal scenario where all causal orderings are known. DOTS generates a small, finite set of orderings based on a heuristic: using different diffusion timesteps (k). Therefore, we run this experiment to validate that, indeed, orderings derived from different k are effective in improving performance. To investigate this, we run DOTS on synthetic data (T =5000) generated from a 3-node graph with τ =1. We repeat the experiment 500 times. The results are shown in Figure 9. Sampling a single causal ordering is underperforming as more instances are needed to arrive at the right graph. On the other hand, as few as four orderings show a substantial performance improvement, which is on the same performance level as ten orderings. Moving further to 15 instances shows a mild improvement, beyond which the performance plateaus. Overall, selecting the number of sampled causal orderings clearly has a large effect on method predictive performance, and while exploring more orderings may guarantee better performance, staying in therangeof4–10mayoftenstriketherightbalancebetweencomputationalcostanddeliveredperformance. The asymptotic gains are in agreement with the theoretical results presented in Figure 3. False positives (spurious edges) are pruned more effectively because conflicting orderings rarely vote for the sameincorrectlink,whilefalse negatives (missedtrueedges)arerescuedwhenatleastoneorderingcaptures the correct ancestor–descendant relation. These findings empirically substantiate our theoretical claim from Section 3: aggregating multiple orderings provides a robust consensus estimate of the transitive closure, correcting errors that any single ordering may introduce. Wealsostudytheinfluenceofthemax lag τ andsoft-votingthresholdθ hyperparametersonperformance max of DOTS, as well as the diversity of orderings obtained at different noise scales k. The results for these can be found in Appendix C. 17 --- Page 18 --- 5.7 Limitations We would like to highlight that the robustness of assumption violations tested in Montagna et al. (2023a), such as faithfulness, unconfoundedness, functional, and others, are not covered in our empirical evaluation. Instead, assuming stationarity and causal sufficiency, our robustness checks investigate variations in sample size, number of variables and size of lagged relationships. As a result, any robustness claims we make throughoutthisworkpertainspecificallytothosedatacharacteristicswetested,andnottheonesinvestigated by Montagna et al. (2023a). Future work should investigate our method’s robustness to non-stationary and confounded settings. Another important consideration is that DOTS relies on CAM pruning and hence inherits its limitations. However, by first recovering the theoretically grounded transitive closure, we provide a stronger foundation for subsequent pruning than single-ordering methods can achieve. 6 Related work 6.1 Ordering-Based Causal Discovery RepresentingaDAGbyoneofitsvalidtopologicalorderings(Verma&Pearl,1990)reducesstructurelearn- ingtoapermutationsearchfollowedbyedgeselection. Earlyworkframedthisideaasadiscreteoptimisation problem: greedyMCMCoverpermutations(Friedman&Koller,2003),hill-climbingwithdynamicprogram- ming(Teyssier&Koller,2005),arc-reversalsearches(Park&Klabjan,2017),andrestricted-MLEprocedures such as CAM (Bühlmann et al., 2014). More recent combinatorial schemes enforce sparsity via ℓ -penalised 0 likelihoods, yielding the “sparsest permutation” estimators with provable consistency guarantees (Raskutti & Uhler, 2018; Solus et al., 2021; Lam et al., 2022). Reinforcement-learning formulations further cast the permutation search as a sequential decision process, amortising exploration across datasets (Wang et al., 2021). Beyondscoreoptimisation,identifiabilitycanbestrengthenedbyexploitingdistributionaloralgebraicasym- metries. In linear additive models, sequentially peeling off leaf nodes from the precision matrix recovers the causal order under heteroscedastic noise assumptions (Ghoshal & Honorio, 2018; Chen et al., 2019). De- terministicfunctionalconstraintsarehandledbyDeterminism-awareGES(DGES),whichfirstclustersexact relations and then performs exact search within each cluster, using determinism itself as an unambiguous ordering cue (Li et al., 2024). Non-Gaussianity offers an alternative route: LiNGAM identifies a unique ordering via independent-component analysis, a principle extended to functional data in Func-LiNGAM (Shimizu et al., 2006). Scalabilitytohigh-dimensional,nonlinearsettingshasrecentlybeenadvancedthroughcontinuousrelaxations and deep generative models. CaPS estimates Hessian diagonals of the log-likelihood to iteratively detect leaves, unifying linear and nonlinear mechanisms and accelerating pruning with a “parent score” metric (Xu et al., 2024). Recent advances in ordering-based causal discovery further refine and generalise score- basedestimationstrategies. SCORE(Rollandetal.,2022)leveragesscorematchingtechniquestoiteratively identify and remove leaf nodes, specifically utilizing variance estimates of the Hessian diagonal. Building upon similar principles, DAS (Montagna et al., 2023c) enhances scalability by efficiently estimating Hessian diagonals, significantly reducing computational overhead. DiffAN trains a denoising diffusion model to approximate the score-function Jacobian, introducing a deciduous update rule that circumvents network retraining during iterative leaf removal, thus scaling ordering discovery to hundreds of variables (Sanchez et al., 2023). More recently, there have been notable efforts to generalise the score-matching framework. For instance, NoGAM(Montagnaetal.,2023b)generalisesordering-basedapproachesbeyondGaussiannoiseassumptions, employing kernelized score estimates to accommodate a wider range of data distributions. Furthermore, Liu etal.(2024)relaxestheassumptionthatallmodelsmustbeeitherlinearornonlinearandconsidersproblems with mixed models, notably also leveraging parallel processing to improve scalability. 18 --- Page 19 --- Together, these developments illustrate a shift from discrete combinatorics to differentiable optimisation, while preserving the core insight that a well-chosen causal ordering sharply narrows the search for a faithful DAG. None of these works explore combining multiple valid causal orderings to recover the full adjacency matrix. 6.2 Hessian of the Log-likelihood Estimating H(logp(x)) is the most expensive task of the ordering algorithm. Our baseline (Rolland et al., 2022)proposeanextensionofLi&Turner(2018)whichutilisesStein’sidentityoveranRBFkernel(Schölkopf & Smola, 2002). Rolland et al.’s method cannot obtain gradient estimates at positions out of the training samples. Therefore, evaluating the Hessian over a subsample of the training dataset is impossible. Other promising kernel-based approaches rely on spectral decomposition to solve this (Shi et al., 2018) issue and constitute promising future directions. Most importantly, computing the kernel matrix is expensive for memory and computation on n. There are, however, methods (Achlioptas et al., 2001; Halko et al., 2011; Si et al., 2017) that help to scale kernel techniques not considered in the present work. Other approaches are also possible with deep likelihood methods such as normalising flows (Durkan et al., 2019; Dinh et al., 2016) and further computing the Hessian via backpropagation. This would require two backpropagation passes giving O(d2) complexity and be less scalable than denoising diffusion. Indeed, preliminary experiments proved impractical in our high-dimensional settings. WeuseDPMsbecausetheycanefficientlyapproximatetheHessianwithasinglebackpropagationpasswhile allowingHessianevaluationonasubsampleofthetrainingdataset. Ithasbeenshown(Song&Ermon,2019) that denoising diffusion can better capture the score than simple denoising (Vincent, 2011) because noise at multiple scales explores regions of low data density. 6.3 Causal Discovery for Time Series Granger’s seminal definition of causality for time series—past X improves the prediction of future Y—still underpins most modern approaches (Granger, 1969). Structural causal models (SCMs) extend this idea to permit intervention semantics, latent variables, and cycles (Bongers et al., 2021). Constraint-based algorithms such as PCMCI and its refinement PCMCI+ adapt the PC procedure to lagged conditional- independence testing, enabling scalable false-discovery control in autocorrelated, high-dimensional settings (Runge,2020). Functional-formassumptionsprovidestrongeridentifiability: TiMINoemploysadditive-noise regressions with independence tests on residuals (Peters et al., 2013), while VARLiNGAM couples a linear VAR with non-Gaussian errors and independent-component analysis to recover causal ordering (Hyvärinen et al., 2010). Score-based and deep-learning methods further relax linearity and stationarity. DYNOTEARS casts struc- ture learning as a single differentiable optimisation problem over lagged and contemporaneous edges with an acyclicity constraint (Pamfil et al., 2020). TCDF trains attention-based CNNs and validates candidates through in-silico interventions (Nauta et al., 2019), whereas CausalFormer augments Transformers with causality-aware attention to handle long sequences (Kong & Lu, 2024). Continuous-time dynamics can now be unveiled with sparse Neural ODEs that yield interpretable differential systems from irregular samples (Aliee et al., 2023). Information-theoretic criteria such as transfer entropy generalise Granger tests to non- linear interactions, though density estimation remains costly in high dimensions (Schreiber, 2000). Most recently, PICK introduces score-matching algorithms to temporal causal discovery while notably reducing pruning time by exploiting the variance of the scores, offering an alternative path to efficiency (Chen et al., 2024). Recentsurveysalsoemphasisepersistentchallenges—hiddenconfounders, non-stationarity, andcom- putationatscale(Gongetal.,2023;Assaadetal.,2022;Moraffahetal.,2021). Ourmethodadvancesthefield by combining the statistical rigour of functional approaches with the scalability of continuous optimisation while accommodating nonlinearities typical of real-world data. 19 --- Page 20 --- 7 Conclusion Inthiswork,weintroducedDOTS,adiffusion-basedapproachleveragingmultiplecausalorderingstoaddress thechallengeoftemporalcausaldiscovery. Whileprevioussingle-orderingmethodswereprimarilydeveloped in the context of static causal discovery, our work extends the causal ordering framework explicitly to the temporal setting. This temporal context inherently incorporates the causal temporality principle, where variablescanonlycausallyinfluencefuturevariables,notpastones. Unliketraditionalsingle-orderingmeth- ods, DOTS effectively captures complementary information by aggregating multiple valid causal orderings, therebyreconstructingthetransitiveclosureoftheunderlyingtemporalDAG.Weformalizedthetheoretical benefits of this multi-ordering strategy, demonstrating its capacity to mitigate spurious dependencies and enhance robustness in causal inference. Our empirical results on synthetic datasets clearly illustrate the su- periority of DOTS compared to existing state-of-the-art baselines in terms of accuracy and scalability, while staying competitive and the best on average on real data. By exploiting the inherent frequency domain characteristics of diffusion steps, our method provides nuanced insights into both coarse and fine-grained temporal causal interactions. Future research directions include exploring additional aggregation strategies for causal orderings, extending our method to non-stationary environments, and further optimizing compu- tational efficiency for large-scale applications. Acknowledgments We would like to thank Ricardo Silva for valuable comments and suggestions. We acknowledge the support of the UKRI AI programme, and the Engineering and Physical Sciences Research Council (EPSRC), for the Causality in Healthcare AI Hub [grant number EP/Y028856/1]. References Dimitris Achlioptas, Frank Mcsherry, and Bernhard Schölkopf. Sampling Techniques for Kernel Methods. In T Dietterich, S Becker, and Z Ghahramani (eds.), Advances in Neural Information Processing Systems (NeurIPS), volume 14. MIT Press, 2001. Hadi Aliee, Stephan Bongers, and Joris Mooij. Beyond predictions in neural odes: Identification and inter- ventions. In Proceedings of the 40th International Conference on Machine Learning, 2023. Charles K Assaad, Emilie Devijver, and Eric Gaussier. Survey and evaluation of causal discovery methods for time series. Journal of Artificial Intelligence Research, 73:767–819, 2022. Paul Beaumont, Ben Horsburgh, Philip Pilgerstorfer, Angel Droth, Richard Oentaryo, Steven Ler, Hiep Nguyen, Gabriel Azevedo Ferreira, Zain Patel, and Wesley Leong. CausalNex, October 2021. URL  Stephan Bongers, Patrick Forré, Jonas Peters, and Joris M Mooij. Foundations of structural causal models with cycles and latent variables. The Annals of Statistics, 49(5):2885–2915, 2021. Peter Bühlmann, Jonas Peters, and Jan Ernest. CAM: Causal additive models, high-dimensional order search and penalized regression. The Annals of Statistics, 42(6):2526 – 2556, 2014. Hao Chen, Kai Yi, Lin Liu, and Yu Guang Wang. Score-matching-based structure learning for temporal data on networks. arXiv preprint arXiv:2412.07469, 2024. WenyuChen, MathiasDrton, andYSamuelWang. Oncausaldiscoverywithanequal-varianceassumption. Biometrika, 106(4):973–980, 12 2019. Yuxiao Cheng, Ziqian Wang, Tingxiong Xiao, Qin Zhong, Jinli Suo, and Kunlun He. Causaltime: Realistic- ally generated time-series for benchmarking of causal discovery. In The Twelfth International Conference on Learning Representations, 2024. URL  David Maxwell Chickering. Learning bayesian networks is np-complete. Learning from data: Artificial intelligence and statistics V, pp. 121–130, 1996. 20 --- Page 21 --- Laurent Dinh, Jascha Sohl-Dickstein Google, Brain Samy, and Bengio Google Brain. DENSITY ESTIMA- TION USING REAL NVP, 12 2016. Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural Spline Flows. In Proc. Advances in Neural Information Processing Systems (NeurIPS), 2019. MichaelEichler. Causalinferenceintimeseriesanalysis,June2012. ISSN1940-6347. URL org/10.1002/9781119945710.ch22. Nir Friedman and Daphne Koller. Being Bayesian About Network Structure. A Bayesian Approach to Structure Discovery in Bayesian Networks. Machine Learning, 50(1):95–125, 2003. Karl J Friston, Lee Harrison, and Will Penny. Dynamic causal modelling. Neuroimage, 19(4):1273–1302, 8 2003. ISSN 1053-8119. AsishGhoshalandJeanHonorio. Learninglinearstructuralequationmodelsinpolynomialtimeandsample complexity. In Amos Storkey and Fernando Perez-Cruz (eds.), Proc. of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume84ofProc. of Machine Learning Research, pp. 1466–1475. PMLR, 8 2018. Chang Gong, Di Yao, Chuzhe Zhang, Wenbin Li, Jingping Bi, Lun Du, and Jin Wang. Causal discovery from temporal data. In Proc. of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD ’23, pp. 5803–5804, New York, NY, USA, 2023. Association for Computing Machinery. C. W. J. Granger. Investigating causal relations by econometric models and cross-spectral methods. Econo- metrica, 37(3):424, August 1969. ISSN 0012-9682. doi: 10.2307/1912791. URL  2307/1912791. N Halko, P G Martinsson, and J A Tropp. Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions. SIAM Review, 53(2):217–288, 2011. Taro Hiraguchi. On the dimension of orders. Science Reports of the Kanazawa University, 4:1–20, 1955. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems (NeurIPS), 33:6840–6851, 2020. Richard Zou Horace He. functorch: JAX-like composable function transforms for PyTorch. ht- tps://github.com/pytorch/functorch, 2021. David Hume. Enquiry Concerning Human Understanding. Clarendon Press, 1904. Aapo Hyvärinen, Kun Zhang, Shohei Shimizu, and Patrik O Hoyer. Estimation of a structural vector autoregression model using non-gaussianity. Journal of Machine Learning Research, 11(5), 2010. A. B. Kahn. Topological sorting of large networks. Commun. ACM, 5(11):558–562, November 1962. ISSN 0001-0782. doi: 10.1145/368996.369025. AntanasKascenas,PedroSanchez,PatrickSchrempf,ChaoyangWang,WilliamClackett,ShadiaS.Mikhael, Jeremy P. Voisey, Keith Goatman, Alexander Weir, Nicolas Pugeault, Sotirios A. Tsaftaris, and Alison Q. O’Neil. The role of noise in denoising models for anomaly detection in medical images. Medical Image Analysis, 90:102963, 2023. ISSN 1361-8415. doi:  URL  XinyuKongandMingLu. Causalformer: Aninterpretabletransformerfortemporalcausaldiscovery. arXiv preprint arXiv:2406.16708, 2024. Wai-Yin Lam, Bryan Andrews, and Joseph Ramsey. Greedy Relaxations of the Sparsest Permutation Al- gorithm. In The 38th Conference on Uncertainty in Artificial Intelligence, 2022. 21 --- Page 22 --- Andrew R. Lawrence, Marcus Kaiser, Rui Sampaio, and Maksim Sipos. Data generating process to evaluate causaldiscoverytechniquesfortimeseriesdata.CausalDiscoveryandCausality-InspiredMachineLearning Workshop at Neural Information Processing Systems, 2020. Loka Li, Haoyue Dai, Hanin Al Ghothani, Biwei Huang, Jiji Zhang, Shahar Harel, Isaac Bentwich, Guangyi Chen, and Kun Zhang. On causal discovery in the presence of deterministic relations. volume 37, pp. 130920–130952, 2024. Yingzhen Li and Richard E Turner. Gradient Estimators for Implicit Models. In International Conference on Learning Representations, 2018. WenqinLiu,BiweiHuang,ErdunGao,QiuhongKe,HowardBondell,andMingmingGong. Causaldiscovery with mixed linear and nonlinear additive noise models: A scalable approach. In Francesco Locatello and Vanessa Didelez (eds.), Proceedings of the Third Conference on Causal Learning and Reasoning, volume 236 of Proceedings of Machine Learning Research, pp. 1237–1263. PMLR, 01–03 Apr 2024. URL  DanielMarbach,ThomasSchaffter,ClaudioMattiussi,andDarioFloreano. Generatingrealisticinsilicogene networks for performance assessment of reverse engineering methods. Journal of computational biology, 16(2):229–239, 2009. FelixMölder,KimPhilippJablonski,BriceLetcher,MichaelBHall,ChristopherHTomkins-Tinch,Vanessa Sochat, Jan Forster, Soohyun Lee, Sven O Twardziok, Alexander Kanitz, et al. Sustainable data analysis with snakemake. F1000Research, 10, 2021. FrancescoMontagna,AtalantiMastakouri,EliasEulig,NicolettaNoceti,LorenzoRosasco,DominikJanzing, Bryon Aragam, and Francesco Locatello. Assumption violations in causal discovery and the robustness of score matching. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), AdvancesinNeuralInformationProcessingSystems,volume36,pp.47339–47378.CurranAssociates,Inc., 2023a. Francesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, Kun Zhang, and Francesco Locatello. Causal discoverywithscorematchingonadditivemodelswitharbitrarynoise. InConference on Causal Learning and Reasoning, pp. 726–751. PMLR, 2023b. Francesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, Kun Zhang, and Francesco Locatello. Scalable causal discovery with score matching. In Conference on Causal Learning and Reasoning, pp. 752–771. PMLR, 2023c. Raha Moraffah, Paras Sheth, Mansooreh Karami, Anchit Bhattacharya, Qianru Wang, Anique Tahir, Ad- rienneRaglin,andHuanLiu. Causalinferencefortimeseriesanalysis: problems,methodsandevaluation. Knowledge and Information Systems, 63(12), 2021. MeikeNauta,DoinaBucur,andChristinSeifert. Causaldiscoverywithattention-basedconvolutionalneural networks. Machine Learning and Knowledge Extraction, 1(1):19, 2019. RoxanaPamfil,NisaraSriwattanaworachai,ShaanDesai,PhilipPilgerstorfer,KonstantinosGeorgatzis,Paul Beaumont, and Bryon Aragam. Dynotears: Structure learning from time-series data. In International Conference on Artificial Intelligence and Statistics, pp. 1595–1605. PMLR, 2020. Young Woong Park and Diego Klabjan. Bayesian Network Learning via Topological Order. Journal of Machine Learning Research, 18(99):1–32, 2017. Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Causal inference on time series using restricted structural equation models. Advances in neural information processing systems, 26, 2013. Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference. MIT Press, 2017. 22 --- Page 23 --- MichelleL.RankinandTeresaMcCormack. Thetemporalpriorityprinciple: atwhatagedoesthisdevelop? Frontiers in Psychology, 4, 2013. ISSN 1664-1078. doi: 10.3389/fpsyg.2013.00178. URL  frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2013.00178. Garvesh Raskutti and Caroline Uhler. Learning directed acyclic graph models based on sparsest permuta- tions. Stat, 7(1):e183, 1 2018. FelixL.Rios,GiusiMoffa,andJackKuipers. Benchpress: ascalableandversatileworkflowforbenchmarking structure learning algorithms for graphical models, 2021. PaulRolland, VolkanCevher, MatthäusKleindessner, ChrisRussell, DominikJanzing, BernhardSchölkopf, and Francesco Locatello. Score Matching Enables Causal Discovery of Nonlinear Additive Noise Models. In Proc. of the 39th International Conference on Machine Learning, volume 162 of Proc. of Machine Learning Research, pp. 18741–18753. PMLR, 2022. David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back- propagating errors. Nature, 323(6088):533–536, 1986. Jakob Runge. Discovering contemporaneous and lagged causal relations in autocorrelated nonlinear time series datasets. In Conference on Uncertainty in Artificial Intelligence, pp. 1388–1397. PMLR, 2020. Jakob Runge, Sebastian Bathiany, Erik Bollt, Gustau Camps-Valls, Dim Coumou, Ethan Deyle, Clark Glymour, Marlene Kretschmer, Miguel D. Mahecha, Jordi Muñoz-Marí, Egbert H. van Nes, Jonas Peters, Rick Quax, Markus Reichstein, Marten Scheffer, Bernhard Schölkopf, Peter Spirtes, George Sugihara, Jie Sun, Kun Zhang, and Jakob Zscheischler. Inferring causation from time series in earth system sciences. Nature Communications, 10(1), June 2019. ISSN 2041-1723. doi: 10.1038/s41467-019-10105-3. URL  Pedro Sanchez, Xiao Liu, Alison Q O’Neil, and Sotirios A. Tsaftaris. Diffusion models for causal discovery via topological ordering. In The Eleventh International Conference on Learning Representations, 2023. URL  Bernhard Schölkopf and Alexander Smola. Learning with kernels. Optimization, and Beyond. MIT press, 1 (2), 2002. Thomas Schreiber. Measuring information transfer. Physical Review Letters, 85(2):461–464, 2000. JiaxinShi,ShengyangSun,andJunZhu. ASpectralApproachtoGradientEstimationforImplicitDistribu- tions. In Jennifer Dy and Andreas Krause (eds.), Proc. of the 35th International Conference on Machine Learning, volume 80 of Proc. of Machine Learning Research, pp. 4644–4653. PMLR, 11 2018. Shohei Shimizu, Patrik O. Hoyer, Aapo Hyvärinen, and Aki Kerminen. A linear non-gaussian acyclic model for causal discovery. In Journal of Machine Learning Research, volume 7, pp. 2003–2030, 2006. Si Si, Cho-Jui Hsieh, and Inderjit S Dhillon. Memory Efficient Kernel Approximation. Journal of Machine Learning Research, 18(20):1–32, 2017. L Solus, Y Wang, and C Uhler. Consistency guarantees for greedy permutation-based causal inference algorithms. Biometrika, 108(4):795–814, 8 2021. Yang Song and Stefano Ermon. Generative Modeling by Estimating Gradients of the Data Distribution. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019. Floris Takens. Detecting strange attractors in turbulence, pp. 366–381. Springer Berlin Heidelberg, 1981. ISBN 9783540389453. doi: 10.1007/bfb0091924. URL  Marc Teyssier and Daphne Koller. Ordering-Based Search: A Simple and Effective Algorithm for Learning Bayesian Networks. In Proc. of the Conference on Uncertainty in Artificial Intelligence (UAI), UAI’05, pp. 584–590, Arlington, Virginia, USA, 2005. AUAI Press. 23 --- Page 24 --- Thomas Verma and Judea Pearl. Causal Networks: Semantics and Expressiveness. In Ross D SHACHTER, Tod S LEVITT, Laveen N KANAL, and John F LEMMER (eds.), Uncertainty in Artificial Intelligence, volume 9 of Machine Intelligence and Pattern Recognition, pp. 69–76. North-Holland, 1990. Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation, 23 (7):1661–1674, 12 2011. XiaoqiangWang,YaliDu,ShengyuZhu,LiangjunKe,ZhitangChen,JianyeHao,andJunWang. Ordering- Based Causal Discovery with Reinforcement Learning. In Zhi-Hua Zhou (ed.), Proc. of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI, pp. 3566–3573. International Joint Con- ferences on Artificial Intelligence Organization, 8 2021. Zhuopeng Xu, Yujie Li, Cheng Liu, and Ning Gui. Ordering-based causal discovery for linear and nonlinear relations. 2024. 24 --- Page 25 --- A Order theory definitions Here, we establish the foundational definitions used in order theory. Definition 1 (Partial Order). Let V be a set. A binary relation ≤ on V is a partial order if, for all x,y,z ∈V, it satisfies: (i) Reflexivity: x≤x. (ii) Antisymmetry: If x≤y and y ≤x, then x=y. (iii) Transitivity: If x≤y and y ≤z, then x≤z. The pair (V,≤) is called a partially ordered set (or poset). Definition 2 (Strict Partial Order). A binary relation ≺ on V is a strict partial order if it is: (i) Irreflexive: For all x∈V, x̸≺x. (ii) Transitive: If x≺y and y ≺z, then x≺z. Definition 3 (Linear Extension). For a poset (V,≤), a linear extension is a total order ⪯ on V such that if x≤y, then x⪯y for all x,y ∈V. That is, ⪯ extends ≤ into a total ordering consistent with the partial order. Definition 4 (ReachabilityRelation). ForadirectedgraphG=(V,E),thereachability relation R⊆V ×V is defined as: (x,y)∈R ⇐⇒ there exists a directed path from x to y in G. IfGisaDAG,RisapartialorderonV (reflexive,antisymmetric,andtransitive),aseveryvertexisreachable from itself via a trivial path. Definition 5 (Transitive Closure). For a directed graph G = (V,E) with reachability relation R, the transitive closure of G is the graph: G+ =(V,E+), where: E+ ={(x,y)∈V ×V :(x,y)∈R and x̸=y}. Thus,G+ containsanedge(x,y)ifandonlyifthereisanon-trivialdirectedpathfromxtoy inG,excluding self-loops. 25 --- Page 26 --- B Supplementary Results B.1 Alternative metrics Figures 10 and 11 supplement our main simulation results. In these, we report precision and recall on both window and summary graphs. The most important observation here is that the top-performing methods almost never provide non-existent edges (i.e. high precision), but some undetected edges still remain (i.e. recall lower than 1). Future work could focus on improving edge detection while maintaining high precision levels. 1.0 0.8 0.6 0.4 0.2 200 1000 2000 5000 sample size Wnoisicerp CAM-C SCORE-C DAS-C NoGAM-C PCMCI PCMCI+ VARLiNGAM DYNOTEARS TCDF DiffAN-C DOTS (ours) 3 4 5 6 1 2 3 number of features number of lags (a) Precision (precision ) W 0.7 0.6 0.5 0.4 0.3 0.2 0.1 200 1000 2000 5000 sample size Wllacer CAM-C SCORE-C DAS-C NoGAM-C PCMCI PCMCI+ VARLiNGAM DYNOTEARS TCDF DiffAN-C DOTS (ours) 3 4 5 6 1 2 3 number of features number of lags (b) Recall (recall ) W Figure 10: Precision and recall (higher is better) on simulated window graphs. TiMINo does not provide window graphs. 26 --- Page 27 --- 1.0 0.8 0.6 0.4 0.2 200 1000 2000 5000 sample size Snoisicerp CAM-C SCORE-C DAS-C NoGAM-C PCMCI PCMCI+ VARLiNGAM DYNOTEARS TCDF TiMINo DiffAN-C DOTS (ours) 3 4 5 6 1 2 3 number of features number of lags (a) Precision (precision ) S 0.7 0.6 0.5 0.4 0.3 0.2 0.1 200 1000 2000 5000 sample size Sllacer CAM-C SCORE-C DAS-C NoGAM-C PCMCI PCMCI+ VARLiNGAM DYNOTEARS TCDF TiMINo DiffAN-C DOTS (ours) 3 4 5 6 1 2 3 number of features number of lags (b) Recall (recall ) S Figure 11: Precision and recall (higher is better) on simulated summary graphs. 27 --- Page 28 --- B.2 Effect of the temporal constraint Figure 12 shows the influence of the temporal constraint applied to score-matching methods to ensure they return a graph that respects the arrow of time. Most methods show some mild performance improvement and mostly in higher precision. Interestingly, diffusion-based methods (DiffAN and DOTS) do not exhibit suchabenefit,whichcouldbepartlyduetoalreadyhighprecisionachievedevenwithouttheconstraint(i.e. not much room for further improvement). constraint CAM no yes SCORE DAS NoGAM DiffAN DOTS (ours) 0.0 0.2 0.4 0.6 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.0 0.2 0.4 0.6 0.8 1.0 F1W recallW precisionW (a) Window graphs constraint CAM no yes SCORE DAS NoGAM DiffAN DOTS (ours) 0.0 0.2 0.4 0.6 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.0 0.2 0.4 0.6 0.8 1.0 F1S recallS precisionS (b) Summary graphs Figure 12: The effect of the temporal constraint on the performance of score-matching algorithms. 28 --- Page 29 --- C Ablations C.1 Maximum lag DOTS has a hyperparameter that sets the maximum lag (τ ) to be considered in the predicted graph. max Here we study its influence on predictive performance. To this end, we run simulations with 3-node graphs, T=5000 and τ ∈{1,2,3}, all repeated 100 times. Figure 13 presents the results. Twomainobservationsemerge. First,selectingτ shorterthangroundtruthcanhaveanobvioussignificant max negative impact on performance as it prevents DOTS from identifying the necessary longer relationships. However, setting τ to values larger than the truth maintains good performance as compared to when max the hyperparameter matches the lag size in the data. This has important practical implications, suggesting that setting τ to larger values may provide more robust results since the right lag size is unknown when max processing real data. 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 1 2 3 4 5 max lag W1F data lag = 1 data lag = 2 data lag = 3 ground truth 1 2 3 4 5 1 2 3 4 5 max lag max lag Figure 13: The relationship between prediction performance (F1 ) and the maximum lag (max lag) hyper- W parameter in DOTS. Each setting (subfigure) includes one type of lag in the data (data lag), which is also highlighted by the vertical ground truth line. Error bars denote 95% confidence intervals. 29 --- Page 30 --- C.2 Soft-voting threshold Here we investigate the sensitivity of DOTS to the soft-voting hyperparameter θ. We run simulations with 3-node graphs, T=5000, τ =1, all repeated 100 times. Figure 14 presents the results. The main observation is a clear trend, in which performance consistently degrades as we increase the threshold. This is consistent with the behaviour we expect as increasing the threshold requires more votes from each ordering for a graph edge to be present, which inevitably leads to increased false negatives and decreased recall (middle subfigure). Interestingly, union of all orderings (θ = 0) provides the best results, and which is notably free from false positives (i.e. perfect precision as in the right-hand-side subfigure). This result shows importance and effectiveness of CAM pruning since without it the number of false positives would likely be very high. This observation also makes θ =0 the recommended default value. 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 W1F 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Wllacer 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Wnoisicerp Figure14: Therelationshipbetweenpredictionperformanceandthesoft-votingthresholdθ hyperparameter in DOTS. Error bars denote 95% confidence intervals. 30 --- Page 31 --- C.3 Diversity of orderings To quantify the diversity of orderings produced during a training run, we computed pairwise distances between all permutations using the Kendall tau metric. This distance reflects the proportion of pairwise disagreementsinitemorderingbetweentwopermutationsandiswidelyusedforcomparingrankedlists. By convertingeachpermutationintoitscorrespondingrankvectorandcomputingnormalizedKendalldistances, weobtainasymmetricdistancematrixthatcaptureshowsimilarordissimilareachorderingistoeveryother within the same run. Figure15visualisesthismatrixasaheatmap. Blocksofdarkerregionsindicatesetsoforderingsthatremain relatively consistent, while lighter bands correspond to greater variability across orderings. The structured patternssuggestthatthemodelexploresmultiple,partiallyoverlappingrankingmodesratherthancollapsing to a single consensus. In particular, similar k values yield similar orderings as shown by the block patterns on the diagonals. In Figure 16 we further investigate predictive performance of individual orderings at different noise scale values k. Each ordering was passed through CAM pruning to get the final performance measure against the true DAG. The figure implies that different noise scales provide different edge-detection capabilities, especially in the [0,30] range. Importantly, however, none of the orderings achieves individually as high of a performance as when all orderings at different noise scales are combined into a single robust prediction. 0 10 0.6 20 0.5 30 40 0.4 50 0.3 60 70 0.2 80 0.1 90 0.0 k k 0 10 20 30 40 50 60 70 80 90 Figure 15: Kendal distance between different orderings obtained at different noise scale values k. 0.90 0.85 0.80 0.75 0.70 0.65 0.60 0.55 0 10 20 30 40 50 60 70 80 90 100 k W1F 0.9 0.8 0.7 0.6 0.5 0.4 0 10 20 30 40 50 60 70 80 90 100 k Wllacer 1.04 1.02 1.00 0.98 0.96 0 10 20 30 40 50 60 70 80 90 100 k Wnoisicerp combined orderings Figure16: Performanceofindividualorderingsobtainedatdifferentnoisescalesk comparedtotheperform- ance achieved with combined orderings. Results are averages over 100 repetitions. Error bars and shaded grey areas denote 95% confidence intervals. 31 --- Page 32 --- D Experimental Details D.1 Hyperparameters We summarise the most important hyperparameters used in conjunction with the methods in Table 3. For the rest of the hyperparameters not mentioned here, we deferred to their default values, which can be found in their respective implementations as per Table 4. In addition, most methods have a hyperparameter that corresponds to the maximum lag size, though can be named differently in their implementations (τ in PCMCI/PCMCI+, lags in VARLiNGAM, p in max DYNOTEARS, max_lag in TiMINo). This hyperparameter can have a non-trivial effect on method’s per- formance. In order to decrease our experiments’ dependence on hyperparameter tuning, we set this hy- perparameter to the true lag value τ in the simulations since we have access to the ground truth. In real data settings (CausalTime), we set the value to 1. Note that some methods (CAM, SCORE, DAS, No- GAM, DiffAN, DOTS) implement this hyperparameter implicitly by creating τ ×d lagged variables based on provided data. While sampling 10 causal orderings in DOTS is a reasonable default (n_ord), we increase it to 20 when processing CausalTime to better showcase our method’s potential. Table 3: Summary of hyperparameters of all methods used in the experiments. Method Hyperparameters CAM alpha=0.05 SCORE α=0.05,η =0.001,η =0.001 G H DAS α=0.05,η =0.001,η =0.001 G H NoGAM α=0.05,η =0.001,η =0.001 G H ridge =0.01,ridge =0.1 α γ PCMCI/PCMCI+ α=0.05,test∈{par_corr, cmi_knn},τ =1 min VARLiNGAM α=0.05,criterion=bic,prune=true DYNOTEARS λ =0.05,λ =0.05,w_threshold=0.01 w a TCDF epochs=5000,layers=2,lr=0.01 kernel_size=4,dilation=4,significance=0.8 DiffAN steps=100,nn_depth=3,batch_size=1024 early_stop=300,lr=0.001 TiMINo α=0.05 DOTS steps=100,nn_depth=3,batch_size=1024 early_stop=300,lr=0.001,n_ord=10 D.2 Implementations of algorithms Table 4: Summary of source code used to run the methods in the experiments. Method Source CAM/SCORE/DAS/NoGAM dodiscover:  PCMCI/PCMCI+ tigramite:  VARLiNGAM lingam:  DYNOTEARS causalnex:  TCDF  DiffAN  TiMINo  32