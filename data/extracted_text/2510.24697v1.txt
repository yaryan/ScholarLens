--- Page 1 --- 2025-10-29 WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling Info-Rich Seeking ZhengweiTao∗((cid:0)),HaiyangShen∗,BaixuanLi∗,WenbiaoYin((cid:0)),JialongWu,KuanLi, ZhongwangZhang,HuifengYin,RuiYe,LiwenZhang,XinyuWang,PengjunXie,JingrenZhou, YongJiang((cid:0)) TongyiLab ,AlibabaGroup    Abstract LargeLanguageModel(LLM)-basedagentshaveemergedasatransformative approachforopen-endedproblemsolving,withinformationseeking(IS)being a core capability that enables autonomous reasoning and decision-making. While prior research has largely focused on improving retrieval depth, we observethatcurrentISagentsoftensufferfromlowsearchefficiency,whichin turnconstrainsoverallperformance. Akeyfactorunderlyingthisinefficiency isthesparsityoftargetentitiesintrainingtasks,whichlimitsopportunitiesfor agentstolearnandgeneralizeefficientsearchbehaviors. Toaddressthesechal- lenges,weproposeWebLeaper,aframeworkforconstructinghigh-coverage IS tasks and generating efficient solution trajectories. We formulate IS as a tree-structuredreasoningproblem,enablingasubstantiallylargersetoftarget entities to be embedded within a constrained context. Leveraging curated Wikipediatables,weproposethreevariantsforsynthesizingIStasks—Basic, Union,andReverse-Union—tosystematicallyincreasebothISefficiencyand efficacy. Finally,wecuratetrainingtrajectoriesbyretainingonlythosethatare simultaneouslyaccurateandefficient,ensuringthatthemodelisoptimizedfor bothcorrectnessandsearchperformance. Extensiveexperimentsonbothbasic andcomprehensivesettings,conductedonfiveISbenchmarks—BrowserComp, GAIA, xbench-DeepSearch, WideSearch, and Seal-0—demonstrate that our method consistently achieves improvements in both effectiveness and effi- ciencyoverstrongbaselines. GAIA BrowseComp xbench-DeepResearch WideSearch 73.2 38.8 SR Item-F1 72.0 68.3 67.4 66.0 63.1 30.0 28.3 26.4 71.0 70.0 69.0 67.0 3.0 2.357.9 2.6 57.7 65.0 48.5 48.3 41.3 14.1 12.2 0.4 WebLeaper Cla -u Sd oe n D- n e4 e et pRO ep se en arAI ch GL -4M . D5 eepS -e Ve 3k .1 Kim -i K2 WebLeape Dr eepS -e Ve 3k . O1 penAI -- mo i4 ni GL -4M .5 Kim -i K2 Cla -u Sd oe n- n4 et WebLeape Dr eepS -e Ve 3k .1 GL -4M .5 ReseaK ri cm hi er Open -AI o3 Cla -u Sd oe n- n4 et WebLeaper Cla -u Sd oe n- n4 et Doubao-1.6 DeepSee -k R1 Figure1: Resultsoncomprehensivetrainingsetting. AllWebLeaperscoresareaveragedoverthreeruns. Themetricofthefirstthreefiguresareaccuracy. “SR”denotesSuccessRateonWideSearch. *Equalcontribution. (cid:0)   1 5202 tcO 82 ]LC.sc[ 1v79642.0152:viXra --- Page 2 --- 1 Introduction TheLLM-basedagentsmarkaparadigmshiftinAI,deliveringtransformativesolutionstochallenges once deemed intractable across diverse domains (Guo et al., 2024; Ye et al., 2023). Among their core capabilities,informationseeking(IS)playsacrucialroleinenablingthecognitiveautonomyofthese agents. This ability not only drives their adaptability in open-ended tasks but also underpins a new generationofpowerfulcommercialsystems,includingOpenAIDeepResearch(OpenAI,2025b),Google’s Gemini(Gemini,2025),andPerplexityAI(Perplexity,2025),Kimi-Researcher(Team,2025). While numerous studies have sought to enhance the IS capabilities of agents through complex ques- tion–answeringpipelinesandadvancedfine-tuningstrategies(Wuetal.,2025a;Lietal.,2025c;b;Tao etal.,2025;Qiaoetal.,2025;Luetal.,2025),mostexistingapproachesprimarilyconcentrateonimproving thesearchdepth,givingcomparativelylittleattentiontosearchefficiency. Ourpreliminaryexperiments indicatethatcurrentLLM-basedagentssearchinefficiently.AsshowninFigure2,thedistributionofvalid actionsforacompetitiveISagentpeaksaround0.04,meaningthatinmostcases,onlyasmallfractionof actionsareeffective(Wongetal.,2025;Xueetal.,2025). Thislowvalid-actionratereflectssuboptimal searchbehaviors,includingredundantqueryreformulations,retrievalofirrelevantinformation,and unnecessarilylongsearchchains. Suchinefficienciesnotonlyincreasecomputationalandtimecostsbut alsolimittheagent’soverallISperformance. 5 4 3 2 1 0 0.0 0.2 0.4 0.6 0.8 1.0 Valid Action Rate ytisneD ecnerruccO evitaleR Thedesignofsynthetictrainingtasksincursthis inefficiency. IntypicalISagentsetups,theagent beginswithasetofknownentitiesandincremen- tally gathers information to infer all target enti- ties. However,priorworkoftenconstructstasks inwhichthetargetentitiesareoverlysparse(Wu etal.,2025a;Lietal.,2025c;b). Suchsparsitylimits 0.04 theagent’sexposuretoinformativecues,reducing opportunitiestolearntolocaterelevantinforma- tionwithina constrainedcontextwindow. As a Figure 2: The distribution of valid actions of the result,theagentspendsmoreactionsprocessing agentbasedontheGPTmodelonoursynthesizedIS irrelevantcontent,weakeningitssearchstrategies, task. Thevalidactionsarethoseseekingthecorrect leadingtolowerperformance. Furthermore,itcan targetentitiesrequiredbythequestion. biasthemeasurementofsearchefficiency,which weproveinalatersection. Thisbiasmakesitdiffi- culttoobtainanaccuratetrainingsignal,therebyobstructingthesystematiclearningofmoreefficient searchbehaviors. Theselimitationsunderscoretheneedtoredesigntrainingtasks,enablingoptimized seekingefficiencyandstrongerIScapabilities. Toaddressthesechallenges,weproposeWebLeaper,aframeworkdesignedwithtwocoreobjectives: (1) toconstructnewIStaskscontainingasubstantiallylargernumberoftargetentities;and(2)togenerate solutiontrajectoriesthatachievebothhighaccuracyandhighefficiency. Forthefirstobjective,wemodel theISprocessasatree-structuredreasoningtask,whichcompactlyaccommodatesmoretargetnodes withinalimitedcontext. Basedonthisformulation,wesystematicallyincreasetaskcomplexitythrough threedatasetvariants. First,leveragingcuratedWikipediatables,wesynthesizeBasicversion,which directlyaddressesthechallengeofentitysparsitybycreatingahigh-densitysearchspacewithinasingle, structuredsource. Tomirrormorerealisticscenariosthatdemandintegratinginformationfrommultiple sources,ourUnionvariantconstructstasksthatrequiresynthesizingfactsacrossdifferentsources,thereby increasingsearchambiguity. Finally,tomitigatetheriskofagentsadoptingsimplistic,keyword-based shortcuts, the Reverse-Union variant reverses the logical flow, compelling the agent to first deduce intermediate entities from scattered clues before completing the main search task. For the second objective,weconstructtask-completiontrajectoriesthatarefilteredaccordingtoInformation-SeekingRate 2 --- Page 3 --- (ISR)andInformation-SeekingEfficiency(ISE),retainingonlythosethatsolvethetaskbothaccuratelyand efficiently. These metrics are then incorporated into the hybrid reward system during the following reinforcementlearningstage. Modelstrainedonthiscurateddatasetaftersupervised-finetuningand reinforcementlearningyieldourfinalISagent. Weconductextensiveexperimentsonbothbasicandcomprehensivesettingstoevaluateourapproach across five benchmarks: BrowserComp (Wei et al., 2025), GAIA (Mialon et al., 2023), Seal-0 (Pham etal.,2025),WideSearch(Wongetal.,2025),andxbench-DeepSearch(Xbench-Team,2025). Ourmethod achievesconsistentimprovementsonallbenchmarks. Ablationstudiesonthedatasetdesignfurther confirmtheeffectivenessofourproposedcomponents. Wesummarizeourcontributionasfollows: • We design a new information-seeking task formulation on a tree-structured reasoning problem, leadingtotheinclusionofasubstantiallylargersetoftargetentitieswithinaconstrainedcontext. Basedonthisformulation,weconstructtheBasic,Union,andReverse-Uniondatasets. • We generate and filter task-solving trajectories using the proposed Information-Seeking Rate (ISR) andInformation-SeekingEfficiency(ISE)metrics,retainingonlythosetrajectoriesthatsolvetasksboth accuratelyandefficiently. ThesemetricsarealsodesignedforourhybridRLrewardsystem. • We conduct extensive experiments on five public IS benchmarks, BrowserComp, GAIA, Xbench- DeepSearch,WideSearch,andSeal-0,achievingconsistentimprovementsoverstrongbaselines. 2 Definitions AnInformation-Seeking(IS)taskchallengesanagenttoansweracomplexnaturallanguagequestion bynavigatingavastinformationspacetoassembleacompletesetofrequiredentities. Thisprocessis inherentlysequential,involvingtheprogressivediscoveryofentities,understandingtheirproperties (attributes),andleveragingrelationshipsbetweenthemtouncoverfurtherentities. Thissectionformally definesthecomponentsofsuchataskandthemetricsforevaluatinganagent’sperformance,emphasizing theimportanceofidentifyingbothfinalandintermediateentitiesinthereasoningchain. 2.1 Information-Seeking Task Anentitye ∈ E isthefundamentalunitofinformation. AnInformation-Seeking(IS)taskistheprocessof identifyingandcollectingaspecificsetoftargetentitiesfromE,basedonaquestion. Formally,anIStask isatuple: T = ⟨q,R⟩,whereqisthenaturallanguagequestionandR ⊂ E isthesetofthetargetentities thatcollectivelysatisfytheconditionsposedbyq. Critically,therequiredsetRincludesnotonlythefinal,explicitanswersbutalsoallintermediateentities thatarenecessarysteppingstonesinthereasoningprocess. Considerthequestion: q :Whichplayerofateaminthe2004–05season,whowasborninthe1990s? (1) Thisteamwasfoundedin1966andisanEastGermanfootballteam. Tosolvethis,anISagentmustseekforinformationonline,andfindthetargetentitysetasanswer: R = {RobertRudwaleit,DannyKukulies,...}. (2) 2.2 Information-Seeking Agent WefocusonanInformation-SeekingAgentthatinteractswithawebenvironmenttosolveanIStaskT withintheReActframework(Yaoetal.,2023). Theagent’soperationisasequentialdecision-making processoccurringoverdiscretetimestepst =1,...,T. Ateachstep,theagentanalyzesitscurrentstate (includingtheinitialquestionandallpreviouslygatheredinformation),generatesathoughtforplanning itsnextmove,executesatool-basedactiontoseeknewinformation,andreceivesanobservationfromthe environment. Thisentireprocessiscapturedintheagenttrajectoryisdefinedas H = (q,τ ,α ,o ,τ ,α ,o ,...,τ ,α ,o ), (3) T 1 1 1 2 2 2 T T T 3 --- Page 4 --- whereτ istheplanningthought,α istheseekingaction,ando istheresultingobservationatstepi. At i i i theendoftheprocess,theagenthasobtainedasetofentitiesO ⊂ E,whichistheunionofallunique entitiesdiscoveredacrossallsteps. 2.3 Quantifying Information Collection and Efficiency ToguideanagenttowardssuccessfullysolvingIStasks,itsperformanceframeworkmustvaluetheentire reasoningprocess,notmerelythefinaloutput. Ourcentralthesisisthatbyexplicitlyquantifyingthe valueofallrequiredinformationdiscovered,wecancreateastrongersignalforlearningeffectivesearch strategies. Tothisend,wedefineprinciplestoformalizetheperformance(thetotalinformationgain)and theefficiency(thegainperaction)oftheagent’scollectionprocess. Information-SeekingRate(ISR) RecallthatRdenotesthesetoftargetground-truthentitiesforthetask, withcardinalityn = |R|.Oisthesetofentitiesactuallyobtainedbytheagentduringitsoperation. The intersectionR∩Othereforecontainsallrequiredentitiesthatweresuccessfullyretrieved. Theinformation collectionratedirectlymeasuresthefractionofrequiredentitiessuccessfullyobtainedbytheagent: |R∩O| |R∩O| ISR= = . (4) |R| n ISR∈ [0,1],andhighervaluesindicatemorethoroughcoverageoftherequiredinformation. Information-Seeking Efficiency (ISE) While ISR measures completeness, the information collection efficiencyreflectstheaveragenumberofactionstepstodiscoverthetargetentity: n ISE= , (5) T whereTisthetotalnumberofstepsofthesolvingtrajectory. HigherISEimpliesgreaterISefficiency. The stabilityofmeasuringISEisimportantforprovidingunbiasedtrainingsignals. Proposition1(VarianceofISE). LetX denotethenumberofstepstheagenttakestodiscoverthei-thnewentity i in R. ThereforeISE = n = n . AssumeX ,...,X bei.i.d.randomvariableswithfinitemeanµ > 0and T ∑ in =1Xi 1 n finitevarianceσ2,X >0almostsurely,then: i (cid:18) (cid:19) 1 Var(ISE) = O . (6) n Thispropositionshowsthatasthenumberoftargetentitiesngrows,measuringISEbecomesamore stableandreliableperformancemetric. ThedetailedproofisprovidedinAppendixA.2. 3 Method ToenhancetheinformationefficiencyoftheISagent,ourapproachtrainsthemodelonacalibratedtask T = ⟨q,R⟩togetherwiththecorrespondingtask-solvingtrajectoryH. InpriorISagenttrainingsetups, thedatasettypicallycontainedonlyalimitednumberoftargetentities(R). Thisdesignsubstantially restricts the potential improvement in information-seeking efficiency and, in turn, limits the agent’s overallcapability. Thelimitationincurstwoproblems: • WithasmallvolumeofR,itisdifficulttotraintheagenttoretrieveinformationefficientlywithina limitedcontextlength. • Ourmethodreliesonmeasuringtheinformation-seekingefficiencyISE.AsshowninEq.(6),asmall setoftargetentitiesintroducesmeasurementbiasintheISEmetric. Toovercometheseshortcomings,weintroduceWebLeaper,anoveldatasynthesisframeworkspecifically designedtoboostinformation-seekingefficiency. Ourmethodconsistsoftwomaincomponents: (1)aQA 4 --- Page 5 --- Question Entities Target Entities Fuzzed Question Entities Version 1: Basic Version 3: Reverse Union Example: PrimaryKey Table Title Union Whowere the Nobel Column Names Prize winners in Literature between 1980 and 1990? Please include their name, country, award year, and gender. Properties Fuzz Version 2: Union Example: Who are the authors from the same Example: country as the 1980s prize-winner that Which authors have won wrote a novel about a group of British both the Nobel Prize in boys stranded on an uninhabited island, Literature and the Booker and who have also won both this reward Prize? For each, provide and the Booker Prize? For each of them, their name, nationality and what is their name, country, and the the year they won the Nobel. Find subtrees with a common set of relations respective years they won each award? Figure3:AnoverviewofWebLeaper.Thereasoningstructureismodeledasatree.Arootentity(question entity)connectstoasetofsecond-layerentities. (a)Version-I(Basic)constructsasimplereasoningtree fromasingleinformationsource. (b)Version-II(Union)createsacomplextaskbyfindingamaximal union between two trees that share a common set of relations within their subtrees (e.g., both have “has_nationality”). (c)Version-III(Reverse-Union)reversesthereasoningprocess. Itprovidesfuzzed clues(third-layerentities)asquestionentities,forcingtheagenttofirstdeduceasecond-layeranchor entity(anentityfromthesecondlayer),thenotherrelevantsubtrees. synthesispipelineforgeneratingcalibratedtasks,and(2)atrajectoryconstructionprocessforproducing realistic task-solving sequences. We describe the QA synthesis pipeline and trajectory construction process in detail in the following subsections. For detailed walkthroughs of the examples for each synthesisversion,pleaserefertoAppendixA.6. 3.1 Entity-Intensive Task Synthesis 3.1.1 Version-I:Basic Inaninformation-seekingtask,thereasoningstructurematters. Weuseatree,denotedasT,torepresent i thisstructure,wherenodesareentitiesandedgesarerelationsbetweenthem.TheISagentmuststartwith someknownentitiesinthetreeandreasonalongtheedgestodeterminethetargetones.Toincorporateas manytargetentitiesaspossible,weusethistreestructureforitscompactandhierarchicalorganization. Synthesizingsuchatask T = ⟨q,R⟩requiresalargevolumeofrelevantentities,whichisnon-trivial. Followingtheone-entity-at-a-timecollectionstrategyofpriorworkisprohibitivelyexpensive. Therefore, we exploit the structured tables contained in Wikipedia articles, which encapsulate rich relational information. These tables naturally provide groups of entities connected by specific relationships, enabling us to efficiently construct the reasoning tree T. We crawled approximately 2 million tables i fromWikipediaandappliedamulti-stagecleaningprocedure,retainingonlylarge,well-formed,and structurallyhomogeneoustables. Thedetaileddatacleaningprocedureandconstructionrationaleare describedinAppendixA.4. ToconstructthereasoningstructureillustratedinFigure3(a),wepopulateitslayersusinginformation fromasingletable. Theentitiesextractedfromthetabletitleformtherootofthetree(i.e.,thequestion entities). Next,weemployanLLMtoselectthemostrepresentative,non-redundantcolumnofvalues fromthetable—typicallytheprimarykey—asthesecond-layerentities(e.g.,“CzesławMiłosz”). An 5 --- Page 6 --- edgebetweentherootentityandasecond-layerentityindicatesthatthetablecontainsthisentity. The third-layerentitiesarederivedfromtheremainingcolumnsofthetable,withtheirvaluesrepresenting attributesofthecorrespondingsecond-layerentity(e.g.,“country: Poland”,“year: 1980”). Inthislayer, anedgesignifiesthatthesecond-layerentitypossessesthegivenpropertydefinedbythethird-layer entity. Eachsecond-layerentityanditsassociatedthird-layerentitiesformasubtree,whichwedenoteasS . i,j Thesesubtrees,eachpossessingasetofrelationsRel(S )thatconnectitslayers,representcohesiveunits i,j ofinformation(e.g.,aspecificlaureateandalltheirdetails). ThefullreasoningtreeT isthuscomposed i ofasetofsuchsubtrees{S }. Thequestionprovidestherootentities,whileallentitiesinthesubtrees i,j (bothsecondandthirdlayers)constitutethefinalanswer. Thedetailedconstructionprocessandthe requiredreasoningpathfortheexampletaskareexplainedinAppendixA.6.1. 3.1.2 Version-II:Union Whileeffective,thereasoningstructureofourbasictasksisderivedfromsinglesources,limitingtheir structuralcomplexityandthescopeofquestionswecanpose. Toaddressthis,weaimtoconstructtasks withamoreintricatereasoningstructurethatspansmultipleinformationsourcesbyunitingreasoning treesfromourBasicversionthatsharesimilarthemesandstructures. Togeneratemorechallengingquestions,weproposeunitingreasoningsubtreesinBasicversionthat sharesimilarthemesandstructures. Anaiveapproach,suchasrandomlycombiningsubtrees,often resultsinsemanticallyincoherentquestions. Tosystematicallydiscoverthemostsubstantialintegration opportunities,ourapproachmodelsthisasaUnionoperation,whichidentifiesmultiplereasoningtrees whoserespectivesubtreessharesomecommonrelations. Theprimarychallengeistosystematicallysearchtheentirecollectionoftreestofindallgroupsthatare suitableforunion. Toavoidacombinatorialexplosionfromenumeratingallpossiblecombinations,we develop an algorithm to efficiently discover only maximal unions. This problem is formally modeled asMaximalBicliqueEnumeration(seeAppendixA.5),whicheffectivelyidentifiesgroupsofreasoning subtreesandtheirsharedsubtreerelations. AsillustratedinFigure3(b),thereasoningtreesfor“NobelPrizeinLiteraturelaureates”and“Booker Prizewinners”bothcontainsubtreeswheresecond-layerentities(authors)areconnectedtothird-layer entitiesviarelationslike“has_nationality”and“has_name”. Ourmethodidentifiesthissharedsubtree structure. Relationsnotsharedacrossallsetsofsubtrees,suchas“has_gender”(presentonlyintheNobel tree),arediscardedduringtheunion. Onceamaximalunionisidentified,weleverageanLLMtosynthesizeaquestionbasedonthecommon featuresoftheselectedsubtrees. Forinstance,thequestion“WhichauthorshavewonboththeNobel PrizeinLiteratureandtheBookerPrize?” requiresidentifyingthetwosetsoflaureatesasintermediate “TargetEntities”andthenfindingtheirintersectiontoproducethefinal“TargetEntities”. Thecomplete walkthroughisinAppendixA.6.2. 3.1.3 Version-III:Reverse-Union While the Union method generates complex, multi-source tasks, a vulnerability remains: an agent couldsolvethequeryandusedirectkeywordsearchesontheconstituentsources(e.g.,search“Nobel Prizewinners,” then“BookerPrizewinners”). Thisapproachcircumventstheintendedsynthesisof information, reducing the cognitive load and failing to stimulate true reasoning capabilities similar toWebSailor(Lietal.,2025c). Toaddressthis,weintroduceReverse-Union,aparadigmdesignedto enforceamorerobustcognitiveworkflowbyreversingthestandardreasoningflow. Asillustratedin Figure3(c),thismethodcombinestwostagestoconstructachallengingtask: • Deductive Fuzz: This stage implements the fuzz by defining the “Question Entities” as a set of 6 --- Page 7 --- descriptivethird-layerentities. Insteadofbeingnameddirectly,acentral“anchor”entity(anentity fromthesecondlayer)isdescribedthroughitscorrespondingthird-layerentities. Intheexample,the description“the1980sprize-winnerthatwroteanovelaboutagroupofBritishboysstrandedonan uninhabitedisland”servesascluesintheformof“QuestionEntities”. Anagentmustfirstdeduce fromthesecluestoidentifytheanchorentity,“WilliamGolding”. • Union-basedSearchConstruction: Afterfuzzingtheanchor, thisstageconstructstheexpansive searchpartofthetask,ensuringtheanchorservesonlyasabridgetothefinalanswer. Toachieve this, wefirstselectaspecificthird-layerentityfromtheanchor’ssubtree(e.g.,hiscountry)toact asapivot. Wethenformulatetheremainderofthequestiontocompelanagenttousethispivot tolaunchanewsearchacrosstheunifiedtrees. ThefinalTargetEntitiesarethusdefinedastheset ofsecond-layerentitiesthatsharethispivotattribute(i.e.,arealsoBritish)andsatisfytheoriginal intersectioncondition(i.e.,winningbothprizes). Bystructuringtasksthisway,Reverse-Unionpreventsagentsfromsucceedingwithsimplekeyword searchingandmandatesamorerobust,multi-stepreasoningprocess. Thedetailedprocessofquestion generationandtherequiredreasoningpathareexplainedinAppendixA.6.3. 3.2 Information-Guided Trajectory Construction Aftersynthesizingthetask,thissectionelaboratesontheconstructionoftask-solvingtrajectories. As showninEq.(3),ouragentsolvesataskwithintheReActframework(Yaoetal.,2023). Weequipthe agentwiththefollowingtools: • SearchThisactionenablestheagenttoconductGooglesearchbyseveralqueries. Theparameters ofthistoolare{queries,filter_year},enablingtemporalfilteringofsearchresults. Thistoolwould returnthetoprelevantURLsandtheirsnippetsastheobservation. • VisitThisactionenablestheagenttovisitmultipleURLs.Theparametersofthistoolare{urls,goal}. Thistoolwouldreturnthesummarizedvisitedparagraphsastheobservation. Aftergeneratingalargesetoftrajectoriesbyexecutingourconstructedtaskswithanopen-sourcemodel, weapplyafilteringproceduretoselecthigh-qualityexamplesfortraining.Ourgoalistoretaintrajectories thatdemonstratebothaccuracyincollectingtherequiredentitiesandefficiencyintheuseofactions,in accordancewiththemetricsdefinedinSection2.3. Specifically,weimposethefollowingselectioncriteria: Coverage Criterion. We require that the trajectory achieve sufficient completeness in information collection. Formally,wekeeponlythosetrajectorieswhoseISRsatisfiesISR> α,whereαisapredefined coverage threshold. To compute ISR, we accumulate the obtained target entities in all actions. We computeISRasEq.(4). Efficiency Criterion. We further require that the trajectory maintain high efficiency in discovering usefulentities. ThistranslatesintoselectingthosetrajectorieswhoseISEsatisfiesISE> β,whereβisa predefinedefficiencythreshold. ForISE,weaccumulatetheobtainedtargetentitiesinVisitactions. The reasonfornotincludingSearchinISEisthatweobserveentitiesfoundinSearcharelesspreciseand wouldbeupdatedbythefollowingVisitaction. WecomputeISRasEq.(5). Throughthisfilteringprocess,weensurethattheretainedtrajectoriesarebothaccurateinacquiringthe targetentitiesandefficientintheiractionusage,providingstrongsupervisionsignalsfortrainingagents toperformpreciseandeffectiveinformation-seeking. 3.3 Reinforcement Learning with Hybrid Reward Systems Followingsupervisedfine-tuning(SFT)onthetrajectoriesgeneratedviaourinformation-guidedmethod (Section3.2),wefurtherenhancetheagent’spolicyusingreinforcementlearning(RL).Acriticalcom- 7 --- Page 8 --- ponentofRListherewardfunction, whichprovidesthetrainingsignal. However, standardreward mechanismsarefundamentallymisalignedwiththeentity-intensivetaskssynthesizedbyWebLeaper. Themostcommonapproach,asimplebinaryreward(e.g.,success/failure),suffersfromextremespar- sity. Thisissueisdramaticallyexacerbatedinoursetting,whereataskmayrequiredozensofentities; rewardingtheagentonlyuponperfectcompletionofsuchalargesetmakespositivefeedbacksorare thateffectivelearningbecomesnearlyimpossible. Furthermore,theverymethodsforimplementingarewardfunction—evenamoregranularone—present their own intractable challenges. On one hand, conventional automated metrics like Exact Match or word-levelF1scoresaretoobrittle. Theycannotgracefullyhandleminorsemanticvariations(e.g.,“USA” vs. “United States”) and would incorrectly penalize the agent, a problem that compounds severely acrossalargeentityset. Ontheotherhand,deployingamoresophisticatedLLM-as-a-Judgetoevaluate correctnessseemspromising,butitstruggleswithscalabilityandreliability. Askingajudgemodelto accuratelyverifyalonglistofentitiesinasingleassessmentimposesahighcognitiveload,leadingto inconsistentscores,whilerunningitforeverysingleentityisprohibitivelyexpensiveforRL.Thisleaves usinapredicament: simplemethodsaretooinaccurate,andaccuratemethodsaretooimpractical. Toovercometheseintertwinedchallenges,wedesignaHybridRewardSystem. Thissystemprovides anuanced,accurate,andcost-effectivetrainingsignal,specificallytailoredtotheuniquedemandsof ourentity-intensivetaskswhilemaintainingcompatibilitywithstandardbenchmarks. Itiscomposedof twocorecomponents: agranular,F-score-basedrewardforoursynthesizedtasks,andtheretentionof conventionalrewardfunctionsforexistingpublicbenchmarkdata. Granular F-Score for Entity-Intensive Tasks. For the approximately 500 entity-intensive QA pairs reservedforRL,wedevelopafine-grainedrewardfunctionbasedontheISRmetric. RecallthatISR= |R∩O| (Equation4)measurestherecalloftheretrievedentities. BuildinguponISRasourmeasureof |R| recall,wedesignedamorecomprehensiverewardsignalthatalsoaccountsforprecision. Anagentcould otherwiseachieveahighscorebyretrievingmanyirrelevantentities. Furthermore,apracticalrewardfunctionmustgracefullyhandleminorsemanticvariations(e.g.,“USA” vs. “UnitedStates”). Therefore,wedefinesoftversionsofprecisionandrecallbyintroducingascoring functions(e ,e ) ∈ [0,1]thatmeasuresthesemanticsimilaritybetweenaretrievedentitye ∈ Oand o r o aground-truthentity e ∈ R. Insteadofamonolithicjudgment, weevaluateattheindividualentity r level. Tobalanceaccuracyandefficiency,wefirstcategorizeentitiesintheground-truthsetRbytheir semantictype(e.g.,personnames,dates,organizations)andassignanappropriateevaluationmodalitys toeachcategory. Forinstance,personnamesmightbeevaluatedusingnear-exactmatchtohandleminor variations,whilemoreabstractconceptsmightrequireatargetedLLM-as-a-Judgeassessment. Basedonthissemanticscoringfunctions,wedefineoursoftrecallR (ageneralizationofISR)andsoft c precisionP: 1 ∑ R = maxs(e ,e ) (7) c |R| er∈Reo∈O o r 1 ∑ P = maxs(e ,e ) (8) |O| eo∈Oer∈R o r Thisformulationcreditstheagentforfindingentitiesthataresemanticallyequivalenttotheground truth. WethenaggregateP andR usingaweightedF-scoretocomputethefinalrewardR . c WebLeaper Thisaddressespotentialbiasesinoursynthesizedground-truthsetR,whichmaybeslightlyover-or under-complete. TherewardR isdefinedas: WebLeaper P ·R R = (1+ω2) c (9) WebLeaper ω2P +R c where ω isahyperparameterthatbalancestheimportanceofprecisionandrecall. Avalueof ω > 1 prioritizesrecall(aligningmorecloselywiththeoriginalgoalofISR),whileω <1emphasizesprecision. 8 --- Page 9 --- Table1: Resultsonmultiplebenchmarks.  WideSearch reportsSuccessRate(SR),Row F1,andItem F1. Boldscoresindicatethehighestvaluesamongallopen- sourceagents. BandCstandforbaseandcomprehensivetrainingsetting. WideSearch Model/Framework BrowseComp GAIA xbench-DS Seal-0 SR Row F1 Item F1 ProprietaryAgents Claude-4-Sonnet 12.2 68.3 64.6 – 2.3 31.7 57.9 OpenAI-o3 49.7 70.5 66.7 18.9 4.5 34.0 52.6 OpenAI DeepResearch 51.5 67.4 – – – – – Open-SourceAgents ASearcher-Web-32B 5.2 52.8 42.1 – – – – DeepDive-32B 14.8 – 50.5 – – – – DeepDiver-V2-38B 13.4 – 53.0 – – – – MiroThinker-32B-DPO-v0.2 13.0 64.1 – – – – – Kimi-K2-Instruct-1T 14.1 57.7 50.0 – 1.1 29.7 54.4 WebExplorer-8B 15.7 50.0 53.7 – – – – WebDancer-QwQ-32B 3.8 51.5 38.3 – 0.0 9.3 34.5 WebSailor-32B 10.5 53.2 53.3 21.3 0.0 2.1 5.5 WebShaper-QwQ-32B – 53.3 35.0 – 0.0 9.9 31.5 WebLeaper-Union B 22.1 69.9 62.3 35.1 4.0 22.2 34.5 WebLeaper-Reverse-Union B 23.0 67.0 66.0 37.2 4.0 25.8 40.8 WebLeaper-Reverse-Union C 38.8 73.2 72.0 48.6 4.0 31.0 48.8 HybridIntegration. FortasksoriginatingfromexistingtrainingQA,weretaintheiroriginal, often binary, reward functions, which we denote as R . Our final hybrid reward function, R , is legacy hybrid thereforeconditionalonthetask’sorigin,ensuringthattheagentisevaluatedappropriatelyforeachdata source:  R (O,R) ifT isfromWebLeaper R (H ,T) = WebLeaper (10) hybrid T R (O,R) otherwise legacy This hybrid reward signal provides rich, fine-grained feedback on our entity-intensive tasks while maintainingcompatibilitywithestablishedevaluationprotocols. Theagent’spolicyisthenoptimized againstthiscomprehensiverewardusingGroupRelativePolicyOptimization(GRPO)(Shaoetal.,2024), enablingittorefineitsinformation-seekingstrategieseffectively. PolicyOptimizationwithHybridReward. Theagent’spolicy,denotedπ parameterizedbyθ,isopti- θ mizedusingGRPO.ForeachtaskT inourRLdataset,wesampleagroupofktrajectories{H ,...,H } 1 k fromthecurrentpolicyπ . EachtrajectoryH isassignedarewardR = R (H,T). Insteadofusing θ i i hybrid i alearnedvaluefunction,GRPOestimatestheadvantageforeachtrajectorybystandardizingitsreward relativetotheothersinthegroup: R −mean({R }k ) Aˆ = i j j=1 (11) i std({R }k )+ϵ j j=1 std where ϵ is a small constant for numerical stability. This group-relative advantage Aˆ is applied to std i everytimestepwithinthetrajectoryH. Thepolicyisthenupdatedbyminimizingaclippedsurrogate i objective,similartoPPO(Schulmanetal.,2017),whichisaveragedoveralltrajectoriesinthegroupand alltimestepsineachtrajectory. TheGRPOlossfunctionis: L (θ) = −E (cid:34) 1 ∑k 1 | ∑H i|(cid:32) min(cid:18) r (θ)Aˆ ,clip(cid:0) r (θ),1−ε,1+ε(cid:1) Aˆ (cid:19)(cid:33)(cid:35) (12) GRPO {H i} ik =1∼πθ k i=1|H i| t=1 i,t i i,t i where r (θ) = πθ(ai,t|si,t) is the importance sampling ratio at timestep t of trajectory i, and ε is the i,t πold(ai,t|si,t) clipping hyperparameter. By optimizing this loss, the policy π learns to favor actions that lead to θ 9 --- Page 10 --- Table2: Ablationstudyontrainingresultsacrossdifferentdatasources(forefficiencyconsiderations, weusetheWideSearch(Englishsubset)andBrowseComp(200subset),whilethefullsetsareusedforthe otherbenchmarks). Numbersinparenthesesdenotethedifferencecomparedtotrainingonlywiththe WebSailor-V2-5kdata. †denotesamixedversionthatincludestheWebSailor-V2-5kdata. DataSource BrowseComp WideSearch GAIA Seal-0 xbench-DS Avg. WebSailor-V2-5k 25.17 33.15 67.69 34.23 60.00 44.05 WebSailor-V2-10k 24.50 38.91 66.02 33.93 62.67 45.21 Basic-5k† 20.67 (-4.50) 32.26 (-0.89) 40.78 (-26.91) 30.03 (-4.20) 58.33 (-1.67) 36.41 (-7.64) Union-5k† 27.50 (+2.33) 41.70 (+8.55) 69.90 (+2.21) 35.14 (+0.82) 62.33 (+2.33) 47.31 (+3.26) Reverse-Union-10k† 27.67 (+2.50) 44.07 (+10.92) 66.99(-0.70) 37.24 (+3.01) 66.00 (+6.00) 48.39 (+4.34) trajectories with higher-than-average rewards within a sampled group, effectively internalizing the complexpreferencesdefinedbyourR function. hybrid 4 Experiments 4.1 Setup Benchmarks WeconductextensiveevaluationsofourmethodonfivechallengingQAbenchmarksthat demandcomplexinformation-seekingcapabilities,namelyBrowseComp(Weietal.,2025),GAIA(Mialon et al., 2023), xbench-DeepSearch (xbench-DS) (Xbench-Team, 2025), Seal-0 (Pham et al., 2025), and WideSearch(Wongetal.,2025). ForGAIA,weadoptthe103-sampletext-onlyvalidationsubset(Lietal., 2025d),whileforallotherbenchmarks,weutilizetheircompletetestsets. Baselines Weselectarepresentativesetofmainstreamandcompetitiveinformation-seekingagentsas ourbaselines,includingproprietaryagents(Claude-4-Sonnet(Anthropic,2025),OpenAI-o3(OpenAI, 2025a),OpenAI DeepResearch(OpenAI,2025b))andopen-sourceagents(ASearcher(Gaoetal.,2025), DeepDive (Lu et al., 2025), DeepDiver-V2 (Team), MiroThinker (Team et al., 2025b), Kimi-K2 (Team etal.,2025a),WebExplorer(Liuetal.,2025),WebDancer(Wuetal.,2025a),WebSailor(Lietal.,2025c), WebShaper(Taoetal.,2025)). TrainingConfigurations Tomaintainthebasicdeepsearchability,wecombineourdatawith5,000 WebSailor-V2(Lietal.,2025b)datatotrainthemodel. Weseparatelymerge5,000WebSailor-V2data withBasic, Union, andReverse-UniondataofWebLeaper, whichstimulatestheISabilitytoalarger degree(withαinISRsetto0.3andβinISEsetto0.1). WeemployQwen3-30B-A3B-Thinking-25071as thebasemodel,trainedusingtheMegatronframework2. Thisisourdefaultbasesettinginwhichmost experimentsareconducted. Comprehensive and Realistic Settings To more rigorously evaluate whether the training data of WebLeapercanremaineffectiveundermorecomprehensiveandrealisticscenarios,weintroducethe comprehensive setting. We mix WebLeaper data into the corpus of Tongyi-DeepResearch-30B-A3B, covering both the supervised fine-tuning and reinforcement learning stages, to examine its overall impactonperformance. Itisworthnotingthatthisservesonlyasasupplementarysettingappliedin certainexperimentalsections. Unlessotherwisespecified,weadoptthebaseWebLeaper experimental configurationbydefault. Evaluation Metrics and Inference Hyper-parameters The overall evaluation follows the settings specifiedbyeachbenchmark.  scoresobtainedviaLLM-as-a-judgeevaluationasthefinalresults. ForWideSearch,wereportthesuccess 1 2 10 --- Page 11 --- GAIA BrowseComp WideSearch 69.9 67.0 68.0 27.5 42.7 41.8 41.7 22.7 22.2 ISR-Only ISE-Only ISR + ISE ISR-Only ISE-Only ISR + ISE ISR-Only ISE-Only ISR + ISE Figure4: Ablationstudyresultsoninformation-guidedtrajectoryconstructionstrategies. rate(SR)forfullyretrievingalltargetresults,alongwithtwoF1scores—Row F1andItem F1—whichare computedusingacombinationofstringmatchingandLLM-as-a-judgeevaluation,inalignmentwiththe officialevaluationprotocol. DuringLLMinference,weconfigurethesamplingparameters(temperature andtop-p)to0.6and0.95,respectively. 4.2 Overall Performance BaseSettingAsshowninTable1,WebLeaperachievesstate-of-the-artperformancecomparedtomain- stream open-source agents on five challenging information-seeking QA benchmarks. Notably, on benchmarks other than BrowseComp and WideSearch, it even delivers performance comparable to, orsurpassing,thatofagentsbuiltonClaude-4-SonnetandOpenAI-o3. Evenonthehighlychallenging BrowseCompbenchmark,WebLeapersignificantlyoutperformsKimi-K2-Instruct-1T,despitethelatter having a much larger parameter scale. It is also worth noting that the Reverse-Union data, which incorporatesgreatertaskcomplexityontopoftheUniondata, employsanfuzzstrategythatfurther facilitates the model’s ability to integrate information-seeking with planning and reasoning, thereby enhancingitsoverallinformation-seekingQAcapability. Overall, the observed performance improvements validate that our proposed approaches—entity- intensive task synthesis and information-guided trajectory construction—significantly enhance the agent’sinformation-seekingcapabilities,evenunderamodestparameterbudget. ComprehensiveSettingWealsotrainourmethodonthecomprehensivesetting,andcompareittomore competitivemethods. TheresultsareshowninFigure1. WebLeaper reaches73.2onGAIA,38.8on BrowseComp,and72.0onxbench-DeepResearch.OntheharderWideSearchbenchmark,WebLeaper also attains the highest Success Rate and Item-F1, clearly outperforming all competitors. These results demonstratethatourapproachgeneralizeswellandremainseffectiveevenwhenevaluatedunderthe comprehensiveandrealistictrainingsetting. 4.3 Capability Gains Induced by Entity-Intensive Task Synthesis Toinvestigatetheeffectivenessofourentity-intensivetasksynthesismethod,weconductacomparative analysisagainsttrainingsolelyontheWebSailor-V2dataset(using5,000and1,000samples,respectively), asyntheticcorpusspecificallydesignedtostimulatetheagent’sdeepsearchcapability. As shown in Table 2, we investigate the impact of different entity-intensive task synthesis strategies throughanablationstudyonallthesebenchmarks. TheBasicsettingexhibitssubstantialdropsacross allthreedatasetscomparedtoWebSailor-V2-5k. Thispoorperformancecanbeattributedtotheinherent limitationsoftheBasicdataconstructionmethod: tasksgeneratedunderthissettingtendtobeoverly simple,allowingthemodeltoinfercompleteanswersfromonlyafewinformationsources. Suchshortcut patterns encourage the model to overfit to superficial cues rather than learning to integrate diverse information,ultimatelyimpairinggeneralization. 11 --- Page 12 --- 42.5 40.0 37.5 35.0 32.5 30.0 27.5 25.0 54 56 58 60 62 64 66 68 70 Average Action Rounds ecnamrofreP WebSailor-V2 70 WebSailor-V2 WideSearch WebLeaper WebLeaper 68 GAIA 66 64 xbench-DS BrowseComp 62 60 20 22 24 26 Average Action Rounds Figure5: EffectivenessandefficiencycomparisonbetweenWebLeaperandWebSailor-V2. In contrast, the Union strategy consistently outperforms WebSailor-V2-5k, achieving an average im- provementof+3.26. Bycombiningheterogeneousinformationsourcesandincreasingthecomplexityof taskconstruction,UnionmitigatestheshortcutprobleminherentinBasic,forcingthemodeltoreason overdispersedandcomplementaryevidence. Thisleadstomorerobustperformanceacrossdatasetsand demonstratestheeffectivenessoftheproposeddataconstructionapproach. Furthermore,comparedtoUnion,Reverse-Unionintroducesacertaindegreeofreasoningcomplexity intotheinformation-seekingprocess,makingitmorechallengingforthemodeltoreadilyidentifywhere tobeginentityretrieval. Thisdesignparticularlyenhancesthemodel’splanninganddecision-making capabilities in information-seeking tasks. The improvement in these abilities is clearly reflected in performance,leadingtosubstantialandwidespreadgainsacrossallbenchmarks. 4.4 Impact of Information-Guided Trajectory Construction Wecomparetheproposedinformation-guidedtrajectoryconstructionstrategiesacrossISR-Only,ISE-Only, andISR+ISEonthreerepresentativebenchmarks—GAIA,BrowseComp,andWideSearch—toexamine theindependentandcombinedeffectsofISEandISR. OnGAIAandBrowseComp,ISR+ISEachievesthebestperformance,suggestingthatintegratingprecision andefficiencyconstraintsproducestrajectoriesthatarebothgoal-directedandconcise,therebyreducing redundantexploration. Thisindicatesthatinmorecomplexbrowsingtasks,relevanceandefficiency constraintscomplementeachothertogeneratehigher-qualitytrajectories. Incontrast,onWideSearch,thethreestrategiesdelivercomparableresults,withperformancedifferences fallingwithinthemarginofvariance. Thissuggeststhatforbroadsearchtasks,thespecificchoiceof trajectoryfilteringplaysalesscriticalrole—likelybecausetrainingonentity-intensivesynthesizeddata alreadyprovidesstrongbroadsearchcapabilities. 4.5 Joint Gains in Efficiency and Effectiveness AsillustratedinFigure5,WebLeaperconsistentlyoutperformsthebaselineintermsofbotheffectiveness andefficiency. IntheWideSearchandBrowseCompbenchmarks,ourapproachachievesmarkedlyhigher performance scores while requiring fewer average action rounds, indicating that the search process is not only more accurate but also more efficient. Similarly, in the GAIA and xbench-DS tasks, our methodimproveseffectivenesswhilesimultaneouslyreducingtheoperationalcost. Thisdemonstrates thatourdesignenablesamoretargetedsearchstrategy,resultinginreducedinteractionstepswithout sacrificing—andinfactenhancing—thequalityoftheresults. Overall,theseresultsvalidatethatourproposedmethodachievessuperiorjointoptimizationofinformation- seekingefficiencyandtaskperformancecomparedtothebaseline. Thisreflectsourkeyinsight: anagent shouldnotmerelylearntosearch,butratherlearntosearchefficientlyandwisely,therebyachievinga betterbalancebetweenefficiencyandeffectiveness. 12 --- Page 13 --- 0.65 0.60 0.55 0.50 0.45 0.40 0.35 20 40 60 80 100 120 140 Step draweR Reward Curve Raw Reward Smoothed Reward Figure 6: Figure shows the training curve of the hybrid reward system, indicating that using the WebLeaperdataleadstoastableincreaseinreward. Weterminatedtheexperimentat135stepswhen webaccessresourceswereexhaustedandevaluatedtheresultsatthispoint. 4.6 Reinforcement Learning using WebLeaper Table3: RLResultsoncomprehensivesetting.  from3rollouts. WideSearchreportsSuccessRate(SR),Row F1,andItem F1. WideSearch BrowseComp GAIA xbench-DS SR Row F1 Item F1 SFT 37.80 69.9 69.0 1.5 23.0 45.4 SFT+RL 38.8 (+1.0) 73.2 (+3.3) 72.0 (+3.0) 4.0 (+2.5) 31.0 (+8.0) 48.5 (+3.1) Wefurtherevaluateourapproachthroughreinforcementlearning,adoptingtheAdditionalSettingsfor MoreComprehensiveandRealisticTraining(seeSection4.1)whereWebLeaperdataismixedintoalarger trainingcorpusforbothSFTandsubsequentRLstages. AsdemonstratedbytheresultsinTable3and Figure6,usingWebLeaperdataforRLyieldsconsistentandsignificantimprovements. Theresultstable showsthatafterRLfine-tuning,themodelcomprehensivelysurpassestheSFT-onlybaselineacrossall benchmarks. This positive performance trend is echoed by the reward curve in Figure 6, which exhibits a stable and continuous upward trajectory throughout the training process. This indicates that the model is effectivelylearningfromtherewardsignalsderivedfromtheWebLeaperdata,progressivelyrefining its information-seeking strategy towards greater efficiency and accuracy. Even with the experiment concludingat135steps,theclearlearningtrendunderscoresthepotentialforfurthergains. TheresultsstronglyvalidatetheeffectivenessoftheWebLeaperdataset. Itnotonlyservesasarobust foundationforsupervisedfine-tuningbutalsoprovidesahigh-qualitysignalforRL,successfullyguiding theagenttomastermoresophisticatedandoptimalinformation-seekingbehaviors. 5 Related Work 5.1 Information Seeking Agent LLM-powered information-seeking agents can be broadly categorized into 3 streams: (1) enhancing coremodelsviasupervisedfine-tuning(Zengetal.,2023;Wuetal.,2025a;Lietal.,2025c;b;Taoetal., 13 --- Page 14 --- 2025; Su et al., 2025; Fang et al., 2025a); (2) advancing agent architecture for improved planning and robustness(Qiaoetal.,2025;Xuetal.,2025a;Lietal.,2025a);and(3)developingmulti-agentsystemsfor collaborativeproblem-solving(Wuetal.,2023;Hongetal.,2024). Ourworkalignswiththefirstcategory butaddressesakeylimitation. Priormethodsoftentrainontasksfocusedoncorrectnesswithsingle-fact answers,whichisinsufficientforlarge-scaleinformationgathering. Wepositthatthenumberofentities inananswer—itsentityrichness—isacriticaldimensionforevaluatinganagent’scompletenessand efficiency. Thispaperaimstobridgethisgapbycreatingandutilizingentity-richQAdatatoenhance agentcapabilitiesforcomprehensiveinformationacquisition. 5.2 Agent Data Synthesis Syntheticdatagenerationispivotalforagenttraining,withprimaryapplicationsintooluse(Wuetal., 2025a;Taoetal.,2025;Shenetal.,2025;Fangetal.,2025b),codegeneration(Jimenezetal.,2024;SHEN etal.,2025;Xuetal.,2025c;Shaoetal.,2025),andGUIautomation(Xuetal.,2025b;Sunetal.,2025;Pahuja etal.,2025).Theseeffortsprimarilycombatdatascarcity.Withintheinformation-seekingdomain,existing datasynthesisapproachesincreasetaskdifficultythroughmulti-stepreasoning(Wuetal.,2025b;a;Tao etal.,2025)orlong-horizonplanning(Qiaoetal.,2025). Wecontendthatsuchmethodsoftenoverlook thesemanticrichnessofthetrainingdataitself. Incontrast,ourapproachcentersonsynthesizingQA datawithhighentity-levelcomplexity. Wehypothesizethatthisfocusondatasemanticsisacrucialand complementarypathtoimprovingagentreasoningandworldknowledgealignment. 6 Conclusion Inthispaper,weaddressedthecriticalchallengeoflowsearchefficiencyinLLM-basedinformation- seekingagents,abottleneckthatconstrainstheiroverallperformance. Wearguedthatthesparsityof targetentitiesinconventionaltrainingtasksisaprimarycontributortothisinefficiency. Toovercomethis, weintroducedWebLeaper,anovelframeworkforconstructingentity-intensiveIStasksandgenerating efficientsolutiontrajectories. ByformulatingISasatree-structuredreasoningproblemandsystematically increasingtaskcomplexitythroughourBasic,Union,andReverse-Uniontasksynthesisvariants,we createdarichtrainingenvironment. Furthermore,ourinformation-guidedtrajectorycuration,using ISRandISEmetrics, ensuresthattheagentlearnsfromsolutionsthatarebothaccurateandefficient. OurextensiveexperimentsdemonstratedthatWebLeaperconsistentlyimprovesperformanceacrossfive challengingbenchmarks,validatingthatenhancingsearchefficiencyisapowerfulleverforboostingthe overallcapabilitiesofISagents. 14 --- Page 15 --- References Anthropic. Introducingclaude4,2025. URL Runnan Fang, Shihao Cai, Baixuan Li, Jialong Wu, Guangyu Li, Wenbiao Yin, Xinyu Wang, Xiaobin Wang, Liangcai Su, Zhen Zhang, Shibin Wu, Zhengwei Tao, Yong Jiang, Pengjun Xie, Fei Huang, andJingrenZhou. Towardsgeneralagenticintelligenceviaenvironmentscaling,2025a. URLhttps: //arxiv.org/abs/2509.13311. RunnanFang,ShihaoCai,BaixuanLi,JialongWu,GuangyuLi,WenbiaoYin,XinyuWang,XiaobinWang, LiangcaiSu,ZhenZhang,etal. Towardsgeneralagenticintelligenceviaenvironmentscaling. arXiv preprintarXiv:2509.13311,2025b. Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, and Yi Wu. Beyond ten turns: Unlocking long-horizon agentic search with large-scale asynchronous rl. arXiv preprintarXiv:2508.07976,2025. Gemini. Geminideepresearch,2025. URL TaichengGuo,XiuyingChen,YaqiWang,RuidiChang,ShichaoPei,NiteshVChawla,OlafWiest,and XiangliangZhang. Largelanguagemodelbasedmulti-agents: Asurveyofprogressandchallenges. arXivpreprintarXiv:2402.01680,2024. SiruiHong,MingchenZhuge,JonathanChen,XiawuZheng,YuhengCheng,JinlinWang,CeyaoZhang, ZiliWang,StevenKaShingYau,ZijuanLin,etal. MetaGPT:Metaprogrammingforamulti-agent collaborativeframework. InTheTwelfthInternationalConferenceonLearningRepresentations,2024. URL  Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R. Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth InternationalConferenceonLearningRepresentations,2024. URL F8yNQM66. BaixuanLi,YunlongFan,TianyiMa,MiaoGao,ChuanqiShi,andZhiqiangGao. Raspberry: Retrieval- augmentedmontecarlotreeself-playwithreasoningconsistencyformulti-hopquestionanswering. In FindingsoftheAssociationforComputationalLinguistics: ACL2025,pp.11258–11276,2025a. KuanLi,ZhongwangZhang,HuifengYin,RuiYe,YidaZhao,LiwenZhang,LituOu,DingchuZhang, XixiWu,JialongWu,XinyuWang,ZileQiao,ZhenZhang,YongJiang,PengjunXie,FeiHuang,and JingrenZhou. Websailor-v2: Bridgingthechasmtoproprietaryagentsviasyntheticdataandscalable reinforcementlearning,2025b. URL KuanLi,ZhongwangZhang,HuifengYin,LiwenZhang,LituOu,JialongWu,WenbiaoYin,BaixuanLi, ZhengweiTao,XinyuWang,WeizhouShen,JunkaiZhang,DingchuZhang,XixiWu,YongJiang,Ming Yan,PengjunXie,FeiHuang,andJingrenZhou. Websailor: Navigatingsuper-humanreasoningfor webagent,2025c. URL Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability. CoRR,abs/2504.21776,2025d. doi: 10.48550/ARXIV.2504.21776. URL rXiv.2504.21776. JuntengLiu,YunjiLi,ChiZhang,JingyangLi,AiliChen,KeJi,WeiyuCheng,ZijiaWu,ChengyuDu, QidiXu,etal. Webexplorer: Exploreandevolvefortraininglong-horizonwebagents. arXivpreprint arXiv:2509.06501,2025. 15 --- Page 16 --- RuiLu,ZhenyuHou,ZihanWang,HanchenZhang,XiaoLiu,YujiangLi,ShiFeng,JieTang,andYuxiao Dong. Deepdive: Advancingdeepsearchagentswithknowledgegraphsandmulti-turnrl. arXiv preprintarXiv:2509.10446,2025. Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmarkforgeneralaiassistants. InTheTwelfthInternationalConferenceonLearningRepresentations, 2023. OpenAI. Introducingopenaio3ando4-mini,2025a. URL 3-and-o4-mini/. OpenAI. Deepresearchsystemcard,2025b. URL ard.pdf. VardaanPahuja,MichaelChang,Hong-LakLee,IgorMordatch,andSergeyLevine. Explorer: Scaling exploration-drivenwebtrajectorysynthesisformultimodalwebagents,2025. Perplexity. Perplexitydeepresearch,2025. URL Thinh Pham, Nguyen Nguyen, Pratibha Zunjare, Weiyuan Chen, Yu-Min Tseng, and Tu Vu. Sealqa: Raisingthebarforreasoninginsearch-augmentedlanguagemodels. arXivpreprintarXiv:2506.01062, 2025. Zile Qiao, Shen Huang, Jialong Wu, Kuan Li, Wenbiao Yin, Xinyu Wang, Liwen Zhang, Baixuan Li, ZhengweiTao,WeizhouShen,XixiWu,YongJiang,PengjunXie,FeiHuang,JunZhang,andJingren Zhou. WebResearcher: Unleashingunboundedreasoningcapabilityinlong-horizonagents,2025. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimizationalgorithms,2017. URL Zheng-XinShao, YeyunGong, YelongShen, JianJiao, RuossJia, YujiuYang, NanDuan, andWeizhu Chen. Case2Code: Scalablesyntheticdataforcodegeneration. InProceedingsofthe31stInternational ConferenceonComputationalLinguistics.AssociationforComputationalLinguistics,2025. ZhihongShao,PeiyiWang,QihaoZhu,RunxinXu,JunxiaoSong,XiaoBi,HaoweiZhang,Mingchuan Zhang,Y.K.Li,Y.Wu,andDayaGuo. Deepseekmath: Pushingthelimitsofmathematicalreasoningin openlanguagemodels,2024. URL HaiyangSHEN,YueLi,DesongMeng,DongqiCai,ShengQi,LiZhang,MengweiXu,andYunMa. Short- cutsbench: Alarge-scalereal-worldbenchmarkforAPI-basedagents. InTheThirteenthInternational ConferenceonLearningRepresentations,2025. URL HaiyangShen,HangYan,ZhongshiXing,MugengLiu,YueLi,ZhiyangChen,YuxiangWang,Jiuzheng Wang,andYunMa. Ragsynth: Syntheticdataforrobustandfaithfulragcomponentoptimization,2025. URL LiangcaiSu,ZhenZhang,GuangyuLi,ZhuoChen,ChenxiWang,MaojiaSong,XinyuWang,KuanLi, JialongWu,XuanzhongChen,ZileQiao,ZhongwangZhang,HuifengYin,ShihaoCai,RunnanFang, ZhengweiTao,WenbiaoYin,ChenxiongQian,YongJiang,PengjunXie,FeiHuang,andJingrenZhou. Scalingagentsviacontinualpre-training,2025. URL QiushiSun,KanzhiCheng,ZichenDing,ChuanyangJin,YianWang,FangzhiXu,ZhenyuWu,Chengyou Jia, Liheng Chen, Zhoumianze Liu, Ben Kao, Guohao Li, Junxian He, Yu Qiao, and Zhiyong Wu. OS-genesis: AutomatingGUIagenttrajectoryconstructionviareversetasksynthesis. InWanxiang Che,JoyceNabende,EkaterinaShutova,andMohammadTaherPilehvar(eds.),Proceedingsofthe63rd AnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),pp.5555–5579, Vienna,Austria,July2025.AssociationforComputationalLinguistics. ISBN979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.277. URL 16 --- Page 17 --- Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang,XinyuWang,YongJiang,PengjunXie,FeiHuang,andJingrenZhou. WebShaper: Agentically datasynthesizingviainformation-seekingformalization,2025. KimiTeam. Kimiresearchertechreport,2025. URL r/. KimiTeam,YifanBai,YipingBao,GuanduoChen,JiahaoChen,NingxinChen,RuijueChen,YanruChen, YuankunChen,YutianChen,etal. Kimik2: Openagenticintelligence. arXivpreprintarXiv:2507.20534, 2025a. MiroMindAITeametal. Mirothinker: Anopen-sourceagenticmodelseriestrainedfordeepresearch andcomplex,long-horizonproblemsolving,2025b. OpenPanguTeam. Openpangudeepdiver-v2: Multi-agentlearningfordeepinformationseeking,2025b. URL JasonWei,ZhiqingSun,SpencerPapay,ScottMcKinney,JeffreyHan,IsaFulford,HyungWonChung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: A simple yet challenging benchmarkforbrowsingagents. arXivpreprintarXiv:2504.12516,2025. RyanWong,JiaweiWang,JunjieZhao,LiChen,YanGao,LongZhang,XuanZhou,ZuoWang,KaiXiang, GeZhang,etal. Widesearch:Benchmarkingagenticbroadinfo-seeking. arXivpreprintarXiv:2508.07999, 2025. JialongWu,BaixuanLi,RunnanFang,WenbiaoYin,LiwenZhang,ZhengweiTao,DingchuZhang,Zekun Xi,GangFu,YongJiang,PengjunXie,FeiHuang,andJingrenZhou. Webdancer: Towardsautonomous informationseekingagency,2025a. URL JialongWu,WenbiaoYin,YongJiang,ZhenglinWang,ZekunXi,RunnanFang,LinhaiZhang,Yulan He,DeyuZhou,PengjunXie,andFeiHuang. WebWalker: BenchmarkingLLMsinwebtraversal. In WanxiangChe,JoyceNabende,EkaterinaShutova,andMohammadTaherPilehvar(eds.),Proceedingsof the63rdAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),pp.10290– 10305,Vienna,Austria,July2025b.AssociationforComputationalLinguistics. ISBN979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.508. URL QingyunWu,GaganBansal,JieyuZhang,YiranWu,BeibinLi,ErkangZhu,LiJiang,XiaoyunZhang, ShaokunZhang,JialeLiu,etal. Autogen: Enablingnext-genllmapplicationsviamulti-agentconversa- tion. arXivpreprintarXiv:2308.08155,2023. Xbench-Team. Xbench-deepsearch,2025. URL WujiangXu,KaiMei,HangGao,JuntaoTan,ZujieLiang,andYongfengZhang. A-mem:Agenticmemory forllmagents,2025a. URL YihengXu,DunjieLu,ZhennanShen,JunliWang,ZekunWang,YuchenMao,CaimingXiong,andTaoYu. AgentTrek: Agenttrajectorysynthesisviaguidingreplaywithwebtutorials. InInternationalConference onLearningRepresentations,2025b. ZhangchenXu,YangLiu,YueqinYin,MingyuanZhou,andRadhaPoovendran. KodCode: Adiverse, challenging,andverifiablesyntheticdatasetforcoding,2025c. ZhenghaiXue,LongtaoZheng,QianLiu,YingruLi,XiaosenZheng,ZejunMa,andBoAn.Simpletir:End- to-endreinforcementlearningformulti-turntool-integratedreasoning. arXivpreprintarXiv:2509.02479, 2025. 17 --- Page 18 --- ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikNarasimhan,andYuanCao. Re- act: Synergizing reasoning and acting in language models. In International Conference on Learning Representations(ICLR),2023. QinghaoYe, HaiyangXu, GuohaiXu, JiaboYe, MingYan, YiyangZhou, JunyangWang, AnwenHu, PengchengShi,YayaShi,etal. mPLUG-Owl: Modularizationempowerslargelanguagemodelswith multimodality. CoRR,abs/2304.14178,2023. AohanZeng,MingdaoLiu,RuiLu,BowenWang,XiaoLiu,YuxiaoDong,andJieTang. AgentTuning: Enabling generalized agent abilities for LLMs. arXiv preprint arXiv:2310.12823, 2023. URL https: //arxiv.org/abs/2310.12823. 18 --- Page 19 --- A Appendix A.1 Declaration on the Use of LLMs WedeclarethattheuseofLLMsduringthepreparationofthismanuscriptwasstrictlylimitedtolanguage- relatedassistance,suchassentencerefinementandgrammaticalcorrection. Allsubstantivecontentwas independentlyauthoredbytheauthorsandrigorouslyreviewedandverifiedfollowinganyLLM-assisted modifications. Duringtheexperiments,allusageofLLMswassolelyforacademicresearchpurposes, with no inappropriate applications. Detailed experimental settings are provided in the Experiments sectionofthispaper. NootherrelianceonLLMsisinvolvedinthiswork. A.2 Proof of Proposition 1 ThisappendixprovidesthedetailedmathematicalderivationforProposition1,aspresentedinSection2.3. ThepurposeofthisproofistoformallyestablishthatthevarianceoftheInformation-SeekingEfficiency (ISE)metricisinverselyproportionalton,thenumberofrequiredentities. Thisproperty,Var(ISE) = O(1/n),demonstratesthatISEbecomesanincreasinglystableandreliableperformancemeasureasthe complexityofthetask(i.e.,thesizeofn)grows. Proof. LetX bethenumberofstepstheagenttakestodiscoverthei-thnewentityintherequiredset i R. Weassume{X}n areindependentandidenticallydistributed(i.i.d.) randomvariableswithmean i i=1 E[X] = µandvarianceVar(X) = σ2. i i Thetotalnumberofstepsis T = ∑n X. Let X betheaveragenumberofstepstofindonerequired i=1 i entity,definedasX = 1 ∑n X = T. Bydefinition,ISE= n/T =1/X. n i=1 i n Fromthepropertiesofi.i.d. randomvariables,themeanandvarianceofXare: E[X] = µ, (13) σ2 Var(X) = . (14) n WeareinterestedinthevarianceofISE,whichisafunctionoftherandomvariableX. Letthisfunction be f(X) =1/X. WecanapproximatethevarianceofISEusingtheDeltamethod,whichstatesthatfora function f withanon-zeroderivativeatµ: Var(f(X)) ≈ (cid:0) f′(E[X])(cid:1)2 Var(X). First,wecomputethederivativeof f(x) =1/x,whichis f′(x) = −x−2. Evaluatingthisderivativeatthe meanµ: f′(µ) = −µ−2. Now,substitutingthisandthevarianceofXfromEquation(14)intotheDeltamethodformula: (cid:16) (cid:17)2 σ2 1 σ2 (cid:18) 1(cid:19) Var(ISE) ≈ −µ−2 · = · = O . n µ4 n n Thiscompletestheproof. A.3 Data Statistics Figure7illustratesthedistributionofourtrainingdata. Figure8displaystheentitycountdistributionofourtrainingdata. Asignificantportionofoursamples containatleast100entities,underscoringtheinherentdifficultyofourdataset. AsformalizedinEquation 6,thiscomplexityiscrucialforrobustlymeasuringefficiency,whichinturnleadstoimprovedoverall performance. 19 --- Page 20 --- Figure7: Thedistributionofourtrainingdata. A.4 Data Cleaning and Basic Task Construction ThissectionelaboratesonthedataprocessingandconstructionmethodologyfortheBasicversiontasks introducedinSection3.1.1. RationaleforTreeStructure Ininformation-seekingtasks,thereasoningstructureisparamount. We choseatreestructureforourbasictasksbecauseitoffersacompactandhierarchicalorganizationof entities. Thisstructureishighlyefficientforrepresentingalargenumberofinterconnectedentitiesthat stemfromacommonqueryconcept,mirroringmanyreal-worldinformation-gatheringscenarios. A reasoningtreeiscomposedofaroot(questionentity)andasetofsubtrees,whereeachsubtreerepresents acohesiveunitofinformation. Multi-StageTableCleaning Toensurethequalityandsuitabilityofthedatausedfortasksynthesis, wecrawledapproximately2milliontablesfromWikipediaandsubjectedthemtoarigorousmulti-stage cleaningprocedure. Thiswasessentialbecauserawwebtablesareoftennoisyandinconsistent. The stageswereasfollows: • SizeFiltering: Wefirstdiscardedtablesthatwereeithertoosmall(fewerthan10rowsor3columns) tocapturemeaningfulrelationalinformation,ortoolarge(morethan200rowsor20columns)tobe processedefficientlyandformacoherenttask. • SemanticandStructuralFiltering: Wethenremovedsemanticallyirrelevantcolumnsthatfrequently appear in web tables, such as those containing serial numbers, notes, or references. Tables with significantformattingerrors(e.g.,numerousmergedcellsthatdisrupttherelationalstructure)were alsoexcluded. • Isomorphism and Homogeneity: Finally, we retained only groups of isomorphic tables (tables sharing the same column headers and structure). This step was crucial for ensuring structural homogeneityacrossourdataset,whichisaprerequisiteforidentifyingcommonsubtreestructures neededfortheUnionoperationdescribedlater. Theresultingcollectioncontainsclean,well-structuredtableswithasetofmeaningfulfieldsascolumns andmultiplerows,whereeachrowcanbetransformedintoasubtree. ReasoningTreePopulation Toconstructthethree-layerreasoningtreefromasingletable,wepopulate thelayersasfollows: • FirstLayer(QuestionEntities): Entitiesmentionedinthetable’stitleorcaptionareextractedto formtherootofthetree. • SecondLayer(RootsofSubtrees): WeemployanLLMtoanalyzethetable’scolumnsandselect onethatcontainsnoduplicateentries. Thiscolumnistreatedasthekey,anditsvaluesbecomethe 20 --- Page 21 --- Figure8: EntityCountDistributioninTrainingData. Asignificantportionofoursamplescontainsat least100entities,underscoringtheinherentdifficultyofourdataset. Thiscomplexity,asformalizedin Equationequation6,iscrucialforrobustlymeasuringefficiency,whichinturncontributestoimproved overallperformance. second-layerentitiesofthetree. Eachoftheseentitiesservesastherootofasubtree. TheLLMis effectiveatidentifyingcolumnslike‘Name’or‘Title’thatservethisuniqueidentificationpurpose. • ThirdLayer(LeavesofSubtrees): Thevaluesintheremainingcolumnsofthetableconstitutethe thirdlayer,representingtheleafentitiesassociatedwitheachsecond-layerentity. A.5 Maximal Union Algorithm for Task Synthesis Thissectionprovidestheformaldefinitionandalgorithmicimplementationfordiscoveringmaximal uniongroups,asintroducedinSection3.1.2. Thecoreofourapproachistoreformulatethesearchfor compatiblereasoningtreesasaMaximalBicliqueEnumeration(referto 1problemonabipartitegraph. ProblemFormulation LetT = {T ,T ,...,T }beourcollectionofbasicreasoningtrees. Wefirstconstructabipartitegraph base 1 2 N G = (U,V,E),whereU = T isthesetofalltrees,andV isthesetofalluniquerelationnamesfound base withinthesubtreesacrossalltreesinT . Anedge(T,v ) ∈ Eexistsiftherelationv ispresentinany base i j j subtreeoftreeT (i.e.,v ∈Rel(T),whereRel(T) = (cid:83) Rel(S )). i j i i k i,k Inthisconstruction,amaximaluniondirectlycorrespondstoamaximalbiclique(U,V),whereU ⊆Uisa setoftreesandV ⊆V isasetoftheircommonrelations. Ourgoalistofindallsuchmaximalbicliques thatsatisfycertainsizeandsemanticconstraints. Formally,weseektofindallmaximalpairs(U,V)that satisfy: findmaximal (U,V) subjectto ∀T ∈ U,V ⊆Rel(T), (15) i i |U| ≥ k ,|V| ≥ m . min min Here,maximalitymeansthatnoothertreecanbeaddedtoU andnootherrelationcanbeaddedtoV withoutviolatingthebicliqueproperty. Solvingthisbyreformulatingitasastandardmaximalbiclique enumerationproblemiscomputationallyefficientcomparedtoanexhaustivesearch. AlgorithmandImplementationDetails • Input: AcollectionofbasereasoningtreesT ;aminimumnumberoftreesforavalidunion,k ; base min 21 --- Page 22 --- aminimumnumberofcommonrelations,m . min • Goal: Tofindallmaximaluniongroups,whicharethesolutions(U,V)toEq.(15)thatalsosatisfythe semanticmatchingcriteriabelow. • SubtreeRelationMatchingCriteria: Toensurethesemanticcoherenceofunions,weimposestrict matchingcriteria. Forrelationsconnectingthesecondandthirdlayers,werequiretheysharethe samestandardizedname,datatype,anddomain. Forthesecond-layerentitiesthemselves(theroots ofthesubtrees), werelaxthisconstraint, requiringonlyamatchindatatypeanddomain. This flexibilityallowsfortheunionoftreeswithconceptuallysimilarbutdifferentlynamedsecond-layer entities(e.g.,fusingatreewhereentitiesare’Authors’withanotherwheretheyare’Writers’). • Output: AsetofmaximaluniongroupsF,whereeachelementisatuple⟨U′,V′⟩thatmeetsthe specifiedcriteria. TheprocessisdetailedinAlgorithm1. Algorithm1:MaximalUnionIdentificationAlgorithm Input: AcollectionofbasereasoningtreesT ,minimumtreesk ,minimumcommonrelations base min m . min Output: AsetofmaximaluniongroupsF. 1 F ← ∅; // 1. Construct the bipartite graph from trees and subtree relations 2 LetUbethesetoftreesfromT baseandV bethesetofuniquestandardizedrelationnamesfound withinthesubtreesofalltreesinT ; base 3 ConstructthegraphG = (U,V,E)whereanedge(u,v) ∈ Eexistsiftreeucontainstherelationvin itssubtrees(i.e.,v ∈Rel(u)); // 2. Enumerate maximal bicliques from the graph 4 B ←EnumerateMaximalBicliques(G); ; // Leverages standard algorithms like MICA or Eclat // 3. Filter and validate bicliques to form final union groups 5 foreachmaximalbiclique(U′,V′)inBdo // Check size constraints from Eq. (1) 6 if|U′| < k minor|V′| < m minthen 7 continue; // Validate semantic compatibility of second-layer entities 8 LetT id,D idbethetypeanddomainofthesecond-layerentitiesofthefirsttreeinU′; 9 is_compatible←true; 10 foreachtreeu ∈U′ do 11 ifu’ssecond-layerentitytype̸= T idordomain̸= D idthen 12 is_compatible←false; 13 break; // If all checks pass, add to the set of valid union groups 14 ifis_compatiblethen 15 F ← F ∪{⟨U′,V′⟩}; 16 returnF; A.6 Detailed Examples of Task Synthesis Thissectionprovidesdetailedexplanationsandreasoningwalkthroughsfortheexamplesofthethree tasksynthesisversionspresentedinSection3andFigure3. 22 --- Page 23 --- A.6.1 Version-I:Basic Thegoalofthebasicversionistocreateataskwithaclear,hierarchicalreasoningstructurederivedfrom asingle,self-containedsetofentities. ExampleQuestion: WhoweretheNobelPrizewinnersinLiteraturebetween1980and1990? Pleaseinclude theirname,country,awardyear,andgender. ConstructionProcess: ThetaskisconstructedfromasingleWikipediatable,formingareasoningtree. ThelayersshowninFigure3(a)arepopulatedasfollows: • FirstLayer(questionentities): Derivedfromthetable’stitleandaspecifiedconstraint,formingthe query’sscope: LiteratureNobelPrize,year1980–1990. • SecondLayer(subtreeroots): Populatedfromthetable’skeycolumn(e.g.,authornames): Czesław Miłosz,WilliamGolding,.... • Third Layer (subtree leaves): Consists of values from the remaining columns, representing at- tributes for each second-layer entity. For example: man, Poland, 1980 for Czesław Miłosz. The edgesconnectingthesecondtothethirdlayerrepresentrelationslike‘has_gender’,‘has_country’, ‘has_award_year’. ReasoningPath: Anagentisexpectedtofollowthishierarchicalstructure: • Identify Scope: Recognize the “Question Entities” from the query: Nobel Prize in Literature, 1980–1990. • RetrieveSecond-LayerEntities: Retrievethesecond-layerentities,whicharetheauthors: Czesław Miłosz,WilliamGolding,.... • GatherAttributes: Foreachsecond-layerentity, followtherelationstoretrievetheirassociated third-layerentities,suchasPoland,1980,manforCzesławMiłosz. A.6.2 Version-II:Union This version increases structural complexity by requiring the agent to perform relational operations acrossdistinctreasoningtrees. ExampleQuestion: WhichauthorshavewonboththeNobelPrizeinLiteratureandtheBookerPrize? Foreach, providetheirname,nationalityandtheyeartheywontheNobel. ConstructionProcess: Onceamaximalunionisidentified(e.g.,betweenthereasoningtreesfor“Nobel Prizelaureates”and“BookerPrizewinners,”whichsharecommonrelationslike“has_nationality”within theirsubtrees),anLLMgeneratesataskrequiringinformationintegration. TheLLMispromptedtofind aninterestingrelationship,suchastheintersectionofthetwosetsofsecond-layerentities(authors),and thenweavethislogicintoanaturallanguagequestion. ReasoningPath: Thetaskisconstructedfromamaximalunionoftwodistinctreasoningtrees. Tosolve this,anagentmust: • RetrieveFirstEntitySet: Identifythefirstconcept,“NobelPrizeinLiterature,”andretrievethefull setofcorrespondingsecond-layerentitiesfromthefirsttree,R . Nobel(T1) • RetrieveSecondEntitySet: Identifythesecondconcept,“BookerPrize,”andretrieveitsfullsetof second-layerentitiesfromthesecondtree,R . Booker(T2) • FindIntersection: Performarelationaljointofindtheintersectionofthetwosetsofsecond-layer entities based on name. The final “Target Entities” are the entities present in both sets, such as {WilliamGolding,J.M.Coetzee,...},alongwiththeirrequestedthird-layerattributes. 23 --- Page 24 --- 0.0200 0.0175 0.0150 0.0125 0.0100 0.0075 0.0050 0.0025 0.0000 0 20 40 60 80 100 Toolcall Count ytisneD Toolcall Count Density 0.040 0.035 0.030 0.025 0.020 0.015 0.010 0.005 0.000 0 20 40 60 80 Search Count ytisneD Search Count Density 0.035 0.030 0.025 0.020 0.015 0.010 0.005 0.000 0 20 40 60 80 Visit Count ytisneD Visit Count Density Figure9: DistributionofSearch,Visit,andtotaltoolcall. A.6.3 Version-III:Reverse-Union Thisversionintroducesachallengingcognitiveworkflowbyintentionallyobfuscatingthequery’sentry points. Motivation and Design: The Union method, while creating multi-source tasks, has a vulnerability: an agent could solve it with simple keyword searches for each source, bypassing deeper reasoning. Reverse-Unioninvertstheinformationflow,forcinganagenttofirstdeduceacore‘anchor’entity(a second-layerentity)fromdescriptivecluesandthenusethatentityasapivottoexpanditssearch. ExampleQuestion: Whoaretheauthorsfromthesamecountryasthe1980sprize-winnerthatwroteanovel aboutagroupofBritishboysstrandedonanuninhabitedisland,andwhohavealsowonboththisrewardandthe BookerPrize? Foreachofthem,whatistheirname,country,andtherespectiveyearstheywoneachaward? ConstructionProcess: TheconstructionbuildsupontheunifiedspacefromVersion-IIwitha“reverse” logic: • Source: WeusetheunifiedinformationspacefromtheNobelandBookerprizeunion. • SelectAnchor: Anentityattheintersectionofthesecondlayersischosenasthe“anchor,” e.g., WilliamGolding. • ObfuscateAnchor: Insteadofnamingtheanchor,uniquedescriptivecluesbasedonitsthird-layer attributesaregenerated: “the1980sprize-winner”and“wroteanovelabout... Britishboys...” These cluesbecomethe‘QuestionEntities’. • CreateUnionTrigger: Athird-layerattributeoftheanchor,hisnationality(British),isselectedas thepivotforthenextstageofthequery. RequiredReasoningProcess: Tosolvethistask,anagentmustexecuteatwo-stageprocess: • DeductionStage:Theagentmustfirstresolvethedescriptiveclues(whicharethird-layerentities)to identifythesecond-layeranchorentity. Theclues“1980sprize-winner”and“novelaboutstranded Britishboys”uniquelypointtoWilliamGolding. Thisinferentialstepiscrucial. • UnionStage: HavingdeducedWilliamGolding,theagentidentifieshisnationality(athird-layer entityinhissubtree): British. Thisbecomesthepivotforthemainquery. Theagentmustthenfind allsecond-layerentitieswho(1)sharethisthird-layerattribute(British)and(2)havewonboththe NobelPrizeandtheBookerPrize. Thisrequiresfilteringtheunifiedentityspacetofindthefinalset of“TargetEntities”,whichincludesauthorslikeWilliamGolding,KazuoIshiguro,andJ.M.Coetzee. 24 --- Page 25 --- B Tool Call Analysis AsshowninFigure9, ourmethodinvolvesasignificantlylargenumberofactions, includingSearch, Visit,andtotaltoolcalls. Thedensitydistributionsindicatethattoolcallsoftenexceedseveraldozenper instance,withmanycasessurpassing50actions. Thishighfrequencyofactionsreflectstheintensive interaction and comprehensive exploration carried out by our approach, ensuring that the method thoroughlyleveragesavailabletoolstoachieveoptimalperformance. 25