--- Page 1 --- Greedy Sampling Is Provably Efficient for RLHF DiWu ChengshuaiShi ElectricalandComputerEngineering PrincetonLanguageandIntelligence UniversityofVirginia PrincetonUniversity Charlottesville,VA22903 Princeton,NJ08540   JingYang CongShen ElectricalandComputerEngineering ElectricalandComputerEngineering UniversityofVirginia UniversityofVirginia Charlottesville,VA22903 Charlottesville,VA22903   Abstract ReinforcementLearningfromHumanFeedback(RLHF)hasemergedasakeytech- niqueforpost-traininglargelanguagemodels. Despiteitsempiricalsuccess,the theoreticalunderstandingofRLHFisstilllimited,aslearningtheKL-regularized targetwithonlypreferencefeedbackposesadditionalchallengescomparedwith canonicalRL.Existingworksmostlystudythereward-basedBradley-Terry(BT) preferencemodel,andextendclassicaldesignsutilizingoptimismorpessimism. Thiswork,instead,considersthegeneralpreferencemodel(whosepracticalrele- vancehasbeenobservedrecently)andobtainsperformanceguaranteeswithmajor, order-wiseimprovementsoverexistingones. Surprisingly,theseresultsarederived fromalgorithmsthatdirectlyusetheempiricalestimates(i.e.,greedysampling), asopposedtoconstructingoptimisticorpessimisticestimatesinpreviousworks. Thisinsighthasadeeprootintheuniquestructuralpropertyoftheoptimalpolicy classundertheKL-regularizedtarget,andwefurtherspecializeittotheBTmodel, highlightingthesurprisingsufficiencyofgreedysamplinginRLHF. 1 Introduction In recent years, a growing amount of attention has been given to Reinforcement Learning from HumanFeedback(RLHF),anessentialcomponentinthepost-trainingstageoflargelanguagemodels (LLMs)(Baietal.,2022;Ouyangetal.,2022;Rafailovetal.,2023;Dongetal.,2023;Zhaoetal., 2023). Specifically,RLHFhasbeenoneofthekeytechniquespoweringthemostwidelyusedLLMs (Achiametal.,2023;Baietal.,2022;Touvronetal.,2023;Teametal.,2023). Withitstremendous empiricalsuccess,researchershavestartedtoseekadeepertheoreticalunderstandingofRLHF(Zhu etal.,2023;Xiongetal.,2023). Compared with canonical reinforcement learning (RL) (Sutton et al., 2018), RLHF has two key characteristics. First,ratherthantheabsoluterewardsignals,thelearningagentinRLHFcommonly observes preference feedback, since during LLM training, it is easier for human annotators to compareanswersthantoprovideabsolutescores. Second,RLHFcommonlyconsidersatargetwith anadditionalpenaltytermastheKullback–Leibler(KL)divergencebetweenthelearnedpolicyanda referencepolicy. ThistargetisoftenreferredtoastheKL-regularizedRLandinthemostcommon single-turnscenario,KL-regularizedcontextualbandits(CB). 39thConferenceonNeuralInformationProcessingSystems(NeurIPS2025). 5202 tcO 82 ]GL.sc[ 1v00742.0152:viXra --- Page 2 --- WhilethesetwokeycharacteristicsposeadditionalchallengesfortheanalysisofRLHF,interesting theoreticalguaranteeshavebeenderived. BasedonZhaoetal.(2024),ithasbeenestablishedthat learninginKL-regularizedRL/CBcanachievearegretupperboundofO(log(T))fortheonline settingwithatimehorizonofT (Zhaoetal.,2025a),andasamplecomplexityofO(ε−1)forthe offlinesettingtoobtainanε-optimalpolicywiththesingle-policycoverage(Zhaoetal.,2025b), bothofwhichareinstance-agnostic. Theseresultshavecleargapstothetraditionallowerbounds √ ofΩ( T)regret(Aueretal.,2008;OsbandandVanRoy,2016)andΩ(ε−2)samplecomplexity (Rashidinejadetal.,2021;Lietal.,2024)incanonicalRL. Despitetheseadvances,twomajorlimitationsexist. First,theaforementionedtightperformance boundsareobtainedbyeitherassumingdirectrewardobservations(Zhaoetal.,2025a,b)orrestricting tothereward-basedBradley-Terry(BT)preferencemodel(Zhaoetal.,2024,2025b). Thus,these resultsarenotapplicabletothegeneralpreferencemodelwithouttheconceptofrewards,whichhas empiricallydemonstratedappealingflexibilityandadaptability(Azaretal.,2024;Munosetal.,2023). Moreover,althoughthepropertiesofKL-regularizedRL/CBhasbeenleveragedforperformance analyses,theyarenotfullyexploitedtofacilitatenewalgorithmdesigns. Specifically,theproposed designsstillfollowtheclassicalprinciplesofoptimism(Aueretal.,2002;Abbasi-Yadkorietal., 2011)orpessimism(Rashidinejadetal.,2021;Jinetal.,2021)inthefaceofuncertainty. Inlightofthesetwolimitations,thisworkcontributestothetheoreticalunderstandingofRLHFby targetingthegeneralpreferencemodelandrevisitingthealgorithmdesigns. Themainresultsare summarizedinthefollowingseeminglycounter-intuitivestatement,whichholdsforbothonlineand offlinesettingsandbothgeneralandBTpreferencemodels: GreedysamplingwithrespecttotheempiricalestimatesisprovablyefficientforRLHF. Thisresultappearssurprising,asdirectlyleveragingtheempiricalestimatesforsamplingistypically undesirableincanonicalRL/CB,bothonlineandoffline. Aswewillillustrateinthiswork,itactually hasadeeprootinthetheoreticalpropertiesintroducedbytheadditionalKL-regularization. Centeredontheabovestatement,thecontributionsofthisworkcanbesummarizedasfollows: •Underthegeneralpreferencemodel,wedemonstratethatforRLHF(inparticular,KL-regularized CB),wecanderivearegretupperboundofO(log(T))withatimehorizonofT intheonlinesetting. Furthermore,intheofflinesetting,asamplecomplexityofO(ε−1)canbeachievedonlyrequiring asingle-policycoverage. Theseonlineandofflineguaranteesarethefirstonesoftheseordersfor √ thegeneralpreferencemodel,largelyimprovingthepreviousresultsofordersO( T)andO(ε−2) obtainedinYeetal.(2024b),respectively. •Moreimportantly,theaboveresultsareobtainedbydirectlyusinggreedysamplingwithrespect to the empirical estimates. As a result, in contrast to previous works that mostly use optimistic or pessimistic estimates, there is no need for constructing upper and lower confidence bounds, whichisoftencomputationallyheavyingeneralscenarios. Thisisthefirsttime,tothebestofour knowledge,thattheprovableefficiencyofgreedysamplingisestablishedforRLHF,regardlessofthe preferencemodels. Technically,thissurprisingresultcomesfromthefundamentalfactthat,under theKL-regularizedobjective,everycandidateoptimalpolicylieswithinaboundedlikelihoodratioof thereferencepolicy–astructuralpropertypreviousstudieshavelargelyoverlooked. •Tofurthergeneralizetheaboveobservation,wederivetheperformanceboundsofgreedysampling undertheBTmodel, whichareagainofordersO(log(T))andO(ε−1)fortheonlineandoffline settings,respectively. Theseresultsmatchtheupperboundsinpreviousworks(Zhaoetal.,2025a,b), whicharederivedunderoptimismandpessimism. •Thetheoreticalresults(i.e.,theefficiencyofdirectlyusingempiricalestimates)arecorroborated withsimulationresults. Inparticular,underbothgeneralandBTpreferencemodels,experimental results demonstrate that the simple greedy sampling approach achieves statistically comparable performancetopriormethodswithmoresophisticatedpolicyconstructions. 2 ProblemFormulationandPreliminaries BasedonthestandardLLMpost-trainingpipelineofRLHF(Ouyangetal.,2022;Achiametal., 2023), thisworkfocusesonthefollowingKL-regularizedcontextualbandits(CB)problemwith 2 --- Page 3 --- preferencefeedback,whichisalsostudiedinthepreviousliteraturewithatheoreticalfocus(Zhuetal., 2023;Xiongetal.,2023;Yeetal.,2024b;Zhaoetal.,2024,2025a,b). Specifically,itisconsidered thatapolicyπ :X →∆(A)(i.e.,anLLM)takesinacontextx∈X (i.e.,aprompt)andperformsan actiona∈A(i.e.,aresponse). Itiscommonlyassumedthatthecontextxisindependentlysampled fromadistributiond ,i.e.,x∼d . ThegoalofthelearningagentinRLHFistofindagoodpolicyπ 0 0 thatbestalignswithhumanvalues. Preferencemodel. AsmentionedinSection1,tofacilitatehumanannotators,akeycharacteristicof RLHFisthatthelearningagentreceivespreferencefeedbackoveractions(typicallyactionpairs) insteadofdirectrewardfeedback. Inparticular,givenacontextxandanactionpair(a1,a2),the learningagentreceivesabinaryfeedbacky ∈{0,1},withy =0meaninga1 ispreferredovera2, denotedasa1 ≻a2andy =0meaninga2ispreferredovera1,denotedasa2 ≻a1(ora1 ≺a2). Thisworkfirstconsidersthefollowinggeneralpreferencemodel,whichdirectlymodelstheprob- abilityofpreferringoneactionovertheothergivenacontext. Ithasattractedincreasingattention recently,duetoitsempiricalflexibilityandadaptability(Azaretal.,2024;Munosetal.,2023). Definition1(GeneralPreferenceModel) ApreferencemodelP:X ×A×A→[0,1]isdefined suchthatthepreferencesignaly ofanyactionpair(a1,a2) ∈ A×Aunderanycontextx ∈ X follows y ∼Ber(P(a1 ≻a2|x,a1,a2)). Withthedefinition,itisnaturallyobservedthatP(a1 ≻ a2|x,a1,a2)+P(a2 ≻ a1|x,a1,a2) = 1. Toeasethepresentation,itisdenotedthatP∗(x,a1,a2):=P(a1 ≻a2|x,a1,a2). AmorecommonlyconsideredpreferencemodelisthefollowingBradley-Terry(BT)model(Bradley andTerry,1952),whichisalsostudiedinthiswork. ItcanbeobservedthattheBTmodelisaspecial caseofthegeneralpreferencemodelbyintroducingalatentrewardfunction. Definition2(Bradley-TerryModel) TheBradley-TerrymodelsatisfiesDefinition1withtheproba- bilityofa ispreferredthana undercontextxmodeledas 1 2 P(a1 ≻a2|x,a1,a2)= exp(R∗(x,a1)) =σ(cid:0) R∗(x,a1)−R∗(x,a2)(cid:1) , exp(R∗(x,a1))+exp(R∗(x,a2)) whereR∗ :X ×A→[0,1]isarewardfunctionthatcapturestheperformanceofoneactiona∈A underacontextx∈X asR∗(x,a). Becausetheunderstandingofgeneralpreferencemodelsremainspreliminaryandmorecomplexthan thatoftheBTmodel,thesubsequentdiscussionfocusesprimarilyontheformer,eventhoughour contributionsaddressboth. Also,tofacilitatethepresentation,wewilluseGP(resp.,BT)todenote thegeneralpreferencemodel(resp.,theBTmodel). Learningobjective. Besidesthepreferencemodel,thesecondkeycharacteristicofRLHFisthata KL-regularizedtargetistypicallyconsidered,whichoriginatesfromitspracticalimplementations (Ziegleretal.,2019;Ouyangetal.,2022;Baietal.,2022;Rafailovetal.,2023). Intermsofthe general preference model, following Munos et al. (2023); Ye et al. (2024b), a game-theoretical perspectiveistakenwheretwoplayers(referredtoasthemax-playerandthemin-player)competein azero-sumgamewiththefollowingvalueforapolicypair(π1,π2): J (π1,π2):=E E (cid:2) P∗(x,a1,a2)−η−1KL(π1,π |x)+η−1KL(π2,π |x)(cid:3) . GP x∼d0 a1∼π1,a2∼π2 0 0 Intheabovedefinition,η ∈Risaparametercontrollingtheregularizationandπ isreferredtoas 0 thereferencepolicy,whichistypicallythemodelcheckpointaftersupervisedfine-tuninginLLM training. Inaddition,thesimplifiednotationKL(π1,π |x):=KL(π1(·|x)||π (·|x))isadoptedfor 0 0 easeofpresentation. Ingeneral,weconsiderthereferencepolicyπ asanon-deterministicpolicy. 0 Forthisgame,ithasbeendemonstratedinMunosetal.(2023);Yeetal.(2024b)thatthereexistsa uniqueNashequilibrium(NE)as (π1,∗,π2,∗)=(π∗ ,π∗ )=argmaxargminJ (π1,π2), GP GP GP π1∈Π π2∈Π 3 --- Page 4 --- whereΠ:={π :support(π)=support(π )}isthesetofpoliciessharingthesamesupportasπ so 0 0 thattheKL-regularizationismeaningful. Whenthereisnoambiguity,wealsorefertotheNEpolicy astheoptimalpolicywithaslightabuseofnotion. AsdefinedinYeetal.(2024b),thefollowingperformancemetricisdefinedforalearnedpolicyπˆ1 withJ∗ :=J (π∗ ,π∗ )as GP GP GP GP ∆ (πˆ1):=J∗ − min J (πˆ1,π2), GP GP GP π2∈Π whichisalwayspositivewhenπˆ1 ̸=π∗ . Apolicyπˆ1issaidtobeε-optimalif∆ (πˆ1)≤ε. When GP GP measuringtheperformanceofasequenceofpolicies{π1 :t∈[T]}(asintheonlinelearningsetting), t thefollowingdefinitionofregretisintroduced: (cid:88) Regret (T):= ∆ (πˆ1). GP GP t t∈[T] WiththeBTmodel,whiletheabovedefinitionscanstillbeapplied,itismoreconvenienttodirectly considertheperformanceintermsofrewards(Xiongetal.,2023;Zhaoetal.,2024). Especially,the valueofapolicyπisdefinedas J (πˆ):=E E (cid:2) R∗(x,a)−η−1KL(π,π |x)(cid:3) . BT x∼d0 a∼π 0 Thevaluedefinitionnaturallyleadstothefollowingperformancemetricofalearnedpolicyπˆ and regretofasequenceofpolicies{πˆ :t∈[T]}: t (cid:88) ∆ (πˆ):=J∗ −J (πˆ); Regret (T):= ∆ (πˆ ), BT BT BT BT BT t t∈[T] whereJ∗ :=J (π∗ )withπ∗ :=argmax J (π)astheoptimalpolicyundertheBTmodel. BT BT BT BT π∈Π BT Notation. Toeasethepresentation,wealsoadoptthefollowingnotations: underageneralpreference model P, the expected probability of preferring an action a over a distribution π is denoted as by P(x,a,π) := E [P(x,a,a′)] and, similarly, the expected probability of preferring a a′∼π(·|x) distributionπ1overtheotheroneπ2asP(x,π1,π2):=E [P(x,a1,a2)]. a1∼π1(·|x),a2∼π2(·|x) 3 PropertiesoftheOptimalPolicyClass Inthissection,thekeypropertiesoftheoptimalpolicyclassfortheKL-regularizedCBproblemare discussed,whicharevitaltothesubsequentalgorithmdesignsandanalyses. Forboththegeneral preferencemodelandtheBTmodel,theoptimalpolicyclasssatisfiesthefollowingproposition. Proposition1(OptimalPolicyClass) ForanyrewardfunctionR∗ andanypreferencemodelP∗, thecorrespondingoptimalpolicyπ∗ andthecorrespondingNEpolicyπ∗ satisfythat BT GP π∗ ,π∗ ∈Π :={π :π (a|x)∝π (a|x)exp(ηf(x,a)),∀f ∈F}, (1) BT GP F f f 0 whereF :={f :f(x,a)∈[0,1],∀(x,a)∈X×A}denotesallthefunctionsmappingcontext-action pairstovaluesin[0,1]. AdetailedproofanddiscussionofProposition1,includingthetwospecialcases(generalpreference modelandBTmodel),canbefoundinAppendixB.Proposition1indicatesthatthelearningagent onlyneedstoconsiderthepoliciesintheoptimalpolicyclassΠ .Inaddition,thefollowingimportant F lemmacharacterizingtheoptimalpolicyclasscanbeestablished. Lemma1 Foranyπ ∈Π ,itholdsthat f F π (a|x) f ∈[exp(−η),exp(η)], ∀(x,a)∈X ×A. π (a|x) 0 TwomajorimplicationsarisefromLemma1. First,anypolicyintheoptimalpolicyclassshould havethesamesupportasthereferencepolicy,andthusbestochastic. Second,thecandidateoptimal policiesandthereferencepolicyhaveaboundedratiobetweenthem. Thesetwoclaimsdonothold incanonicalsettingsofRLorbandits(i.e.,withoutKL-regularization): theoptimalpolicyclassis typicallyconsideredtoonlycontaindeterministicpolicies,andthus,theratiobetweenanytwoof themcanbeinfinite. 4 --- Page 5 --- 4 GreedySamplingforOnlineLearning 4.1 AlgorithmDesign WeconsiderasetoffunctionsP ⊂(X ×A×A→R)(andsetoffunctionsR⊂(X ×A→R) under the BT model) which provides a set of candidates to approximate P∗ (and R∗ for the BT model). ThefunctionP inP satisfiesP(x,a1,a2)+P(x,a2,a1)=1,andweformallypresentthe greedysamplingalgorithmforonlineRLHFinAlgorithm1. Ineachiteration(Step4),actiona1 t isdrawnfromthelearnedgreedysamplingpolicyπˆ1 anda2 fromthereferencepolicyπ . After t t 0 observingthepreferencelabely ,astandardmaximumlikelihoodestimation(MLE)isperformedon i thecurrentlyaccessibledata(Step6)inthefunctionclassP (andRundertheBTmodel) InStep7,thealgorithmdirectlyupdatesπˆ1 astheoptimalpolicywithrespecttotheempirical t+1 estimatesfromMLE.Thispolicyisreferredtoasgreedysamplingbecauseitusestheempirical estimate“asis”. Thisisamajordeparturefromexistingonlinetheoretically-soundRLHFdesigns,all ofwhichuseoptimismeitherbyaddingabonustermtotheempiricalestimateswhendesigningπˆ1 t (Zhaoetal.,2025a;Jinetal.,2020)orbysamplinga2fromacarefully-constructedenhancerpolicy t πˆ2whichmaximizestheuncertaintyrelativetotheπˆ1(Yeetal.,2024b;Xiongetal.,2023). t t Algorithm1OnlineRLHFwithGreedySampling 1: Input:parameterη,referencepolicyπ ,GP:preferencemodelclassP,BT:rewardfunctionclassR 0 2: Initializeπˆ1 ←π t 0 3: fort=1,...,T do 4: Samplecontextx ∼d andtwoactionsa1 ∼πˆ1(·|x ),a2 ∼π (·|x ) i 0 t t i t 0 i 5: Observepreferencelabely ∈{0,1} i 6: Performthemaximumlikelihoodestimationwith{(x ,a1,a2,y )}t : i i i i i=1 GP: Pˆ ←argmax(cid:88) y logP(x ,a1,a2)+(1−y )logP(x ,a2,a1); t i i i i i i i i P∈P i∈[t] BT: Rˆ ←argmax(cid:88) y logσ(R(x,a1)−R(x,a2))+(1−y )logσ(R(x,a2)−R(x,a1)) t i i i i i i R∈R i∈[t] (cid:40) GP: theNEpolicyassociatedwithPˆ 7: Obtainπˆ1 ← t t+1 BT: theoptimalpolicyassociatedwithRˆ t 8: endfor 4.2 TheoreticalAnalysis ThetheoreticalperformanceguaranteeofgreedysamplingintheonlinesettingforRLHFisestab- lishedinthissection. Specifically,wederivenovelregretboundsforboththegeneralpreference modelandtheBTmodel. Thestandardrealizabilityassumptionisfirstintroduced,whichindicates thatthetruepreferencemodelP∗isintheconsideredfunctionclassP. Assumption1 ItisassumedthatP isfinite,i.e.,N :=|P|<∞,andrealizable,i.e.,P∗ ∈P. P Forclarityofpresentation,weassumethatP hasfinitecardinality;thisassumptioncanberelaxedto aninfinitefunctionclassusingstandardcoveringnumberarguments,asinZhaoetal.(2024). Furthermore,weintroducethefollowingconceptoftheEluderdimension,whichservesasacom- plexitymeasureoftheconsideredfunctionclass. Definition3(EluderDimension,GeneralPreferenceModel) Under the general preference model, for any D = {(x ,a1,a2)}t−1, we define the uncertainty of (x,a1,a2) with respect t−1 i i i i=1 toP as |P (x,a1,a2)−P (x,a1,a2)| U (λ,x,a1,a2;P,D )= sup 1 2 , GP t−1 P1,P2∈P (cid:113) λ+(cid:80) i∈[t−1](P 1(x,a1 i,a2 i)−P 2(x,a1 i,a2 i))2 andthecorrespondingEluderdimensionas d (P,λ,T):= sup (cid:88) min(cid:8) 1,[U (λ,x,a1,a2;P,D )]2(cid:9) . GP GP t t t−1 x1:T,a1 1:T,a2 1:T t∈[T] 5 --- Page 6 --- Especially,theuncertaintyU measureshowdifferentthenewlysampleddata(x,a1,a2)isfrom GP thehistorydataD ,andiswidelyadoptedintheRLliteraturewithgeneralfunctionapproximation t (Zhang,2023;Yeetal.,2024a).TheEluderdimensioncapturesthecumulativeeffectofout-of-sample dataoverT rounds(Zhaoetal.,2025a;RussoandVanRoy,2013;Zhang,2023). Theorem1(Online,GeneralPreferenceModel) UnderAssumption1,foranyδ >0,withproba- bilityatleast1−δ,theregretofAlgorithm1forthegeneralpreferencemodelsatisfies Regret (T)=O(exp(η)d (P,λ,T)log(N T/δ)). GP GP P Tothebestofourknowledge,Theorem1isthefirstanalysisofRLHFunderthegeneralpreference model that can achieve an O(log(T)) regret bound. Ye et al. (2024b) has discussed the sample √ complexityforthegeneralpreferencemodelinonlineRLHF,whoseresultimpliesanO( T)regret boundthatissignificantlyimprovedbytheaboveresult. Moreremarkably,Theorem1isachievedby usingonlygreedysampling,withouttheaidofoptimismneededinYeetal.(2024b). Proofsketch: Toeasethepresentation,theexpectationx∼d isomitted. Toestablishtheregret 0 bound, we first analyze the regret that occurs in each step: J (π1,∗,π2,∗)−min J (πˆ1,π). GP π∈Π GP t Since a direct analysis of this per-step regret is difficult, our first novel idea is to bound it by J (π˜1,π˜2)−J (πˆ1,π˜2)whereπ˜2 isthebestresponseofπˆ1 andπ˜1 isthebestresponseofπ˜2, GP t t GP t t t t t t both under the ground-truth preference model P∗. This conversion is important as it aligns the min-player’spolicytobethesame(i,e.,π˜2)inbothterms. Thesecondmainnoveltyistoleveragea t decompositiondetailedinLemma2thatdelicatelymakesuseofthepowerofKL-regularizationto obtainthefollowingbound,whereπˆ2 :=πˆ1andπf isanauxiliarypolicyintheoptimalpolicyclass: t t t J (π˜1,π˜2)−J (πˆ1,π˜2)≤ηE (cid:2)(cid:0) Pˆ (x,a,πˆ2)−P∗(x,a,π˜2)(cid:1)2(cid:3) GP t t GP t t a∼πf t−1 t t t ≤2ηE (cid:2)(cid:0) P∗(x,a,πˆ2)−Pˆ (x,a,πˆ2)(cid:1)2(cid:3) +2ηE (cid:2)(cid:0) P∗(x,a,π˜2)−P∗(x,a,πˆ2)(cid:1)2(cid:3) . a∼πf t t−1 t a∼πf t t t t Bothtermsontheleft-handsidecanbefurtherboundedasO(E (cid:2)(cid:0) b (x,a1,a2)(cid:1)2(cid:3) ) a1∼π0,a2∼π0 t−1 (ignoringtheηdependency)withaconfidenceintervaldenotedasb (x,a1,a2)viaaconcentration t−1 resultbetweenbyP∗ andPˆ (c.f. Lemma7),wherethepropertystatedinLemma1aboutthe t−1 optimalpolicyclass isalso leveragedto convertall intermediatepolicies to π . Aggregatingthe 0 single-step regrets leads to the regret bound Regret (T) = O(d (P,λ,T)log(N T/δ)). The GP GP P completeproofcanbefoundinAppendixD. ■ Next,theanalysisisextendedtotheBradley-Terrymodel. Similarly,thecorrespondingrealizability assumptionandtheEluderdimensionareintroduced. Assumption2 ItisassumedthatRisfinite,i.e.,N :=|R|<∞,andrealizable,i.e.,R∗ ∈R. R Definition4(EluderDimension,Bradley-TerryModel) UndertheBradley-Terrymodel,forany sequenceD ={(x ,a1,a2)}t−1,wedefinetheuncertaintyof(x,a1,a2)withrespecttoRas: t−1 i i i i=1 U (λ,x,a1,a2;R,D ) BT t−1 |R (x,a1)−R (x,a2)−R (x,a1)+R (x,a2)| = sup 1 1 2 2 , (cid:113) R1,R2∈R λ+(cid:80)t i=1(R 1(x,a1 i)−R 1(x,a2 i)−R 2(x,a1 i)+R 2(x,a2 i))2 andthecorrespondingEluderdimensionas (cid:88) d (R,λ,T):= sup min{1,[U (λ,x,a1,a2;R,D )]2}. BT BT t t t−1 x1:T,a1 1:t,a2 1:T t∈[T] Theorem2(Online,Bradley-TerryModel) UnderAssumption2,foranyδ >0,withprobability atleast1−δ,theregretofAlgorithm1fortheBradley-Terrymodelsatisfies: Regret (T)=O(exp(η)d (R,λ,T)log(N T/δ)). BT BT R 6 --- Page 7 --- Theorem2establishesalogarithmicregretboundforgreedysamplingintheBradley-Terrymodel, whichsharesthesameorderastheregretachievedbyusingoptimisminZhaoetal.(2025a). The proofofTheorem2largelyfollowsthatofTheorem1andcanbefoundinAppendixD.2. Finally,we remarkthatboththeoremsfocusonthescalingbehaviorwithrespecttoT. Inboththegeneralmodel andBTmodel,greedysamplingachieveslogarithmicregretwhilecircumventingtheneedtosolve thecostlyoptimizationproblemthatisrequiredinoptimism/pessimism-baseddesigns,atthecostof incurringanadditionalmultiplicativefactorofexp(η)intheboundsfromTheorems1and2. Amore refinedanalysisofotherconstants(suchasthedependencyonη)andcomparisonwiththeprevious boundscanbefoundinthedetailedproofinAppendixD.2aswellasthediscussioninAppendixA.2. 5 GreedySamplingforOfflineLearning 5.1 AlgorithmDesign Algorithm2OfflineRLHFwithGreedySampling 1: Input: parameterη,referencepolicyπ ,pre-collecteddataD = {(x ,a1,a2,y )}m ,GP: preference 0 0 i i i i i=1 modelclassP,BT:rewardfunctionclassR 2: PerformthemaximumlikelihoodestimationwithD : 0 GP: Pˆ ←argmax(cid:88) y logP(x ,a1,a2)+(1−y )logP(x ,a2,a1); i i i i i i i i P∈P i∈[m] BT: Rˆ ←argmax(cid:88) y logσ(R(x,a1)−R(x,a2))+(1−y )logσ(R(x,a2)−R(x,a1)) i i i i i i R∈R i∈[m] (cid:40) GP: theNEpolicyassociatedwithPˆ 3: Obtainπˆ ← BT: theoptimalpolicyassociatedwithRˆ TheproposeddesignfortheofflinesettingwithboththegeneralpreferencemodelandtheBTmodel isdescribedinAlgorithm2. Here,MLEisperformedwiththepre-collectedofflinedatasetD (Step 0 2)andusestheempiricalestimatestocalculatetheoutputpolicyπˆ(Step3),i.e.,asagreedysampling policy. 5.2 TheoreticalAnalysis AlthoughAlgorithm2reliessolelyongreedysamplingusingtheempiricalestimates,thetheoretical resultsinthissectionguaranteeitsefficiency. Wefirstintroducetheconceptofdatacoverage. Definition5(DataCoverage) Given an offline dataset D, let µ1(a|x),µ2(a|x) be the empirical distributionsofthefirstactiona1andsecondactiona2ofD,respectively. Thecoveragecoefficient ofDtothereferencepolicyπ1,π2isdefinedas π1(a1|x)π2(a2|x) C(D,(π1,π2))= max . a1,a2,xµ1(a1|x)µ2(a2|x) Thecoveragecoefficient(Zhaoetal.,2024;Songetal.,2024)measureshowwelltheofflinedataset Dcoversthepolicypair(π1,π2),asthispairinC(D,(π1,π2))representsthechoiceof(a1,a2)in thedataset. Theorem3(Offline,GeneralPreferenceModel) SupposethatAssumption1hold. Withprobabil- ityatleast1−δandform≥8η(exp(2η)+4η2exp(8η))C(D ,(π ,π ))log(N /δ)/ϵ,theoutput 0 0 0 P policyofAlgorithm2isϵ-optimal. Theorem4(Offline,Bradley-TerryModel) Suppose that Assumption 2 hold. With probability at least 1 − δ and for m ≥ 6ηexp(2η + 1)C(D ,(π ,π ))log(N /δ)/ϵ, the output policy of 0 0 0 R Algorithm2isϵ-optimal. WenotethatbothTheorems3and4guaranteesamplecomplexityO(1/ϵ). Toourbestknowledge, Theorem3isthefirstresultinofflineRLHFwiththegeneralpreferencemodelthatachievessample 7 --- Page 8 --- complexity of O(1/ϵ). For the BT model, Zhao et al. (2025b) also achieves the order of O(1/ϵ) with pessimism, and our result establishes the same order but only using greedy sampling with respecttotheempiricalestimates. Thisisanimportantadvantage,asagreedypolicyneedsfewer computational resources compared to using pessimism in offline RLHF, which requires solving optimizationproblemsateverystepthatareoftencomputationallydemanding(Zhaoetal.,2025b; Yeetal.,2024b;Zhuetal.,2023). Similarlytotheonlinecase,wepresentaproofsketchofTheorem3inthefollowing,andtheproof ofTheorem4followssimilarly. WedeferthecompleteproofstoAppendixC. Proofsketch: SimilartotheproofofTheorem1,wecanboundthesuboptimalgapas J (π1,∗,π2,∗)−J (πˆ1,π˜2)≤J (π˜1,π˜2)−J (πˆ1,π˜2) GP GP GP GP ≤2ηE (cid:2)(cid:0) P∗(x,a,π˜2)−Pˆ(x,a,π˜2)(cid:1)2(cid:3) +2ηE (cid:2)(cid:0) Pˆ(x,a,π˜2)−Pˆ(x,a,πˆ2)(cid:1)2(cid:3) . π f′ π f′ With this decomposition, we only need to bound the difference of two policies πˆ and π˜ . The 2 2 secondnovelideaisthatthesetwopoliciescanbeconnectedbyP∗andtheMLEresultPˆ. Bythis observation,wecanboundtheregretas (cid:16) (cid:17) J (π1,∗,π2,∗)−J (πˆ1,π˜2)=O E [(Pˆ(x,a1,a2)−P∗(x,a1,a2))2] , GP GP x∼d0,a1∼µ1,a2∼µ2 whereµ1andµ2arethecorrespondingofflinedatadistributions. Theremaininganalysisisrelatively straightforward,focusingonboundingthedeviationoftheMLEestimatePˆ fromthetrueparameter P∗. ThisisresolvedinLemma4,whichthencompletestheproof. ■ (a)Stepregret,GP (b)Cumulativeregret,GP Figure1: Thecomparisonbetweengreedysamplingandoptimismunderthegeneralpreferencemodel. (a)Stepregret,BT (b)Cumulativeregret,BT Figure2: ThecomparisonbetweengreedysamplingandoptimismundertheBradley-Terrymodel. 8 --- Page 9 --- 6 Experiment Experimentsareconductedtocorroboratethetheoreticalfindings. Weprimarilyfocusonthemore challengingonlinesetting,whichisalsopopularinpracticalLLMdevelopment(i.e.,usingiterative training as in Xiong et al. (2023); Guo et al. (2024); Xiong et al. (2024)). For both the general preferencemodelandtheBTmodel,weconsiderthelinearsettingwithrandomlysampledcontext vectorsand6fixedactions. Inparticular,thegeneralpreferencemodelisconsideredtobealinearone withdimensionk×k×kwhiletheBTmodelwithdimensionk×k,wherekissetto5. Detailed experimentalsetupsandimplementationdetailsaredeferredtoAppendixE. 6.1 Baselines We particularly compare with previous optimism-based designs in Xiong et al. (2023); Ye et al. (2024b);Zhaoetal.(2025a). Forthegeneralpreferencemodel, theoriginalmethodproposedin Yeetal.(2024b)(i.e.,computinganenhancerpolicyπˆ2 basedonuncertainty)iscomputationally t heavyanddifficulttoimplement. Wethusadoptatournament-styleprocedurethatwasusedinYe etal.(2024b). Inparticular,a1 isstillsampledfromthegreedypolicyπˆt asinAlgorithm1,and 1 severalcandidateresponsesaresampledfromπˆt withthebestofthemadoptedasa2,whichensures 1 acertainlevelofoptimism. Similarrejection-basedsamplingasinDongetal.(2023)isalsoadopted. Tosimulatedifferentdegreesofoptimism,weuse1and3asthenumberofresponsesselectedinthe tournament. In the BT model with linear rewards, as considered in Xiong et al. (2023); Zhao et al. (2025a), closed-formoptimismisadopted. Inparticular, ateachstept, theoptimisticestimateRˆ (x,a)+ t β||ϕ(x,a)−E [ϕ(x,π )]|| isleveragedtoconstructπˆ1forsamplingthefirstanswerwhile x∼d0 0 Σ− t1 t the second answer is obtained from π as in Algorithm 1, where ϕ(x,a) is the linear feature of 0 thecontext-actionpairandΣ istheregularizedGrammatrixfromthecollecteddata. Toconsider t differentlevelsofoptimism,β =0.3and0.5areused. Itisnotedthatwhenβ equalszero,thebonus termsvanish,whichreturnstogreedysampling. Inbothsettings,theregularizationcoefficientηis setto1. 6.2 Results The experimental results are plotted in Figure 1 and Figure 2. We can see that both the greedy sampling and the use of optimism (tournament number equals 1 and 3 and the bonus coefficient β = 0.3and0.5)haveverysimilarconvergencerate(especiallyw.r.t. thescalingwithT)forthe trainingprocessforboththegeneralpreferencemodelandtheBTmodel,whichcorroboratesthe maintheoreticalresultsinSection4, i.e., greedysamplingisprovablyefficient. Takingadeeper look,wecanseethatthecumulativeregretofgreedysamplingisslightlyhigherthanthatofusing optimismasiterationincreases. Thisisalignedwithourtheoreticalresultthatthegreedysampling algorithmhasaslightlyworseconstant(seethediscussioninAppendixA.2). 7 RelatedWorks TheoreticalstudiesofbanditsandRL.Overthepastfewyears,afairlycomprehensivepictureof canonicalbanditsandRLhasemerged(LattimoreandSzepesvári,2020;Agarwaletal.,2019). To obtainsublinearregretsintheonlinesetting,oneofthemostwidelyadoptedapproachesistouse optimisticestimates,i.e.,optimisminthefaceofuncertainty(Aueretal.,2002;Abbasi-Yadkorietal., 2011;Azaretal.,2017;Jinetal.,2018,2020). Ontheotherhand,intheofflinesetting,theprinciple of “pessimism” (i.e., using conservative estimates to construct policies) has become a dominant approach(Rashidinejadetal.,2021;Xieetal.,2021a,b;Jinetal.,2021;YinandWang,2021;Xiong etal.,2022),whichdemonstratesitssuperiorityviaonlyrequiringthesingle-policycoverage(i.e., offlinedatacoveringtheoptimalpolicy). Despitetheirstatisticalefficiency,oneshortcomingisthe requiredconstructionofconfidenceboundsoroptimizationwithinaconfidenceset,whichistypically computationallyinfeasible,exceptinsimplesettingssuchastabularandlinearones. Alongthisline ofinvestigation,thetheoreticalstudiesonpolicygradientmethods(especiallyonproximalpolicy optimization(PPO)(Schulmanetal.,2017))arealsorelevantintermsoftheanalyticaltools,suchas Caietal.(2020);ZhongandZhang(2023);Shermanetal.(2023). However,theseworksstillfocus onlearningthecanonicalRLtarget(i.e.,withoutKLregularization)usingtherewardfeedback. 9 --- Page 10 --- RLHFwiththeBTmodel. TheBradley-Terry(BT)modelisthemostwidelyadoptedpreference modelinRLHFandhasbeenthebasisformanyempiricalsuccesses,eitherusinglogisticlossfor rewardmodeltraining(Ziegleretal.,2019;Stiennonetal.,2020;Ouyangetal.,2022;Baietal., 2022)orconnectingwiththeKL-regularizedobjectivefordirectpolicyoptimization(Rafailovetal., 2023;Xiongetal.,2024;Rafailovetal.,2024). ThemajorityoftheoreticalstudiesonRLHFare alsofocusedontheBTmodel,withsomeearlystudiesinvestigatingthecanonicaltargetofreward- maximization(i.e.,withoutKLregularization)(Pacchianoetal.,2021;Wangetal.,2023;Zhuetal., 2023;Zhanetal.,2023). UndertheKL-regularizedcontextualbandits(CB)formulation(which alignsmorecloselywithpracticalimplementations),Xiongetal.(2023)derivesthefirstprovably efficientalgorithmofRLHF,whichisthenfollowedbyalineofworksXieetal.(2024);Zhong etal.(2024);Cenetal.(2024). Recently,basedonthetechniquesproposedinZhaoetal.(2024),it isrevealedthatduetotheKL-regularization,sharperperformanceguaranteescanbeobtainedfor boththeonline(Zhaoetal.,2025a)andofflinesettings(Zhaoetal.,2025b),asdetailedinSection1. However,theseresultsarestillbasedondesignsthatutilizeoptimisticorpessimisticestimates. RLHFwiththegeneralpreferencemodel.WhiletheBTmodelhasshownstrongempiricalsuccess, recentworkincreasinglyexploresmoreflexibleandadaptablepreferencemodelsthatavoidimplicit rewards. Inparticular,Azaretal.(2024)(IPO)andMunosetal.(2023)(Nash-MD)popularizethe approachofleveragingthegeneralpreferencemodel,i.e.,directlymodelpreferencesamonganswer pairs without introducing rewards. The corresponding theoretical analysis dates back to dueling bandits(YueandJoachims,2009;Dudíketal.,2015),whichisfurtherextendedbyWangetal.(2023) onthenon-KL-regularizedtarget. ThemostrelevantworktoourpaperisYeetal.(2024b),which studiestheKL-regularizedtargetunderboththeonlineandofflinesettingsusingtheprinciplesof optimismandpessimism. ThisworkfurtherimprovestheperformanceboundsinYeetal.(2024b) anddemonstratestheefficiencyofdirectlyusinggreedysampling. Recentworks(Zhangetal.,2024, 2025)alsostudythegeneralpreferencemodelwhilefocusingmoreontheperformanceguaranteefor theplanningprobleminsteadofthelearningone,asinYeetal.(2024b)andthiswork. 8 Conclusion This work investigated reinforcement learning from human feedback (RLHF) under the KL- regularizedcontextualbanditsframework. UnderboththegeneralpreferencemodelandtheBradley- Terry(BT)model,itwasdemonstratedthattheseeminglysimplegreedysamplingwithrespectto empiricalestimatesisprovablyefficient. Inparticular,itachievesregretsofO(log(T))intheonline settingandsamplecomplexityofO(ε−1)withthesingle-policycoverageintheofflinesetting. These resultsarereportedforthefirsttimeunderthegeneralpreferencemodel,andalsomatchprevious performanceboundsundertheBTmodelwhileeliminatingtheneedforconstructingconfidence bounds(thusresultinginmuchlowercomputationaloverhead). Thekeytechnicalinsightisthat KLregularizationconfineseverycandidateoptimalpolicywithinaboundedlikelihood-ratioregime around the reference policy. Simulation results further corroborated the effectiveness of greedy samplingacrossbothpreferencemodels. Acknowledgments TheauthorsthankWeiXiongandChenluYefortheirhelpfulfeedbackontheinitialdraftofthispaper. TheworkofDiWuandCongShenwaspartiallysupportedbytheU.S.NationalScienceFoundation (NSF)undergrants2143559and2313110,andtheUniversityofVirginiaGrandChallengeResearch Investments–DigitalTechnologySmartInfrastructure(StrategicInvestmentFundAward#200). The workofJingYangwaspartiallysupportedbytheU.S.NSFundergrants2531023and2531789. References Abbasi-Yadkori,Y.,Pál,D.,andSzepesvári,C.(2011). Improvedalgorithmsforlinearstochastic bandits. AdvancesinNeuralInformationProcessingSystems,24. Achiam,J.,Adler,S.,Agarwal,S.,Ahmad,L.,Akkaya,I.,Aleman,F.L.,Almeida,D.,Altenschmidt, J.,Altman,S.,Anadkat,S.,etal.(2023). GPT-4technicalreport. arXivpreprintarXiv:2303.08774. 10 --- Page 11 --- Agarwal,A.,Jiang,N.,Kakade,S.M.,andSun,W.(2019). Reinforcementlearning: Theoryand algorithms. CSDept.,UWSeattle,Seattle,WA,USA,Tech.Rep,32:96. Auer,P.,Cesa-Bianchi,N.,andFischer,P.(2002). Finite-timeanalysisofthemultiarmedbandit problem. Machinelearning,47:235–256. Auer,P.,Jaksch,T.,andOrtner,R.(2008). Near-optimalregretboundsforreinforcementlearning. AdvancesinNeuralInformationProcessingSystems,21. Azar,M.G.,Guo,Z.D.,Piot,B.,Munos,R.,Rowland,M.,Valko,M.,andCalandriello,D.(2024). Ageneraltheoreticalparadigmtounderstandlearningfromhumanpreferences. InInternational ConferenceonArtificialIntelligenceandStatistics,pages4447–4455.PMLR. Azar,M.G.,Osband,I.,andMunos,R.(2017). Minimaxregretboundsforreinforcementlearning. InInternationalconferenceonmachinelearning,pages263–272.PMLR. Bai,Y.,Jones,A.,Ndousse,K.,Askell,A.,Chen,A.,DasSarma,N.,Drain,D.,Fort,S.,Ganguli,D., Henighan,T.,etal.(2022). Trainingahelpfulandharmlessassistantwithreinforcementlearning fromhumanfeedback. arXivpreprintarXiv:2204.05862. Bradley,R.A.andTerry,M.E.(1952). Rankanalysisofincompleteblockdesigns: I.themethodof pairedcomparisons. Biometrika,39(3/4):324–345. Cai,Q.,Yang,Z.,Jin,C.,andWang,Z.(2020). Provablyefficientexplorationinpolicyoptimization. InInternationalConferenceonMachineLearning,pages1283–1294.PMLR. Cen,S.,Mei,J.,Goshvadi,K.,Dai,H.,Yang,T.,Yang,S.,Schuurmans,D.,Chi,Y.,andDai,B. (2024). Value-incentivized preference optimization: A unified approach to online and offline RLHF. arXivpreprintarXiv:2405.19320. Dong,H.,Xiong,W.,Goyal,D.,Zhang,Y.,Chow,W.,Pan,R.,Diao,S.,Zhang,J.,Shum,K.,and Zhang,T.(2023). RAFT:Rewardrankedfinetuningforgenerativefoundationmodelalignment. arXivpreprintarXiv:2304.06767. Dudík,M.,Hofmann,K.,Schapire,R.E.,Slivkins,A.,andZoghi,M.(2015). Contextualdueling bandits. InConferenceonLearningTheory,pages563–587.PMLR. Guo,S.,Zhang,B.,Liu,T.,Liu,T.,Khalman,M.,Llinares,F.,Rame,A.,Mesnard,T.,Zhao,Y., Piot,B.,etal.(2024). DirectlanguagemodelalignmentfromonlineAIfeedback. arXivpreprint arXiv:2402.04792. Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M. I. (2018). Is Q-learning provably efficient? AdvancesinNeuralInformationProcessingSystems,31. Jin,C.,Yang,Z.,Wang,Z.,andJordan,M.I.(2020). Provablyefficientreinforcementlearningwith linearfunctionapproximation. InConferenceonlearningtheory,pages2137–2143.PMLR. Jin,Y.,Yang,Z.,andWang,Z.(2021).IspessimismprovablyefficientforofflineRL?InInternational ConferenceonMachineLearning,pages5084–5096.PMLR. Lattimore,T.andSzepesvári,C.(2020). BanditAlgorithms. CambridgeUniversityPress. Li,G.,Shi,L.,Chen,Y.,Chi,Y.,andWei,Y.(2024). Settlingthesamplecomplexityofmodel-based offlinereinforcementlearning. TheAnnalsofStatistics,52(1):233–260. Munos, R., Valko, M., Calandriello, D., Azar, M. G., Rowland, M., Guo, Z. D., Tang, Y., Geist, M.,Mesnard,T.,Michi,A.,etal.(2023). Nashlearningfromhumanfeedback. arXivpreprint arXiv:2312.00886,18. Osband,I.andVanRoy,B.(2016). Onlowerboundsforregretinreinforcementlearning. arXiv preprintarXiv:1608.02732. Ouyang,L.,Wu,J.,Jiang,X.,Almeida,D.,Wainwright,C.,Mishkin,P.,Zhang,C.,Agarwal,S., Slama,K.,Ray,A.,etal.(2022). Traininglanguagemodelstofollowinstructionswithhuman feedback. AdvancesinNeuralInformationProcessingSystems,35:27730–27744. 11 --- Page 12 --- Pacchiano, A., Saha, A., andLee, J.(2021). DuelingRL:reinforcementlearningwithtrajectory preferences. arXivpreprintarXiv:2111.04850. Rafailov,R.,Hejna,J.,Park,R.,andFinn,C.(2024). FromrtoQ*: Yourlanguagemodelissecretly aQ-function. arXivpreprintarXiv:2404.12358. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. (2023). Direct preferenceoptimization: Yourlanguagemodelissecretlyarewardmodel. AdvancesinNeural InformationProcessingSystems,36:53728–53741. Rashidinejad,P.,Zhu,B.,Ma,C.,Jiao,J.,andRussell,S.(2021). Bridgingofflinereinforcement learningandimitationlearning: Ataleofpessimism. AdvancesinNeuralInformationProcessing Systems,34:11702–11716. Russo, D. and Van Roy, B. (2013). Eluder dimension and the sample complexity of optimistic exploration. AdvancesinNeuralInformationProcessingSystems,26. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy optimizationalgorithms. arXivpreprintarXiv:1707.06347. Sherman,U.,Cohen,A.,Koren,T.,andMansour,Y.(2023). Rate-optimalpolicyoptimizationfor linearMarkovdecisionprocesses. arXivpreprintarXiv:2308.14642. Song,Y.,Swamy,G.,Singh,A.,Bagnell,J.A.,andSun,W.(2024). Theimportanceofonlinedata: Understandingpreferencefine-tuningviacoverage. Stiennon,N., Ouyang,L., Wu,J., Ziegler,D., Lowe, R.,Voss,C.,Radford, A.,Amodei, D.,and Christiano, P. F. (2020). Learning to summarize with human feedback. Advances in Neural InformationProcessingSystems,33:3008–3021. Sutton, R. S., Barto, A. G., et al. (2018). Reinforcement learning: An introduction. MIT press Cambridge. Team,G.,Anil,R.,Borgeaud,S.,Alayrac,J.-B.,Yu,J.,Soricut,R.,Schalkwyk,J.,Dai,A.M.,Hauth, A., Millican, K., etal.(2023). Gemini: afamilyofhighlycapablemultimodalmodels. arXiv preprintarXiv:2312.11805. Touvron,H.,Martin,L.,Stone,K.,Albert,P.,Almahairi,A.,Babaei,Y.,Bashlykov,N.,Batra,S., Bhargava,P.,Bhosale,S.,etal.(2023). Llama2: Openfoundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288. Wang, Y., Liu, Q., and Jin, C. (2023). Is RLHF more difficult than standard RL? a theoretical perspective. AdvancesinNeuralInformationProcessingSystems,36:76006–76032. Xie,T.,Cheng,C.-A.,Jiang,N.,Mineiro,P.,andAgarwal,A.(2021a). Bellman-consistentpessimism forofflinereinforcementlearning. AdvancesinNeuralInformationProcessingSystems,34:6683– 6694. Xie, T., Foster, D. J., Krishnamurthy, A., Rosset, C., Awadallah, A., and Rakhlin, A. (2024). Exploratorypreferenceoptimization: HarnessingimplicitQ*-approximationforsample-efficient RLHF. arXivpreprintarXiv:2405.21046. Xie,T.,Jiang,N.,Wang,H.,Xiong,C.,andBai,Y.(2021b). Policyfinetuning: Bridgingsample- efficientofflineandonlinereinforcementlearning. AdvancesinNeuralInformationProcessing Systems,34:27395–27407. Xiong,W.,Dong,H.,Ye,C.,Wang,Z.,Zhong,H.,Ji,H.,Jiang,N.,andZhang,T.(2023). Iterative preference learning from human feedback: Bridging theory and practice for RLHF under KL- constraint. arXivpreprintarXiv:2312.11456. Xiong,W.,Shi,C.,Shen,J.,Rosenberg,A.,Qin,Z.,Calandriello,D.,Khalman,M.,Joshi,R.,Piot, B.,Saleh,M.,etal.(2024). Buildingmathagentswithmulti-turniterativepreferencelearning. arXivpreprintarXiv:2409.02392. 12 --- Page 13 --- Xiong,W.,Zhong,H.,Shi,C.,Shen,C.,Wang,L.,andZhang,T.(2022). Nearlyminimaxoptimal offlinereinforcementlearningwithlinearfunctionapproximation: Single-agentMDPandmarkov game. arXivpreprintarXiv:2205.15512. Ye,C.,Xiong,W.,Gu,Q.,andZhang,T.(2024a). Corruption-robustalgorithmswithuncertainty weightingfornonlinearcontextualbanditsandMarkovdecisionprocesses. Ye, C., Xiong, W., Zhang, Y., Dong, H., Jiang, N., andZhang, T.(2024b). Onlineiterativerein- forcementlearningfromhumanfeedbackwithgeneralpreferencemodel. AdvancesinNeural InformationProcessingSystems,37:81773–81807. Yin, M. and Wang, Y.-X. (2021). Towards instance-optimal offline reinforcement learning with pessimism. AdvancesinNeuralInformationProcessingSystems,34:4065–4078. Yue,Y.andJoachims,T.(2009). Interactivelyoptimizinginformationretrievalsystemsasadueling banditsproblem.InProceedingsofthe26thAnnualInternationalConferenceonMachineLearning, pages1201–1208. Zhan,W.,Uehara,M.,Sun,W.,andLee,J.D.(2023). Provablereward-agnosticpreference-based reinforcementlearning. arXivpreprintarXiv:2305.18505. Zhang,T.(2023). Mathematicalanalysisofmachinelearningalgorithms. CambridgeUniversity Press. Zhang, Y., Yu, D., Ge, T., Song, L., Zeng, Z., Mi, H., Jiang, N., and Yu, D. (2025). Improv- ing LLM general preference alignment via optimistic online mirror descent. arXiv preprint arXiv:2502.16852. Zhang, Y., Yu, D., Peng, B., Song, L., Tian, Y., Huo, M., Jiang, N., Mi, H., and Yu, D. (2024). IterativeNashpolicyoptimization: AligningLLMswithgeneralpreferencesviano-regretlearning. arXivpreprintarXiv:2407.00617. Zhao,H.,Ye,C.,Gu,Q.,andZhang,T.(2024). SharpanalysisforKL-regularizedcontextualbandits andRLHF. arXivpreprintarXiv:2411.04625. Zhao, H., Ye, C., Xiong, W., Gu, Q., and Zhang, T. (2025a). Logarithmic regret for online KL- regularizedreinforcementlearning. arXivpreprintarXiv:2502.07460. Zhao, Q., Ji, K., Zhao, H., Zhang, T., and Gu, Q. (2025b). Nearly optimal sample complexity ofofflineKL-regularizedcontextualbanditsundersingle-policyconcentrability. arXivpreprint arXiv:2502.06051. Zhao, Y., Joshi, R., Liu, T., Khalman, M., Saleh, M., and Liu, P.J. (2023). SLiC-HF:Sequence likelihoodcalibrationwithhumanfeedback. arXivpreprintarXiv:2305.10425. Zhong,H.,Shan,Z.,Feng,G.,Xiong,W.,Cheng,X.,Zhao,L.,He,D.,Bian,J.,andWang,L.(2024). DPOmeetsPPO:ReinforcedtokenoptimizationforRLHF. arXivpreprintarXiv:2404.18922. Zhong,H.andZhang,T.(2023). Atheoreticalanalysisofoptimisticproximalpolicyoptimizationin linearMarkovdecisionprocesses. AdvancesinNeuralInformationProcessingSystems,36:73666– 73690. Zhu,B.,Jordan,M.,andJiao,J.(2023). Principledreinforcementlearningwithhumanfeedback frompairwiseork-wisecomparisons. InInternationalConferenceonMachineLearning,pages 43037–43067.PMLR. Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. (2019). Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593. 13 --- Page 14 --- A Discussions A.1 BroadImpacts Thisworkfocusesonthetheoreticalstudyofreinforcementlearningfromhumanfeedback(RLHF) anddemonstratesthatitisprovablyefficienttousegreedysamplinginboththegeneralpreference modelandtheBradley-Terry(BT)model,underboththeonlineandofflinesettings. Whileacknowl- edgingtheneedforresponsibleusageoftheproposedmethod,wedonotforeseemajornegative societalimpactsduetothetheoreticalnatureofthiswork. A.2 LimitationsandFutureDirections Inthefollowing,somedirectionsworthfurtherinvestigationarelisted. • From the theoretical perspective, it would be interesting to further tighten the obtained performanceboundsofgreedysampling. Inparticular,asillustratedinthelaterproof,the boundscontainmultiplicativeconstantsofexp(η),whichmaybeworthfurtherstudieson itsnecessity. • Ontheempiricalside, asmentionedinthemainpaper, thetheoreticaldesignsbasedon optimismorpessimismareoftendifficulttoimplement,asthebonustermortheoptimization intheconfidencesetcanbecomputationallydemanding. Withthetheoreticalguarantees andsomeempiricalevidenceontheeffectivenessofgreedysamplinginthiswork,more empirical,large-scaleexperimentswouldbehelpfultocomparegreedysamplingwithother methodsfurther. • Thisworkfocusesonthesingle-stepsetting,i.e.,acontextualbanditproblem,asitisthe most widely adopted RLHF scenario. Still, with a growing interest in performing post- traininginmulti-turnscenarios(Xiongetal.,2024),itwouldbeapromisingdirectionto extendthetheoreticalresultsinthisworktomulti-stepRL. A.3 ComparisontoRelatedWorks Tables1and2compareourresultswithpriorworks,emphasizingthedependenceonthehorizonT (forregret)andthesub-optimalitygapε(forsamplecomplexity). Itcanbeobservedthatunderthe generalperformancemodel,thisworksubstantiallyimprovesthepreviousresultsestablishedinYe etal.(2024b). UndertheBTmodel,thisworkmatchesthepreviousperformanceboundsinZhao etal.(2025a,b)whilenotrequiringoptimismorpessimism. Itisnotedthatintheonlinesetting,the conversionfromregretstosamplecomplexitiesisperformedviaLemma14. Table1: ComparisonsintheonlinesettingunderthegeneralpreferencemodelandtheBTmodel. Setting Algorithm Regret SampleComplexity Optimism GP Yeetal.(2024b) − O(cid:101)(cid:0) 1/ϵ2(cid:1) ✓ (cid:0) (cid:1) GreedySampling(Theorem1) O(cid:101)(log(T)) O(cid:101) 1/ϵ × Xiongetal.(2023) − O(cid:101)(cid:0) 1/ϵ2(cid:1) ✓ BT Zhaoetal.(2025a) O(cid:101)(log(T)) O(cid:101)(cid:0) 1/ϵ(cid:1) ✓ (cid:0) (cid:1) GreedySampling(Theorem2) O(cid:101)(log(T)) O(cid:101) 1/ϵ × B OptimalPolicyClassandValueDecomposition TheoptimalpolicyundertheBTmodeliswell-knowntobeaGibbsdistributionwithrespecttothe rewardfunction,asstatedinthefollowingproposition,whoseproofcanbefoundinZhang(2023, Proposition7.16)andRafailovetal.(2023). Proposition2(OptimalPolicy,BTModel) ForanyrewardfunctionR∗,thecorrespondingoptimal policyπ∗ satisfiesthatπ∗ (a|x)∝π (x|a)exp(ηR∗(x,a)). BT BT 0 14 --- Page 15 --- Table2: ComparisonsintheofflinesettingunderthegeneralpreferencemodelandtheBTmodel. Setting Algorithm SampleComplexity Pessimism GP Yeetal.(2024b) O(cid:101)(cid:0) 1/ϵ2(cid:1) ✓ (cid:0) (cid:1) GreedySampling(Theorem3) O(cid:101) 1/ϵ × Xiongetal.(2023) O(cid:101)(cid:0) 1/ϵ2(cid:1) ✓ BT Zhaoetal.(2025b) O(cid:101)(cid:0) 1/ϵ(cid:1) ✓ (cid:0) (cid:1) GreedySampling(Theorem4) O(cid:101) 1/ϵ × Forthegeneralpreferencemodel,thefollowingpropositioncanbeestablished,demonstratingthat theequilibriumpolicystillfollowsasimilarstructure. Proposition3(EquilibriumPolicy,GeneralPreferenceModel) For any preference model P∗, thecorrespondingNEpolicyπ∗ satisfiesthatπ∗ (a|x)∝π (a|x)exp(ηP∗(x,a,π∗ )). GP GP 0 GP Proof: Recallthat (π1,∗,π2,∗)=argmax argminE P∗(x,π1,π2)−η−1KL(π1,π |x)+η−1KL(π2,π |x), x∼d0 0 0 π1∈Π π2∈Π andLemma4inYeetal.(2024b)showsthatπ1,∗isequaltoπ2,∗andwedenotethemasπ∗ . Thus, GP wehave π1,∗ =argmax E P∗(x,π1,π2,∗)−η−1KL(π1,π |x)+η−1KL(π2,∗,π |x) x∼d0 0 0 π1∈Π =argmax E P∗(x,π ,π2,∗)−η−1KL(π ,π |x), x∼d0 1 1 0 π1∈Π and π2,∗ =argmin E P∗(x,π1,∗,π2)−η−1KL(π1,∗,π |x)+η−1KL(π2,π |x) x∼d0 0 0 π2∈Π =argmin E P∗(x,π1,∗,π2)+η−1KL(π2,π |x) x∼d0 0 π2∈Π =argmax E [−P∗(x,π1,∗,π2)−η−1KL(π2,π |x)]. x∼d0 0 π2∈Π TakingR(x,a)=P∗(x,a,π2,∗)and−P∗(x,π1,∗,a)inProposition2leadstothedesiredresult. ■ Proposition1canthenbeestablishedbyaggregatingtheabovetwopropositionstogether. To facilitate further discussions, the following notation is introduced: for the function f(x,a) : X ×A→R,itisdenotedthat (cid:88) Z (x):= π (a|x)exp(ηf(x,a)) f 0 a∈A π (a|x)∝π (a|x)exp(ηf(x,a)) f 0 V(π,f):=E [f(x,a)−η−1KL(π(·|x)||π (·||x))]. x∼d0,a∼π 0 Also,theclosed-formsolutiontoaKL-regularizedobjective maxE [f(x,a)−KL(π,π |x)] π∈Π x∼d0,a∼π 0 isdenotedasπ∗(a|x)∝π (a|x)exp(ηf(x,a)). Furthermore,whenthereisnoambiguity,wewill 0 omitx∼d intheexpectationandusethefollowingsimplifiednotation: 0 E [f(x,a)]:=E [f(x,a)]. π x∼d0,a∼π(·|x) 15 --- Page 16 --- Lemma2(ValueDecomposition) Foranyfunctionf∗(x,a),f(x,a):X ×A→R,wehave V(π ,f∗)−V(π ,f∗)≤η·E (cid:2)(cid:0) f(x,a)−f∗(x,a)(cid:1)2(cid:3) f∗ f π f′ wheref′(·,·)=γf(·,·)+(1−γ)f∗(·,·)forsomeγ ∈[0,1]. Proof: ThefollowingprooflargelyfollowsLemma3.9inZhaoetal.(2024),whichisincludedhere forcompleteness. Foranyfunctionf∗(x,a),f(x,a):X ×A→R,with (cid:88) J(f):=logZ (x)−η π (a|x)·∆(x,a) f f a∈A itcanbeobservedthat V(π ,f∗)−V(π ,f∗) f∗ f (cid:20) (cid:21) (cid:20) (cid:21) 1 π (a|x) 1 π (a|x) =E f∗(x,a)− log f∗ −E f∗(x,a)− log f πf∗ η π (a|x) πf η π (a|x) 0 0 1 (cid:20) π (a|x)·exp(cid:0) ηf∗(x,a)(cid:1)(cid:21) 1 (cid:20) π (a|x)·exp(cid:0) ηf∗(x,a)(cid:1)(cid:21) = E log 0 − E log 0 η πf∗ π (a|x) η πf π (a|x) f∗ f (cid:20) (cid:21) = 1 E (cid:2) logZ (x)(cid:3) − 1 E (cid:2) logZ (x)(cid:3) −E (cid:88) π (a|x)·(cid:0) f∗(x,a)−f(x,a)(cid:1) η x∼d0 f∗ η x∼d0 f x∼d0 f a∈A 1 = E [J(f∗)−J(f)] η x∼d0 wherethefirstequalityfollowsfromthedefinitionoftheKL-divergence. Furthermore, the first derivative of J(f) with respect to ∆(x,a) := f(x,a) − f∗(x,a) can be obtainedas (cid:20) (cid:21) ∂J(f) ∂ (cid:88) = logZ (x)−η π (a′|x)·∆(x,a′) ∂∆(x,a) ∂∆(x,a) f f a′∈A 1 (cid:0) (cid:1) = ·π (a|x)exp η·f(x,a) ·η−η·π (a|x) Z (x) 0 f f (cid:0) (cid:1) (cid:2) (cid:0) (cid:1)(cid:3)2 π (a|x)·exp η·f(x,a) π (a|x)·exp η·f(x,a) 0 0 −η·∆(x,a)· ·η+η·∆(x,a)· ·η Z (x) [Z (x)]2 f f +η (cid:88) π 0(a′|x)·exp(cid:0) η·f(x,a′)(cid:1) ·η·∆(x,a′)· π 0(a|x)·exp(cid:0) η·f(x,a)(cid:1) Z (x) Z (x) f f a′∈A\{a} (cid:88) =−η2π (a|x)∆ (x,a)+η2 π (a′|x)π (a|x)∆(x,a′). f f f f a′∈A Then, via the mean value theorem, there exists an f′(·,·) = γf(·,·)+(1−γ)f∗(·,·) for some γ ∈[0,1]suchthat 1 E [J(f∗)−J(f)] η x∼d0 (cid:20) (cid:21) = 1 E η2 (cid:88) π (a|x)·γ·(cid:0) f(x,a)−f∗(x,a)(cid:1)2 η x∼d0 f′ a∈A (cid:20) (cid:21) − 1 E γη2 (cid:88) (cid:88) π (a |x)π (a |x)(cid:0) f(x,a )−f∗(x,a )(cid:1)(cid:0) f(x,a )−f∗(x,a )(cid:1) η x∼d0 f′ 1 f′ 2 1 1 2 2 a1∈Aa2∈A ≤η·E (cid:2)(cid:0) f(x,a)−f∗(x,a)(cid:1)2(cid:3) π f′ wherethelastinequalityholdssince (cid:88) (cid:88) π (a |x)π (a |x)(cid:0) f(x,a )−f∗(x,a )(cid:1)(cid:0) f(x,a )−f∗(x,a )(cid:1) f′ 1 f′ 2 1 1 2 2 a1∈Aa2∈A =(cid:2)E [f(x,a)−f∗(x,a)](cid:3)2 ≥0. a∼π f′(·|x) Theproofisthenconcluded. ■ 16 --- Page 17 --- C ProofsforOfflineGreedyRLHF Whileweheavilyfocusontheonlinesettinginthemainpaper,wewillstartwiththeproofsforthe offlinesettingbeforethosefortheonlinesettinginordertofacilitatethepresentationoftheanalyses. C.1 GeneralPreferenceModel Wefirstintroducethefollowinglemmastoboundtheerrorofthemaximumlikelihoodestimates. Lemma3 GiventhetrainingdataD ={(x ,a1,a2,y )}n ,withprobability1−δandtheMLE i i i i i=1 estimatorPˆ satisfiesthat n (cid:88) (Pˆ(x ,a1,a2)−P∗(x ,a1,a2))2 ≤logN P. i i i i i i δ i=1 Proof: For any fixed function P ∈ P, we first upper bound its logarithmic moment generating functionas logEexp(cid:18) (cid:88)n log P(y i|x i,a1 i,a2 i) (cid:19) P∗(y |x ,a1,a2) i=1 i i i i (cid:115) =logEexp(cid:18)n (cid:88)−1 log P(y i|x i,a1 i,a2 i) (cid:19) +log2E P(y n|x n,a1 n,a2 n) i=1 P∗(y i|x i,a1 i,a2 i) yn|xn,a1 n,a2 n P∗(y n|x n,a1 n,a2 n) =logEexp(cid:18)n (cid:88)−1 log P(y i|x i,a1 i,a2 i) (cid:19) +log(cid:16) 1−H(cid:0) P(y |x ,a1,a2)∥P∗(y |x ,a1,a2)(cid:1)2(cid:17) P∗(y |x ,a1,a2) n n n n n n n n i=1 i i i i ≤logEexp(cid:18)n (cid:88)−1 log P(y i|x i,a1 i,a2 i) (cid:19) −H(cid:16) P(y |x ,a1,a2)∥P∗(y |x ,a1,a2)(cid:1)(cid:17)2 P∗(y |x ,a1,a2) n n n n n n n n i=1 i i i i n ≤...≤−(cid:88) H(cid:16) P(y |x ,a1,a2)∥P∗(y |x ,a1,a2)(cid:1)(cid:17)2 , (2) i i i i i i i i i=1 whereH(P||Q)istheHellingerdistancedefinedby (cid:90) (cid:16)(cid:112) (cid:112) (cid:17)2 H(P||Q)2 := p(z)− q(z) dµ(z). Ω Wecontinuetolower-boundtheHellingerdistanceby n (cid:88)(cid:16) H(P(y |x ,a1,a2)∥P∗(y |x ,a1,a2)(cid:1)(cid:17)2 i i i i i i i i i=1 n ≥(cid:88)(cid:16) TV(P(y |x ,a1,a2)∥P∗(y |x ,a1,a2)(cid:1)(cid:17)2 i i i i i i i i i=1 n (cid:88) = (P(x ,a1,a2)−P∗(x ,a1,a2))2, (3) i i i i i i i=1 wheretheinequalityusesthefactthatforanydistributionp,q,H(p,q) ≥ TV(p,q)accordingto TheoremB.9ofZhang(2023). Then,byinvokingLemma10,weobtainforanyP ∈P,withprobabilityatleast1−δ, (cid:88)n log P(y i|x i,a1 i,a2 i) ≤log(N /δ)+logEexp(cid:18) (cid:88)n log P(y i|x i,a1 i,a2 i) (cid:19) P∗(y |x ,a1,a2) P P∗(y |x ,a1,a2) i=1 i i i i i=1 i i i i n ≤−(cid:88) H(cid:16) P(y |x ,a1,a2)∥P∗(y |x ,a1,a2)(cid:1)(cid:17)2 +log(N /δ) i i i i i i i i P i=1 n (cid:88) ≤− (P(x ,a1,a2)−P∗(x ,a1,a2))2+log(N /δ), i i i i i i P i=1 17 --- Page 18 --- wherethesecondinequalityusesEqn.(2),andthelastinequalityusesEqn.(3). BytakingP asPˆ, sincePˆ istheMLE,weget (cid:88)n (Pˆ(x ,a1,a2)−P∗(x ,a1,a2))2 ≤(cid:88)n logP∗(y i|x i,a1 i,a2 i) +log(N /δ) i i i i i i Pˆ(y |x ,a1,a2) P i=1 i=1 i i i i ≤log(N /δ), P whichconcludestheproof. ■ Lemma4 Considertwoarbitrarypoliciesπ ,π ,andasetofdata{(x ,a1,a2,y )}n generated 1 2 i i i i i=1 i.i.d. fromthegeneralpreferencemodelP∗andpoliciesπ ,π . SupposethatPˆ istheMLEestimate, 1 2 withprobabilityatleast1−δ,itholdsthat n N E [(Pˆ(x,a1,a2)−P∗(x,a1,a2))2]≤2log P. 2 x∼d0,a1∼π1,a2∼π2 δ Proof: BythemultiplicativeChernoffbounds(refertoLemma11andRemark2),withprobabilityat least1−δ,foranyP ∈P,wehave n E [(P(x,a1,a2)−P∗(x,a1,a2))2] 2 x∼d0,a1∼π1,a2∼π2 n ≤(cid:88) (P(x ,a1,a2)−P∗(x ,a1,a2))2+log(N P). i i i i i i δ i=1 TakingP =Pˆ andusingLemma3,wecanget n E [(Pˆ(x,a1,a2)−P∗(x,a1,a2))2] 2 x∼d0,a1∼π1,a2∼π2 n ≤(cid:88) (Pˆ(x ,a1,a2)−P∗(x ,a1,a2))2+log(N P) i i i i i i δ i=1 N ≤2log( P), δ whichconcludestheproof. ■ WearenowreadytoproveTheorem3. Proof: [ProofofTheorem3]Thefollowingnotationsarefirstintroduced: (π1,∗,π2,∗)=argmax argminE P∗(x,π1,π2)−η−1KL(π1,π |x)+η−1KL(π2,π |x), x∼d0 0 0 π1∈Π π2∈Π (πˆ1,πˆ2)=argmax argminE Pˆ(x,π1,π2)−η−1KL(π1,π |x)+η−1KL(π2,π |x), x∼d0 0 0 π1∈Π π2∈Π π˜2 =argminE P∗(x,πˆ1,π)+η−1KL(π,π |x)=argminJ (πˆ1,π), x∼d0 0 GP π∈Π π∈Π π˜1 =argmaxE P∗(x,π,π˜2)−η−1KL(π,π |x)=argmaxJ (π,π˜2), x∼d0 0 GP π∈Π π∈Π anditcanbenoticedthatthesuboptimalgapcanbede-compositedas J (π1,∗,π2,∗)−J (πˆ1,π˜2) GP GP =[J (π1,∗,π2,∗)−J (π1,∗,π˜2)]+[J (π1,∗,π˜2)−J (π˜1,π˜2)]+[J (π˜1,π˜2)−J (πˆ1,π˜2)] GP GP GP GP GP GP ≤J (π˜1,π˜2)−J (πˆ1,π˜2), GP GP wheretheinequalityholdsasthefirsttwotermsarenegativeduetotheabovedefinitions. Recallthatwehave π1,∗(a|x)∝π (a|x)exp(ηP∗(x,a,π2,∗)), π2,∗(a|x)∝π (a|x)exp(−ηP∗(x,π1,∗,a)). 0 0 18 --- Page 19 --- WithLemma2,wecanget J (π˜1,π˜2)−J (πˆ1,π˜2) GP GP =E P∗(x,π˜1,π˜2)−η−1KL(π˜1,π |x)+η−1KL(π˜2,π |x) x∼d0 0 0 −(E P∗(x,πˆ1,π˜2)−η−1KL(πˆ1,π |x)+η−1KL(π˜2,π |x)) x∼d0 0 0 =E P∗(x,a,π˜2)−η−1KL(π˜1,π |x) x∼d0,a∼π˜1 0 −(E P∗(x,a,π˜2)−η−1KL(πˆ1,π |x)) x∼d0,a∼πˆ1 0 ≤ηE [(P∗(x,a,π˜2)−Pˆ(x,a,πˆ2))2] x∼d0,a∼π f′ ≤2ηE (cid:2)(cid:0) P∗(x,a,π˜2)−Pˆ(x,a,π˜2)(cid:1)2(cid:3) +2ηE (cid:2)(cid:0) Pˆ(x,a,π˜2)−Pˆ(x,a,πˆ2)(cid:1)2(cid:3) . π f′ π f′ Then,itcanbeboundedthat E (cid:2)(cid:0) P∗(x,a,π˜2)−Pˆ(x,a,π˜2)(cid:1)2(cid:3) π f′ =E (cid:2)(cid:0)E [P∗(x,a1,a2)−Pˆ(x,a1,a2)](cid:1)2(cid:3) x∼d0,a1∼π f′ a2∼π˜2 ≤E (cid:2)(cid:0) P∗(x,a1,a2)−Pˆ(x,a1,a2)(cid:1)2(cid:3) x∼d0,a1∼π f′,a2∼π˜2 ≤exp(2η)E (cid:2)(cid:0) P∗(x,a1,a2)−Pˆ(x,a1,a2)(cid:1)2(cid:3) x∼d0,a1∼π0,a2∼π0 ≤exp(2η)C(D ,(π ,π ))E (cid:2)(cid:0) P∗(x,a1,a2)−Pˆ(x,a1,a2)(cid:1)2(cid:3) 0 0 0 x∼d0,a1∼µ1,a2∼µ2 4exp(2η)C(D ,(π ,π )) ≤ 0 0 0 log(N /δ) n P where the first inequality is putting the square into the expectation, the second inequality is by Lemma1,thethirdinequalityisbytheassumptionofdatacoverage,andthefinalinequalityisby Lemma4. Fortheotherterm2ηE (cid:2)(cid:0) Pˆ(x,a,π˜2)−Pˆ(x,a,πˆ2)(cid:1)2(cid:3) ,weboundthatbyseveralsteps. First,itis π f′ noticedthat πˆ2(a|x)∝π (a|x)exp(−ηPˆ(x,πˆ1,a)), π˜2(a|x)∝π (a|x)exp(−ηP∗(x,πˆ1,a)), 0 0 andwecorrespondinglydefine Z′(x)=(cid:88) π (a|x)exp(−ηPˆ(x,πˆ1,a)), Z′′(x)=(cid:88) π (a|x)exp(−ηP∗(x,πˆ1,a)). 0 0 a a Itcanbeobservedthat |Z′(x)−Z′′(x)|=|(cid:88) π (a|x)[exp(−ηPˆ(x,πˆ1,a))−exp(−ηP∗(x,πˆ1,a))]| 0 a ≤η|(cid:88) π (a|x)(Pˆ(x,πˆ1,a)−P∗(x,πˆ1,a))| 0 a =η|E [Pˆ(x,a1,a2)−P∗(x,a1,a2)]| (4) a1∼πˆ1,a2∼π0 wheretheinequalityisfromthemeanvaluetheoremandthefactthattheboundofthederivativeof exp(−ηx)withrespecttoxisboundedin[−η,−ηexp(−η)]forx∈[0,1]. Withthefollowingrelationship (cid:88) (cid:88) 1= π (a|x)≥Z′(x),Z′′(x)≥ π (a|x)·exp(−η)=exp(−η), (5) 0 0 a a 19 --- Page 20 --- itcanbeestablishedthat |Pˆ(x,a′,π˜2)−Pˆ(x,a′,πˆ2)| =|(cid:88) (π˜2(a|x)−πˆ2(a|x))Pˆ(x,a′,a)| a (cid:88) ≤ |π˜2(a|x)−πˆ2(a|x)| a =(cid:88) |π 0(a|x)exp(−ηP∗(x,πˆ1,a)) − π 0(a|x)exp(−ηPˆ(x,πˆ1,a)) | Z′′(x) Z′(x) a ≤exp(η)(cid:88) |π (a|x)exp(−ηP∗(x,πˆ1,a))− Z′′(x)π 0(a|x)exp(−ηPˆ(x,πˆ1,a)) | 0 Z′(x) a ≤exp(η)(cid:88) |π (a|x)(exp(−ηP∗(x,πˆ1,a))−exp(−ηPˆ(x,πˆ1,a))| 0 a +exp(η)(cid:88) |Z′(x)−Z′′(x) π (a|x)exp(−ηPˆ(x,πˆ1,a))| Z′(x) 0 a ≤ηexp(η)(cid:88) π (a|x)|(P∗(x,πˆ1,a)−Pˆ(x,πˆ1,a))| 0 a +exp(η)(cid:88) π (a|x)η|E a1∼πˆ1,a2∼π0[Pˆ(x,a1,a2)−P∗(x,a1,a2)]| 0 exp(−η) a ≤ηexp(η)E |E [Pˆ(x,a′,a)−P∗(x,a′,a)]| a∼π0 a′∼πˆ1 +ηexp(2η)|E [Pˆ(x,a1,a2)−P∗(x,a1,a2)]| a1∼πˆ1,a2∼π0 ≤ηexp(3η)E |E [Pˆ(x,a′,a)−P∗(x,a′,a)]| a∼π0 a′∼π0 +ηexp(4η)|E [Pˆ(x,a1,a2)−P∗(x,a1,a2)]| a1∼π0,a2∼π0 ≤2ηexp(4η)E E [|Pˆ(x,a1,a2)−P∗(x,a1,a2)|], a1∼π0 a2∼π0 wherethesecondinequalityisfromEquation(5),thefourthinequalityisfromEquations(5)and(4), andthelasttwoinequalitiesarebyLemma1. Thus,wehave (Pˆ(x,a′,π˜2)−Pˆ(x,a,πˆ2))2 ≤4η2exp(8η)E E [(Pˆ(x,a1,a2)−P∗(x,a1,a2))2] (6) a1∼π0 a2∼π0 ≤4η2exp(8η)C(D ,(π ,π ))E E [(Pˆ(x,a1,a2)−P∗(x,a1,a2))2]. 0 0 0 a1∼µ1 a2∼µ2 Combiningtheaboveresults,weget J (π1,∗,π2,∗)−J (πˆ1,π˜2) GP GP ≤2ηC(D ,(π ,π ))(exp(2η)+4η2exp(8η))E [(Pˆ(x,a1,a2)−P∗(x,a1,a2))2] 0 0 0 x∼d0,a1,a2∼µ1,µ2 8ηC(D ,(π ,π ))(exp(2η)+4η2exp(8η)) (cid:18) N (cid:19) ≤ 0 0 0 log P , m δ andwhen 8ηC(D ,(π ,π ))(exp(2η)+4η2exp(8η)) (cid:18) N (cid:19) m≥ 0 0 0 log P , ϵ δ thesuboptimalgapislessthanϵ. ■ C.2 TheBradley-TerryModel Similarly,undertheBTmodel,westartwithboundingtheestimationerroroftheMLE. 20 --- Page 21 --- Lemma5 GiventhetrainingdataD ={(x ,a1,a2,y )}n ,withprobability1−δandtheMLE i i i i i=1 estimatorRˆsatisfiesthefollowingestimation: (cid:88)n (cid:104) Rˆ(x ,a1)−Rˆ(x ,a2)−(cid:0) R∗(x ,a1)−R∗(x ,a2)(cid:1)(cid:105)2 ≤2elog(cid:18) N R(cid:19) . i i i i i i i i δ i=1 Proof: SubstitutingP(x ,a1,a2)withσ(R(x ,a1)−R(x ,a2))inLemma3,weimmediatelyget i i i i i i i (cid:88)n (cid:104) σ(cid:16) Rˆ(x ,a1)−Rˆ(x ,a2)(cid:17) −σ(cid:0) R∗(x ,a1)−R∗(x ,a2)(cid:1)(cid:105)2 ≤log(cid:18) N R(cid:19) . i i i i i i i i δ i=1 Withthefactthatσ′(r)=σ(r)(1−σ(r))≥ 1 (astherewardisassumedtobeboundedin[0,1]),it 2e canbeestablishedthat n (cid:88)(cid:104) Rˆ(x ,a1)−Rˆ(x ,a2)−(cid:0) R∗(x ,a1)−R∗(x ,a2)(cid:1)(cid:105)2 i i i i i i i i i=1 n ≤2e(cid:88)(cid:104) σ(Rˆ(x ,a1)−Rˆ(x ,a2))−σ(cid:0) R∗(x ,a1)−R∗(x ,a2)(cid:1)(cid:105)2 i i i i i i i i i=1 (cid:18) (cid:19) N ≤2elog R , δ whichconcludestheproof. ■ Lemma6 Considerarbitrarypoliciesπ1,π2,andasetofcontext-actionpairs{(x ,a1,a2,y )}n i i i i i=1 generatedi.i.d. fromtheBTmodelwherea1 ∼π1,a2 ∼π2. SupposethatRˆistheMLEestimator. i i Wehavewithprobabilityatleast1−δ, (cid:18) (cid:19) E (cid:2)(cid:0) Rˆ(x,a1)−Rˆ(x,a2)−(R∗(x,a1)−R∗(x,a2))(cid:1)2(cid:3) ≤ 6e log N R . x∼d0,a1∼π1,a2∼π2 n δ Proof: SimilartotheproofofLemma4,withprobabilityatleast1−δ,foranyR∈R,wehave n E (cid:2)(cid:0) R(x,a1)−R(x,a2)−(R∗(x,a1)−R∗(x,a2))(cid:1)2(cid:3) 2 x∼d0,a1∼π1,a2∼π2 n (cid:18) (cid:19) ≤(cid:88)(cid:0) R(x ,a1)−R(x ,a2)−(R∗(x ,a1)−R∗(x ,a2))(cid:1)2 +log N R . i i i i i i i i δ i=1 BytakingR=RˆandusingLemma5,wecanget n E (cid:2)(cid:0) Rˆ(x,a )−Rˆ(x,a )−(R∗(x,a )−R∗(x,a ))(cid:1)2(cid:3) 2 x∼d0,a1∼π1,a2∼π2 1 2 1 2 n (cid:18) (cid:19) ≤(cid:88)(cid:0) Rˆ(x ,a1)−Rˆ(x ,a2)−(R∗(x ,a1)−R∗(x ,a2))(cid:1)2 +log N R i i i i i i i i δ i=1 (cid:18) (cid:19) (cid:18) (cid:19) N N ≤(2e+1)log R ≤3elog R , δ δ whichprovesthelemma. ■ Proof: [ProofofTheorem4]First,theoutputpolicyπˆ satisfies πˆ =argmaxE [Rˆ(x,a)−KL(π,π |x)] π 0 π∈Π =argmaxE [Rˆ(x,a)−b(x)−KL(π,π |x)], π 0 π∈Π 21 --- Page 22 --- foranyfunctionb(x),whichmeansthepoliciesinducedbyRˆ(x,a)andRˆ(x,a)−b(x)arethesame. Thus,with b(x):=E [Rˆ(x,a′)−R∗(x,a′)] a′∼π0(·|x) andLemma2,wehave J (π∗)−J (πˆ) BT BT ≤ηE [(Rˆ(x,a)−b(x)−R∗(x,a))2] π f′ =ηE [(Rˆ(x,a1)−R∗(x,a1)−E (Rˆ(x,a2)−R∗(x,a2)))2] x∼d0,a1∼π f′ a2∼π0 =ηE [(E [Rˆ(x,a1)−R∗(x,a1)−(Rˆ(x,a2)−R∗(x,a2))])2] x∼d0,a1∼π f′ a2∼π0 =ηE [(Rˆ(x,a1)−R∗(x,a1)−(Rˆ(x,a2)−R∗(x,a2)))2] x∼d0,a1∼π f′,a2∼π0 ≤ηexp(2η)E [(Rˆ(x,a1)−R∗(x,a1)−(Rˆ(x,a2)−R∗(x,a2)))2] x∼d0,a1∼π0,a2∼π0 ≤ηexp(2η)C(D ,(π ,π ))E [(Rˆ(x,a1)−R∗(x,a1)−(Rˆ(x,a2)−R∗(x,a2)))2] 0 0 0 µ1,µ2 (cid:18) (cid:19) N 1 ≤6ηexp(2η+1)C(D ,(π ,π ))log R , (7) 0 0 0 δ m wherethelastthreeinequalitiesuseLemma1,Lemma6andDefinition5. Taking (cid:18) (cid:19) N m≥6ηexp(2η+1)C(D ,(π ,π ))log R /ϵ, 0 0 0 δ wehaveJ (π∗)−J (πˆ)≤ϵ. ■ BT BT D ProofsforOnlineRLHFwithGreedySampling D.1 GeneralPreferenceModel DefinebGP(x,a1,a2)=min{1,β ·U (λ,x,a1,a2;P ,D )}and t T;GP GP t t (cid:40) t (cid:41) P = P ∈P :(cid:88) (P(x,a1,a2)−Pˆ(x,a1,a2))2+λ≤β2 , t i i t i i T;GP i=1 whereβ2 =2log(N T/δ)andλ≤β2 /2. T;GP P T;GP Lemma7 UnderAlgorithm1,wehavewithprobabilityatleast1−δforallt∈[T],theuniform optimismeventthatE = {Pˆ(x,a1,a2)+bGP(x,a1,a2)−P∗(x,a1,a2) > 0,∀(x,a1,a2) ∈ X × t t t A×A}holdstrue. Proof: ByLemma3,wehavethat,forallt∈[T]withprobabilityatleast1−δ n (cid:18) (cid:19) (cid:88) (Pˆ(x ,a1,a2)−P∗(x ,a1,a2))2 ≤log N PT = 1 β2 . (8) i i i i i i δ 2 T;GP i=1 Hence,wededucethatforany(x,a1,a2)∈X ×A×A, |P (x,a1,a2)−P (x,a1,a2)| |Pˆ(x,a1,a2)−P∗(x,a1,a2)|≤ sup 1 2 t (cid:113) P1,P2∈Pt λ+(cid:80)t i=1(P 1(x,a1 i,a2 i)−P 2(x,a1 i,a2 i))2 (cid:118) (cid:117) t ·(cid:117) (cid:116)λ+(cid:88) (Pˆ(x,a1,a2)−P∗(x,a1,a2))2 t i i i i i=1 (cid:114) 1 ≤U (λ,x,a1,a2;P ,D ) λ+ β2 GP t t 2 T;GP ≤U (λ,x,a1,a2;P ,D )β , GP t t T;GP 22 --- Page 23 --- whichconcludestheproof. ■ Proof: [ProofofTheorem1]Similartotheproofintheofflinesetting,wedefine (πˆ1,πˆ2)=argmax argminE [Pˆ (x,π1,π2)−η−1KL(π1,π |x)+η−1KL(π2,π |x)], t t x∼d0 t−1 0 0 π1∈Π π2∈Π π˜2 =argminE P∗(x,πˆ1,π)+η−1KL(π,π |x), t x∼d0 t 0 π∈Π π˜1 =argmaxE P∗(x,π,π˜2)−η−1KL(π,π |x). t x∼d0 t 0 π∈Π Conditioningontheevent∪ E inLemma7,wecanboundthedesiredregretasfollows. For t∈[T] t eachstep,asshownintheproofoftheofflinesetting,wehave J (π1,∗,π2,∗)−J (πˆ1,π˜2)≤ηE (cid:2)(cid:0) Pˆ (x,a,πˆ2)−P∗(x,a,π˜2)(cid:1)2(cid:3) . (9) GP GP t t πf t−1 t t t ByLemma7,wecanboundEquation(9)as: E (cid:2)(cid:0) P∗(x,·,π˜2)−Pˆ (x,·,πˆ2)(cid:1)2(cid:3) πf t t−1 t t ≤E (cid:2)(cid:0) P∗(x,·,π˜2)−P∗(x,·,πˆ2)+P∗(x,·,πˆ2)−Pˆ (x,·,πˆ2)(cid:1)2(cid:3) πf t t t t−1 t t ≤2E (cid:2)(cid:0) P∗(x,·,π˜2)−P∗(x,·,πˆ2)(cid:1)2 +(cid:0) P∗(x,·,πˆ2)−Pˆ (x,·,πˆ2)(cid:1)2(cid:3) πf t t t t−1 t t =2E (cid:2)(cid:0) P∗(x,·,π˜2)−P∗(x,·,πˆ2)(cid:1)2(cid:3) +2E (cid:2)(cid:0) P∗(x,·,πˆ2)−Pˆ (x,·,πˆ2)(cid:1)2(cid:3) πf t t πf t t−1 t t t ≤2E (cid:2)(cid:0) P∗(x,·,π˜2)−P∗(x,·,πˆ2)(cid:1)2(cid:3) +2E (cid:2)(cid:0) bGP (x,·,πˆ2)(cid:1)2(cid:3) πf t t πf t−1 t t t ≤8η2exp(8η)E E [(Pˆ(x,a1,a2)−P∗(x,a1,a2))2]+2E (cid:2)(cid:0) bGP (x,·,πˆ2)(cid:1)2(cid:3) a1∼π0 a2∼π0 π tf t−1 t ≤(8η2exp(8η)+2exp(2η))E E (cid:2)(cid:0) bGP (x,a1,a2)(cid:1)2(cid:3) ) a1∼π0 a2∼π0 t−1 ≤(8η2exp(9η)+2exp(3η))E E (cid:2)(cid:0) bGP (x,a1,a2)(cid:1)2(cid:3) ), a1∼π t1 a2∼π0 t−1 wheretheboundofE (cid:2)(cid:0) P∗(x,·,π˜2)−P∗(x,·,πˆ2)(cid:1)2(cid:3) followsthetechnicalanalysisintheoffline πf t t t setting(refertoEquation(6))andthelasttwoinequalitiesfollowLemma1. Thus,wehave (cid:88) Regret (T)= J (π1,∗,π2,∗)−J (πˆ1,π˜2) GP GP GP t t t∈[T] (cid:88) ≤(8η3exp(9η)+2ηexp(3η)) E [bGP (x,a1,a2)2] x∼d0,a1∼π t1,a2∼π0 t−1 t∈[T] (cid:88) ≤(8η3exp(9η)+2ηexp(3η)) E [min{1,U (λ,x,a1,a2;P ,D )}2]β2 x∼d0,a1∼π t1,a2∼π0 GP t t T;GP t∈[T] ≤4ηexp(3η)(4η2exp(6η)+1)log(N T/δ)d (P,λ,T). P GP Therefore,wehavethefinalresult Regret (T)=O(log(N T/δ)d (P,λ,T)), GP P GP where T (cid:88) d (P,λ,T):= sup min{1,[U (λ,x ,a1,a2;P,D )]2}. GP GP t t t t−1 x1:T,a1 1:T,a2 1:T t=1 ■ D.2 TheBradley-TerryModel Similar to the proof for the general preference model, we define bBT(x,a1,a2) = min{1,β · t T;BT U (λ,x,a1,a2;R ,D )}and BT t t t R ={R∈R:(cid:88) (Rˆ (x ,a1)−Rˆ (x ,a2)−(R∗(x ,a1)−R∗(x ,a2)))2+λ≤β2 }, t t i i t i i i i i i T;BT i=1 whereβ2 =4elog(N T/δ)andλ≤β2 /2. T;BT R T;BT 23 --- Page 24 --- Lemma8 UnderAlgorithm1,wehavewithprobabilityatleast1−δforallt∈[T],theuniform optimismeventthat E ={Rˆ (x,a1)−Rˆ (x,a2)+bBT(x,a1,a2)−(R∗(x,a1)−R∗(x,a2))>0, t t t t ∀(x,a1,a2)∈X ×A×A} holdstrue. Proof: ByLemma5,forallt∈[T],withprobabilityatleast1−δ, (cid:88)t (cid:104) Rˆ(x ,a1)−Rˆ(x ,a2)−(cid:0) R∗(x ,a1)−R∗(x ,a2)(cid:1)(cid:105)2 ≤2elog(cid:18) N RT(cid:19) = 1 β2 . i i i i i i i i δ 2 T;BT i=1 Hence,wededucethatforany(x,a1,a2)∈X ×A×A, |Rˆ (x,a1)−Rˆ (x,a2)−(R∗(x,a1)−R∗(x,a2))| t t |R (x,a1)−R (x,a2)−R (x,a1)+R (x,a2)| ≤ sup 1 1 2 2 (cid:113) R1,R2∈R λ+(cid:80)t i=1(R 1(x i,a1 i)−R 1(x i,a2 i)−R 2(x i,a1 i)+R 2(x i,a2 i))2 (cid:118) (cid:117) t ·(cid:117) (cid:116)λ+(cid:88) (Rˆ (x ,a1)−Rˆ (x ,a2)−(R∗(x ,a1)−R∗(x ,a2)))2 t i i t i i i i i i i=1 (cid:114) 1 ≤U (λ,x,a1,a2;R ,D ) λ+ β2 BT t t 2 T;BT ≤U (λ,x,a1,a2;R ,D )β , BT t t T;BT whichconcludestheproof. ■ Proof: [Proof of Theorem 2] Conditioning on the event ∪ E in Lemma 8, we can bound the t∈[T] desireregretasfollows. Forsingle-stepregret,fromtheanalysisfortheofflinesetting(Equation(7)), wehave J (π∗)−J (πˆ ) BT BT t ≤ηE [(Rˆ (x,a1)−R∗(x,a1)−(Rˆ (x,a2)−R∗(x,a2)))2] x∼d0,a1∼π f′,a2∼π0 t−1 t−1 ≤ηE [bBT (x,a1,a2)2] x∼d0,a1∼π f′,a2∼π0 t−1 ≤ηexp(2η)E [bBT (x,a1,a2)2], x∼d0,a1∼π t1,a2∼π0 t−1 wherethelastinequalityisbyLemma1. Thus (cid:88) Regret (T)= J (π∗)−J (πˆ ) BT BT BT t t∈[T] (cid:88) ≤ηexp(2η) E [bBT (x,a1,a2)2] x∼d0,a1∼π t1,a2∼π0 t−1 t∈[T] (cid:88) ≤ηexp(2η) E [min{1,U (λ,x,a1,a2;R ,D )}2]β2 x∼d0,a1∼π t1,a2∼π0 BT t t T;BT t∈[T] T (cid:88) ≤4ηexp(2η+1)log(N T/δ) sup min{1,[U (λ,x,a1,a2;R,D )]2} R BT t−1 x1:T,a1 1:T,a2 1:T t=1 ≤4ηexp(2η+1)log(N T/δ)d (R,λ,T), R BT whichconcludestheproof. ■ Remark1 Theabovetheoreticalresultsindicatethatthegreedyalgorithmssufferfromsomeextra constantofη(e.g.exp(η))comparedtousingoptimism.Atthesametime,aswehavediscussedinthe mainpaper,thebonustermsinoptimismareusuallycomputationallyintensive,whereasthegreedy policyenjoyslesscomputationalcomplexity. Additionally, itisunclearwhethertheseadditional dependenciesonηarefundamental–wewillexplorehowtotightentheseboundsinfuturework. 24 --- Page 25 --- E ExperimentDetails E.1 ImplementationConsiderationsoftheGeneralPreferenceModel Nashequilibriumoracleapproximationviaaniterationmethod. GivenafunctionP ∈P,the Nashequilibriumis (π1,∗,π2,∗)=(π ,π )=argmax min J (π1,π2). (10) P P P P GP π1∈Π π2∈Π However,Equation(10)doesnothaveaclosedform,whichposesachallengeintheexperiment. We notethatitcanbetransformedtoafixed-pointproblembyProposition3: π (a|x)exp(η(cid:80) π(a′|x)P(x,a,a′)) π =f(π ), f(π)= 0 a′ . P P (cid:80) π (a|x)exp(η(cid:80) π(a′|x)P(x,a,a′)) a 0 a′ Inthisspirit,weusetheiterationmethodtosolvetheabovefixed-pointproblem,andtheiterationhas beenshowntoconvergetothefixed-point(i.e.,theNashpoint)inourexperiment. Achieve optimism by enhancer and rejection sampling. One way to achieve optimism is by choosingπ2 astheenhancerthatmaximizestheuncertaintytoπ1 (Yeetal.,2024b;Xiongetal., 2023). Our experiment also adopts this method to simulate the online RLHF with optimism. In thegeneralcase,π2 doesnotadmitaclosedform. Thus,weuseatournament-styleprocedureto getthebestresponse(andrejectallotherresponses),andtakethebestresponsesatπ2 (Yeetal., 2024b;Dongetal.,2023). Thenumberoftournamentcompetitorscanbeusedtocontrolthelevelof optimism. E.2 ExperimentSetup InboththegeneralpreferencemodelandtheBTmodel,welimitthescopetothelinearcaseand assumetheactionandcontextarevectorsinRk. Inthegeneralpreferencemodel,weparameterize thepreferenceoracleP∗ ∈P byatensorM (withsizek×k×k),andintheBradley-Terrymodel, weparameterizetherewardfunctionR∗ ∈RbyamatrixW (withsizek×k). Wegivetheexact formbelow. Generalpreferencemodel. WeparameterizethepreferenceoracleP ∈P byatensorM (with sizek×k×k)as (a1)T(xM)a2 P :X ×A×A→[0,1], (x,a1,a2)(cid:55)→ ∈[0,1]. (a1)T(xM)a2+(a2)T(xM)a1 Bradley-Terrymodel. WeparameterizetherewardfunctionR ∈ RbyamatrixW (withsize k×k)as R:X ×A→[0,1], (x,a)(cid:55)→xTWa. Forimplementation,wechoosek =5andfirstuniformlyrandomlysamplefrom[0,1]toconstructthe ground-truthpreferencemodelparametersM∗andW∗. Wesimilarlysample6vectorsfrom[0,1]5 astheactionsetA. Ineachiteration,werandomlysampleavectorfromtheuniformdistribution in[0,1]5 asthecontextvector,andthenwesampleactionpairs(a1,a2)basedonthepolicies. We runthetrajectoryforT iterationsandrepeattheexperiments5times,computingtheaveragesand standarddeviations. E.3 AdditionalExperiments Tostudytheinfluenceoftheregularizationcoefficientη,wefurtherconductexperimentsevaluating theperformanceofthealgorithmunderdifferentηvalues. Theexperimentsettingremainsthesame as Section E.2. We choose η = 1,2,3 in the greedy sampling algorithm under both the general preferencemodelandtheBTmodel. TheresultsarepresentedinFigure3. Wecanseethatunderall ηvalues,thegreedysamplingconvergesasourtheoremhassuggested. 25 --- Page 26 --- (a)Stepregret,GP (b)Cumulativeregret,GP Figure3: Thecomparisonofdifferentregularizationcoefficientsηunderthegeneralpreferencemodel. (a)Stepregret,BT (b)Cumulativeregret,BT Figure4: ComparisonofdifferentregularizationcoefficientsηundertheBradley-Terrymodel. F AuxiliaryLemmas Lemma9(Freedman’sInequality) LetM,v >0befixedconstants. Let{X }n beastochastic i i=1 process,{G } beasequenceofσ-fields,andX beG -measurable,whilealmostsurely i i i i n (cid:88) E[X |G ]=0,|X |≤M, and E[X2|G ]≤v. i i i i i−1 i=1 Thenforanyδ >0,withprobabilityatleast1−δ,itholdsthat n (cid:88) (cid:112) 2 X ≤ 2vlog(1/δ)+ Mlog(1/δ). i 3 i=1 Lemma10(MartingaleExponentialInequalities) Consider a sequence of random functions ξ (Z ),...,ξ (Z ),...withrespecttofiltration{F }. Wehaveforanyδ ∈(0,1)andλ>0: 1 1 t t t (cid:34) n n (cid:35) (cid:88) log(1/δ) 1 (cid:88) P ∃n>0:− ξ ≥ + logE exp(−λξ ) ≤δ, i λ λ Z(y) i i i=1 i=1 whereZ =(Z(x),Z(y))andZ =(Z ,...,Z ). t t t t 1 t 26 --- Page 27 --- Lemma11(MultiplicativeChernoffBounds) AssumethatX ∈[0,1]withEX =µ. Thenforall ϵ>0, (cid:16) (cid:17) (cid:104)−2nµϵ2(cid:105) P X¯ ≥(1+ϵ)µ ≤exp n 2+ϵ (cid:16) (cid:17) (cid:104)−2nµϵ2(cid:105) P X¯ ≤(1−ϵ)µ ≤exp . n 2 Moreover,fort>0,wehave (cid:114) (cid:16) 2µt t (cid:17) P X¯ ≥µ+ + ≤exp(−t). n n 3n Proof: RefertotheproofofCorollary2.18inZhang(2023). Remark2 The multiplicative Chernoff bounds (Lemma 11) can be expressed as follows. With probabilityatleast1−δ: (cid:114) 2µln(1/δ) µ≤X¯ + . n n Itimpliesthatforanyγ ∈(0,1): ln(1/δ) X¯ ≥(1−γ)µ− . n 2γn ■ Lemma12 Supposea,b≥0. Ifx2 ≤a+b·x,thenx2 ≤2b2+2a. Proof: Bysolvingtherootofquadraticpolynomialq(x):=x2−b·x−a,weobtainmax{x ,x }= √ √ 1 2 (b+ b2+4a)/2. Hence,wehavex≤(b+ b2+4a)/2providedthatq(x)≤0. Thenwefurther have x2 ≤ 1(cid:16) b+(cid:112) b2+4a(cid:17)2 ≤ 1 ·2(cid:0) b2+b2+4a(cid:1) ≤2b2+2a. (11) 4 4 ■ Lemma13 LetX bearandomvariableand0≤X ≤M,wehave Var(X)≤ME(X). Proof: FromtheBhatia-Davisinequality,wehave Var(X)≤(M −E(X))E(X)≤ME(X). ■ Lemma14(Online-to-batchconversion) Ifanalgorithmhasasublinearregretofc†·logT,then thealgorithmfindsanϵ-optimalpolicywithatmostΘ(cid:101)(cid:0) c†/ϵ(cid:1) samples,whereΘ(cid:101) omitslogarithmic termsofc†/ϵ. Herec†isaproblem-dependentconstant. Proof: Wedenotethepolicysequenceas{π1,··· ,πT}. Then,bydefinitionofregret,weknow T Regret(T)=TV∗(x )−(cid:88) Vπt (x ) 1 1 1 1 t=1 ≤c†logT. 27 --- Page 28 --- Weconsidertheuniformpolicyπ˜ :=Uniform(π1,··· ,πT). Itfollowsthat T V∗(x )−Vπ˜(x )=V∗(x )− 1 (cid:88) Vπt (x )≤c†logT . 1 1 1 1 1 1 T 1 1 T t=1 Itsufficestoprovethat logT c† ≤ϵ, T whichisequivalenttosolving T ≤exp(Tϵ/c†). ByusingtheLambertW function,wecanprovethat W(1)c† T ≥ , ϵ whereW(1)≥log(1/ϵ)−loglog(1/ϵ). ■ 28