{
  "abstract": "Abstract Parallelthinkingexpandsexplorationbreadth,complementingthedeepexplo- rationofinformation-seeking(IS)agentstofurtherenhanceproblem-solving capability. However,conventionalparallelthinkingfacestwokeychallengesin thissetting: inefficiencyfromrepeatedlyrollingoutfromscratch,anddifficulty inintegratinglong-horizonreasoningtrajectoriesduringanswergeneration,as limitedcontextcapacitypreventsfullconsiderationofthereasoningprocess. Toaddresstheseissues,weproposePARALLELMUSE,atwo-stageparadigm designedfordeepISagents. Thefirststage,Functionality-SpecifiedPartial Rollout,partitionsgeneratedsequencesintofunctionalregionsandperforms uncertainty-guidedpathreuseandbranchingtoenhanceexplorationefficiency. Thesecondstage,CompressedReasoningAggregation,exploitsreasoningre- dundancytolosslesslycompressinformationrelevanttoanswerderivationand synthesizeacoherentfinalanswer. Experimentsacrossmultipleopen-source agents and benchmarks demonstrate up to 62% performance improvement witha10\u201330%reductioninexploratorytokenconsumption.",
  "introduction": "1 Introduction Deepinformation-seeking (IS)agents1 (OpenAI,2025b; Team,2025a;b) canactively uncoverhard-to- accessinformation,extendinglargelanguagemodelsbeyondstatictrainingdataandempoweringthem toreasonoverreal-worldknowledge. Thiscapabilityemergesfromacontinualloopofenvironmental2 interactionandinternalreasoning,throughwhichtheagentincrementallybuildsreasoningdepthwithin asingleexecutiontoeffectivelysolvecomplexproblems(Wuetal.,2025c;a;Lietal.,2025c;Taoetal.,2025; Lietal.,2025b).Inthissetting,parallelthinkingprovidesanaturalformoftest-timescaling:byexpanding thenumberofparallelexplorationpaths,itbroadenstheagent\u2019ssearchwhilemaintainingreasoning depthalongeachpath,therebyenhancingoverallperformancewithoutalteringmodelparameters. As commonly recognized, parallel thinking can be viewed as a two-stage process (Li et al., 2025a), involvinganinitialstageofexploratorysamplingandasubsequentstagededicatedtoanswergeneration across sampled candidates. In this work, we extend this paradigm to the setting of deep IS agents, \u2020Equalcontribution. (cid:0)   1Theagentsdiscussedinthisworkarefunction-callingagentsthatadheretothestandardReAct(Yaoetal.,2023) paradigm,operatingthroughaniterativethink\u2192toolcallloop. 2Thisworkfocusesondeepinformation-seekingagents,wheretheterm\u201cenvironment\u201dspecificallyreferstothe webenvironmentorinformationsourceswithwhichtheagentinteracts. 1 5202 tcO 82 ]LC.sc[ 1v89642.0152:viXra --- Page 2 --- referredtoasPARALLELMUSE. Weanalyzehowthecharacteristicsofeachstagemanifestunderagentic conditionsandproposesystematicoptimizationstrategiesderivedfromthesepilotobservations. First, in the exploratorysamplingstage, conventional rollout strategies in parallel thinking typically restartfromscratchateachiteration,resamplingtheentireexplorationspace(AI,2025;Fuetal.,2025; Zengetal.,2025). Duringcertainreasoningphases,however,explorationdiversityisinherentlylow, makingrepeatedrolloutsinefficientandtoken-expensive. Priorworkintroducespartialrolloutmethods thatestimateexplorationpotentialviauncertaintyandselectivelybranchwhereuncertaintyishigh(Hou etal.,2025;Dongetal.,2025;Lietal.,2025e),buttheseapproachesassumefunctionalhomogeneityacross tokens,implyingthatalltokenscontributeequallytoexplorationandexhibitsimilaruncertainty. Thisassumptionholdsinpurelyreasoning-orientedtaskssuchasmathematicsorcodingbutfailsin agenticISsettings,wherethemodelmustgeneratebothreasoningandtool-callactions. Thesebehaviors naturallyformdistinctfunctionalregionswithdifferentuncertaintypatterns.Motivatedbythisobservation, weproposetheFunctionality-SpecifiedPartialRolloutmethodasthefirststageofthePARALLELMUSE framework. Themethodsegmentsthegeneratedsequenceintofunctionalregions,estimatesuncertainty independentlywithineach,andselectivelyexpandsrolloutsforreasoningstepswithhigherexploration potential. Thisenablesbehavior-levelestimationofexplorationpotential,allowingtargetedexploration acrossdifferentfunctionalbehaviors,andimprovingoverallefficiencyinagentictasks. Second,intheanswergenerationstage,parallelthinkingproducesmultiplereasoningcandidatesfrom whichasingleanswermustbederived,typicallythroughanswerselection(Wangetal.,2022;Fuetal., 2025)oransweraggregation(Jiangetal.,2023;Liangetal.,2024;Zhangetal.,2025b;Qiaoetal.,2025). Incomplexagentictaskswithvastsamplingspaces,thecorrectanswermaynotdominatenumerically, asitoftenconstitutesonlyasmallfractionofallpossiblesampledoutcomes. Moreover,thecontinual incorporation of external, non\u2013model-generated information shifts the output distribution, further hindering confidence calibration (Jang et al., 2024). As a",
  "results": "results show that the effectiveness of branching based on different functional-region uncertain- tiesvariesacrossmodels,reflectingtheirinherentbehavioralandcapabilitydifferences. Forinstance, GPT-OSS-120Bbenefitslessfromreasoning-basedbranching,asitsstrongadaptivereasoningmecha- nismalreadyyieldsconsistentlyhigh-qualityreasoningwithlimitedexplorationpotential. Incontrast, DeepSeek-V3.1-Temploysfunctioncallingoutsideofthethinkingmode,resultinginweakerinternal reasoningcapacityandthushighersamplingpotentialinreasoningsteps. Theseinsightsheuristically informourchoiceoffunctional-regionuncertaintyforpartialrolloutinPARALLELMUSE. We also observe that partial rollout consistently outperforms full from-scratch rollout in most cases. Thisimprovementarisesfrommoretargetedexploration. IndeepIStasks,whereinteractionwiththe webenvironmentinducesanextremelylargeexplorationspace,unguidedrolloutsstruggletoidentify effectivesearchpathsandoftenfallintolocaloptima. Incontrast, uncertainty-guidedpartialrollout functionsanalogouslytoMonte-CarloTreeSearch(MCTS)(Browneetal.,2012): whileMCTSreuses high-rewardtrajectories,PARALLELMUSEreuseslow-uncertainty(low-potential)pathsandselectively 3Weomitresultsofpartialrolloutwithoutfunctional-regiondistinction(i.e.,treatingalltokensashomogeneous), asourpreliminaryexperimentsshowthatthissettingperformscomparablytofullfrom-scratchrolloutsandprovides noobservablegainsfrombranchingathigh-uncertaintysteps. 9 --- Page 10 --- expands exploration at high-uncertainty steps. This strategy allocates the limited sampling budget towardregionswithgreaterexpectedexplorationgain,enhancingbothefficiencyandeffectiveness. 4.4 Performance Gains from Compressed Reasoning Aggregation 80 70 60 50 40 BrowseComp BrowseComp-zh GAIA HLE erocS In Section 4.3, we examined the perfor- mancegainsarisingfromthefirst-stagepar- No Scaling 80.6 tialrolloutoftheproposedPARALLELMUSE. Majority Vote 77.778.6 Inthissection,weisolateandanalyzethe Weighted Vote 73.6 Ours effectivenessofitssecond-stageanswerag- gregationmethod,independentlyassessing 63.5 62.0 itscontributiontooverallperformance. 60.0 56.8 57.1 53.6 As shown in Figure 4, even without the 52.2 51.0 sampling(exploration)gainsfromthefirst- 45.3 stage partial rollout, the proposed Com- 42.7 40.1 pressed Reasoning Aggregation (the second- 38.5 stageofPARALLELMUSE)aloneyieldsthe mostsignificantimprovement.Notably,this Figure 4: Performance gains from different answer gen- approachperformsnear-losslesscompres- eration",
  "methods": "methods, with sampling fixed to 8 from-scratch sionovereachagenticreasoningtrajectory rolloutstoisolatesampling(exploration)effects. to efficiently integrate reasoning informa- tionwithoutinvokingadditionaltoolcalls for secondary verification. By maximizing the exploitation of existing sampled information during aggregation,itachievesabalancedimprovementinbothefficiencyandsolutionquality. 4.5 Efficiency Gains through Context Reuse and Trajectory Compression Figure 5: Efficiency gains using PARALLELMUSE. (i) (Left) Token reduction through context reuse in ourpartialrolloutmethod. Wetakethetokenconsumptionpertrajectoryofthefrom-scratchrolloutas thebaseline. Thegreenbarsrepresentthetokencostafterapplyingpartialrollout(thenumbersabove indicatetheratiorelativetothebaseline),whiletheremainingbluebarsshowtheproportionoftokens saved. (ii)(Right)Comparisonofcontexttokenusagebeforeandaftertrajectorycompression. WeconductadetailedanalysisoftheefficiencygainsachievedbyourproposedPARALLELMUSE,which primarilyarisefromtwocomplementarysources: (i) Token reduction via context reuse in Functionality-Specified Partial Rollout. As shown in Figure 5 (Left), our method (Partial Rollout) achieves up to 28% token savings by effectively reusingcontextinsteadofregeneratingitfromscratch(RolloutfromScratch). Theefficiencygain increaseswithsamplingscale,indicatingbetterscalability. (ii) ContextefficiencyviatrajectorycompressioninCompressedReasoningAggregation. Asshown 10 --- Page 11 --- inFigure5(Right),compressingtheagenticreasoningtrajectoryreducescontexttokenusageby upto99%relativetothefulltrajectory,achievinganalmostcompletecompression. Thisenables multi-trajectoryreasoningaggregationwithincontextlimitsandimprovesprocessingefficiency. Insummary,theproposedPARALLELMUSEisnotaconventionaltest-timescalingstrategythatimproves performancebyaggressivelysacrificingefficiency. Instead,byleveragingatask-informeddesign,itscales computationwhereitmattersmost,allocatingadditionaltokensandreasoningcapacityasneededto high-utilityregionswhileeliminatingmostredundantoravoidablecomputation. 4.6 Impact of Model Capability on Compressed Reasoning Aggregation In the proposed PARALLELMUSE, the compres- sionprocessinCompressedReasoningAggregation Table4:Performancegainsfromusingstrongermod- canbeviewedasextractingandreconstructingthe els for Compressed Reasoning Aggregation. Rollout agent\u2019sinternalinformationstategraphG(defined configurationdetailedinTable1. inequation2)fromthefullagenticreasoningtra- jectory. Thisgraph,describedbythecompressed RolloutModel AggregationModel BrowseComp report,encapsulatesallinformationnecessaryfor GPT-OSS-20B 49.0 answerderivation. Hence,thequalityofcompres- GPT-OSS-20B GPT-OSS-120B\u2191 50.5 GPT-5\u2191\u2191 55.5 siondependsonthefidelityofthisextractionand Tongyi-DR-30B-A3B 65.0 reconstruction,whichdirectlyaffectssubsequent Tongyi-DR-30B-A3B GPT-5\u2191 66.0 aggregationperformance. Toexaminewhetherastrongermodelcanperform higher-qualitycompressionandyieldbetteraggregation,weevaluatethesettingwheretheCompressed ReasoningAggregationstageisexecutedbymodelsstrongerthanthoseusedforthefirst-stagepartial rollout. AsshowninTable4,whenthefirst-stagesamplingisconductedwithGPT-OSS-20B,replacing itwithastrongerGPT-OSS-120Bfortheaggregationstageleadstoaclearperformanceimprovement. FurthersubstitutionwithGPT-5bringscontinuousgains,andasimilartrendisobservedontheTongyi- DR-30B-A3Bmodel,confirmingthatthecompressedreporteffectivelyrepresentstheagent\u2019sinternal informationstategraphandthathigher-qualitygraphreconstructionenhancesoverallperformance. This resultalsosuggestsapracticalinsightformulti-agentdesign(Hanetal.,2024;Lietal.,2024): combining modelsofdifferentstrengthscanbalanceefficiencyandperformance. 5",
  "experiments": "Experiments Wefocusonevaluatingtheeffectivenessandefficiencyofapplying PARALLELMUSE toexistingdeep information-seeking(IS)agents. Comprehensiveexperimentsareconductedtoexaminetheimpactofits twostagesbothindividuallyandjointly,assessinghoweachcontributestooveralltaskperformance. 4.1 Setup Benchmarks. WeevaluatePARALLELMUSEonfourchallengingdeepISbenchmarks:BrowseComp(Wei etal.,2025),BrowseComp-zh(Zhouetal.,2025),GAIA(Mialonetal.,2023),andHumanity\u2019sLastExam (HLE)(Phanetal.,2025). Thesebenchmarksjointlyassessbothdeepsearchandreasoningcapabilities, withBrowseCompandBrowseComp-zhplacinggreateremphasisondeepsearch,HLEfocusingmore onreasoning,andGAIAprovidingabalancedevaluationacrossbothdimensions. For efficient text-only evaluation, we use sampled subsets from large-scale datasets: 200 randomly selectedtasksfromBrowseComp,157search-focusedtext-onlytasksfromHLE,and103text-onlytasks fromGAIA(Lietal.,2025d),whileusingthefull289-tasksetforBrowseComp-zh. Tools. WeadoptthestandardtoolconfigurationcommonlyusedindeepISagents(Wuetal.,2025a;Li etal.,2025c;Taoetal.,2025;Lietal.,2025b;Qiaoetal.,2025),whichincludestwocoretoolsforinteracting withthewebenvironmentandretrievingexternalinformation: \u2022 Search: PerformsbatchedGooglequeriesandreturnsthetop-10rankedresultsforeach. \u2022 Visit: FetcheswebpagesfrommultipleURLsandextractsinformationrelevanttothegivengoal. AgentModels. Weselectfouropen-sourceagentmodelswithdiverseparameterscalesandadvanced tool-usecapabilitiesfordeepIStasks: GPT-OSS-20B(OpenAI,2025a),GPT-OSS-120B,DeepSeek-V3.1- Terminus(DeepSeek-V3.1-T,671B)(Liuetal.,2024),andTongyi-DeepResearch-30B-A3B(Tongyi-DR-30B- A3B)(Team,2025b). Allagentmodelsareinvokedundertheofficialfunction-callingprotocol. Unless otherwisespecified,weusethesameagentmodeltoperformbothstagesofthePARALLELMUSE. Baselines. Inadditiontothestandardinferencebaselinewithoutanyparallelthinking(NoScaling), we compare PARALLELMUSE against several mainstream parallel thinking baselines. These include: (i) Self-Consistency(MajorityVote) (Wang et al., 2022), which se- lects the most frequent answer across multiple trajectories as Table1: Defaultsettingsofourpro- the final output; (ii) Max#ToolCall (Zeng et al., 2025), which posedPARALLELMUSE. heuristically chooses the answer derived from the trajectory with the largest number of environment interactions; and (iii) Hyper-Parameters Values DeepConf(WeightedVote)(Fuetal.,2025),whichweightsanswers SamplingBudgetN 8 by the model\u2019s confidence over each trajectory and selects the #InitialRolloutM 1 answerwiththehighestfinalscore. BranchingPPLTop-K 2 #BranchingTimesperStep 3 EvaluationMetricsandHyper-Parameters. Allevaluationsare performedundertheLLM-as-a-Judgeparadigm(Guetal.,2024), 7 --- Page 8 --- using the official evaluation prompts and judging models specified by each benchmark\u2019s released configuration. FortheNoScalingmethod,wereporttheaveragepassrateoverNindependentrollouts, whileforparallelthinkingmethods,whichyieldasinglefinalanswerfrom N rollouts,wereportthe passrateofthatfinaloutput. ForourproposedPARALLELMUSE,unlessotherwisespecified,thedefault hyper-parametersettingsarelistedinTable1. Toensurefaircomparisonandreproducibility,allagent model\u2013specifichyper-parametersarealignedwiththeirofficialoptimalconfigurationsfortoolusage. 4.2 Overall Performance Table2: Overallperformance. Scoresmarkedwith\u2021representfull-benchmarkresults,whereasunmarked scorescorrespondtoourbenchmarksettings. BothPARALLELMUSEandotherparallelthinkingbaselines are evaluated under the default configurations as described in Section 4.1. The specific strategy for selectingfunctionalregionsinPARALLELMUSE\u2019spartialrolloutisdiscussedinSection4.3. Model/Framework Method BrowseComp BrowseComp-zh GAIA HLE Closed-SourceDeepInformation-SeekingAgents Claude-4-Sonnet NoScaling 12.2\u2021 29.1 68.3 20.3\u2021 OpenAI-o3 NoScaling 49.7\u2021 58.1 70.5 26.6\u2021 KimiResearcher NoScaling \u2013 \u2013 \u2013 26.9\u2021 OpenAIDeepResearch NoScaling 51.5\u2021 42.9 67.4 26.6\u2021 ChatGPTAgent NoScaling 68.9\u2021 \u2013 \u2013 41.6\u2021 Open-SourceDeepInformation-SeekingAgents NoScaling 30.9 28.6 63.4 24.2 MajorityVote 44.0 38.8 69.9 24.2 GPT-OSS-20B Max#ToolCall 17.0 19.0 58.3 26.1 WeightedVote 41.0 37.0 68.9 31.2 PARALLELMUSE 49.0 44.3 72.8 32.5 NoScaling 34.9|33.8\u2021 36.0 74.3 36.3 MajorityVote 48.5 46.7 77.7 43.3 GPT-OSS-120B Max#ToolCall 17.5 26.3 68.9 36.9 WeightedVote 48.0 45.7 82.5 45.2 PARALLELMUSE 56.5 54.3 85.4 45.9 NoScaling 23.2 36.1 61.0 25.0|21.7\u2021 MajorityVote 30.0 45.0 70.9 26.1 DeepSeek-V3.1-T Max#ToolCall 17.5 28.0 57.3 27.4 WeightedVote 29.5 45.0 70.9 28.0 PARALLELMUSE 39.0 50.2 74.8 37.6 NoScaling 51.0|43.4\u2021 45.3 73.6 38.5|32.9\u2021 MajorityVote 60.0 56.8 77.7 40.1 Tongyi-DR-30B-A3B Max#ToolCall 41.0 36.3 75.7 38.2 WeightedVote 62.0 53.6 78.6 42.7 PARALLELMUSE 65.0 57.1 79.6 52.2 Wereporttheperformanceofclosed-sourcedeepISagentsacrossallbenchmarksandcomparethemwith open-sourceagentsequippedwithourproposedPARALLELMUSEandseveralrepresentativeparallel thinkingbaselines. AsshowninTable2,PARALLELMUSEconsistentlyachievesthehighestperformance gainsoverallbaselinesacrosseveryagentmodelandbenchmark. Notably,whenappliedtoTongyi-DR- 30B-A3B,itattainsperformancecomparabletoorsurpassingthatofmostclosed-sourceagents. 8 --- Page 9 --- ItisimportanttonotethatwefurtherobservethattheWeightedVote,whichreliesonself-estimatedconfi- dence,underperformsMajorityVoteacrossallmodelsexceptTongyi-DR-30B-A3B.Thiscanbeattributed to confidence miscalibration in agentic settings: as agents repeatedly integrate external, non\u2013model- generated content (e.g., tool responses), their internal probability distributions shift, degrading the reliabilityofconfidencescores(Jangetal.,2024;Chhikara,2025). TheexceptionistheHLEbenchmark, whichemphasizesreasoningwithlimitedexternalinteraction,andTongyi-DR-30B-A3B,whichbenefits fromcontinualpre-training(Suetal.,2025)thatimprovescalibrationoverSearchandVisittoolresponses. Incontrast,ourproposedPARALLELMUSEavoidsconfidence-basedanswerselectionaltogether,mitigat- ingthissourceofbiasandyieldingconsistentandsubstantialimprovementsacrossallagentmodels. 4.3 Analysis of Partial Rollout over Distinct Functional Regions Table 3: Performance comparison between full from-scratch rollouts and partial rollouts guided by functional-regionuncertainty. DetailedconfigurationsarelistedinTable1. AgentModel FunctionalRegion BrowseComp BrowseComp-zh GAIA HLE FromScratch: NoRegion 34.9 36.0 74.2 36.3 GPT-OSS-120B Partial: Reasoning 37.9 43.1 76.1 36.3 Partial: Exploration 39.9 41.6 77.9 37.5 Partial: Mixed 38.1 42.9 76.7 37.1 FromScratch: NoRegion 23.2 36.1 61.0 25.0 DeepSeek-V3.1-T Partial: Reasoning 26.5 39.8 60.2 26.4 Partial: Exploration 23.8 37.9 60.6 25.3 Partial: Mixed 23.4 38.1 60.3 25.1 To analyze the effect of identifying branching steps based on different functional regions in partial rollout,wereporttheaveragepassrateafter8rollouts,asshowninTable3. TheFromScratchsetting denotesfullrolloutswithoutreusingcontext,wherefunctionalregionselectionisnotapplicable. Forour proposedPARALLELMUSE3,weevaluatethreefunctionality-specifiedpartialrolloutstrategies: using uncertaintyfromtheReasoningregiononly,usinguncertaintyfromtheExplorationregiononly,anda Mixedconfigurationwherehalfofthetop-2branchingstepsareselectedfromeachregion. TheMixed settingactsasacompromise,integratinguncertaintycuesfrombothregions. The",
  "related_work": "Related Work 5.1 Deep Information-Seeking Agents Deepinformation-seeking(IS)agentsareautonomoussystemsdesignedtoengageinmulti-stepinter- actionwithexternalinformationenvironments(suchastheweb)andintegrateretrieveddatathrough reasoninginordertoaddresscomplexknowledge-intensivetasks. Thedevelopmentofsuchagentshasbenefitedfrombothproprietaryandopen-sourceinitiatives. Onthe proprietaryside,systemsfrommajorresearchorganizationshavesetbenchmarksfordeepexploration and reasoning capabilities (OpenAI, 2025b; Team, 2025a; AI, 2025; Perplexity, 2025; Anthropic, 2025; DeepMind,2025),thoughtheirarchitecturesandtrainingprotocolsremainclosed. Ontheopen-source front,communityeffortshaveadvancedtransparencyandreproducibilityindeepISagentdesign(Zhang etal.,2025a;Wuetal.,2025c;a;Lietal.,2025c;b;Taoetal.,2025;Gengetal.,2025;Sunetal.,2025;Gao etal.,2025;Liuetal.,2025;Luetal.,2025;Team,2025b),drivingcontinuousprogressinthisdomain. Inthiswork,wefurtherexploretheuniquecharacteristicsofdeepISagentsandproposePARALLELMUSE toexploitthesepropertiesmoreeffectively,therebyenhancingbothcapabilityandefficiency. 11 --- Page 12 --- 5.2 Parallel Thinking for Test-Time Scaling Parallel thinking (Wang et al., 2025) serves as an effective test-time scaling strategy for reasoning, particularlyinagenticsettingsthatrequiredeepandcomplexinteractions.Itgeneratesmultiplereasoning trajectoriestocapturediversereasoningbehaviorsandjointlydeterminesthefinalanswer. Conceptually,parallelthinkingfollowsatwo-stageparadigm(Lietal.,2025a): exploratorysamplingand answergeneration. Thefirststageexploresdiversereasoningpathsthroughindependentsampling(Wei etal.,2022;Zengetal.,2025),structuredrollouts,orintermediatepartialrollouts(AI,2025;Dongetal., 2025; Hou et al., 2025; Li et al., 2025e) where branches share context but remain flexible. In agentic reasoningwithvastexplorationspaces,independentandpartialrolloutsaregenerallymoreeffective andefficientthanstructuredones. Thesecondstagefocusesonsynthesizingresultsviaeitheranswer selection(Wangetal.,2022;Fuetal.,2025),whichisefficientbutoftenbiased,oransweraggregation(Jiang etal.,2023;Liangetal.,2024;Zhangetal.,2025b;Qiaoetal.,2025),whichismorestablebutchallenged bytheneedtoidentifywhichintermediatereasoningcontributesmosttothefinaloutcome. Mostexistingparallelthinkingmethodsinherittheassumptionsofpurereasoningtasks. Buildingon a detailed analysis of agentic reasoning, especially in deep IS tasks, we propose PARALLELMUSE, a paradigmthatfullyleveragesthesepropertiestomoreeffectivelyunlockthepotentialofdeepISagents. 6",
  "conclusion": "Conclusion This work investigates the challenges of applying parallel thinking to deep information-seeking (IS) agents. Conventionalparallelthinkingstrategiesoftenwastecomputationthroughredundantrollouts andstruggletointegratelong-horizonreasoningduetolimitedcontextcapacity. Buildingonanin-depth analysisofthecharacteristicsofdeepIStasks,weincorporatetheseinsightsintomethoddesignand proposePARALLELMUSE,atwo-stageparadigmthatenhancesbothexplorationefficiencyandreasoning aggregation. Experimentalresultsacrossmultipleopen-sourceagentsandbenchmarksdemonstratethat PARALLELMUSEachievessubstantialperformanceimprovementswhilegreatlyreducingexploratory tokenconsumption,highlightingitseffectivenessforefficientandscalabledeepISreasoning. 7 Limitations and Future Work In this work, we focus primarily on question-answering\u2013oriented deep IS tasks, where the toolset is limitedtoSearchandVisit. WhilethisconfigurationisoptimalfordeepIStasks,moregeneralagentic tasksofteninvolveabroaderrangeoftools(Fangetal.,2025),leadingtosubstantiallylargerexploration spaces. Designingeffectiveparallelthinkingstrategiesundersuchcomplextoolconfigurationstoextend applicabilitytogeneralagenticsettingsremainsanopendirectionforfutureresearch. 12 --- Page 13 ---",
  "references": "References SkyworkAI. Skywork-deepresearch.  GeneMAmdahl.Validityofthesingleprocessorapproachtoachievinglargescalecomputingcapabilities. InProceedingsoftheApril18-20,1967,springjointcomputerconference,pp.483\u2013485,1967. Anthropic. Introducingclaude4,2025. URL Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp Rohlfshagen,StephenTavener,DiegoPerez,SpyridonSamothrakis,andSimonColton. Asurveyof montecarlotreesearchmethods. IEEETransactionsonComputationalIntelligenceandAIingames,4(1): 1\u201343,2012. PrateekChhikara. Mindtheconfidencegap: Overconfidence,calibration,anddistractoreffectsinlarge languagemodels. arXivpreprintarXiv:2502.11028,2025. GoogleDeepMind. Gemini2.5,2025. URL ni-model-thinking-updates-march-2025/. GuantingDong, HangyuMao, KaiMa, LichengBao, YifeiChen, ZhongyuanWang, ZhongxiaChen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprintarXiv:2507.19849,2025. RunnanFang,ShihaoCai,BaixuanLi,JialongWu,GuangyuLi,WenbiaoYin,XinyuWang,XiaobinWang, LiangcaiSu,ZhenZhang,etal. Towardsgeneralagenticintelligenceviaenvironmentscaling. arXiv preprintarXiv:2509.13311,2025. YichaoFu,XueweiWang,YuandongTian,andJiaweiZhao. Deepthinkwithconfidence. arXivpreprint arXiv:2508.15260,2025. Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, and Yi Wu. Beyond ten turns: Unlocking long-horizon agentic search with large-scale asynchronous rl. arXiv preprintarXiv:2508.07976,2025. XinyuGeng,PengXia,ZhenZhang,XinyuWang,QiuchenWang,RuixueDing,ChenxiWang,Jialong Wu,YidaZhao,KuanLi,etal. Webwatcher: Breakingnewfrontierofvision-languagedeepresearch agent. arXivpreprintarXiv:2508.05748,2025. JiaweiGu,XuhuiJiang,ZhichaoShi,HexiangTan,XuehaoZhai,ChengjinXu,WeiLi,YinghanShen, ShengjieMa,HonghaoLiu,etal. Asurveyonllm-as-a-judge. arXivpreprintarXiv:2411.15594,2024. ShanshanHan,QifanZhang,YuhangYao,WeizhaoJin,andZhaozhuoXu. Llmmulti-agentsystems: Challengesandopenproblems. arXivpreprintarXiv:2402.03578,2024. ZhenyuHou,ZiniuHu,YujiangLi,RuiLu,JieTang,andYuxiaoDong.Treerl:Llmreinforcementlearning withon-policytreesearch. arXivpreprintarXiv:2506.11902,2025. ChaeyunJang,HyungiLee,SeanieLee,andJuhoLee. Calibrateddecision-makingthroughllm-assisted retrieval. arXivpreprintarXiv:2411.08891,2024. DongfuJiang,XiangRen,andBillYuchenLin. Llm-blender: Ensemblinglargelanguagemodelswith pairwiserankingandgenerativefusion. InProceedingsofthe61stAnnualMeetingoftheAssociationfor ComputationalLinguistics(Volume1: LongPapers),pp.14165\u201314178,2023. BaixuanLi,YunlongFan,TianyiMa,MiaoGao,ChuanqiShi,andZhiqiangGao. Raspberry: Retrieval- augmentedmontecarlotreeself-playwithreasoningconsistencyformulti-hopquestionanswering. In FindingsoftheAssociationforComputationalLinguistics: ACL2025,pp.11258\u201311276,2025a. 13 --- Page 14 --- HaoyangLi,YimingLi,AnxinTian,TianhaoTang,ZhanchaoXu,XuejiaChen,NicoleHu,WeiDong, QingLi,andLeiChen. Asurveyonlargelanguagemodelaccelerationbasedonkvcachemanagement. arXivpreprintarXiv:2412.19442,2024. KuanLi,ZhongwangZhang,HuifengYin,RuiYe,YidaZhao,LiwenZhang,LituOu,DingchuZhang, XixiWu,JialongWu,XinyuWang,ZileQiao,ZhenZhang,YongJiang,PengjunXie,FeiHuang,and JingrenZhou. Websailor-v2: Bridgingthechasmtoproprietaryagentsviasyntheticdataandscalable reinforcementlearning,2025b. URL KuanLi,ZhongwangZhang,HuifengYin,LiwenZhang,LituOu,JialongWu,WenbiaoYin,BaixuanLi, ZhengweiTao,XinyuWang,WeizhouShen,JunkaiZhang,DingchuZhang,XixiWu,YongJiang,Ming Yan,PengjunXie,FeiHuang,andJingrenZhou. Websailor: Navigatingsuper-humanreasoningfor webagent,2025c. URL Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability. CoRR,abs/2504.21776,2025d. doi: 10.48550/ARXIV.2504.21776. URL rXiv.2504.21776. YizhiLi,QingshuiGu,ZhoufutuWen,ZiniuLi,TianshunXing,ShuyueGuo,TianyuZheng,XinZhou, XingweiQu,WangchunshuZhou,etal. Treepo: Bridgingthegapofpolicyoptimizationandefficacy andinferenceefficiencywithheuristictree-basedmodeling. arXivpreprintarXiv:2508.17445,2025e. TianLiang,ZhiweiHe,WenxiangJiao,XingWang,YanWang,RuiWang,YujiuYang,ShumingShi,and ZhaopengTu. Encouragingdivergentthinkinginlargelanguagemodelsthroughmulti-agentdebate. InProceedingsofthe2024ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.17889\u201317904, 2024. AixinLiu,BeiFeng,BingXue,BingxuanWang,BochaoWu,ChengdaLu,ChenggangZhao,Chengqi Deng,ChenyuZhang,ChongRuan,etal.DeepSeek-V3technicalreport.arXivpreprintarXiv:2412.19437, 2024. JuntengLiu,YunjiLi,ChiZhang,JingyangLi,AiliChen,KeJi,WeiyuCheng,ZijiaWu,ChengyuDu, QidiXu,etal. Webexplorer: Exploreandevolvefortraininglong-horizonwebagents. arXivpreprint arXiv:2509.06501,2025. RuiLu,ZhenyuHou,ZihanWang,HanchenZhang,XiaoLiu,YujiangLi,ShiFeng,JieTang,andYuxiao Dong. Deepdive: Advancingdeepsearchagentswithknowledgegraphsandmulti-turnrl. arXiv preprintarXiv:2509.10446,2025. Gr\u00e9goire Mialon, Cl\u00e9mentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmarkforgeneralaiassistants. InTheTwelfthInternationalConferenceonLearningRepresentations, 2023. OpenAI. gpt-oss-120b&gpt-oss-20bmodelcard,2025a. URL OpenAI. Deepresearchsystemcard,2025b. URL ard.pdf. Perplexity. Perplexitydeepresearch,2025. URL LongPhan,AliceGatti,ZiwenHan,NathanielLi,JosephinaHu,HughZhang,ChenBoCalvinZhang, MohamedShaaban,JohnLing,SeanShi,etal. Humanity\u2019slastexam. arXivpreprintarXiv:2501.14249, 2025. 14 --- Page 15 --- Zile Qiao, Shen Huang, Jialong Wu, Kuan Li, Wenbiao Yin, Xinyu Wang, Liwen Zhang, Baixuan Li, ZhengweiTao,WeizhouShen,XixiWu,YongJiang,PengjunXie,FeiHuang,JunZhang,andJingren Zhou. WebResearcher: Unleashingunboundedreasoningcapabilityinlong-horizonagents,2025. Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen, Chenxi Wang, Maojia Song, Xinyu Wang, Kuan Li, Jialong Wu, Xuanzhong Chen, et al. Scaling agents via continual pre-training. arXiv preprint arXiv:2509.13310,2025. ShuangSun,HuatongSong,YuhaoWang,RuiyangRen,JinhaoJiang,JunjieZhang,FeiBai,JiaDeng, WayneXinZhao,ZhengLiu,etal. Simpledeepsearcher: Deepinformationseekingviaweb-powered reasoningtrajectorysynthesis. arXivpreprintarXiv:2505.16834,2025. Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang,XinyuWang,YongJiang,PengjunXie,FeiHuang,andJingrenZhou. WebShaper: Agentically datasynthesizingviainformation-seekingformalization,2025. KimiTeam. Kimiresearchertechreport,2025a. URL r/. Tongyi DeepResearch Team. Tongyi deepresearch: A new era of open-source ai researchers. https: //github.com/Alibaba-NLP/DeepResearch,2025b. XuezhiWang,JasonWei,DaleSchuurmans,QuocLe,EdHuaihsinChi,andDennyZhou.Self-consistency improveschainofthoughtreasoninginlanguagemodels. ArXiv,abs/2203.11171,2022. ZiqiWang,BoyeNiu,ZipengGao,ZhiZheng,TongXu,LinghuiMeng,ZhongliLi,JingLiu,YilongChen, ChenZhu,etal. Asurveyonparallelreasoning. arXivpreprintarXiv:2510.12164,2025. JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,DennyZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural informationprocessingsystems,35:24824\u201324837,2022. JasonWei,ZhiqingSun,SpencerPapay,ScottMcKinney,JeffreyHan,IsaFulford,HyungWonChung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: A simple yet challenging benchmarkforbrowsingagents. arXivpreprintarXiv:2504.12516,2025. JialongWu,BaixuanLi,RunnanFang,WenbiaoYin,LiwenZhang,ZhengweiTao,DingchuZhang,Zekun Xi,GangFu,YongJiang,PengjunXie,FeiHuang,andJingrenZhou. Webdancer: Towardsautonomous informationseekingagency,2025a. URL JialongWu,ZhenglinWang,LinhaiZhang,YilongLai,YulanHe,andDeyuZhou. SCOPE:Optimizing key-valuecachecompressioninlong-contextgeneration. InWanxiangChe,JoyceNabende,Ekaterina Shutova,andMohammadTaherPilehvar(eds.),Proceedingsofthe63rdAnnualMeetingoftheAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 10775\u201310790, Vienna, Austria, July 2025b. AssociationforComputationalLinguistics. ISBN979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.52 9. URL JialongWu,WenbiaoYin,YongJiang,ZhenglinWang,ZekunXi,RunnanFang,LinhaiZhang,YulanHe, DeyuZhou,PengjunXie,andFeiHuang. Webwalker: Benchmarkingllmsinwebtraversal,2025c. URL ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikNarasimhan,andYuanCao. Re- act: Synergizing reasoning and acting in language models. In International Conference on Learning Representations(ICLR),2023. WeihaoZeng,KeqingHe,ChuqiaoKuang,XiaoguangLi,andJunxianHe. Pushingtest-timescaling limitsofdeepsearchwithasymmetricverification. arXivpreprintarXiv:2510.06135,2025. 15 --- Page 16 --- DingchuZhang,YidaZhao,JialongWu,BaixuanLi,WenbiaoYin,LiwenZhang,YongJiang,YufengLi, KeweiTu,PengjunXie,andFeiHuang. Evolvesearch: Aniterativeself-evolvingsearchagent,2025a. URL QiyuanZhang,FuyuanLyu,ZexuSun,LeiWang,WeixuZhang,ZhihanGuo,YufeiWang,IrwinKing, Xue Liu, and Chen Ma. What, how, where, and how well? a survey on test-time scaling in large languagemodels. CoRR,2025b. PeilinZhou,BruceLeon,XiangYing,CanZhang,YifanShao,QichenYe,DadingChong,ZhilingJin, ChenxuanXie,MengCao,etal.Browsecomp-zh:Benchmarkingwebbrowsingabilityoflargelanguage modelsinchinese. arXivpreprintarXiv:2504.19314,2025. 16"
}