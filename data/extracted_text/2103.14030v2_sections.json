{
  "abstract": "Abstract This paper presents a new vision Transformer, called SwinTransformer,thatcapablyservesasageneral-purpose backbone for computer vision. Challenges in adapting Transformerfromlanguagetovisionarisefromdifferences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shiftedwindowingschemebringsgreaterefficiencybylim- iting self-attention computation to non-overlapping local Figure1.(a)TheproposedSwinTransformerbuildshierarchical windowswhilealsoallowingforcross-windowconnection. featuremapsbymergingimagepatches(showningray)indeeper This hierarchical architecture has the flexibility to model layersandhaslinearcomputationcomplexitytoinputimagesize due to computation of self-attention only within each local win- at various scales and has linear computational complexity dow(showninred). Itcanthusserveasageneral-purposeback- with respect to image size. These qualities of Swin Trans- bone for both image classification and dense recognition tasks. former make it compatible with a broad range of vision (b) In contrast, previous vision Transformers [20] produce fea- tasks, including image classification (87.3 top-1 accuracy ture maps of a single low resolution and have quadratic compu- onImageNet-1K)anddensepredictiontaskssuchasobject tationcomplexitytoinputimagesizeduetocomputationofself- detection (58.7 box AP and 51.1 mask AP on COCO test- attentionglobally. dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the- greaterscale[30,76],moreextensiveconnections[34],and artbyalargemarginof+2.7boxAPand+2.6maskAPon moresophisticatedformsofconvolution[70,18,84]. With COCO,and+3.2mIoUonADE20K,demonstratingthepo- CNNsservingasbackbonenetworksforavarietyofvision tential of Transformer-based models as vision backbones. tasks,thesearchitecturaladvanceshaveledtoperformance The hierarchical design and the shifted window approach improvementsthathavebroadlyliftedtheentirefield. also prove beneficial for all-MLP architectures. The code Ontheotherhand,theevolutionofnetworkarchitectures andmodelsarepubliclyavailableat innaturallanguageprocessing(NLP)hastakenadifferent com/microsoft/Swin-Transformer. path, where the prevalent architecture today is instead the Transformer [64]. Designed for sequence modeling and transduction tasks, the Transformer is notable for its use",
  "introduction": "1.Introduction of attention to model long-range dependencies in the data. Its tremendous success in the language domain has led re- Modeling in computer vision has long been dominated searchers to investigate its adaptation to computer vision, byconvolutionalneuralnetworks(CNNs). Beginningwith whereithasrecentlydemonstratedpromisingresultsoncer- AlexNet [39] and its revolutionary performance on the tain tasks, specifically image classification [20] and joint ImageNet image classification challenge, CNN architec- vision-languagemodeling[47]. tureshaveevolvedtobecomeincreasinglypowerfulthrough In this paper, we seek to expand the applicability of *Equalcontribution.\u2020InternsatMSRA.\u2021Contactperson. Transformer such that it can serve as a general-purpose 1202 guA 71 ]VC.sc[ 2v03041.3012:viXra --- Page 2 --- backbone for computer vision, as it does for NLP and as CNNs do in vision. We observe that significant chal- lengesintransferringitshighperformanceinthelanguage domain to the visual domain can be explained by differ- ences between the two modalities. One of these differ- ences involves scale. Unlike the word tokens that serve as the basic elements of processing in language Trans- formers, visual elements can vary substantially in scale, a problem that receives attention in tasks such as object de- Figure2.Anillustrationoftheshiftedwindowapproachforcom- putingself-attentionintheproposedSwinTransformerarchitec- tection [42, 53, 54]. In existing Transformer-based mod- ture. In layer l (left), a regular window partitioning scheme is els [64, 20], tokens are all of a fixed scale, a property un- adopted, and self-attention is computed within each window. In suitable for these vision applications. Another difference thenextlayerl+1(right),thewindowpartitioningisshifted,re- is the much higher resolution of pixels in images com- sultinginnewwindows.Theself-attentioncomputationinthenew pared to words in passages of text. There exist many vi- windowscrossestheboundariesofthepreviouswindowsinlayer siontaskssuchassemanticsegmentationthatrequiredense l,providingconnectionsamongthem. prediction at the pixel level, and this would be intractable for Transformer on high-resolution images, as the compu- shifted window approach has much lower latency than the tational complexity of its self-attention is quadratic to im- sliding window method, yet is similar in modeling power agesize. Toovercometheseissues, weproposeageneral- (see Tables 5 and 6). The shifted window approach also purpose Transformer backbone, called Swin Transformer, provesbeneficialforall-MLParchitectures[61]. which constructs hierarchical feature maps and has linear TheproposedSwinTransformerachievesstrongperfor- computational complexity to image size. As illustrated in manceontherecognitiontasksofimageclassification,ob- Figure1(a),SwinTransformerconstructsahierarchicalrep- ject detection and semantic segmentation. It outperforms resentationbystartingfromsmall-sizedpatches(outlinedin theViT/DeiT[20,63]andResNe(X)tmodels[30,70]sig- gray)andgraduallymergingneighboringpatchesindeeper nificantly with similar latency on the three tasks. Its 58.7 Transformer layers. With these hierarchical feature maps, box AP and 51.1 mask AP on the COCO test-dev set sur- theSwinTransformermodelcanconvenientlyleveragead- pass the previous state-of-the-art",
  "results": "Results Table 3 lists the mIoU, model size (#param), inductivebiasthatencouragescertaintranslationinvariance FLOPsandFPSfordifferentmethod/backbonepairs. From isstillpreferableforgeneral-purposevisualmodeling,par- theseresults,itcanbeseenthatSwin-Sis+5.3mIoUhigher ticularly for the dense prediction tasks of object detection (49.3vs. 44.0)thanDeiT-Swithsimilarcomputationcost. andsemanticsegmentation. It is also +4.4 mIoU higher than ResNet-101, and +2.4 mIoU higher than ResNeSt-101 [78]. Our Swin-L model Different self-attention",
  "experiments": "Experiments With the cyclic-shift, the number of batched windows re- mainsthesameasthatofregularwindowpartitioning,and WeconductexperimentsonImageNet-1Kimageclassi- thus is also efficient. The low latency of this approach is fication [19], COCO object detection [43], and ADE20K showninTable5. semanticsegmentation[83]. Inthefollowing,wefirstcom- pare the proposed Swin Transformer architecture with the previous state-of-the-arts on the three tasks. Then, we ab- Relative position bias In computing self-attention, we latetheimportantdesignelementsofSwinTransformer. follow [49, 1, 32, 33] by including a relative position bias B \u2208RM2\u00d7M2 toeachheadincomputingsimilarity: 4.1.ImageClassificationonImageNet-1K \u221a Attention(Q,K,V)=SoftMax(QKT/ d+B)V, (4) Settings Forimageclassification,webenchmarkthepro- posedSwinTransformeronImageNet-1K[19],whichcon- whereQ,K,V \u2208 RM2\u00d7d arethequery,keyandvaluema- tains 1.28M training images and 50K validation images from 1,000 classes. The top-1 accuracy on a single crop trices;disthequery/keydimension,andM2 isthenumber isreported. Weconsidertwotrainingsettings: of patches in a window. Since the relative position along eachaxisliesintherange[\u2212M+1,M\u22121],weparameter- \u2022 Regular ImageNet-1K training. This setting mostly izeasmaller-sizedbiasmatrixB\u02c6 \u2208 R(2M\u22121)\u00d7(2M\u22121),and follows [63]. We employ an AdamW [37] optimizer valuesinBaretakenfromB\u02c6. for 300 epochs using a cosine decay learning rate 4Tomakethewindowsize(M,M)divisiblebythefeaturemapsizeof scheduler and 20 epochs of linear warm-up. A batch (h,w),bottom-rightpaddingisemployedonthefeaturemapifneeded. size of 1024, an initial learning rate of 0.001, and a 5 --- Page 6 --- weight decay of 0.05 are used. We include most of (a)RegularImageNet-1Ktrainedmodels theaugmentationandregularizationstrategiesof[63] image throughputImageNet method #param.FLOPs intraining,exceptforrepeatedaugmentation[31]and size (image/s)top-1acc. RegNetY-4G[48] 2242 21M 4.0G 1156.7 80.0 EMA [45], which do not enhance performance. Note RegNetY-8G[48] 2242 39M 8.0G 591.6 81.7 thatthisiscontraryto[63]whererepeatedaugmenta- RegNetY-16G[48] 2242 84M 16.0G 334.7 82.9 tioniscrucialtostabilizethetrainingofViT. EffNet-B3[58] 3002 12M 1.8G 732.1 81.6 EffNet-B4[58] 3802 19M 4.2G 349.4 82.9 \u2022 Pre-training on ImageNet-22K and fine-tuning on EffNet-B5[58] 4562 30M 9.9G 169.1 83.6 ImageNet-1K. We also pre-train on the larger EffNet-B6[58] 5282 43M 19.0G 96.9 84.0 ImageNet-22K dataset, which contains 14.2 million EffNet-B7[58] 6002 66M 37.0G 55.1 84.3 imagesand22Kclasses. WeemployanAdamWopti- ViT-B/16[20] 3842 86M 55.4G 85.9 77.9 mizerfor90epochsusingalineardecaylearningrate ViT-L/16[20] 3842 307M 190.7G 27.3 76.5 schedulerwitha5-epochlinearwarm-up.Abatchsize DeiT-S[63] 2242 22M 4.6G 940.4 79.8 DeiT-B[63] 2242 86M 17.5G 292.3 81.8 of4096,aninitiallearningrateof0.001,andaweight DeiT-B[63] 3842 86M 55.4G 85.9 83.1 decay of 0.01 are used. In ImageNet-1K fine-tuning, Swin-T 2242 29M 4.5G 755.2 81.3 wetrainthemodelsfor30epochswithabatchsizeof Swin-S 2242 50M 8.7G 436.9 83.0 1024, a constant learning rate of 10\u22125, and a weight Swin-B 2242 88M 15.4G 278.1 83.5 decayof10\u22128. Swin-B 3842 88M 47.0G 84.7 84.5 (b)ImageNet-22Kpre-trainedmodels image throughputImageNet method #param.FLOPs",
  "methods": "methods. We hope that Swin Transformer\u2019s cialtostabilizethetrainingofViT.Anincreasingdegreeof strongperformanceonvariousvisionproblemswillencour- stochasticdepthaugmentationisemployedforlargermod- ageunifiedmodelingofvisionandlanguagesignals. els, i.e. 0.2,0.3,0.5 for Swin-T, Swin-S, and Swin-B, re- AsakeyelementofSwinTransformer, theshiftedwin- spectively. dow based self-attention is shown to be effective and effi- For fine-tuning on input with larger resolution, we em- cient on vision problems, and we look forward to investi- ploy an adamW [37] optimizer for 30 epochs with a con- gatingitsuseinnaturallanguageprocessingaswell. stant learning rate of 10\u22125, weight decay of 10\u22128, and thesamedataaugmentationandregularizationsasthefirst Acknowledgement stageexceptforsettingthestochasticdepthratioto0.1. We thank many colleagues at Microsoft for their help, ImageNet-22K pre-training We also pre-train on the inparticular,LiDongandFuruWeiforusefuldiscussions; larger ImageNet-22K dataset, which contains 14.2 million BinXiao,LuYuanandLeiZhangforhelpondatasets. imagesand22Kclasses. Thetrainingisdoneintwostages. Forthefirststagewith2242 input, weemployanAdamW A1.DetailedArchitectures optimizer for 90 epochs using a linear decay learning rate scheduler with a 5-epoch linear warm-up. A batch size of ThedetailedarchitecturespecificationsareshowninTa- 4096, an initial learning rate of 0.001, and a weight decay ble7,whereaninputimagesizeof224\u00d7224isassumedfor of0.01areused. InthesecondstageofImageNet-1Kfine- allarchitectures. \u201cConcatn\u00d7n\u201dindicatesaconcatenation tuning with 2242/3842 input, we train the models for 30 of n\u00d7n neighboring features in a patch. This operation epochswithabatchsizeof1024,aconstantlearningrateof resultsinadownsamplingofthefeaturemapbyarateofn. 10\u22125,andaweightdecayof10\u22128. \u201c96-d\u201d denotes a linear layer with an output dimension of 96. \u201cwin. sz. 7\u00d77\u201d indicates a multi-head self-attention A2.2.ObjectdetectiononCOCO modulewithwindowsizeof7\u00d77. For an ablation study, we consider four typical ob- A2.DetailedExperimentalSettings ject detection frameworks: Cascade Mask R-CNN [29, 6], ATSS [79], RepPoints v2 [12], and Sparse RCNN [56] in A2.1.ImageclassificationonImageNet-1K mmdetection [10]. For these four frameworks, we utilize thesamesettings: multi-scaletraining[8,56](resizingthe The image classification is performed by applying a input such that the shorter side is between 480 and 800 global average pooling layer on the output feature map of while the longer side is at most 1333), AdamW [44] opti- the last stage, followed by a linear classifier. We find this mizer(initiallearningrateof0.0001,weightdecayof0.05, strategytobeasaccurateasusinganadditionalclassto- andbatchsizeof16),and3xschedule(36epochswiththe ken as in ViT [20] and DeiT [63]. In evaluation, the top-1 learningratedecayedby10\u00d7atepochs27and33). accuracyusingasinglecropisreported. For system-level comparison, we adopt an improved HTC[9](denotedasHTC++)withinstaboost[22],stronger Regular ImageNet-1K training The training settings multi-scale training [7] (resizing the input such that the mostlyfollow[63]. Forallmodelvariants, weadoptade- shortersideisbetween400and1400whilethelongerside faultinputimageresolutionof2242. Forotherresolutions isatmost1600),6xschedule(72epochswiththelearning suchas3842,wefine-tunethemodelstrainedat2242 reso- rate decayed at epochs 63 and 69 by a factor of 0.1), soft- lution,insteadoftrainingfromscratch,toreduceGPUcon- NMS[5],andanextraglobalself-attentionlayerappended sumption. at the output of last stage and ImageNet-22K pre-trained 9 --- Page 10 --- downsp.rate Swin-T Swin-S Swin-B Swin-L (outputsize) concat4\u00d74,96-d,LN concat4\u00d74,96-d,LN concat4\u00d74,128-d,LN concat4\u00d74,192-d,LN 4\u00d7 (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) stage1 win.sz.7\u00d77, win.sz.7\u00d77, win.sz.7\u00d77, win.sz.7\u00d77, (56\u00d756) \u00d72 \u00d72 \u00d72 \u00d72 dim96,head3 dim96,head3 dim128,head4 dim192,head6 concat2\u00d72,192-d,LN concat2\u00d72,192-d,LN concat2\u00d72,256-d,LN concat2\u00d72,384-d,LN 8\u00d7 (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) stage2 win.sz.7\u00d77, win.sz.7\u00d77, win.sz.7\u00d77, win.sz.7\u00d77, (28\u00d728) \u00d72 \u00d72 \u00d72 \u00d72 dim192,head6 dim192,head6 dim256,head8 dim384,head12 concat2\u00d72,384-d,LN concat2\u00d72,384-d,LN concat2\u00d72,512-d,LN concat2\u00d72,768-d,LN 16\u00d7 (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) stage3 win.sz.7\u00d77, win.sz.7\u00d77, win.sz.7\u00d77, win.sz.7\u00d77, (14\u00d714) \u00d76 \u00d718 \u00d718 \u00d718 dim384,head12 dim384,head12 dim512,head16 dim768,head24 concat2\u00d72,768-d,LN concat2\u00d72,768-d,LN concat2\u00d72,1024-d,LN concat2\u00d72,1536-d,LN 32\u00d7 (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) stage4 win.sz.7\u00d77, win.sz.7\u00d77, win.sz.7\u00d77, win.sz.7\u00d77, (7\u00d77) \u00d72 \u00d72 \u00d72 \u00d72 dim768,head24 dim768,head24 dim1024,head32 dim1536,head48 Table7.Detailedarchitecturespecifications. modelasinitialization. Weadoptstochasticdepthwithra- Swin-T Swin-S Swin-B tioof0.2forallSwinTransformermodels. input top-1 throughput top-1 throughput top-1 throughput size acc (image/s) acc (image/s) acc (image/s) A2.3.SemanticsegmentationonADE20K 2242 81.3 755.2 83.0 436.9 83.3 278.1 2562 81.6 580.9 83.4 336.7 83.7 208.1 ADE20K [83] is a widely-used semantic segmentation 3202 82.1 342.0 83.7 198.2 84.0 132.0 dataset,coveringabroadrangeof150semanticcategories. 3842 82.2 219.5 83.9 127.6 84.5 84.7 Ithas25Kimagesintotal,with20Kfortraining,2Kforval- Table 8. Swin Transformers with different input image size on idation,andanother3Kfortesting.WeutilizeUperNet[69] ImageNet-1Kclassification. inmmsegmentation[16]asourbaseframeworkforitshigh efficiency. Backbone OptimizerAPboxAPb 5o 0xAPb 7o 5x APmaskAPm 50askAPm 75ask Intraining,weemploytheAdamW[44]optimizerwith R50 SGD 45.0 62.9 48.8 38.5 59.9 41.4 aninitiallearningrateof6\u00d710\u22125,aweightdecayof0.01, AdamW 46.3 64.3 50.5 40.1 61.7 43.4 SGD 47.8 65.9 51.9 40.4 62.9 43.5 a scheduler that uses linear learning rate decay, and a lin- X101-32x4d AdamW 48.1 66.5 52.4 41.6 63.9 45.2 ear warmup of 1,500 iterations. Models are trained on 8 SGD 48.8 66.9 53.0 41.4 63.9 44.7 GPUswith2imagesperGPUfor160Kiterations. Foraug- X101-64x4d AdamW 48.3 66.4 52.3 41.7 64.0 45.1 mentations,weadoptthedefaultsettinginmmsegmentation Table 9. Comparison of the SGD and AdamW optimizers for of random horizontal flipping, random re-scaling within ResNe(X)t backbones on COCO object detection using the Cas- ratio range [0.5, 2.0] and random photometric distortion. cadeMaskR-CNNframework. Stochastic depth with ratio of 0.2 is applied for all Swin Transformer models. Swin-T, Swin-S are trained on the A3.2.DifferentOptimizersforResNe(X)tonCOCO standard setting as the previous approaches with an input of512\u00d7512. Swin-BandSwin-Lwith\u2021indicatethatthese Table 9 compares the AdamW and SGD optimizers of two models are pre-trained on ImageNet-22K, and trained theResNe(X)tbackbonesonCOCOobjectdetection. The withtheinputof640\u00d7640. Cascade Mask R-CNN framework is used in this compar- Ininference,amulti-scaletestusingresolutionsthatare ison. While SGD is used as a default optimizer for Cas- [0.5, 0.75, 1.0, 1.25, 1.5, 1.75]\u00d7 of that in training is em- cade Mask R-CNN framework, we generally observe im- ployed. When reporting test scores, both the training im- provedaccuracybyreplacingitwithanAdamWoptimizer, agesandvalidationimagesareusedfortraining,following particularly for smaller backbones. We thus use AdamW commonpractice[71]. for ResNe(X)t backbones when compared to the proposed SwinTransformerarchitectures. A3.MoreExperiments A3.3.SwinMLP-Mixer A3.1.Imageclassificationwithdifferentinputsize We apply the proposed hierarchical design and the Table8liststheperformanceofSwinTransformerswith shifted window approach to the MLP-Mixer architec- different input image sizes from 2242 to 3842. In general, tures [61], referred to as Swin-Mixer. Table 10 shows the a larger input resolution leads to better top-1 accuracy but performanceofSwin-MixercomparedtotheoriginalMLP- withslowerinferencespeed. Mixer architectures MLP-Mixer [61] and a follow-up ap- 10 --- Page 11 --- image throughputImageNet endobjectdetectionwithtransformers.InEuropeanConfer- method #param.FLOPs size (image/s)top-1acc. enceonComputerVision,pages213\u2013229.Springer,2020.3, MLP-Mixer-B/16[61] 2242 59M 12.7G - 76.4 6,9 ResMLP-S24[62] 2242 30M 6.0G 715 79.4 [9] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaox- ResMLP-B24[62] 2242 116M 23.0G 231 81.0 iao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Swin-T/D24 2562 28M 5.9G 563 81.6 Shi,WanliOuyang,etal. Hybridtaskcascadeforinstance (Transformer) segmentation. InProceedingsoftheIEEE/CVFConference Swin-Mixer-T/D24 2562 20M 4.0G 807 79.4 on Computer Vision and Pattern Recognition, pages 4974\u2013 Swin-Mixer-T/D12 2562 21M 4.0G 792 79.6 4983,2019. 6,9 Swin-Mixer-T/D6 2562 23M 4.0G 766 79.7 [10] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Swin-Mixer-B/D24 2242 61M 10.4G 409 80.3 Xiong,XiaoxiaoLi,ShuyangSun,WansenFeng,ZiweiLiu, (noshift) JiaruiXu,etal. Mmdetection: Openmmlabdetectiontool- Swin-Mixer-B/D24 2242 61M 10.4G 409 81.3 boxandbenchmark.arXivpreprintarXiv:1906.07155,2019. Table10.PerformanceofSwinMLP-MixeronImageNet-1Kclas- 6,9 sification. Dindictesthenumberofchannelsperhead. Through- [11] Liang-ChiehChen,YukunZhu,GeorgePapandreou,Florian putismeasuredusingtheGitHubrepositoryof[68]andaV100 Schroff, and Hartwig Adam. Encoder-decoder with atrous GPU,following[63]. separableconvolutionforsemanticimagesegmentation. In ProceedingsoftheEuropeanconferenceoncomputervision (ECCV),pages801\u2013818,2018. 7 proach, ResMLP [61]. Swin-Mixer performs significantly [12] YihongChen,ZhengZhang,YueCao,LiweiWang,Stephen better than MLP-Mixer (81.3% vs. 76.4%) using slightly Lin,andHanHu. Reppointsv2: Verificationmeetsregres- smallercomputationbudget(10.4Gvs. 12.7G).Italsohas sionforobjectdetection. InNeurIPS,2020. 6,7,9 betterspeedaccuracytrade-offcomparedtoResMLP[62]. [13] Cheng Chi, Fangyun Wei, and Han Hu. Relationnet++: Theseresultsindicatetheproposedhierarchicaldesignand Bridgingvisualrepresentationsforobjectdetectionviatrans- theshiftedwindowapproacharegeneralizable. formerdecoder. InNeurIPS,2020. 3,7 [14] Krzysztof Marcin Choromanski, Valerii Likhosherstov,",
  "conclusion": "Conclusion ferent position embedding approaches. Swin-T with rela- tive position bias yields +1.2%/+0.8% top-1 accuracy on This paper presents Swin Transformer, a new vision ImageNet-1K, +1.3/+1.5 box AP and +1.1/+1.3 mask AP Transformer which produces a hierarchical feature repre- 8 --- Page 9 --- ImageNet COCO ADE20k When training from scratch with a 2242 input, we em- Backbone top-1top-5 APbox APmask mIoU ployanAdamW[37]optimizerfor300epochsusingaco- slidingwindow Swin-T 81.4 95.6 50.2 43.5 45.8 sinedecaylearningrateschedulerwith20epochsoflinear Performer[14] Swin-T 79.0 94.2 - - - warm-up. A batch size of 1024, an initial learning rate of shiftedwindow Swin-T 81.3 95.6 50.5 43.7 46.1 0.001, a weight decay of 0.05, and gradient clipping with Table 6. Accuracy of Swin Transformer using different",
  "references": "References David Dohan, Xingyou Song, Andreea Gane, Tamas Sar- los,PeterHawkins,JaredQuincyDavis,AfrozMohiuddin, [1] HangboBao,LiDong,FuruWei,WenhuiWang,NanYang, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, XiaodongLiu,YuWang,JianfengGao,SonghaoPiao,Ming and Adrian Weller. Rethinking attention with performers. Zhou,etal. Unilmv2: Pseudo-maskedlanguagemodelsfor In International Conference on Learning Representations, unifiedlanguagemodelpre-training. InInternationalCon- 2021. 8,9 ferenceonMachineLearning,pages642\u2013652.PMLR,2020. 5 [15] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and [2] JoshBeal, EricKim, EricTzeng, DongHukPark, Andrew HuaxiaXia. Dowereallyneedexplicitpositionencodings Zhai,andDmitryKislyuk. Towardtransformer-basedobject forvisiontransformers? arXivpreprintarXiv:2102.10882, detection. arXivpreprintarXiv:2012.09958,2020. 3 2021. 3 [3] IrwanBello,BarretZoph,AshishVaswani,JonathonShlens, [16] MMSegmentation Contributors. MMSegmentation: and Quoc V. Le. Attention augmented convolutional net- Openmmlab semantic segmentation toolbox and bench- works,2020. 3 mark.  [4] Alexey Bochkovskiy, Chien-Yao Wang, and Hong- mmsegmentation,2020. 8,10 Yuan Mark Liao. Yolov4: Optimal speed and accuracy of [17] EkinDCubuk,BarretZoph,JonathonShlens,andQuocV objectdetection. arXivpreprintarXiv:2004.10934,2020. 7 Le. Randaugment: Practical automated data augmenta- [5] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and tion with a reduced search space. In Proceedings of the LarryS.Davis. Soft-nms\u2013improvingobjectdetectionwith IEEE/CVF Conference on Computer Vision and Pattern onelineofcode. InProceedingsoftheIEEEInternational RecognitionWorkshops,pages702\u2013703,2020. 9 ConferenceonComputerVision(ICCV),Oct2017. 6,9 [18] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong [6] ZhaoweiCaiandNunoVasconcelos. Cascader-cnn: Delv- Zhang,HanHu,andYichenWei. Deformableconvolutional ingintohighqualityobjectdetection. InProceedingsofthe networks. InProceedingsoftheIEEEInternationalConfer- IEEEConferenceonComputerVisionandPatternRecogni- enceonComputerVision,pages764\u2013773,2017. 1,3 tion,pages6154\u20136162,2018. 6,9 [19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, [7] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han andLiFei-Fei. Imagenet: Alarge-scalehierarchicalimage Hu.Gcnet:Non-localnetworksmeetsqueeze-excitationnet- database. In2009IEEEconferenceoncomputervisionand worksandbeyond. InProceedingsoftheIEEE/CVFInter- patternrecognition,pages248\u2013255.Ieee,2009. 5 nationalConferenceonComputerVision(ICCV)Workshops, [20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Oct2019. 3,6,7,9 Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, [8] NicolasCarion,FranciscoMassa,GabrielSynnaeve,Nicolas MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl- Usunier,AlexanderKirillov,andSergeyZagoruyko.End-to- vainGelly,JakobUszkoreit,andNeilHoulsby. Animageis 11 --- Page 12 --- worth16x16words: Transformersforimagerecognitionat theIEEE/CVFInternationalConferenceonComputerVision scale. InInternationalConferenceonLearningRepresenta- (ICCV),pages3464\u20133473,October2019. 2,3,5 tions,2021. 1,2,3,4,5,6,9 [34] GaoHuang,ZhuangLiu,LaurensVanDerMaaten,andKil- [21] Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Golnaz Ghiasi, ian Q Weinberger. Densely connected convolutional net- Mingxing Tan, Yin Cui, Quoc V Le, and Xiaodan Song. works. InProceedingsoftheIEEEconferenceoncomputer Spinenet: Learning scale-permuted backbone for recogni- visionandpatternrecognition,pages4700\u20134708,2017.1,2 tionandlocalization. InProceedingsoftheIEEE/CVFCon- [35] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil- ferenceonComputerVisionandPatternRecognition,pages ianQWeinberger. Deepnetworkswithstochasticdepth. In 11592\u201311601,2020. 7 European conference on computer vision, pages 646\u2013661. [22] Hao-Shu Fang, Jianhua Sun, Runzhong Wang, Minghao Springer,2016. 9 Gou, Yong-Lu Li, and Cewu Lu. Instaboost: Boosting [36] David H Hubel and Torsten N Wiesel. Receptive fields, instance segmentation via probability map guided copy- binocularinteractionandfunctionalarchitectureinthecat\u2019s pasting.InProceedingsoftheIEEE/CVFInternationalCon- visual cortex. The Journal of physiology, 160(1):106\u2013154, ferenceonComputerVision,pages682\u2013691,2019. 6,9 1962. 3 [23] JunFu, JingLiu, HaijieTian, YongLi, YongjunBao, Zhi- [37] Diederik P Kingma and Jimmy Ba. Adam: A method for wei Fang, and Hanqing Lu. Dual attention network for stochastic optimization. arXiv preprint arXiv:1412.6980, scenesegmentation. InProceedingsoftheIEEEConference 2014. 5,9 on Computer Vision and Pattern Recognition, pages 3146\u2013 [38] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan 3154,2019. 3,7 Puigcerver, JessicaYung, SylvainGelly, andNeilHoulsby. Big transfer (bit): General visual representation learning. [24] JunFu,JingLiu,YuhangWang,YongLi,YongjunBao,Jin- arXivpreprintarXiv:1912.11370,6(2):8,2019. 6 hui Tang, and Hanqing Lu. Adaptive context network for scene parsing. In Proceedings of the IEEE/CVF Interna- [39] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. tional Conference on Computer Vision, pages 6748\u20136757, Imagenet classification with deep convolutional neural net- 2019. 7 works. In Advances in neural information processing sys- tems,pages1097\u20131105,2012. 1,2 [25] Kunihiko Fukushima. Cognitron: A self-organizing multi- [40] YannLeCun,Le\u00b4onBottou,YoshuaBengio,PatrickHaffner, layeredneuralnetwork. Biologicalcybernetics,20(3):121\u2013 et al. Gradient-based learning applied to document recog- 136,1975. 3 nition. ProceedingsoftheIEEE,86(11):2278\u20132324, 1998. [26] GolnazGhiasi,YinCui,AravindSrinivas,RuiQian,Tsung- 2 YiLin,EkinDCubuk,QuocVLe,andBarretZoph.Simple [41] YannLeCun,PatrickHaffner,Le\u00b4onBottou,andYoshuaBen- copy-pasteisastrongdataaugmentationmethodforinstance gio. Object recognition with gradient-based learning. In segmentation. arXivpreprintarXiv:2012.07177,2020. 2,7 Shape,contourandgroupingincomputervision,pages319\u2013 [27] JiayuanGu, HanHu, LiweiWang, YichenWei, andJifeng 345.Springer,1999. 3 Dai. Learningregionfeaturesforobjectdetection. InPro- [42] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, ceedings of the European Conference on Computer Vision Bharath Hariharan, and Serge Belongie. Feature pyramid (ECCV),2018. 3 networks for object detection. In The IEEE Conference [28] KaiHan,AnXiao,EnhuaWu,JianyuanGuo,ChunjingXu, onComputerVisionandPatternRecognition(CVPR),July andYunheWang.Transformerintransformer.arXivpreprint 2017. 2 arXiv:2103.00112,2021. 3 [43] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays, [29] KaimingHe,GeorgiaGkioxari,PiotrDolla\u00b4r,andRossGir- PietroPerona,DevaRamanan,PiotrDolla\u00b4r,andCLawrence shick.Maskr-cnn.InProceedingsoftheIEEEinternational Zitnick. Microsoft coco: Common objects in context. In conferenceoncomputervision,pages2961\u20132969,2017. 6, European conference on computer vision, pages 740\u2013755. 9 Springer,2014. 5 [30] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. [44] Ilya Loshchilov and Frank Hutter. Decoupled weight de- Deep residual learning for image recognition. In Proceed- cayregularization. InInternationalConferenceonLearning ingsoftheIEEEconferenceoncomputervisionandpattern Representations,2019. 6,9,10 recognition,pages770\u2013778,2016. 1,2,4 [45] Boris T Polyak and Anatoli B Juditsky. Acceleration of [31] EladHoffer,TalBen-Nun,ItayHubara,NivGiladi,Torsten stochastic approximation by averaging. SIAM journal on Hoefler,andDanielSoudry.Augmentyourbatch:Improving controlandoptimization,30(4):838\u2013855,1992. 6,9 generalizationthroughinstancerepetition.InProceedingsof [46] SiyuanQiao,Liang-ChiehChen,andAlanYuille.Detectors: theIEEE/CVFConferenceonComputerVisionandPattern Detectingobjectswithrecursivefeaturepyramidandswitch- Recognition,pages8129\u20138138,2020. 6,9 able atrous convolution. arXiv preprint arXiv:2006.02334, [32] HanHu,JiayuanGu,ZhengZhang,JifengDai,andYichen 2020. 2,7 Wei. Relation networks for object detection. In Proceed- [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya ingsoftheIEEEConferenceonComputerVisionandPattern Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Recognition,pages3588\u20133597,2018. 3,5 Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen [33] HanHu,ZhengZhang,ZhendaXie,andStephenLin. Local Krueger, and Ilya Sutskever. Learning transferable visual relationnetworksforimagerecognition. InProceedingsof modelsfromnaturallanguagesupervision,2021. 1 12 --- Page 13 --- [48] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, markforefficienttransformers. InInternationalConference Kaiming He, and Piotr Dolla\u00b4r. Designing network design onLearningRepresentations,2021. 8 spaces. In Proceedings of the IEEE/CVF Conference on [61] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lu- Computer Vision and Pattern Recognition, pages 10428\u2013 casBeyer,XiaohuaZhai,ThomasUnterthiner,JessicaYung, 10436,2020. 6 Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario [49] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee, Lucic,andAlexeyDosovitskiy. Mlp-mixer: Anall-mlpar- SharanNarang, MichaelMatena, Yanqi Zhou, WeiLi, and chitectureforvision,2021. 2,10,11 PeterJ.Liu. Exploringthelimitsoftransferlearningwitha [62] HugoTouvron,PiotrBojanowski,MathildeCaron,Matthieu unifiedtext-to-texttransformer. JournalofMachineLearn- Cord, Alaaeldin El-Nouby, Edouard Grave, Gautier Izac- ingResearch,21(140):1\u201367,2020. 5 ard,ArmandJoulin,GabrielSynnaeve,JakobVerbeek,and [50] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Herve\u00b4Je\u00b4gou.Resmlp:Feedforwardnetworksforimageclas- Bello,AnselmLevskaya,andJonShlens. Stand-aloneself- sificationwithdata-efficienttraining,2021. 11 attentioninvisionmodels. InAdvancesinNeuralInforma- [63] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco tionProcessingSystems,volume32.CurranAssociates,Inc., Massa,AlexandreSablayrolles,andHerve\u00b4 Je\u00b4gou. Training 2019. 2,3 data-efficient image transformers & distillation through at- [51] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U- tention. arXivpreprintarXiv:2012.12877,2020. 2,3,5,6, net: Convolutionalnetworksforbiomedicalimagesegmen- 9,11 tation. InInternationalConferenceonMedicalimagecom- [64] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko- puting and computer-assisted intervention, pages 234\u2013241. reit,LlionJones,AidanNGomez,\u0141ukaszKaiser,andIllia Springer,2015. 2 Polosukhin.Attentionisallyouneed.InAdvancesinNeural InformationProcessingSystems,pages5998\u20136008,2017.1, [52] K. Simonyan and A. Zisserman. Very deep convolutional 2,4 networksforlarge-scaleimagerecognition. InInternational [65] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, ConferenceonLearningRepresentations,May2015. 2,4 ChaoruiDeng,YangZhao,DongLiu,YadongMu,Mingkui [53] Bharat Singh and Larry S Davis. An analysis of scale in- Tan,XinggangWang,etal. Deephigh-resolutionrepresen- variance in object detection snip. In Proceedings of the tationlearningforvisualrecognition. IEEEtransactionson IEEE conference on computer vision and pattern recogni- patternanalysisandmachineintelligence,2020. 3 tion,pages3578\u20133587,2018. 2 [66] WenhaiWang,EnzeXie,XiangLi,Deng-PingFan,Kaitao [54] Bharat Singh, Mahyar Najibi, and Larry S Davis. Sniper: Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Efficientmulti-scaletraining. InAdvancesinNeuralInfor- Pyramid vision transformer: A versatile backbone for mationProcessingSystems, volume31.CurranAssociates, dense prediction without convolutions. arXiv preprint Inc.,2018. 2 arXiv:2102.12122,2021. 3 [55] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon [67] XiaolongWang,RossGirshick,AbhinavGupta,andKaim- Shlens, Pieter Abbeel, and Ashish Vaswani. Bottle- ing He. Non-local neural networks. In IEEE Conference neck transformers for visual recognition. arXiv preprint on Computer Vision and Pattern Recognition, CVPR 2018, arXiv:2101.11605,2021. 3 2018. 3 [56] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chen- [68] Ross Wightman. Pytorch image mod- feng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan els.  Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end pytorch-image-models,2019. 6,11 object detection with learnable proposals. arXiv preprint [69] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and arXiv:2011.12450,2020. 3,6,9 JianSun. Unifiedperceptualparsingforsceneunderstand- [57] ChristianSzegedy,WeiLiu,YangqingJia,PierreSermanet, ing. In Proceedings of the European Conference on Com- Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent puterVision(ECCV),pages418\u2013434,2018. 7,8,10 Vanhoucke, and Andrew Rabinovich. Going deeper with [70] SainingXie,RossGirshick,PiotrDolla\u00b4r,ZhuowenTu,and convolutions. In Proceedings of the IEEE conference on KaimingHe. Aggregatedresidualtransformationsfordeep computer vision and pattern recognition, pages 1\u20139, 2015. neural networks. In Proceedings of the IEEE Conference 2 on Computer Vision and Pattern Recognition, pages 1492\u2013 [58] MingxingTanandQuocLe. Efficientnet:Rethinkingmodel 1500,2017. 1,2,3 scalingforconvolutionalneuralnetworks. InInternational [71] MinghaoYin,ZhuliangYao,YueCao,XiuLi,ZhengZhang, ConferenceonMachineLearning,pages6105\u20136114.PMLR, Stephen Lin, and Han Hu. Disentangled non-local neural 2019. 3,6 networks. In Proceedings of the European conference on [59] MingxingTan,RuomingPang,andQuocVLe.Efficientdet: computervision(ECCV),2020. 3,7,10 Scalable and efficient object detection. In Proceedings of [72] LiYuan,YunpengChen,TaoWang,WeihaoYu,YujunShi, the IEEE/CVF conference on computer vision and pattern FrancisEHTay, JiashiFeng, andShuichengYan. Tokens- recognition,pages10781\u201310790,2020. 7 to-token vit: Training vision transformers from scratch on [60] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, imagenet. arXivpreprintarXiv:2101.11986,2021. 3 DaraBahri,PhilipPham,JinfengRao,LiuYang,Sebastian [73] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object- Ruder, and Donald Metzler. Long range arena : A bench- contextual representations for semantic segmentation. In 13 --- Page 14 --- 16thEuropeanConferenceComputerVision(ECCV2020), August2020. 7 [74] YuhuiYuanandJingdongWang. Ocnet:Objectcontextnet- work for scene parsing. arXiv preprint arXiv:1809.00916, 2018. 3 [75] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun,JunsukChoe,andYoungjoonYoo. Cutmix: Regular- izationstrategytotrainstrongclassifierswithlocalizablefea- tures. InProceedingsoftheIEEE/CVFInternationalCon- ferenceonComputerVision,pages6023\u20136032,2019. 9 [76] SergeyZagoruykoandNikosKomodakis.Wideresidualnet- works. InBMVC,2016. 1 [77] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and DavidLopez-Paz. mixup: Beyondempiricalriskminimiza- tion. arXivpreprintarXiv:1710.09412,2017. 9 [78] HangZhang,ChongruoWu,ZhongyueZhang,YiZhu,Zhi Zhang, Haibin Lin, Yue Sun, Tong He, Jonas Mueller, R Manmatha, etal. Resnest: Split-attentionnetworks. arXiv preprintarXiv:2004.08955,2020. 7,8 [79] ShifengZhang, ChengChi, YongqiangYao, ZhenLei, and Stan Z Li. Bridging the gap between anchor-based and anchor-freedetectionviaadaptivetrainingsampleselection. In Proceedings of the IEEE/CVF Conference on Computer VisionandPatternRecognition,pages9759\u20139768,2020. 6, 9 [80] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Explor- ingself-attentionforimagerecognition. InProceedingsof theIEEE/CVFConferenceonComputerVisionandPattern Recognition,pages10076\u201310085,2020. 3 [81] SixiaoZheng, JiachenLu, HengshuangZhao, XiatianZhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang,PhilipHSTorr,etal. Rethinkingsemanticsegmen- tation from a sequence-to-sequence perspective with trans- formers. arXiv preprint arXiv:2012.15840, 2020. 2, 3, 7, 8 [82] ZhunZhong,LiangZheng,GuoliangKang,ShaoziLi,and YiYang.Randomerasingdataaugmentation.InProceedings oftheAAAIConferenceonArtificialIntelligence,volume34, pages13001\u201313008,2020. 9 [83] BoleiZhou, HangZhao, XavierPuig, TeteXiao, SanjaFi- dler,AdelaBarriuso,andAntonioTorralba.Semanticunder- standingofscenesthroughtheade20kdataset. International JournalonComputerVision,2018. 5,7,10 [84] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. De- formableconvnetsv2: Moredeformable, betterresults. In Proceedings of the IEEE Conference on Computer Vision andPatternRecognition,pages9308\u20139316,2019. 1,3 [85] XizhouZhu,WeijieSu,LeweiLu,BinLi,XiaogangWang, andJifengDai. Deformable{detr}: Deformabletransform- ersforend-to-endobjectdetection. InInternationalConfer- enceonLearningRepresentations,2021. 3 14"
}