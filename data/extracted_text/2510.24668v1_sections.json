{
  "abstract": "ABSTRACT Languageagentshavedemonstratedremarkablepotentialinwebsearchandinfor- mationretrieval. However,thesesearchagentsassumeuserqueriesarecomplete andunambiguous,anassumptionthatdivergesfromrealitywhereusersbeginwith incomplete queries requiring clarification through interaction. Yet most agents lackinteractivemechanismsduringthesearchprocess, andexistingbenchmarks cannotassessthiscapability. Toaddressthisgap,weintroduceINTERACTCOMP, abenchmarkdesignedtoevaluatewhethersearchagentscanrecognizequeryam- biguityandactivelyinteracttoresolveitduringsearch. Followingtheprincipleof easytoverify,interacttodisambiguate,weconstruct210expert-curatedquestions across 9 domains through a target-distractor",
  "methods": "methodology. We deliberately pairantargetentitywithasimilarpopularentity(thedistractor),craftingquestionsusingonlytheir sharedattributeswhilehidingdistinctiveinformationascontext. Thisconstructionensuresthat: (1) questions admit multiple plausible interpretations including the popular distractor, making direct answeringunreliable;(2)thetargetanswerpossessesalldescribedattributes,ensuringverifiability; and(3)distinctiveattributeshiddenincontextprovidecleardisambiguationpathsthroughinterac- tion. Algorithm1formalizesthispipeline, whichwedetailinthefollowingsubsectionsalongside ourtwo-stageverificationprocess. 3.2.1 CONSTRUCTIONPROCESS Annotatorsreceivethefollowinginstruction: \u201cYouneedtofindapairofentitiesthataresimilarbutdifferinpopularity. Usetheirshared attributestoconstructanambiguousquestion,andreservetheremainingdistinctiveattributesto formthecontext.\u201d Followingthisinstruction,theconstructionproceedsinfoursteps: (1)EntitySelection: annotators identify a lesser-known target and a popular distractor sharing overlapping characteristics; (2) At- tributeCategorization: attributesareclassifiedasshared(commontoboth)ordistinctive(unique to target); (3) Question Formulation: only shared attributes are used to create questions admit- ting multiple plausible candidates; (4) Context Formation: distinctive attributes are reserved as 4 --- Page 5 --- context, ensuring question-context pairs uniquely identify the target while questions alone remain ambiguous. 3.2.2 VERIFICATIONPROCESS Weimplementatwo-stageverificationprotocoltoensuredataqualityandinteractionnecessity. Stage1: CompletenessVerification. Independentannotatorsvalidatethreerequirements: (1)the targetanswermustpossessallattributesdescribedinboththequestionandcontext,(2)thequestion- context combination must admit only one valid answer with no plausible alternatives, and (3) in- stanceswhereannotatorsidentifyvalidalternativeanswersarediscardedandreconstructed. Stage 2: Interaction Necessity Validation. We verify that questions truly require interaction through two complementary checks. First, we manually confirm questions cannot be confidently resolvedthroughdirectwebsearch,checkingthefirstfiveGoogleresultpages. Second,weconduct automatedtestingwiththreecapablemodels(GPT-5,GPT-5-mini,Claude-Sonnet-4)across5-round trials where models have access to search but no interaction. Questions successfully answered by twoormoremodelswithoutinteractionareflaggedasinsufficientlyambiguousandundergorevision tostrengthentheirambiguity. 3.3 DATASTATISTICS Figure2: Topicdistributionandquestion/contextlengthstatisticsinINTERACTCOMP. Inthissection,wepresentstatisticsonthetopicdistribution,questionandcontextlengthdistribution ofourcuratedINTERACTCOMPdataset. Topic distribution. Figure 2 presents the distribution of samples across 9 topic domains in the INTERACTCOMPdataset. ThemostrepresentedcategoriesincludeScience&Engineering(21.3%), Humanities(18.0%),andEntertainment(16.6%). ThedatasetalsofeaturesBusiness&Economics (11.8%), Law & Politics (8.5%), and Sports (7.1%). Conversely, domains like Medicine & Life Science(5.7%),Academic&Research(4.7%),andGeneralKnowledge&Misc. (6.2%)havefewer samples. Question and Context Length distribution. Figure 2 illustrates the distribution of question and context lengths in the INTERACTCOMP dataset. Question length predominantly ranges between 40 to 80 words, with the majority falling within this interval. Context length shows a broader distribution,typicallyspanningfrom40toover200words,withpeakfrequencyinthe60-100word range. These distributions demonstrate that questions are concise yet informative, while contexts providecomprehensivedisambiguationinformation. 5 --- Page 6 --- Languagedistribution. The INTERACTCOMP datasetcomprisesbilingualinstanceswithEnglish accountingfor139samples(66.19%)andChinesecontributing71samples(33.81%),enablingeval- uationofinteractioncapabilitiesacrossdifferentlinguisticcontexts. 4",
  "introduction": "1 INTRODUCTION Language agents have demonstrated remarkable potential across diverse domains, including code generation (Zhang et al., 2025; Hong et al., 2024b) , data analysis (Hong et al., 2024a; Li et al., 2025b;a), information retrieval (Geng et al., 2025; Song et al., 2025), and decision-making (Liu etal.,2025a;Liangetal.,2025).Anotabletrendistherapiddevelopmentofsearchagents(OpenAI, 2025d;Google,2025b), whichcanhandlecomplexuserqueriesandgatherinformationacrossthe internetbyperformingsearch,browse,andreasoningactions(Mialonetal.,2023;Weietal.,2025). However, these advanced search agents assume user queries are complete and unambiguous. In practice,usersbeginwithincompletequeriesadmittingmultipleplausibleinterpretations,andonly throughinteractioncanthetrueintentbeidentified. Yetmostsearchagentslackinteractivemecha- nismsduringsearch. Commercialagents(OpenAI,2025d)engageinasingleclarification,withno further interaction once search begins. When faced with ambiguity, agents confidently commit to assumedqueries,leadingtoincorrectanswersandwastedcomputationalresources. Existing benchmarks cannot assess this capability. Search benchmarks like GAIA (Mialon et al., 2023)andBrowseComp(Weietal.,2025)provideallnecessaryresourcesupfront,enablingagents \u2217Theseauthorscontributedequallytothiswork.  1 5202 tcO 82 ]LC.sc[ 1v86642.0152:viXra --- Page 2 --- Acc(%) (a) BrowseComp Acc(%) (b) InteractComp ChatGPT Agent 70 SOTA (OpenAI) 70 SOTA 60 SOTA (Others) Ultra8x 60 Language Models Language Models Deep Research 50 Ultra 50 40 40 Grok 4 30 30 Claude-4-Opus 20 20 DeepSeek-R1GPT-5 10 OpenAI o1 Sonar pG re om Ai Pn Ii 2.5 Pro 10 GPT-4o Qwen-2.5-72B GPT-4.1 GPT-4o Qwen-2.5-72B 0 0 May Sep Jan May Sep May Sep Jan May Sep Time (May 2024 Sep 2025) Time (May 2024 Sep 2025) Figure 1: Despite rapid progress on complete search queries (BrowseComp: seven-fold over 15 months),agentperformanceonambiguous,interaction-dependentqueries(InteractComp)hasstag- natedaround6-14%. Thisgrowingdisparityrevealsacriticalblindspotinagentdevelopment. toproceedwithoutclarifyingambiguousintent. InteractionbenchmarkslikeIN3(Qianetal.,2024) and Tau-Bench (Yao et al., 2024) focus on general conversation but lack grounding in verifiable search tasks. Neither addresses the question: Can agents recognize query ambiguity and actively interact to gather disambiguating information during search? Without proper assessment of this capability,wecannotdeterminewhetherrecentadvancesinsearchagentstranslatetohandlingreal- worldscenarioswhereuserintentmustbeuncoveredratherthanassumed. Motivated by this gap, we introduce INTERACTCOMP, a benchmark designed to evaluate whether searchagentscanrecognizeambiguityandactivelyinteracttoresolveit. Ourdesignfollowsacore principle: easytoverify,interacttodisambiguate. Questionshaveshort,verifiableanswers(1-2 words) that are answerable with enough context, yet require interaction to obtain specific details neededfordisambiguation. Weachievethisthroughatarget-distractordesign: questionsuseonly shared attributes of a lesser-known target and a popular alternative, creating genuine ambiguity that search alone cannot resolve. Agents must interact with simulated users to uncover distinctive attributes not given in the initial query. INTERACTCOMP contains 210 expert-curated questions across 9 domains in both English and Chinese, validated to ensure interaction is necessary and answersareverifiable. Systematicevaluationof17modelsconfirmsourdesignprincipleandrevealsastrikingfailurepat- tern. When provided complete disambiguating context, models achieve strong performance with the best reaching 71.50% accuracy, validating questions are answerable once information is com- plete. However,eventhebestmodelachievesonly13.73%inthefullinteractionsetting,withmost models in single digits. This 5\u00d7 performance gap exposes the core problem: models fail not due tosearchandreasoningdeficits,butsystematicoverconfidencethatpreventsthemfromengagingin interactiondespitehavingaccesstoit. Scaling",
  "experiments": "EXPERIMENTS 4.1 EXPERIMENTALSETUP To systematically evaluate agent capabilities across different interaction paradigms, we design a controlledexperimentalframeworkthatisolatesandmeasurestheincrementalcontributionofcore agentcapabilities: knowledgerecall,informationretrieval,andinteractiveclarification. Agent Architecture: We employ the ReAct framework (Yao et al., 2023) as our base architec- ture, implementingthreecomplementaryconfigurations: (1)Answer-only: directresponsegenera- tion testing pure knowledge recall, (2) Answer+Search: incorporating web search for information retrieval, and (3) Answer+Search+Interact: adding interactive clarification through interact with responder. This design enables measurement of capability increments while maintaining architec- tural consistency. To further investigate interaction behavior, we implement a forced-interaction variantforablationstudiesthatrequiresminimuminteractionthresholdsbeforeanswergeneration. ImplementationdetailsareprovidedinAppendixA.1. Models: We evaluate across diverse model families including proprietary models (GPT-4o-mini, GPT-4o, GPT-4.1, GPT-5, OpenAI o3, Grok-4, Doubao-1.6, Claude-Sonnet-4, Claude-Opus-4, Claude-3.5-Sonnet) and open-weight models (GLM-4.5, Kimi-K2, Deepseek-V3.1, Deepseek-R1, Qwen3-235B-A22B,Qwen2.5). Followingestablishedbenchmarkingpractices,westandardizepa- rameters where supported: temperature=0.6, top p=0.95.We employ GPT-4o (temperature=0.0) as ourgrader,providinggroundtruth,agentresponse,andquestioncontextforbinarycorrectnessjudg- ments.WeimplementrespondersimulationusingGPT-4o(temperature=1.0)thatprovidesstructured feedbackwhenagentsemploytheinteractaction. Metrics: We evaluate agents across five key dimensions: (1) Interaction Metrics: Round (aver- age number of conversation turns) and percentage of rounds where interact actions are used (IR) measuring behavioral patterns and action utilization; (2) Performance Metrics: Accuracy (Acc.) measuring the percentage of correctly answered queries, and Calibration Error (C.E.) measuring confidencecalibrationusing5confidencebins;and(3)Cost: measuredinUSDreflectingcomputa- tionalresourcesusageforpracticaldeploymentconsiderations. 4.2 MAINRESULTS Table2presentscomprehensiveresultsacross17models,revealingstrikingpatternsinhowdifferent architectureshandleambiguousqueries. Theresultsexposefundamentallimitationseveninstate- of-the-art systems, with the highest-performing model (GPT-5) achieving only 13.73% accuracy, demonstratingthebenchmark\u2019schallengingnature. Diverse Interaction Patterns Across Models. Models exhibit dramatically different interaction strategies,creatingdistinctbehavioralprofiles. GPT-4o-ministandsoutasanextremecase: itasks questionsin73.95%ofavailablerounds,byfarthehighestinteractionrate,yetachievesonly7.14% accuracy\u2014closetoGLM-4.5whichbarelyinteracts(0.25%IR).Thissuggeststhatexcessiveques- tioning without clear purpose can be counterproductive. Conversely, DeepSeek-R1 demonstrates morebalancedbehaviorwith44.72%IRyielding13.08%accuracy,thehighestamongopen-weight models,indicatingthatwillingnesstointeractcantranslatetobetterperformancewhenusedeffec- tively. Calibration Quality Correlates with Interaction Patterns. A remarkable finding is that models withhigherinteractionratesoftenexhibitsuperiorcalibration. GPT-4o-mini\u2019saggressivequestion- ing strategy, while not improving accuracy,",
  "related_work": "RELATED WORK Search Benchmarks and Agents. Recent benchmarks evaluate search agents along two dimen- sions. Web-scalesearchbenchmarkslikeBrowseComp(Weietal.,2025)assessinformationgather- ingacrosstheentirewebwithcompletequeries,spawningvariantsforChinese(Zhouetal.,2025a), multimodalcontent(Lietal.,2025d),andenhancedquestions(Chenetal.,2025). Tool-augmented benchmarkslikeGAIA(Mialonetal.,2023)andWebWatcher(Gengetal.,2025)additionallyre- quire agents to handle multimedia and perform computations. These benchmarks have motivated diverseagentdesigns. ReinforcementlearningapproacheslikeR1-Searcher(Songetal.,2025)and Search-R1(Jinetal.,2025)learnintegratedsearch-reasoningpatterns,whiledatasynthesismethods like WebSailor (Li et al., 2025c) and WebExplorer (Liu et al., 2025b) enhance long-horizon capa- bilities. Additionally,bothmanuallydesignedandself-designedsearchagents(Zhangetal.,2025; Zeng et al., 2025; Teng et al., 2025) have achieved strong performance through careful workflow engineering. Interaction Benchmarks and Agents. Complementary to search benchmarks, recent work eval- uates agents\u2019 interaction capabilities. SWEET-RL (Zhou et al., 2025b) proposes ColBench for multi-turncollaborativereasoningwithRL-basedcreditassignmentacrossturns. UserBench(Qian etal.,2025a)andUserRL(Qianetal.,2025b)creategymenvironmentsfortrainingagentsonuser- centrictaskswheregoalsareunderspecifiedandpreferencesemergeincrementally.IN3(Qianetal., 2024) and Tau-Bench (Yao et al., 2024) evaluate implicit intention understanding and tool-agent- user interaction respectively. These benchmarks collectively reveal that current models struggle with proactive clarification and user alignment\u2014for instance, agents uncover fewer than 30% of userpreferencesthroughactivequestioninginUserBench. However,thesebenchmarksprimarilyfocusongeneralconversationalsettingsortool-usescenarios, lacking grounding in search tasks where intent errors lead to objectively wrong retrieval",
  "results": "result as (cid:44)\u2192 observation. You should think step by step, and output the action you want to execute. ### Evidence first Before answering, you MUST: 1. Identify ALL missing information dimensions (time, scope, context, conditions etc.) 2. Systematically gather evidence for each dimension 3. Verify key assumptions through multiple sources/questions 4. Only answer when you can confidently justify each part of your response **Critical**: Most questions have hidden complexities. Your initial understanding is (cid:44)\u2192 likely incomplete. ### Using ask When the ask action is available, you may pose closed-ended questions to fill gaps such (cid:44)\u2192 as time, scope, conditions, relationships, or quantities. - Do **not** ask the user to confirm a complete candidate answer or entity name. (cid:44)\u2192 request neutral attributes or other missing evidence instead. **Important: When you choose the ask action, you can only ask closed-ended, yes/no (cid:44)\u2192 questions. The user will only respond with \"yes\", \"no\", or \"I don't know\".** ## Available actions: {actions} ## Output Format When you output the action, you should output the action name and parameters in the json format, and only one (cid:44)\u2192 action. Such as, ```json {{ \"action\": \"\", \"params\": {{ \"<param_name>\": \"<param_value>\" 13 --- Page 14 --- }} }} ``` Before output, you should think step by step. ## Question {question} \"\"\" ACT_PROMPT = \"\"\" ## Memory {memory} ## Observation Last action: {last_action} Observation: {last_observation} ## Question {question} ## Action You should output the action you want to execute. Output your next action in JSON format, e.g. ```json {{ \"action\": \"\", \"params\": {{ \"<param_name>\": \"<param_value>\" }} }} ``` ## ROUNDS Current round: {round_info} You have only one opportunity to provide your final answer. Use your remaining rounds wisely to collect evidence and test your theories before (cid:44)\u2192 committing to an answer. The above shows your remaining action rounds. \"\"\" FINAL_ROUND_ACT_PROMPT = \"\"\" Given the question and information you have gathered, output the final answer. ## Round {round_info} ## Memory {memory} ## Question {question} ## Action You should output the answer action, you can think step by step before you output the (cid:44)\u2192 answer. Return the final answer action in JSON, for example: ```json {{ \"action\": \"answer\", \"params\": {{ \"answer\": \"<param_value>\", \"confidence\": \"<param_value>\" }} }} ``` \"\"\" A.2 RESPONDERSIMULATION We implement a controlled responder simulation using GPT-4o (temperature=1.0) that provides structuredfeedbackwhenagentsemploytheaskaction.Uponreceivingagentqueries,theresponder evaluates questions against available context and responds with one of three standardized options: \u201dyes\u201d,\u201dno\u201d,or\u201dIdon\u2019tknow\u201d. Theresponderstates consistsofthegivencontextandinteraction r history, with transitions T : (s ,q ) \u2192 o \u2208 {yes,no,unknown} conditioned on context- r r agent r 14 --- Page 15 --- questionalignment. WhilemaintainingresponsediversitythroughLLMgeneration,theconstrained outputformatensuresevaluationconsistency. Thecompleteresponderpromptsaredetailedbelow. Prompt RESPONDER_PROMPT = \"\"\" You are a specialized Q&A agent. Think step by step before you output the answer. Rules: - Reply with exactly one of: yes, no, or i don't know. - Treat the context as the entire truth. - Use only the provided CONTEXT to judge the yes/no question. - Answer **yes** only if the context clearly states the proposition is correct. - Answer **no** if the context contradicts the proposition (for example it states an (cid:44)\u2192 incompatible attribute). - If the context neither confirms nor denies it, answer **i don't know**. - Do not rely on outside knowledge, analogies, or multi-hop guesses. Compare the (cid:44)\u2192 relevant words directly. CONTEXT {context} QUESTION {question} Output: yes | no | i don't know \"\"\" A.3 EVALUATIONPROTOCOL Wevalidatesimulationreliabilitythroughrepeatedsamplingacrossidenticalcontext\u2013questionpairs acrossk =3trials,indicatingstablebehaviordespitethestochasticgenerationprocess. Weemploy GPT-4O (temperature = 0.0) as our grader, providing ground truth, agent response, and ques- tioncontextforbinarycorrectnessjudgments. Graderreliabilityisvalidatedthroughspot-checking againsthumanevaluation. Thecompleteresponderpromptsaredetailedbelow. Prompt GRADING_PROMPT = \"\"\" \\nYou are an impartial grader. Question: {question} Predicted Answer: {predicted_answer} Correct Answer: {correct_answer} CRITICAL GRADING INSTRUCTIONS: 1. The predicted answer must match the CORRECT ANSWER 2. Look for EXACT name matches or clear",
  "conclusion": "CONCLUSION This paper presents INTERACTCOMP, a benchmark designed to evaluate a critical yet overlooked capabilityofsearchagents:recognizingandresolvingambiguousqueriesthroughactiveinteraction. Whileexistingsearchbenchmarkshavedrivenremarkableprogressinretrievalandreasoning,they uniformly assume users provide complete queries from the outset\u2014an assumption that diverges from real-world behavior where users begin with incomplete information needs. By constructing questionsthatareeasytoverifyoncesufficientcontextisgatheredyetimpossibletodisambiguate withoutinteraction,INTERACTCOMPsystematicallyevaluateswhetheragentscanrecognizeambi- guityandactivelyseekclarificationduringsearch. Our evaluation of 17 models reveals systematic overconfidence as the primary bottleneck rather thancapabilitydeficits. Modelsachieve68-72%accuracywhenprovidedcompletecontextbutonly 13.73%withinteractionavailable,severelyunderutilizingclarifyingquestionsdespitetheiraccess. Forcedinteractionexperimentsconfirmthisisastrategicfailure\u2014whencompelledtointeract, ac- curacy doubles, demonstrating latent capabilities current strategies fail to engage. Longitudinal analysis reinforces this diagnosis: while BrowseComp performance improved seven-fold over 15 months, INTERACTCOMP scores remained stagnant, exposing a critical blind spot where progress in retrieval has not translated to progress in interaction. Beyond diagnosis, the grounded nature ofsearchprovidescleanrewardsignalsfortraining,makingINTERACTCOMPwell-suitedforrein- forcement learning approaches to develop uncertainty-aware, actively interactive agents. We hope this benchmark provides the foundation for systematic progress on this neglected but essential di- mensionofagentdevelopment. 9 --- Page 10 ---",
  "references": "references to the same entity 3. Consider different languages, translations, or alternative names as potential (cid:44)\u2192 matches 4. Be strict: partial matches or vague similarities should be 'no' IMPORTANT: Give ONLY one score: - 'yes': The predicted answer correctly identifies the same entity as the correct (cid:44)\u2192 answer - 'no': The predicted answer is wrong, matches the popular answer, or refers to a (cid:44)\u2192 different entity Respond with ONLY 'yes' or 'no', nothing else.\"\"\" 15 --- Page 16 --- A.4 DATACONSTRUCTIONPIPELINE TableA1: DataConstructionPipeline: Step-by-StepExample Step Component ExampleContent TargetEntityA Hornussen(Swissteamstrikingsport) Step1 DistractorB Baseball(globallypopularteambat-and-ballsport) SharedAttributes Team-basedstrikinggame;offense/defensealternation;play- Step2 ers take turns hitting; projectiles reach very high speeds (>100 mph); protective gear required; governed by formal associationsorleagues. Distinctive Hornussen: strikesaplasticpuck(\u201cNouss\u201d)withwhip-like Attributes swing using a long wooden rod; defenders intercept with wooden boards; fan-shaped field \u223c300m; 18\u201320 defenders spreadinwideformation;scoringdependsondistance/land- ingpoint. Step3 Ambiguous \u201cWhichteam-basedstrikingsportfeaturestwosidesalternat- QuestionQ ingoffenseanddefense,whereindividualssequentiallyhita high-speedprojectileandteammatescoordinatetointercept it in the air? Outcomes depend on whether the projectile is intercepted or lands within the valid playing field. Defense relies on wide positioning and collaboration, all offensive players take turns striking, flight speeds often exceed 100 mph, protective gear is required due to impact risk, and the sportisgovernedbylong-standingassociationsorleagues.\u201d Step4 Contextual \u2013 Struck object is a plastic puck, resembling an ice hockey Information puck. \u2013Strikingmethodusesawhip-likeswingwithalongwooden rod. \u2013Defendersusewoodenboardstoblockthepuckinmid-air. \u2013Field: fanshape,\u223c300mlong,10\u201312\u00b0angle. \u2013Defensiveline: 18\u201320players. \u2013Scoring: distance/landing-based. Step5 ReasoningPath Q gives a plausible candidate set (e.g., Baseball vs Hor- nussen). AddingcontextclarifiesuniqueHornussenfeatures (puck,whipswing,fan-shapedfield,defensiveboards),lead- ingtotheuniqueanswer=Hornussen. 16 --- Page 17 --- A.5 ROUNDSCONSTRAINTSRESULTS Table A2: Performance comparison of models under varying average interaction levels. Metrics includeaccuracy(%),expectedcalibrationerror(ECE,%),andaverageroundsofinteraction(Inter- action). Interaction Accuracy(%) ECE(%) Setting GPT-5 1.14 14.0 71.50 SCALING 1.76 16.0 71.54 SCALING 1.90 20.0 70.06 SCALING 4.40 24.0 63.34 FORCED 6.10 20.0 69.02 FORCED 8.32 32.0 54.68 FORCED 9.40 40.0 48.20 FORCED 11.56 40.0 46.86 FORCED DeepSeek-Chat 0.38 10.0 77.00 SCALING 0.74 8.0 80.30 SCALING 1.54 10.0 75.20 SCALING 5.40 20.0 62.30 FORCED 6.68 22.0 52.60 FORCED 9.26 22.0 61.30 FORCED 10.82 16.0 66.40 FORCED 12.62 24.0 61.20 FORCED Claude-Sonnet-4 0.16 6.0 79.90 SCALING 0.70 4.0 80.24 SCALING 0.78 8.0 81.84 SCALING 3.12 12.0 75.80 FORCED 4.26 12.0 76.90 FORCED 6.10 10.0 76.00 FORCED 8.02 16.0 69.10 FORCED 10.46 18.0 68.40 FORCED GPT-4o-mini 2.00 4.0 49.50 SCALING 3.62 8.0 47.60 SCALING 2.76 8.0 33.20 SCALING 14.50 4.0 65.50 FORCED 16.50 4.0 69.70 FORCED 14.88 2.0 62.60 FORCED 11.18 6.0 56.10 FORCED 14.92 2.0 66.70 FORCED 17"
}