{
  "abstract": "Abstract\u2014Infrastructure-basedperceptionplaysacrucialrole in intelligent transportation systems, offering global situational awarenessandenablingcooperativeautonomy.However,existing camera-based detection models often underperform in such sce- nariosduetochallengessuchasmulti-viewinfrastructuresetup, diverse camera configurations, degraded visual inputs, and vari- ous road layouts. We introduce MIC-BEV, a Transformer-based bird\u2019s-eye-view (BEV) perception framework for infrastructure- based multi-camera 3D object detection. MIC-BEV flexibly sup- portsavariablenumberofcameraswithheterogeneousintrinsic and extrinsic parameters and demonstrates strong robustness under sensor degradation. The proposed graph-enhanced fusion module in MIC-BEV integrates multi-view image features into Cam1 Cam2 the BEV space by exploiting geometric relationships between c)Camera-BEVGrid Geometric Relation cameras and BEV cells alongside latent visual cues. To support t fora rin inin frg asa tn rud cte uv ra el -u ba at sio en d, ow be jecin tt dro etd eu cc tie onM ,2 feI a, ta urs iy nn gth de ivti ec rsd ea cta as met - Cam3 \ud835\udf11 ! Cam2 \ud835\udc67$\ud835\udf11$ \ud835\udf11\u2019 Cam2 \ud835\udee5\ud835\udc65\",$ \ud835\udc66 \ud835\udc65 e Er xa tec no sn ivfi egu er xa pt ei ro in ms, enr to sad onla by oo tu hts M, a 2n Id anen dvi tr ho enm ree an l-t wal orc lo dnd di at tio an ses t. \ud835\udc67!=\ud835\udc67%&\u2019 \ud835\udee5\ud835\udc65\",$ Cam \ud835\udc671 \ud835\udc66 \ud835\udeff\",$ \ud835\udee5\ud835\udc66\",$ Cam1 \ud835\udc45- RoScenes demonstrate that MIC-BEV achieves state-of-the-art \ud835\udc51\",$ \ud835\udee5\ud835\udc66\",$ \ud835\udc67\u2019 \ud835\udc65 \ud835\udee5\ud835\udc65\",! \ud835\udc51),\u2019 \ud835\udeff),\u2019 p cher af lo ler nm ga inn gce ci on n3 dD itioo nb sje ,c it nd ce lute dc it ni gon e. xI tt ra el mso ere wm eaa ti hn es rro ab nu dst su en nd soer r \ud835\udee5\ud835\udc66\",! \ud835\udee5\ud835\udc65\",! \ud835\udc51),\u2019 BEVGrid Ca\ud835\udee5 m\ud835\udc66\" 3,! \ud835\udeff\"\ud835\udc51 ,!\",!\ud835\udee5\ud835\udc66),\u2019 \ud835\udee5\ud835\udc65),\u2019 degradation. These",
  "results": "results second) and mAP on the M2I-Normal testing set. demonstrate that introducing moderate stochastic masking duringtrainingprovidesafavorablebalancebetweenstandard Model mAP(M2I-Normal) FPS accuracy and robustness. LSS 0.446 17.52 PETR 0.596 13.33 BEVFormer 0.637 4.76 TABLEX:Effectoftrainingcameramaskingprobability(p m) PETRv2 0.651 9.17 on performance over the M2I normal and robust testing sets. StreamPETR 0.677 11.11 BEVNeXt 0.681 10.20 mAP\u2191 GeoBEV 0.690 6.25 MaskingRatepm UVTR 0.698 9.09 M2I-Normal M2I-Robust DETR3D 0.701 11.76 0.00 0.764 0.598 MIC-BEV 0.767 4.65 0.25 0.767 0.647 0.50 0.757 0.650 0.75 0.742 0.652 F. Ablation Studies 3) Influence of Relation Encoding: We conduct an ab- 1) ComponentAblation: Weperformacomponentablation lation study to evaluate the contribution of latent camera on the M2I test set to evaluate the contribution of each key features, distance-based relations, and angle-based relations moduleinMIC-BEV,includingcameramasking,theauxiliary withintheGNN-basedfusionmodule.Startingfromabaseline BEV segmentation head, and the Relation-Enhanced Spatial that excludes all three components, we observe a mAP of Cross-Attention (ReSCA) module. As shown in Table IX, 0.691andNDSof0.727.Introducingonlygeometricrelations removing any component leads to a consistent performance (distance and angle) without camera features yields a notable drop under both the M2I-Normal and M2I-Robust settings. improvement (mAP: 0.729, NDS: 0.745), highlighting the Enabling random camera masking improves robustness to importance of spatial structure among infrastructure-mounted sensor failures by encouraging the model to rely on com- cameras. Using only camera features provides comparable plementary camera views. The auxiliary BEV segmentation gains (mAP: 0.725, NDS: 0.740), indicating that latent image head further enhances spatial reasoning, providing geometric featuresarebeneficialbutslightlylesseffectivethangeometric priors that guide detection. Finally, by studying the effect of relations. Combining camera features with distance relations the ReSCA module, we replace it with a regular spatial cross- furtherboostsperformanceto0.761mAPand0.765NDS.The attention layer without graph-based weighting, and observe a full model, which incorporates all three components, achieves clear reduction in both mAP and NDS. This confirms that thebestresults(mAP:0.767,NDS:0.771),demonstratingthat the GNN weighting mechanism in ReSCA effectively learns latentcamerafeaturesandgeometricrelationsarecomplemen- view-dependent importance for multi-camera fusion, leading tary and essential for multi-view feature fusion. to stronger overall performance. TABLE IX: Ablation study of MIC-BEV components on TABLE XI: Influence of incorporating camera features, dis- the M2I-Normal testing set. Cam. Masking: random camera tance,andanglerelationsintheGNN-basedmulti-viewfusion masking; BEV Seg.: the auxiliary BEV segmentation head; module. ReSCA: Relation-enhanced spatial cross-attention module M2I-Normal Cam.Feature Distances Angles Cam. BEV M2I-Normal M2I-Robust mAP\u2191 NDS\u2191 ReSCA Masking Seg. mAP\u2191 NDS\u2191 mAP\u2191 NDS\u2191 \u2717 \u2717 \u2717 0.691 0.727 \u2717 \u2717 \u2717 0.637 0.678 0.513 0.593 \u2717 \u2713 \u2713 0.729 0.745 \u2713 \u2717 \u2717 0.649 0.689 0.574 0.631 \u2713 \u2717 \u2717 0.725 0.740 \u2713 \u2713 \u2717 0.691 0.727 0.597 0.647 \u2713 \u2713 \u2717 0.761 0.765 \u2713 \u2713 \u2713 0.767 0.771 0.647 0.678 \u2713 \u2713 \u2713 0.767 0.771 2) Influence of Masking Rate: We conduct an ablation 4) Robustness across Random Seeds: To evaluate con- studytoanalyzetheeffectoftheview-maskingprobabilityp sistency under random perturbations, we assess MIC-BEV\u2019s m during training on the M2I-Normal and Robust testing set. As stability across different test-time degradations. The robust shown in Table X, applying moderate masking enhances ro- test set is constructed by randomly applying image blur or bustness against sensor degradation, while maintaining strong maskingonecameraviewforeachsample.WecompareMIC- performance under normal conditions. Specifically, when p BEV against BEVFormer and DETR3D over three random m increases from 0.0 to 0.25, the model\u2019s mAP for the M2I- seeds, reporting the mean and standard deviation of mAP Robust set improves by 8%, indicating that exposure to in Table XII. MIC-BEV achieves both the highest average simulated sensor failures effectively encourages the model to accuracy and the lowest variance, indicating greater stability rely on complementary visual cues from the remaining views. and reliability under random sensor degradation. --- Page 16 --- 16 TABLE XII: Detection performance across random seeds for [4] Z.Bai,G.Wu,X.Qi,Y.Liu,K.Oguchi,andM.J.Barth,\u201cInfrastructure- the M2I robust testing set. basedobjectdetectionandtrackingforcooperativedrivingautomation: A survey,\u201d in 2022 IEEE Intelligent Vehicles Symposium (IV). IEEE, M2I-Robust 2022,pp.1366\u20131373. Method [5] M. Lo\u00a8tscher, N. Baumann, E. Ghignone, A. Ronco, and M. Magno, mAP\u2191 NDS\u2191 \u201cAssessingtherobustnessoflidar,radaranddepthcamerasagainstill- BEVFormer 0.513\u00b10.009 0.593\u00b10.008 reflecting surfaces in autonomous vehicles: An experimental study,\u201d in DETR3D 0.540\u00b10.007 0.580\u00b10.008 2023 IEEE 9th World Forum on Internet of Things (WF-IoT). IEEE, MIC-BEV 0.647\u00b10.006 0.678\u00b10.007 2023,pp.1\u20136. [6] W.Jiang,H.Xiang,X.Cai,R.Xu,J.Ma,Y.Li,G.H.Lee,andS.Liu, \u201cOptimizingtheplacementofroadsidelidarsforautonomousdriving,\u201d inProceedingsoftheIEEE/CVFInternationalConferenceonComputer VI. CONCLUSIONS Vision,2023,pp.18381\u201318390. [7] T.-H.Kim,G.-H.Jo,H.-S.Yun,K.-S.Yun,andT.-H.Park,\u201cPlacement We propose MIC-BEV, a Transformer-based framework method of multiple lidars for roadside infrastructure in urban environ- ments,\u201dSensors,vol.23,no.21,p.8808,2023. formulti-camerainfrastructure-basedperception,togetherwith [8] L. Kloeker, G. Joeken, and L. Eckstein, \u201cEconomic analysis of smart M2I, a comprehensive synthetic dataset and benchmark en- roadsideinfrastructuresensorsforconnectedandautomatedmobility,\u201din compassing diverse road layouts, camera placements, illumi- 2023IEEE26thInternationalConferenceonIntelligentTransportation Systems(ITSC). IEEE,2023,pp.2331\u20132336. nation conditions, and adverse weather. MIC-BEV introduces [9] W.Wang,Y.Lu,G.Zheng,S.Zhan,X.Ye,Z.Tan,J.Wang,G.Wang, a camera-BEV spatial relation-aware attention mechanism andX.Li,\u201cBevspread:Spreadvoxelpoolingforbird\u2019s-eye-viewrepre- that explicitly models geometric relations between each cam- sentationinvision-basedroadside3dobjectdetection,\u201dinProceedingsof theIEEE/CVFConferenceonComputerVisionandPatternRecognition, era and BEV cell through a graph neural network, enabling 2024,pp.14718\u201314727. adaptive multi-view fusion under heterogeneous configura- [10] L. Yang, X. Zhang, J. Yu, J. Li, T. Zhao, L. Wang, Y. Huang, tions. In addition, a dual-level BEV segmentation head jointly C. Zhang, H. Wang, and Y. Li, \u201cMonogae: Roadside monocular 3d objectdetectionwithground-awareembeddings,\u201dIEEETransactionson learns map-level and object-level priors to enhance spatial Intelligent Transportation Systems, vol. 25, no. 11, pp. 17587\u201317601, reasoning, while camera masking strategies such as random 2024. dropout and Gaussian blur improve robustness against sensor [11] Z. Li, Z. Chen, A. Li, L. Fang, Q. Jiang, X. Liu, and J. Jiang, \u201cUnsuperviseddomainadaptationformonocular3dobjectdetectionvia degradation, occlusion, and partial failures. self-training,\u201d in European conference on computer vision. Springer, Extensive",
  "introduction": "INTRODUCTION Feature hp IN inF teR llA igS eT ntR tU raC nT spU oR rtE at- ib oa ns se yd stp ee mrc se ,p pt ri oo vn idis ina gk ce ry itie cn alab sule pr pf oo rr t Weights \ud835\udf14!,# \ud835\udf14),\u2019\ud835\udf14), \ud835\udf14. ),/ Cam3 0 Weight 1 for traffic monitoring, situational awareness, and cooperative autonomy in urban environments [1]\u2013[3]. Sensors deployed at intersections, crosswalks, and merging zones offer a strategic advantage for observing traffic participants from elevated viewpoints, providing broader and more stable observations. This spatial advantage facilitates long-term monitoring and enhances the ability to detect dynamic objects [4]. Although LiDAR sensors have been widely adopted for infrastructure- basedobjectdetectionduetotheiraccurate3Dmeasurements, they remain costly, maintenance-intensive, and sensitive to mounting and calibration errors [5]\u2013[7]. In contrast, cameras are significantly more affordable, scalable, and easier to de- ploy. They also provide rich semantic information that en- hancessceneunderstandinginlarge-scalesensingapplications, All authors are with the University of California, Los Angeles (UCLA), CA90095,USA.Email:{yun666,zhz03,jwu7,zhiyuh,zeweizhou,meng925,  \u2217Correspondingauthor:ZhiyuHuang. Edge-Enhanced GNN Assign weight to views a)DiverseRoadGeometry&CameraConfiguration Cam2 Cam1 Cam3 2-cam: L-shapedroad 3-cam: 4-way intersection 4-cam:5-way intersection 4-cam:4-way intersection b)Multi-Infrastructure Camera Views \ud835\udc5d BEV cell \ud835\udc5d \ud835\udc5d Cam3 3DView BEVView \ud835\udc51\",$ \ud835\udc51\",! \ud835\udee5\ud835\udc66),\u2019 \ud835\udee5\ud835\udc65),\u2019 Cam1 Cam2 Cam3 Flatten Edgegp,n g),. hp Fig. 1: Overview of the proposed MIC-BEV framework for multi-camera infrastructure perception. (a) Example scenariosofinfrastructuresensingwithdiverseroadgeometry and camera configuration. (b) Multiple cameras capture the scenefromdifferentviewpointsandprojectobjectsontoade- finedBEVgrid.(c)MIC-BEVencodesthegeometricrelations betweeneachcameraandBEVcell,suchasdistances,angles, and height differences, to construct edge features for a graph neural network (GNN). (d) The GNN in MIC-BEV performs geometricrelation-awarefusion,assigningimportanceweights to each camera for adaptive multi-view feature aggregation. making them an attractive alternative for infrastructure-based perception [8]. While single-camera infrastructure perception systems have been extensively studied [9]\u2013[11], their spatial coverage and robustness are inherently limited, particularly under occlusion 5202 tcO 82 ]VC.sc[ 1v88642.0152:viXra --- Page 2 --- 2 or in complex scenes. Multi-camera infrastructure sensing that MIC-BEV achieves strong and consistent perfor- addresses these limitations by integrating information from mance, and validating its adaptability to heterogeneous multiple viewpoints to achieve more comprehensive and re- infrastructure layouts and robustness. silient scene understanding [12]. Nonetheless, it introduces several critical challenges. 1) Spatially distributed sensors. II. RELATEDWORK Cameras deployed across large spatial distances often have A. Camera-based BEV Perception overlapping fields of view with significant perspective dif- BEV representations have become a dominant paradigm ferences and occlusions. These multi-view conditions make in camera-based 3D perception, offering a unified spatial spatial alignment and feature fusion across views challenging. abstraction across multi-view inputs. Early works such as 2) Variability in camera configurations. Unlike vehicle- OFT[14]andCADDN[15]projectmonocularimagefeatures mounted sensors that follow consistent mounting patterns, into BEV space to enable 3D object detection. Lift-Splat- infrastructure cameras are deployed with diverse quantities, Shoot [16] advances this by lifting 2D image features into spatiallayouts,orientations,fieldsofview(FoV),anddegrees 3Dfrustumsusingpredicteddepthdistributionsandprojecting of overlap. Each intersection has a distinct design, requiring them into BEV space through vertical accumulation, while models to adapt to a wide range of installation configurations. BEVDet [17] improves efficiency for multi-view settings. 3) Sensor reliability and robustness. Infrastructure cameras BEVDepth [18] further enhances depth modeling through may degrade or fail over time without immediate detection or LiDAR-guided supervision, and GeoBEV [19] improves the repair.Therefore,perceptionmodelsmustmaintainrobustness geometric fidelity of BEV representations via radial-Cartesian againstmissing,corrupted,orlow-qualityvisualinputsduring sampling and centroid-aware depth supervision. real-world deployment. Transformer-basedmethodsfurtheradvanceBEVdetection. Toaddressthesechallenges,weproposeMIC-BEV(Multi- DETR3D [20] extends DETR [21] to 3D detection by sam- Infrastructure Camera Bird\u2019s-Eye-View Transformer), an ef- pling image features at learned 3D",
  "references": "REFERENCES queries,\u201dinConferenceonrobotlearning. PMLR,2022,pp.180\u2013191. [21] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and [1] H.Yu,W.Yang,J.Zhong,Z.Yang,S.Fan,P.Luo,andZ.Nie,\u201cEnd- S. Zagoruyko, \u201cEnd-to-end object detection with transformers,\u201d in to-endautonomousdrivingthroughv2xcooperation,\u201dAAAI,2025. European conference on computer vision. Springer, 2020, pp. 213\u2013 [2] M. Tang, D. Yu, P. Li, C. Song, P. Zhao, W. Xiao, and N. Chen, 229. \u201cAmulti-sceneroadsidelidarbenchmarktowardsdigitaltwinsofroad [22] Y.Liu,T.Wang,X.Zhang,andJ.Sun,\u201cPetr:Positionembeddingtrans- intersections,\u201d ISPRS Annals of the Photogrammetry, Remote Sensing formationformulti-view3dobjectdetection,\u201dinEuropeanconference andSpatialInformationSciences,vol.10,pp.341\u2013348,2024. oncomputervision. Springer,2022,pp.531\u2013548. [3] Y.Li,D.Ma,Z.An,Z.Wang,Y.Zhong,S.Chen,andC.Feng,\u201cV2x- [23] Y.Liu,J.Yan,F.Jia,S.Li,A.Gao,T.Wang,andX.Zhang,\u201cPetrv2: sim: Multi-agent collaborative perception dataset and benchmark for A unified framework for 3d perception from multi-camera images,\u201d in autonomous driving,\u201d IEEE Robotics and Automation Letters, vol. 7, Proceedings of the IEEE/CVF international conference on computer no.4,pp.10914\u201310921,2022. vision,2023,pp.3262\u20133272. --- Page 17 --- 17 [24] Z. Li, W. Wang, H. Li, E. Xie, C. Sima, T. Lu, Q. Yu10568349, and [44] W. Chen, Y. Su, J. Zuo, C. Yang, C. Yuan, C. Qian, C.-M. Chan, J.Dai,\u201cBevformer:learningbird\u2019s-eye-viewrepresentationfromlidar- Y. Qin, Y. Lu, R. Xie et al., \u201cAgentverse: Facilitating multi-agent col- cameraviaspatiotemporaltransformers,\u201dIEEETransactionsonPattern laborationandexploringemergentbehaviorsinagents,\u201darXivpreprint AnalysisandMachineIntelligence,2024. arXiv:2308.10848,vol.2,no.4,p.6,2023. [25] S. Wang, Y. Liu, T. Wang, Y. Li, and X. Zhang, \u201cExploring object- [45] G.Liu,Y.Hu,C.Xu,W.Mao,J.Ge,Z.Huang,Y.Lu,Y.Xu,J.Xia, centrictemporalmodelingforefficientmulti-view3dobjectdetection,\u201d Y.Wangetal.,\u201cTowardscollaborativeautonomousdriving:Simulation inProceedingsoftheIEEE/CVFinternationalconferenceoncomputer platformandend-to-endsystem,\u201dIEEETransactionsonPatternAnalysis vision,2023,pp.3621\u20133631. andMachineIntelligence,2025. [26] J. Huang and G. Huang, \u201cBevdet4d: Exploit temporal cues in [46] H. Xiang, Z. Zheng, X. Xia, R. Xu, L. Gao, Z. Zhou, X. Han, multi-camera 3d object detection,\u201d 2022. [Online]. Available: https: X. Ji, M. Li, Z. Meng et al., \u201cV2x-real: a largs-scale dataset for //arxiv.org/abs/2203.17054 vehicle-to-everything cooperative perception,\u201d in European Conference [27] J. Park, C. Xu, S. Yang, K. Keutzer, K. Kitani, M. Tomizuka, and onComputerVision. Springer,2024,pp.455\u2013470. W. Zhan, \u201cTime will tell: New outlooks and a baseline for temporal [47] R. Hao, S. Fan, Y. Dai, Z. Zhang, C. Li, Y. Wang, H. Yu, W. Yang, multi-view3dobjectdetection,\u201darXivpreprintarXiv:2210.02443,2022. J. Yuan, and Z. Nie, \u201cRcooper: A real-world large-scale dataset for [28] Z.Li,S.Lan,J.M.Alvarez,andZ.Wu,\u201cBevnext:Revivingdensebev roadsidecooperativeperception,\u201dinProceedingsoftheIEEE/CVFcon- frameworks for 3d object detection,\u201d in Proceedings of the IEEE/CVF ferenceoncomputervisionandpatternrecognition,2024,pp.22347\u2013 conference on computer vision and pattern recognition, 2024, pp. 22357. 20113\u201320123. [48] X.Zhu,H.Sheng,S.Cai,B.Deng,S.Yang,Q.Liang,K.Chen,L.Gao, [29] Z. Yang, J. Mao, W. Yang, Y. Ai, Y. Kong, H. Yu, and W. Zhang, J. Song, and J. Ye, \u201cRoscenes: A large-scale multi-view 3d dataset \u201cLidar-based end-to-end temporal perception for vehicle-infrastructure forroadsideperception,\u201dinEuropeanConferenceonComputerVision. cooperation,\u201d IEEE Internet of Things Journal, vol. 12, no. 13, pp. Springer,2024,pp.331\u2013347. 22862\u201322874,2025. [49] S. Fan, Z. Wang, X. Huo, Y. Wang, and J. Liu, \u201cCalibration-free [30] W. Zimmer, J. Wu, X. Zhou, and A. C. Knoll, \u201cReal-time and robust bev representation for infrastructure perception,\u201d in 2023 IEEE/RSJ 3d object detection with roadside lidars,\u201d in Proceedings of the 12th International Conference on Intelligent Robots and Systems (IROS). InternationalScientificConferenceonMobilityandTransport:Mobility IEEE,2023,pp.9008\u20139013. InnovationsforGrowingMegacities. Springer,2023,pp.199\u2013219. [50] A.Vaghela,D.Lu,A.A.Verma,B.Chakravarthi,H.Wei,andY.Yang, \u201cMc-bevro: Multi-camera bird eye view road occupancy detection for [31] H. Yu, Y. Tang, E. Xie, J. Mao, J. Yuan, P. Luo, and Z. Nie, trafficmonitoring,\u201darXivpreprintarXiv:2502.11287,2025. \u201cVehicle-infrastructurecooperative3dobjectdetectionviafeatureflow [51] J.Jia,G.Yi,andY.Shi,\u201cRopebev:Amulti-cameraroadsideperception prediction,\u201d2023.[Online].Available: networkinbird\u2019s-eye-view,\u201darXivpreprintarXiv:2409.11706,2024. [32] W. Zimmer, J. Birkner, M. Brucker, H. T. Nguyen, S. Petrovski, [52] K.He,X.Zhang,S.Ren,andJ.Sun,\u201cDeepresiduallearningforimage B.Wang,andA.C.Knoll,\u201cInfradet3d:Multi-modal3dobjectdetection recognition,\u201dinProceedingsoftheIEEEconferenceoncomputervision basedonroadsideinfrastructurecameraandlidarsensors,\u201din2023IEEE andpatternrecognition,2016,pp.770\u2013778. IntelligentVehiclesSymposium(IV). IEEE,2023,pp.1\u20138. [53] T.-Y.Lin,P.Dolla\u00b4r,R.Girshick,K.He,B.Hariharan,andS.Belongie, [33] X. Bai, Z. Hu, X. Zhu, Q. Huang, Y. Chen, H. Fu, and C.-L. Tai, \u201cFeaturepyramidnetworksforobjectdetection,\u201dinProceedingsofthe \u201cTransfusion: Robust lidar-camera fusion for 3d object detection with IEEEconferenceoncomputervisionandpatternrecognition,2017,pp. transformers,\u201dinProceedingsoftheIEEE/CVFconferenceoncomputer 2117\u20132125. visionandpatternrecognition,2022,pp.1090\u20131099. [54] X.Zhu,W.Su,L.Lu,B.Li,X.Wang,andJ.Dai,\u201cDeformableDETR: [34] Z. Liu, H. Tang, A. Amini, X. Yang, H. Mao, D. Rus, and S. Han, Deformabletransformersforend-to-endobjectdetection,\u201darXivpreprint \u201cBevfusion:Multi-taskmulti-sensorfusionwithunifiedbird\u2019s-eyeview arXiv:2010.04159,2020. representation,\u201darXivpreprintarXiv:2205.13542,2022. [55] P.Velic\u02c7kovic\u00b4,G.Cucurull,A.Casanova,A.Romero,P.Lio,andY.Ben- [35] Z. Meng, Y. Zhang, Z. Zheng, Z. Zhao, and J. Ma, \u201cAgentalign: gio,\u201cGraphattentionnetworks,\u201darXivpreprintarXiv:1710.10903,2017. Misalignment-adapted multi-agent perception for resilient inter-agent [56] S. Brody, U. Alon, and E. Yahav, \u201cHow attentive are graph attention sensorcorrelations,\u201darXivpreprintarXiv:2412.06142,2024. networks?\u201darXivpreprintarXiv:2105.14491,2021. [36] H.Hu,Z.Liu,S.Chitlangia,A.Agnihotri,andD.Zhao,\u201cInvestigating [57] R. Xu, H. Xiang, Z. Tu, X. Xia, M.-H. Yang, and J. Ma, \u201cV2x-vit: theimpactofmulti-lidarplacementonobjectdetectionforautonomous Vehicle-to-everything cooperative perception with vision transformer,\u201d driving,\u201dinProceedingsoftheIEEE/CVFconferenceoncomputervision inEuropeanconferenceoncomputervision. Springer,2022,pp.107\u2013 andpatternrecognition,2022,pp.2550\u20132559. 124. [37] Z. Zheng, Y. Zhang, Z. Meng, J. Liu, X. Xia, and J. Ma, \u201cInspe: [58] Z. Zhou, H. Xiang, Z. Zheng, S. Z. Zhao, M. Lei, Y. Zhang, T. Cai, Rapid evaluation of heterogeneous multi-modal infrastructure sensor X. Liu, J. Liu, M. Bajji et al., \u201cV2xpnp: Vehicle-to-everything spatio- placement,\u201d2025. temporal fusion for multi-agent perception and prediction,\u201d arXiv [38] X. Ye, M. Shu, H. Li, Y. Shi, Y. Li, G. Wang, X. Tan, and E. Ding, preprintarXiv:2412.01812,2024. \u201cRope3d: The roadside perception dataset for autonomous driving and [59] W.Zimmer,G.A.Wardana,S.Sritharan,X.Zhou,R.Song,andA.C. monocular 3d object detection task,\u201d in Proceedings of the IEEE/CVF Knoll,\u201cTumtrafv2xcooperativeperceptiondataset,\u201dinProceedingsof Conference on Computer Vision and Pattern Recognition, 2022, pp. theIEEE/CVFconferenceoncomputervisionandpatternrecognition, 21341\u201321350. 2024,pp.22668\u201322677. [39] H. Yu, Y. Luo, M. Shu, Y. Huo, Z. Yang, Y. Shi, Z. Guo, H. Li, [60] H. Xiang, Z. Zheng, X. Xia, S. Z. Zhao, L. Gao, Z. Zhou, T. Cai, X. Hu, J. Yuan et al., \u201cDair-v2x: A large-scale dataset for vehicle- Y.Zhang,andJ.Ma,\u201cV2x-realo:Anopenonlineframeworkanddataset infrastructure cooperative 3d object detection,\u201d in Proceedings of the forcooperativeperceptioninreality,\u201dECCV,2024. IEEE/CVFconferenceoncomputervisionandpatternrecognition,2022, [61] A.Dosovitskiy,G.Ros,F.Codevilla,A.Lopez,andV.Koltun,\u201cCarla: pp.21361\u201321370. An open urban driving simulator,\u201d in Conference on robot learning. [40] L.Yang,K.Yu,T.Tang,J.Li,K.Yuan,L.Wang,X.Zhang,andP.Chen, PMLR,2017,pp.1\u201316. \u201cBevheight: A robust framework for vision-based roadside 3d object [62] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, detection,\u201d in Proceedings of the IEEE/CVF Conference on Computer A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, \u201cnuscenes: A VisionandPatternRecognition,2023,pp.21611\u201321620. multimodal dataset for autonomous driving,\u201d in Proceedings of the [41] L.Yang,T.Tang,J.Li,K.Yuan,K.Wu,P.Chen,L.Wang,Y.Huang, IEEE/CVFconferenceoncomputervisionandpatternrecognition,2020, L. Li, X. Zhang et al., \u201cBevheight++: Toward robust visual centric 3d pp.11621\u201311631. objectdetection,\u201dIEEETransactionsonPatternAnalysisandMachine [63] P.Sun,H.Kretzschmar,X.Dotiwalla,A.Chouard,V.Patnaik,P.Tsui, Intelligence,2025. J. Guo, Y. Zhou, Y. Chai, B. Caine et al., \u201cScalability in perception [42] H. Shi, C. Pang, J. Zhang, K. Yang, Y. Wu, H. Ni, Y. Lin, R. Stiefel- for autonomous driving: Waymo open dataset,\u201d in Proceedings of the hagen, and K. Wang, \u201cCobev: Elevating roadside 3d object detection IEEE/CVFconferenceoncomputervisionandpatternrecognition,2020, with depth and height complementarity,\u201d IEEE Transactions on Image pp.2446\u20132454. Processing,2024. [64] Y. Li, Y. Chen, X. Qi, Z. Li, J. Sun, and J. Jia, \u201cUnifying voxel- [43] J. Jinrang, Z. Li, and Y. Shi, \u201cMonouni: A unified vehicle and basedrepresentationwithtransformerfor3dobjectdetection,\u201dAdvances infrastructure-sidemonocular3dobjectdetectionnetworkwithsufficient inNeuralInformationProcessingSystems,vol.35,pp.18442\u201318455, depth clues,\u201d Advances in Neural Information Processing Systems, 2022. vol.36,pp.11703\u201311715,2023.",
  "methods": "methods. on Neural Networks and Learning Systems, vol. 32, no. 1, pp. 4\u201324, 2021. OnRoScenes,MIC-BEVgeneralizeseffectivelytolarge-scale [14] T. Roddick, A. Kendall, and R. Cipolla, \u201cOrthographic feature highway scenes and complex urban intersections, maintaining transform for monocular 3d object detection,\u201d 2018. [Online]. superior detection stability under limited overlap and chal- Available: [15] L. Zhang, Y. Liu, X. Wang, Y. He, G. Li, Y. Zhang, C. Liu, Z. Jiang, lenging lighting. Qualitative analyses further reveal that the and Y. Liu, \u201cCaddn: A content-aware downsampling-based detection relation-enhanced attention dynamically emphasizes informa- methodforsmallobjectsinremotesensingimages,\u201dIEEETransactions tiveviewpoints,suppressesredundantordistortedfeatures,and onGeoscienceandRemoteSensing,vol.63,pp.1\u201317,2025. [16] J. Philion and S. Fidler, \u201cLift, splat, shoot: Encoding images from improves the detection of small or distant targets. arbitrary camera rigs by implicitly unprojecting to 3d,\u201d in European Beyondstaticobjectdetection,futureresearchwillfocuson conferenceoncomputervision. Springer,2020,pp.194\u2013210. extending MIC-BEV toward multi-object tracking and trajec- [17] J. Huang, G. Huang, Z. Zhu, Y. Ye, and D. Du, \u201cBevdet: High- performancemulti-camera3dobjectdetectioninbird-eye-view,\u201darXiv tory forecasting to capture dynamic interactions among road preprintarXiv:2112.11790,2021. users. Another direction is to explore real-time deployment [18] Y. Li, Z. Ge, G. Yu, J. Yang, Z. Wang, Y. Shi, J. Sun, and Z. Li, through lightweight backbones and knowledge distillation for \u201cBevdepth: Acquisition of reliable depth for multi-view 3d object de- tection,\u201dinProceedingsoftheAAAIconferenceonartificialintelligence, edge devices. Finally, we plan to expand the M2I benchmark vol.37,2023,pp.1477\u20131485. with additional real-world data to bridge the simulation-to- [19] J.Zhang,Y.Zhang,Y.Qi,Z.Fu,Q.Liu,andY.Wang,\u201cGeobev:Learn- realitygapandenablecomprehensiveevaluationacrossdiverse inggeometricbevrepresentationformulti-view3dobjectdetection,\u201din ProceedingsoftheAAAIConferenceonArtificialIntelligence,vol.39, urban contexts. no.9,2025,pp.9960\u20139968. [20] Y.Wang,V.C.Guizilini,T.Zhang,Y.Wang,H.Zhao,andJ.Solomon, \u201cDetr3d: 3d object detection from multi-view images via 3d-to-2d",
  "experiments": "experiments demonstrate that MIC-BEV consis- 2022,pp.245\u2013262. tently achieves state-of-the-art performance across both syn- [12] R.Xu,Z.Tu,H.Xiang,W.Shao,B.Zhou,andJ.Ma,\u201cCobevt:Cooper- ativebird\u2019seyeviewsemanticsegmentationwithsparsetransformers,\u201d thetic (M2I) and real-world (RoScenes) datasets. On M2I, arXivpreprintarXiv:2207.02202,2022. the model shows strong detection accuracy across normal, [13] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu, \u201cA robust, and extreme weather settings, with a significant im- comprehensive survey on graph neural networks,\u201d IEEE Transactions provement in mAP compared to leading baseline"
}