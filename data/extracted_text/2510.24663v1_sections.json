{
  "abstract": "Abstract Agentic tool use has gained traction with the rise of agentic tool calling, yet mostexistingworkoverlooksthecomplexityofmulti-turntoolinteractions. We introduceOrchDAG,asyntheticdatagenerationpipelinethatmodelstoolexecution asdirectedacyclicgraphs(DAGs)withcontrollablecomplexity. Usingthisdataset, webenchmarkmodelperformanceandproposeagraph-basedrewardtoenhance RLVR training.",
  "experiments": "EXPERIMENTS Inthissection,wedescribethegenerateddataset,focusingonthedistributionoftopologicaldifficulty intheDAGtemplatesandtheproportionofsingle-turnversusmulti-turnsettings. Forsimplicity,we restrictevaluationtothetwo-turnsetting,leavingextensionstolongerhorizonsforfuturework. To ensureindependencebetweentrainingandtestdata,weconstructthemfromtwodisjointsetsofdata pointsfromAPIGEN[16]andTOOLACE[14].,asdescribedin3.1. Table 1: Synthetic Data Distribution (Height and width are hyperparameters controlling DAG complexity,andsuccessrateistheproportionofdatathatpassesrule-basedvalidation) Type data# Multi-turnproportion AverageHeight AverageWidth SucessfulRate Training 1800 30% 2.50\u00b10.12 3.4\u00b10.24 0.6 Test 250 25% 2.7\u00b10.08 3.1\u00b10.14 0.7 4.1 TaskDifficulty Weinvestigatetwocentralquestions: (1)Giventhedesigneddifficultyofoursyntheticdata,isthe datasetsolvableinprinciple? Adatasetthatcannotbesolvedevenbyadvancedclosed-sourcemodels suchasClaude4orGPT-4owouldlackpracticalutility. (2)Ifitissolvable,doesthedatasetoffera sufficientlevelofchallengetomeaningfullyevaluatemodelperformance? Tobeginwith,weevaluate severalclosed-sourceandopen-sourcemodelsbyprovidingthemwiththesystempromptdefinedin Section3.1,andmeasurewhethertheycancorrectlypredicttheDAGbyanalyzingtooldependencies. Inthemulti-turnsetting,modelsmustgeneratetheDAGbyconsideringboththeavailabletoolsand theobservationsfrompreviousturns.  focushereisonassessingthetaskdifficultyintroducedbythedataset. Table2:  wererun10timeswiththetemperatureofthebaseLLMssetto0.1) Models ZeroShot Oneshot Threeshots GPT-4o (0.18\u00b10.03) (0.22\u00b10.02) (0.24\u00b10.04) Claude4 (0.15\u00b10.01) (0.23\u00b10.03) (0.22\u00b10.01) Claude3.7 (0.16\u00b10.04) (0.18\u00b10.03) (0.23\u00b10.01) Claude3.5 (0.08\u00b10.02) (0.09\u00b10.03) (0.08\u00b10.03) DeepSeek-R1 (0.12\u00b10.02) (0.14\u00b10.01) (0.11\u00b10.04) Qwen2.53B (0\u00b10) (0\u00b10) (0.02\u00b10.01) Qwen2.57B (0.02\u00b10.01) (0.03\u00b10.02) (0.03\u00b10.02)  20.23%forQwen3-4Band26.55%for Qwen3-8B.FromTable2,wecanseethatGPT-4omaintainsthehighestaccuracyforthezero-shot setting and the three-shots setting with nearly same performance with Claude 4 in the one-shot 6 --- Page 7 --- setting. TheaccuracyforGPT-4oandalsoforClaude4showsthatourdatasetissolvablehowever theperofmranceforClaude3.5,Qwen2.53Bwithaccuracy0,andQwen2.57Bdemonstratesthe challengeofourdatasetstocurrentLLMs. Comparisonbetweentheone-shotandthree-shotresults showsthatprovidingadditionalexamplesdoesnotnecessarilyimproveLLMperformanceinDAG prediction. 4.2 AnalysisoftheGraph-basedRewardShapinginOrchDAG AsdiscussedinSection3.2,therewardsignalfromToolRL[19]canbesparseinourdata.,which makesitdifficultforreinforcementlearningalgorithmssuchasGRPO[22],DAPO[35],andGiGPO [9]toefficientlyimprovethepolicyLLM,evenwithlargerollouts. WeapplyToolRLonthetraining single-turndatasetandevaluateitonthetestsetinthesingle-turnsetting. Inthissetup,wetrain Qwen2.5withGRPOusingToolRL,andconverttheToolCallsitgeneratesintopredictedDAGs,since theoutputsfollowastandardizedJSONformat.Weuse8\u00d7100A100GPUswithVerl[23]tocomplete thetraining. Weevaluateperformanceusingtwometrics: Accuracy/stepandAccuracy/user_query. Accuracy/step measures correctness at the step level, where each individual action in a turn is assessed independently; a step may be correct even if the final tool execution graph is incorrect. Accuracy/user_querymeasurescorrectnessatthefullquerylevel,requiringtheentiretoolexecution graphtobecorrect. Table3: ToolRLPerformanceonOrchDAGSingle-turnTestDataset(Thedefinitionsoffine-grained andcoarserewardsaregiveninToolRL Model(Qwen2.5) Acc/step Acc/user_query 3BCoarse 0.517 0 3BFinegrained 0.540 0 7BCoarse 0.609 0 7BFinegrained 0.594 0 From Table 3, we observe that ToolRl performs reasonably well on certain steps within a single turn;however,itstrugglestomaintainacoherentoverviewoftheentireexecution. Incontrast,as showninTable2, Qwen2.5-7Bachieves2%accuracyinpredictingtheDAG.Thisindicatesthat for complex tool executions, it may be advantageous to first establish a high-level plan, such as aDAG,toguidethesubsequentexecution. Wesubsequentlyfine-tuneQwen2.5usingGRPOon thetrainingsingle-turndataset,guidedbytheproposedgraph-basedreward. Weevaluatedifferent hyperparametersettings: theuseof entropyregularization andtheKLloss, the choiceofrollout number,andthenumberoftrainingstepsfortheoptimizer. Table4: PerformanceofGraph-BasedRewardonOrchDAGSingle-turnTestDataset(Wereport resultsusingAcc/user_queryastheevaluationmetric. Thecolumnsindicatethenumberoftraining steps,andndenotestherolloutnumber.) Model(Qwen2.5)/Steps 15 30 45 60 3BKLn=4 0 - - - 7Bn=4 0.184 0.253 0.241 - 7BKLn=4 0.184 0.276 0.276 - 7BKLEntropyn=4 0.195 0.276 0.253 - 7BKLn=8 0.23 0.33 0.402 0.391 InTable4,weobservethatmodelsizehasaclearimpactonperformance. Moreover,therollout numberplaysacrucialrole,consistentwiththeintuitionthatlargerrolloutnumbersenablegreater exploration [24], thereby increasing the likelihood of reaching the correct DAG. To evaluate the effectivenessoftheGED-basedrewarddesign,weconductanablationstudyusingacoarserreward: thepredictedDAGreceivesarewardof1ifitexactlymatchestheground-truthDAG,and0otherwise. Usingthe7BmodelwithKLandn=8,theaccuracyremains0evenafter15trainingsteps. Wethenextendourexperimentstothemulti-turnsetting. Inthissetting,wetrainQwen2.5onthe entiretrainingsetwithGRPO,usingboththeinformationfrompreviousturnsandthenewuserquery asinput,andevaluateperformanceonthefullOrchDAGtestdataset. BasedonTable4,wereport performanceonlyatthe45thtrainingstep. 7 --- Page 8 --- Table5: PerformanceofGraph-BasedRewardonOrchDAGSingle/Multi-turnTestDataset(We reportresultsusingAcc/user_queryastheevaluationmetric. Thecolumnsindicatethethreemulti- turnscenariosdefinedby2) Model(Qwen2.5)/Steps scenario1 scenario2 scenario3 7BKLEntropyn=4 0.112 0.125 0.218 7BKLn=8 0.156 0.203 0.352 FromTable5,weobservethatinthemulti-turnsetting,performancedecreasesacrossbothexperi- mentalsetups. ThelargestdropsoccurinScenario1(tool-callingerror)andScenario2(requiring informationfromthepreviousturn),sincethesetasksdependnotonlyonthenewuserqueryand thesystempromptbutalsooninformationcarriedoverfromearlierturns. Incontrast,thedropin Scenario3issmaller,asthenewuserqueryiscompletelyindependentofpriorturns. Todemonstrate generalizability,wefurtherevaluatethetrainedmodelonStableToolBench[11],measuringsolvable passratesacrossL1,L2,andL3categories. AtaskisconsideredsuccessfulwhenthepredictedDAG matchesthegroundtruth. WechooseStableToolBenchforitsinherentcomplexityintoolinteractions. InStableToolBench,GPT-4-0613(CoT)achievessolvablepassratesof45.5(L1instruction),57.4 (L1category),48.8(L1tool),43.0(L2instruction),46.5(L2category),and48.1(L3instruction). Underthesameevaluation,ourmodelattains47.1,56.4,47.2,41.3,44.8,and50.7,respectively. 4.3 TrainingInsightAnalysis As shown in Table 4, performance generally improves as the number of training steps increases. However,whenweextendtrainingbeyondthisrange,weobserveasignificantdropinperformance aroundstep51. InspiredbyDAPO[35],Wehypothesizethattheperformancedropmaybecaused by a low-entropy situation, where the model becomes overly confident and thus fails to explore sufficiently. 3 2.5 2 1.5 1 0.5 0 0 2 4 6 81012141618202224262830323436384042444648505254565860 draweR GAD DAG Reward vs Step 0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0 1 3 5 7 911131517192123252729313335373941434547495153555759 Step DAPO GRPO yportnE Entropy vs Step Step DAPO GRPO Figure3: PerformanceandEntropyAnalysiswithDAPOandGRPO AfterapplyingDAPO,weobservethattheperformancecollapsenolongeroccurs,andtheentropy remainsatarelativelyhigherlevelinthelaterstagesoftraining. 5",
  "introduction": "1 INTRODUCTION LargeLanguageModels(LLMs)([3,5,6,26,36])havebeenattheforefrontofadvancingartificial intelligence,markingsignificantbreakthroughsindiversefields. TheplanningcapabilitiesofLLMs, particularly their ability to use tools ([33, 34]), enable them not only to execute commands and performwebsearchesbutalsotoenhancetheiradvancedmathematicalreasoningabilities. LLM Compiler[12]anditssubsequentwork([7,8])proposeconstructingtoolingusageasadirectedacyclic graph(DAG)toenabletheparallelexecutionofindependenttools,therebyimprovingtool-calling efficiency.CodeAct[28]andCodePlan[29]proposeleveragingthegenerationofpseudo-Pythoncode tooutlinehigh-levelreasoningprocessesforcomplexmulti-stepreasoningtasks,whereeachtool usageisrepresentedasafunctioncallwithinthecode. ReWOO[30]proposesamodularframework that decouples the reasoning process from the external observations of each tool usage, thereby reducingtokenconsumptionandimprovingefficiency. [15]clusterstheprovidedtoolsintogroups oftoolkits,plansatthetoolkitlevel,andreplansbyselectingtoolswithinthesametoolkitiferror comesout. [17]proposesamethodcalledPredictive-Decoding,whichleveragesModelPredictive Controlfromtheoptimalcontrolfieldtomitigateearlyerrorsinplanningandpromotenon-myopic planning,therebyenhancingoverallaccuracy. ReasonFlux[31]proposesaframeworkinwhichthe LLMreasonsovertemplatefields,executestoolsbasedonthetemplates,andemploysreinforcement learningtoimproveplanningaccuracyusinganactioncompletionreward. Intheagenticsetting,LLMsareevolvingbeyondpurelytextualreasoningtowarddynamicagents capableofplanning,tooluse,andmulti-step(alsomulti-turn)execution. TheintroductionofGroup Relative Policy Optimization (GRPO) [22] further inspired the development of Reinforcement LearningwithVerifiableReward(RLVR)foragentictooluse,drivenbyitsefficiency. ThexLAM \u2217WorkisdoneduringinternshipinAmazon.Equalcontribution. 39thConferenceonNeuralInformationProcessingSystems(NeurIPS2025)Workshop:Multi-TurnInteractions inLargeLanguageModels. 5202 tcO 82 ]IA.sc[ 1v36642.0152:viXra --- Page 2 --- [37]suiteintroducespurpose-built\u201clargeactionmodels\u201doptimizedforfunctioncalling,offering strongbaselinesandopenresourcesformulti-turntoolexecution. Llama-Nemotron[2]extendsthis trajectorywithefficientreasoningmodesandscalableinference,enablingmodelstodynamically switch behaviors across long conversations. ToolRL [19] systematically studies reinforcement learningrewarddesigns\u2014coveringgranularity,temporalstructure,andsignaltypes\u2014toimprove generalizationinmulti-turntool-integratedreasoning,whileOTC[27]complementsthisbyexplicitly balancing accuracy and tool-call efficiency to maintain productivity over prolonged interactions. KimiK2[25]showsthatstabilizinglong-contexttrainingandusingmulti-stageRLleadstorobust performanceacrossmulti-roundsoftwareengineering,math,andagentictasks.Theseworkshighlight thatadvancingagenticLLMsinmulti-turnsettingsrequiresnotonlylargerormoreefficientmodels, butalsoprincipledrewarddesign,cost-awaretool-usestrategies,andscalablesystempipelines. Recentadvancesinevaluatingagenticmodelshaveledtonewbenchmarksandenvironmentsfor assessingperformanceinrealistic,interactivescenarios. ACEBench[4]overcomeslimitsofprior evaluationsbyintroducingastructuredbenchmarkwithNormal,Special,andAgentcategoriesto testatomic-leveltooluseacrosssimple,complex,multi-agent,andambiguousinstructionsettings. Complementingthis,BFCL(v3)[18]standardizesfunction-callingbenchmarksacrossreal-world contexts, supporting serial and parallel invocations in multiple languages through an AST-based evaluation.\u03c4-Bench,\u03c42-Bench,andUserBench[32,1,20]togetherextendevaluationfromstructured tool\u2013userinteractionstocontrolledbidirectionalagentcooperationandfinallytofullyuser-centric, dynamicenvironments,progressivelyenrichingtherealismandrobustnessofagentassessment. Priorworkontoolusehasmainlystudiedgeneralreal-worldAPIs[21],suchassendemailormake calendar,alongwithrelatedfunctionsinwebsearchsystemslikeManus. Meanwhile,currentmulti- turnsettingsmainlyfocusoncomputer-usetasks[18],suchasmanipulatingfilesintheoperating system. Inindustrialsettings,however,anagentmayneedtoworkwithhundredsofdomaintools, includingAPIsandpipelineendpoints,andmayalsointeractwithotherdomainagentstoproduce a complete answer to a user query. The complexity usually arises from three aspects: (1) the dependenciesamongtoolscanbeintricate,(2)theoutputofatoolisoftenrepresentedasaJSON file with many fields, and (3) a key output field from one tool may serve as an input to another, butwithdifferentfieldnames. Moreover,inmulti-turnsettings,theenvironmentmayexecutethe requiredtoolsbutencountertime-outsorruntimeerrorsintheirresponses. Therefore,itisimportant to construct a dataset that not only evaluates current models but also pushes their capabilities in complexmulti-turntoolinteraction,whichisessentialforbuildingrobustandreliableagents. Insummary,thisworkmakesseveralpivotalcontributions: \u2022 Wedesignasyntheticmulti-turndatagenerationpipelineOrchDAGforagentictooluse,where each round of tool execution for a user query is represented as a DAG. The complexity of the generateddataiscontrolledbyapipelinehyperparameter. \u2022 Usingtheconstructeddataset,wefirstevaluatethecurrentmodel\u2019sperformanceandthenintroduce agraph-basedrewardderivedfromtheDAGforRLVRtraining. \u2022 Extensiveexperimentsshowtheeffectivenessofourapproach,emphasizingthevalueofincorpo- ratingthetopologicalstructureoftoolexecutiongraphsandtheimportanceofcontrollingdata complexityinmulti-turntooluse. 2 PRELIMINARY 2.1 LLMReasoningwithToolsforMulti-turnSettings For the first turn, given a user query x and a pretrained LLM \u03c1 (\u00b7), the LLM generates an tool \u03b8 execution plan represented as a graph with p = {P ,...,P } \u223c \u03c1 (p | T,D,x), where p is 1 n \u03b8 the plan list after topological sorting, T is the set of available tools, and D is the collection of descriptionsforallavailabletools. Ateachstept, theLLMgeneratesanintermediatereasoning outputr \u223c\u03c1 (r |T,D,x,p,O ,...,O )andexecutestheplanstepP toobtaintheobservation t \u03b8 t 1 t\u22121 t O . ThefinalresponseisthengeneratedasR\u223c\u03c1 (R|T,D,x,p,O ,...,O ). t \u03b8 1 n Inlaterturns,ausermayissueanirrelevantqueryrequiringacompletelynewtoolexecutiongraph,or adependentquerythatbuildsonpartialoutputsfromearliertoolexecutionsorresponses.Additionally, sometoolsmayreturnerrors(e.g.,timeouts),requiringunfinishedexecutionpathsfrompriorqueries toberescheduled. 2 --- Page 3 --- 2.2 ToolExecutionasDAG Given a plan p generated by the LLM, we represent it as a directed graph G = (V,E), where V = (v ,...,v ) is the set of nodes and E = (e ,...,e ) is the set of edges. The node v 1 n 1 m 1 corresponds to the user query, and v represents the final node that aggregates observations and n returnstheresponse. Theintermediatenodesv ,...,v correspondtotoolcalls,eachassociated 2 n\u22121 withanattributethatstoresitstoolpayloadinJSONformat. Anedgee \u2208E denotesadependency i betweentwotools,whereanoutputkeyfromthesourcetoolservesasaninputkeytothetargettool. WerepresentthetoolgraphasanorderedlistoftasksinaJSON-liketextstyle. Eachtaskcontains four fields : task_id, toolname, payload, and dependencies. A task can be associated with multipledependencies. Ataskcanbeexpressedas{task_id: task_4,toolname: name,payload: {param1: val1,param2: $2.outputkey1,param3: $3.outputkey4},dependencies: [task_2, task_3]}. 3",
  "methods": "METHODOLOGY Inreal-worlddomains,APIspecificationsandorchestrationsareoftenconsiderablymorecomplex, makingitchallengingforLLMspretrainedongeneralpublicdatatogenerateaccurateandreliable plansfordiverseuserqueries. DrawinginspirationfromLLMCompiler[12,8],forqueriesinvolving complextoolinteractions,itisadvantageoustofirstconstructatoolexecutionDAG.ThisDAGserves asablueprintforexecutingtoolssequentiallyorinparallel,withsubsequentreplanningguidedby boththeexecutionresultsofthecurrentDAGandanynewuserqueriesinlaterturns. 3.1 OrchDAG\u2013SyntheticDataGenerationPipelineforMulti-TurnToolUse AsdiscussedinSection1,thedesignofthedatagenerationpipelinefollowsseveralkeyprinciplesto betterreflectreal-worldtool-usescenarios: (1)thecomplexityofthetoolexecutionDAGforeach syntheticuserqueryshouldbecontrollablethroughpipelinehyperparameters,(2)thesystemprompt providedtotheLLMmustincludeirrelevanttools(bothschemaanddescription)sothatthemodel learnstoidentifyandselectonlytherelevantones,(3)theoutputpayloadofeachtoolshouldcontain multiplefields,typicallyfourorfive,and(4)atleastonekeyoutputfieldfromatoolshouldserveas aninputtoanothertool,butwithadifferentfieldname,tocaptureschemamisalignmentcommonly observedinpractice. Moreover,inmulti-turnsettings,thedatashouldcapturescenarioswherecertainnodesinthetool executionDAGfail. Whentheuserissuesafollow-upquery,thecorrespondingDAGinthedataset should exclude nodes that have already been executed in the previous turn, while reusing their availableresultswheneverapplicable. Inlightoftheserequirements,wedevelopagraph-baseddata generationpipelineimplementedwithLangGraph2,accompaniedbyasetofvalidationfunctionsto ensurethequalityofthegenerateddatapoints. As shown in Figure 1, the data generation pipeline begins by collecting real high-quality tools fromexistingbenchmarks,Specifically,weleverageAPIGEN[16]andTOOLACE[14]. Toensure sufficientcomplexity,weretainonlyexampleswherethefinalanswerinvolvesmorethantwodistinct functionsrepresentedinJSONformat. Notably,weextracttoolsdirectlyfromtheanswersratherthan fromthesystem-prompttoollists.Thisdesignchoiceeliminatestheneedforadditionalcategorization orclusteringsteps,whichcouldintroduceunnecessaryuncertainty,whilenaturallyyieldingasmaller andmorecoherentsetofrelatedtools. Afterfiltering,APIGENcontributes2,542datapointsand TOOLACEcontributes1,005. Foreachdatapoint,supposeitcontainsfourrealtools;theseareplacedasthefirstlayerofthetool executionDAG.BasedonthehyperparametersoftheDAG(heightandwidth),wethenrandomly sampleatopologicalordertoobtainaDAGtemplate(seeFigure1). Accordingtothistemplate, wesynthesizethetoolsforeachnodelayerbylayerunderthefollowingconditions: (1)eachinput keymustdependonanoutputkeyfromoneofitsparentnodesintheDAG,and(2)thefieldnames shouldnotremainidenticalacrossinstancesbutinsteadvaryrandomly. AfterpopulatingtheDAG template, we obtain the tool execution DAG, which is then used to prompt the LLM to generate thecorrespondinguserquery,conditionedontheDAGandafew-shotsetofexamples. Finally,we 2 3 --- Page 4 --- Initiating real tools as the first layer of DAG Sample Topological order to get DAG synthesize APIs for every node at each layer of the DAG D N SE Node 1 at Node n at Last node layer i layer i at layer i Receive Is it the last layer? No Data generating\u2026 Get the Tool execution DAG with the synthetic APIs Get the synthetic user query Figure1: DataGenerationPipelineforasingleturn augment the system prompt with irrelevant tools to encourage the model to discriminate among availableoptions. Tools Timeout from previous turn random Single turn generation User asks a query which needs previous pipeline ToolCall but also new tools User asks an irrelevant queries which only need new tools Figure2: Extensionofsingle-turndatagenerationpipelinetomulti-turnsettings We further extend the generation pipeline to the multi-turn setting by attaching three additional nodestothefinalnode(seeFigure2),withonlyonebeingactivatedduringgeneration. Thesenodes correspondtothreepossiblemulti-turnscenarios: (1)theuserissuesacompletelyirrelevantquery, (2)theuserposesaquerythatrequiresanewsetoftoolswhilealsodependingonthepreviousfinal responseorintermediatetooloutputs,and(3)atoolexecutionerroroccurs,suchasatimeout. In case(1),theoutcomeisacompletelynewDAG;incase(2),thenewDAGmustexplicitlyencode cross-turndependenciesthroughthetaskidentifiersfromthepreviousDAG;andincase(3), the resultingDAGreducestoapartialsubgraphoftheoriginalDAG.Weadoptthefinaldatageneration formatintroducedinToolRL[19]. AnexampleofagenerateddatasampleisshowninFigure3.1. Thequalityofthedata,particularlythesyntheticdata,iscritical. Toensurereliability,weincorporate arule-basedverificationmechanismintothegenerationpipeline. Alltools,planDAGs,toolcalls, andobservationsproducedbytheLLMarerequiredtobeinJSONformat. Thefirstverificationlayer checksJSONvalidity. ForplanDAGs,weapplyAST[18]matchingateachnodetoguaranteethat theLLMonlyreferencestheprovidedtoolswiththecorrectargumentnames. Wefurthervalidate symbolicargumentsbycomparingeachreferencedkeyagainsttheJSONschemaofthepredecessor\u2019s output. EverytoolcallisverifiedagainsttheplanDAGandtheprecedingtoolcallobservations, ensuring adherence to the plan and the correct use of return values as inputs for dependent calls. 4 --- Page 5 --- Likewise,eachobservationismatchedtoitscorrespondingtoolcall,anditsreturnvalueischecked againstthetool\u2019sJSONschema. Ifanyverificationstepfailsduringgeneration,theLLMisrequired torestarttheprocess. Ourpipelineenablesthegenerationofdiversesyntheticqueriesderivedfromhigh-qualityrealAPIs, each requiring resolution through a plan DAG. The difficulty of the queries is controlled by the topologicalstructureofrandomlygeneratedDAGtemplates. Sincethesetemplatesvaryinstructure, a fixed workflow for synthetic data generation is impractical. Instead, the graph-based pipeline providesaflexibleend-to-endframeworkforproducingsuchdata. Finally,therule-basedverification mechanism ensures reliability: it not only checks JSON validity to guarantee compatibility with downstream benchmarks, but also leverages AST matching to validate the correctness of DAG instantiationsduringdatageneration. SyntheticDataSample SystemPrompt: Youareadialogueassistantdesignedtoleveragetoolcallstosolveusertasksandprovide structuredresponses. AvailableTools Inyourresponse,youcanusethefollowingtools: {{ToolList}} Stepsforeachturn 1. Think: Retrievetherelevantcontextandevaluatethecurrenttool. 2. DAG:Produceatasklistdefinedhere2.2 3. Respond: Ifaresponseisneeded,generateonewhilemaintainingconsistencyacrossuser queries. SynetheticUserQuery: ... </think>...</think>[Thethinkblockisabsentinthesyntheticdatabutincludedduringthe trainingstage.] </DAG>realDAGgeneratedfrompipeline</DAG> <tool_call>toolcall1stDAGlayer</tool_call> <obs>observation1stDAGlayer</obs> ... <tool_call>toolcalllastDAGlayer</tool_call> <obs>observationlastDAGlayer</obs> </response>... </response> NewUserQuery: ... </think>...</think> </DAG>newDAGbasedonthethreescenariosdefinedinthemulti-turnsettings3.1. </DAG> ... 3.2 OrchDAG\u2013Graph-basedRewardDerivedfromtheDAGforRLVRtraining Duetotheintricatetoolinteractionstructureinherentinthesyntheticdata,theformatreward,correct- nessreward,andparametermatchingrewarddefinedin[19]mayremainsparse,evenwheninitiating alargenumberofrollouts. Moreover,thisrewarddoesnotaccountforstructuraldependenciesamong tools;thus,norewardisgivenwhentheLLMcorrectlypredictspartialdependenciesforthesetools. To account for structural dependencies, and given that we have access to the ground-truth DAG duringsyntheticdatageneration,following[13],weuseaweightedGraphEditDistance(GED)as 5 --- Page 6 --- therewardsignalateachturn. GED[10]measuresthedistancebetweentwographsbyapplying operationssuchasedgedeletion,edgeinsertion,nodeinsertion,ornoderelabelingtotransformone graphintoanisomorphicformoftheother. Wedefinetherewardforeachturnas GED(g ,g ) R =R +\u03b1R , where R =1\u2212 1 2 Total Format DAG DAG GED(g ,\u2205)+GED(g ,\u2205) 1 2 Hereg isthepredictedDAG,g istheground-truthDAG.Wedefinethefollowingnodeequivalence 1 2 whencalculatingGED:thetoolname,parameternames,andparametervaluesaretreatedasasingle unit,andequivalenceisevaluatedattheleveloftheentiretoolcall. TherewardR isassigned Format avalueof1iftheoutputcontainsthespecialtokensinthecorrectorder,and0otherwise. \u03b1isthe hyperparameterusedtobalancethetwotypesofrewards. Inthisrewarddesign,wenotonlyprovide credittoLLMsforpartiallycorrectingthepathduringrollouts,butalsomaketherewardsdenser comparedtothepreviousdesign. Themulti-turnsettingisnaturallysupported,astheground-truth DAGisavailableateachturn. 4",
  "conclusion": "CONCLUSION Insummary,weintroduceOrchDAG,asyntheticmulti-turndatagenerationpipelinethatmodels toolexecutionasDAGswithcontrollablecomplexity. Leveragingthisdataset,weevaluatemodel performanceandproposeagraph-basedrewardtoenhanceRLVRtraining. Extensiveexperiments validatetheeffectivenessofourapproach,underscoringtheimportanceofexploitingthetopological structureoftoolexecutiongraphsandmanagingdatacomplexityinmulti-turntooluse. 8 --- Page 9 --- Nonetheless,ourmethodremainslimitedinthatitdoesnotyetaddressmulti-turnscenariosinvolving implicitdependencies,suchasfileoperationsincomputer-usetasks. Infuturework,weaimtoextend ourframeworktocapturetheseimplicitdependencycases. 9 --- Page 10 ---",
  "references": "References [1] Victor Barres, Honghua Dong, Soham Ray, Xujie Si, and Karthik Narasimhan. \u03c42-bench: Evaluatingconversationalagentsinadual-controlenvironment,2025. [2] AkhiadBercovich,ItayLevy,IzikGolan,MohammadDabbah,RanEl-Yaniv,OmriPuny,Ido Galil,ZachMoshe,TomerRonen,NajeebNabwani,IdoShahaf,OrenTropp,EhudKarpas,Ran Zilberstein,JiaqiZeng,SoumyeSinghal,AlexanderBukharin,YianZhang,TugrulKonuk,Ger- aldShen,AmeyaSunilMahabaleshwarkar,BilalKartal,YoshiSuhara,OlivierDelalleau,Zijia Chen,ZhilinWang,DavidMosallanezhad,AdiRenduchintala,HaifengQian,DimaRekesh,Fei Jia,SomshubraMajumdar,VahidNoroozi,WasiUddinAhmad,SeanNarenthiran,Aleksander Ficek,MehrzadSamadi,JocelynHuang,SiddharthaJain,IgorGitman,IvanMoshkov,WeiDu, ShubhamToshniwal,GeorgeArmstrong,BranislavKisacanin,MatveiNovikov,DariaGitman, EvelinaBakhturina,PrasoonVarshney,MakeshNarsimhan,JanePolakScowcroft,JohnKamalu, DanSu,KezhiKong,MarkusKliegl,RabeehKarimi,YingLin,SanjeevSatheesh,JupinderPar- mar,PritamGundecha,BrandonNorick,JosephJennings,ShrimaiPrabhumoye,SyedaNahida Akter,MostofaPatwary,AbhinavKhattar,DeepakNarayanan,RogerWaleffe,JimmyZhang, Bor-YiingSu,GuyueHuang,TerryKong,ParthChadha,SahilJain,ChristineHarvey,Elad Segal,JiningHuang,SergeyKashirsky,RobertMcQueen,IzzyPutterman,GeorgeLam,Arun Venkatesan,SherryWu,VinhNguyen,ManojKilaru,AndrewWang,AnnaWarno,Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov,ScotJunkin,OleksandrRomanenko,PedroLarroy,MonikaKatariya,MarcoRovinelli, VijiBalas, NicholasEdelman, AnahitaBhiwandiwalla, MuthuSubramaniam, SmitaIthape, KarthikRamamoorthy,YutingWu,SugunaVarshiniVelury,OmriAlmog,JoyjitDaw,Denys Fridman,ErickGalinkin,MichaelEvans,ShaonaGhosh,KatherineLuna,LeonDerczynski, NikkiPope,EileenLong,SethSchneider,GuillermoSiman,TomaszGrzegorzek,PabloRibalta, MonikaKatariya,ChrisAlexiuk,JoeyConway,TrishaSaar,AnnGuan,KrzysztofPawelec, ShyamalaPrayaga,OleksiiKuchaiev,BorisGinsburg,OluwatobiOlabiyi,KariBriski,Jonathan Cohen,BryanCatanzaro,JonahAlben,YonatanGeifman,andEricChung. Llama-nemotron: Efficientreasoningmodels,2025. [3] TomB.Brown,BenjaminMann,NickRyder,MelanieSubbiah,JaredKaplan,PrafullaDhariwal, ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,Alec Radford,IlyaSutskever,andDarioAmodei. Languagemodelsarefew-shotlearners,2020. [4] ChenChen,XinlongHao,WeiwenLiu,XuHuang,XingshanZeng,ShuaiYu,DexunLi,Shuai Wang, Weinan Gan, Yuefeng Huang, Wulong Liu, Xinzhi Wang, Defu Lian, Baoqun Yin, YashengWang,andWuLiu. Acebench: Whowinsthematchpointintoolusage?,2025. [5] AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, TojuDuke,AnselmLevskaya,SanjayGhemawat,SunipaDev,HenrykMichalewski,Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan,HyeontaekLim,BarretZoph,AlexanderSpiridonov,RyanSepassi,DavidDohan,Shivani Agrawal,MarkOmernick,AndrewM.Dai,ThanumalayanSankaranarayanaPillai,MariePellat, AitorLewkowycz,EricaMoreira,RewonChild,OleksandrPolozov,KatherineLee,Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, KathyMeier-Hellstern,DouglasEck,JeffDean,SlavPetrov,andNoahFiedel. Palm: Scaling languagemodelingwithpathways. JournalofMachineLearningResearch,24(240):1\u2013113, 2023. [6] DeepSeek-AI,DayaGuo,DejianYang,HaoweiZhang,JunxiaoSong,RuoyuZhang,Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z.F.Wu,ZhibinGou,ZhihongShao,ZhuoshuLi,ZiyiGao,AixinLiu,BingXue,Bingxuan Wang,BochaoWu,BeiFeng,ChengdaLu,ChenggangZhao,ChengqiDeng,ChenyuZhang, 10 --- Page 11 --- ChongRuan,DamaiDai,DeliChen,DongjieJi,ErhangLi,FangyunLin,FucongDai,Fuli Luo,GuangboHao,GuantingChen,GuoweiLi,H.Zhang,HanBao,HanweiXu,Haocheng Wang,HonghuiDing,HuajianXin,HuazuoGao,HuiQu,HuiLi,JianzhongGuo,JiashiLi, JiaweiWang,JingchangChen,JingyangYuan,JunjieQiu,JunlongLi,J.L.Cai,JiaqiNi,Jian Liang, JinChen, KaiDong, KaiHu, KaigeGao, KangGuan, KexinHuang, KuaiYu, Lean Wang,LecongZhang,LiangZhao,LitongWang,LiyueZhang,LeiXu,LeyiXia,Mingchuan Zhang,MinghuaZhang,MinghuiTang,MengLi,MiaojunWang,MingmingLi,NingTian, PanpanHuang,PengZhang,QianchengWang,QinyuChen,QiushiDu,RuiqiGe,Ruisong Zhang,RuizhePan,RunjiWang,R.J.Chen,R.L.Jin,RuyiChen,ShanghaoLu,Shangyan Zhou,ShanhuangChen,ShengfengYe,ShiyuWang,ShuipingYu,ShunfengZhou,Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T.Wang,WangdingZeng,WanjiaZhao,WenLiu,WenfengLiang,WenjunGao,WenqinYu, WentaoZhang,W.L.Xiao,WeiAn,XiaodongLiu,XiaohanWang,XiaokangChen,Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, XuhengLin,X.Q.Li,XiangyueJin,XiaojinShen,XiaoshaChen,XiaowenSun,Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei,YangZhang,YanhongXu,YaoLi,YaoZhao,YaofengSun,YaohuiWang,YiYu,Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma,YiyuanLiu,YongqiangGuo,YuanOu,YuduanWang,YueGong,YuhengZou,YujiaHe, YunfanXiong,YuxiangLuo,YuxiangYou,YuxuanLiu,YuyangZhou,Y.X.Zhu,Yanhong Xu,YanpingHuang,YaohuiLi,YiZheng,YuchenZhu,YunxianMa,YingTang,YukunZha, YutingYan,Z.Z.Ren,ZehuiRen,ZhangliSha,ZheFu,ZheanXu,ZhendaXie,Zhengyan Zhang,ZhewenHao,ZhichengMa,ZhigangYan,ZhiyuWu,ZihuiGu,ZijiaZhu,ZijunLiu, ZilinLi,ZiweiXie,ZiyangSong,ZizhengPan,ZhenHuang,ZhipengXu,ZhongyuZhang, andZhenZhang. Deepseek-r1: Incentivizingreasoningcapabilityinllmsviareinforcement learning,2025. [7] LutfiErenErdogan,NicholasLee,SiddharthJha,SehoonKim,RyanTabrizi,SuhongMoon, Coleman Hooper, Gopala Anumanchipalli, Kurt Keutzer, and Amir Gholami. Tinyagent: Functioncallingattheedge,2024. [8] LutfiErenErdogan,NicholasLee,SehoonKim,SuhongMoon,HirokiFuruta,GopalaAnu- manchipalli,KurtKeutzer,andAmirGholami. Plan-and-act: Improvingplanningofagentsfor long-horizontasks,2025. [9] LangFeng,ZhenghaiXue,TingcongLiu,andBoAn. Group-in-grouppolicyoptimizationfor llmagenttraining,2025. [10] XinboGao,BingXiao,DachengTao,andXuelongLi. Asurveyofgrapheditdistance. Pattern Anal.Appl.,13(1):113\u2013129,February2010. [11] Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, MaosongSun,andYangLiu. Stabletoolbench: Towardsstablelarge-scalebenchmarkingon toollearningoflargelanguagemodels,2024. [12] SehoonKim,SuhongMoon,RyanTabrizi,NicholasLee,MichaelW.Mahoney,KurtKeutzer, andAmirGholami. Anllmcompilerforparallelfunctioncalling,2024. [13] Shengjie Liu, Alex Lu, Li Dong, Jason Zhu, Manish Gawali, and Alice Zhou. Toposem: In-contextplanningwithsemantically-informedtoolinggraphsimilarity. 2025. [14] Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, WeinanGan, ZhengyingLiu, YuanqingYu, ZezhongWang, YuxianWang, WuNing, Yutai Hou,BinWang,ChuhanWu,XinzhiWang,YongLiu,YashengWang,DuyuTang,Dandan Tu,LifengShang,XinJiang,RuimingTang,DefuLian,QunLiu,andEnhongChen. Toolace: Winningthepointsofllmfunctioncalling,2025. [15] YanmingLiu,XinyuePeng,JiannanCao,ShiBo,YuweiZhang,XuhongZhang,ShengCheng, Xun Wang, Jianwei Yin, and Tianyu Du. Tool-planner: Task planning with clusters across multipletools,2025. 11 --- Page 12 --- [16] Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Shirley Kokane, Juntao Tan, WeiranYao,ZhiweiLiu,YihaoFeng,RitheshMurthy,LiangweiYang,SilvioSavarese,JuanCar- losNiebles,HuanWang,ShelbyHeinecke,andCaimingXiong. Apigen: Automatedpipeline forgeneratingverifiableanddiversefunction-callingdatasets,2024. [17] Chang Ma, Haiteng Zhao, Junlei Zhang, Junxian He, and Lingpeng Kong. Non-myopic generationoflanguagemodelsforreasoningandplanning,2024. [18] ShishirG.Patil,HuanzhiMao,CharlieCheng-JieJi,FanjiaYan,VishnuSuresh,IonStoica,and JosephE.Gonzalez. Theberkeleyfunctioncallingleaderboard(bfcl): Fromtoolusetoagentic evaluationoflargelanguagemodels. InForty-secondInternationalConferenceonMachine Learning,2025. [19] ChengQian,EmreCanAcikgoz,QiHe,HongruWang,XiusiChen,DilekHakkani-T\u00fcr,Gokhan Tur,andHengJi. Toolrl: Rewardisalltoollearningneeds,2025. [20] ChengQian,ZuxinLiu,AksharaPrabhakar,ZhiweiLiu,JianguoZhang,HaolinChen,HengJi, WeiranYao,ShelbyHeinecke,SilvioSavarese,CaimingXiong,andHuanWang. Userbench: Aninteractivegymenvironmentforuser-centricagents,2025. [21] YujiaQin,ShihaoLiang,YiningYe,KunlunZhu,LanYan,YaxiLu,YankaiLin,XinCong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, MarkGerstein,DahaiLi,ZhiyuanLiu,andMaosongSun. Toolllm: Facilitatinglargelanguage modelstomaster16000+real-worldapis,2023. [22] ZhihongShao,PeiyiWang,QihaoZhu,RunxinXu,JunxiaoSong,XiaoBi,HaoweiZhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematicalreasoninginopenlanguagemodels,2024. [23] GuangmingSheng,ChiZhang,ZilingfengYe,XibinWu,WangZhang,RuZhang,Yanghua Peng, HaibinLin, andChuanWu. Hybridflow: Aflexibleandefficientrlhfframework. In ProceedingsoftheTwentiethEuropeanConferenceonComputerSystems,EuroSys\u201925,page 1279\u20131297.ACM,March2025. [24] SakshamSahaiSrivastavaandVaneetAggarwal. Atechnicalsurveyofreinforcementlearning techniquesforlargelanguagemodels,2025. [25] KimiTeam,YifanBai,YipingBao,GuanduoChen,JiahaoChen,NingxinChen,RuijueChen, YanruChen,YuankunChen,YutianChen,ZhuofuChen,JialeiCui,HaoDing,MengnanDong, AngangDu,ChenzhuangDu,DikangDu,YulunDu,YuFan,YichenFeng,KelinFu,BofeiGao, HongchengGao,PeizhongGao,TongGao,XinranGu,LongyuGuan,HaiqingGuo,Jianhang Guo,HaoHu,XiaoruHao,TianhongHe,WeiranHe,WenyangHe,ChaoHong,YangyangHu, ZhenxingHu,WeixiaoHuang,ZhiqiHuang,ZihaoHuang,TaoJiang,ZhejunJiang,XinyiJin, YongshengKang,GuokunLai,ChengLi,FangLi,HaoyangLi,MingLi,WentaoLi,Yanhao Li,YiweiLi,ZhaoweiLi,ZhemingLi,HongzhanLin,XiaohanLin,ZongyuLin,Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang Liu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu, Zhengying Liu,EnzheLu,LijunLu,ShenglingMa,XinyuMa,YingweiMa,ShaoguangMao,JieMei, XinMen,YiboMiao,SiyuanPan,YeboPeng,RuoyuQin,BowenQu,ZeyuShang,Lidong Shi,ShengyuanShi,FeifanSong,JianlinSu,ZhengyuanSu,XinjieSun,FloodSung,Heyi Tang, Jiawen Tao, Qifeng Teng, Chensi Wang, Dinglu Wang, Feng Wang, Haiming Wang, JianzhouWang,JiaxingWang,JinhongWang,ShengjieWang,ShuyiWang,YaoWang,Yejie Wang,YiqinWang,YuxinWang,YuzhiWang,ZhaojiWang,ZhengtaoWang,ZhexuWang, ChuWei,QianqianWei,WenhaoWu,XingzheWu,YuxinWu,ChenjunXiao,XiaotongXie, WeiminXiong,BoyuXu,JingXu,JinjingXu,L.H.Xu,LinXu,SutingXu,WeixinXu,Xinran Xu,YangchuanXu,ZiyaoXu,JunjieYan,YuziYan,XiaofeiYang,YingYang,ZhenYang, ZhilinYang,ZonghanYang,HaotianYao,XingchengYao,WenjieYe,ZhuoruiYe,Bohong Yin,LonghuiYu,EnmingYuan,HongbangYuan,MengjieYuan,HaobingZhan,DehaoZhang, HaoZhang, WanluZhang, XiaobinZhang, YangkunZhang, YizhiZhang, YongtingZhang, Yu Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Haotian Zhao, Yikai Zhao, Huabin Zheng,ShaojieZheng,JianrenZhou,XinyuZhou,ZaidaZhou,ZhenZhu,WeiyuZhuang,and XinxingZu. Kimik2: Openagenticintelligence,2025. 12 --- Page 13 --- [26] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,CristianCantonFerrer,MoyaChen,GuillemCucurull,DavidEsiobu,JudeFernandes, JeremyFu,WenyinFu,BrianFuller,CynthiaGao,VedanujGoswami,NamanGoyal,Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,IsabelKloumann,ArtemKorenev,PunitSinghKoura,Marie-AnneLachaux,Thibaut Lavril,JenyaLee,DianaLiskovich,YinghaiLu,YuningMao,XavierMartinet,TodorMihaylov, PushkarMishra,IgorMolybog,YixinNie,AndrewPoulton,JeremyReizenstein,RashiRungta, KalyanSaladi,AlanSchelten,RuanSilva,EricMichaelSmith,RanjanSubramanian,Xiao- qingEllenTan,BinhTang,RossTaylor,AdinaWilliams,JianXiangKuan,PuxinXu,Zheng Yan,IliyanZarov,YuchenZhang,AngelaFan,MelanieKambadur,SharanNarang,Aurelien Rodriguez,RobertStojnic,SergeyEdunov,andThomasScialom. Llama2: Openfoundation andfine-tunedchatmodels,2023. [27] HongruWang,ChengQian,WanjunZhong,XiusiChen,JiahaoQiu,ShijueHuang,BowenJin, MengdiWang,Kam-FaiWong,andHengJi. Actinglessisreasoningmore! teachingmodelto actefficiently,2025. [28] XingyaoWang,YangyiChen,LifanYuan,YizheZhang,YunzhuLi,HaoPeng,andHengJi. Executablecodeactionselicitbetterllmagents,2024. [29] JiaxinWen,JianGuan,HongningWang,WeiWu,andMinlieHuang. Unlockingreasoning potentialinlargelangaugemodelsbyscalingcode-formplanning,2024. [30] BinfengXu,ZhiyuanPeng,BowenLei,SubhabrataMukherjee,YuchenLiu,andDongkuan Xu. Rewoo: Decouplingreasoningfromobservationsforefficientaugmentedlanguagemodels, 2023. [31] LingYang,ZhaochenYu,BinCui,andMengdiWang. Reasonflux: Hierarchicalllmreasoning viascalingthoughttemplates,2025. [32] ShunyuYao,NoahShinn,PedramRazavi,andKarthikNarasimhan. \u03c4-bench: Abenchmarkfor tool-agent-userinteractioninreal-worlddomains,2024. [33] ShunyuYao,DianYu,JeffreyZhao,IzhakShafran,ThomasL.Griffiths,YuanCao,andKarthik Narasimhan. Treeofthoughts: Deliberateproblemsolvingwithlargelanguagemodels,2023. [34] ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikNarasimhan,andYuan Cao. React: Synergizingreasoningandactinginlanguagemodels,2023. [35] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, TiantianFan,GaohongLiu,LingjunLiu,XinLiu,HaibinLin,ZhiqiLin,BoleMa,Guangming Sheng,YuxuanTong,ChiZhang,MofanZhang,WangZhang,HangZhu,JinhuaZhu,Jiaze Chen,JiangjieChen,ChengyiWang,HongliYu,YuxuanSong,XiangpengWei,HaoZhou, JingjingLiu,Wei-YingMa,Ya-QinZhang,LinYan,MuQiao,YonghuiWu,andMingxuan Wang. Dapo: Anopen-sourcellmreinforcementlearningsystematscale,2025. [36] AohanZeng,XiaoLiu,ZhengxiaoDu,ZihanWang,HanyuLai,MingDing,ZhuoyiYang,Yifan Xu,WendiZheng,XiaoXia,WengLamTam,ZixuanMa,YufeiXue,JidongZhai,Wenguang Chen, Peng Zhang, Yuxiao Dong, and Jie Tang. Glm-130b: An open bilingual pre-trained model,2023. [37] JianguoZhang,TianLan,MingZhu,ZuxinLiu,ThaiHoang,ShirleyKokane,WeiranYao, JuntaoTan,AksharaPrabhakar,HaolinChen,ZhiweiLiu,YihaoFeng,TulikaAwalgaonkar, RitheshMurthy,EricHu,ZeyuanChen,RanXu,JuanCarlosNiebles,ShelbyHeinecke,Huan Wang,SilvioSavarese,andCaimingXiong. xlam: Afamilyoflargeactionmodelstoempower aiagentsystems,2024. 13"
}