{
  "abstract": "Abstract WestudytheparametercomplexityofrobustmemorizationforReLUnetworks: thenumberofparametersrequiredtointerpolateanygivendatasetwith\u03f5-separation betweendifferentlylabeledpoints,whileensuringpredictionsremainconsistent withina\u00b5-ballaroundeachtrainingsample. Weestablishupperandlowerbounds ontheparametercountasafunctionoftherobustnessratio\u03c1=\u00b5/\u03f5. Unlikeprior work, weprovideafine-grainedanalysisacrosstheentirerange\u03c1 \u2208 (0,1)and obtain tighter upper and lower bounds that improve upon existing",
  "results": "results \u2225f (x)\u2212f (x )\u2225 \u2264 5\u03f5 . Since f 1 1 i 2 6 D\u2032\u2032 2 5-robustlymemorizeD\u2032\u2032,wehave 6 f(x)=f (f (x))=f (f (x ))=y . 2 1 2 1 i i Inparticular,f(x)=y foranyx\u2208B (x ,\u03c1\u03f5 ),concludingthatf isa\u03c1-robustmemorizerD. i 2 i D Regardingthenumberofnonzeroparameterstoconstructf,noticethatf consistsof(d+1)m= 1 O\u02dc(Nd\u03c12) nonzero parameters as m = O\u02dc(N\u03c12). f consists of O\u02dc(Nm) = O\u02dc(N2\u03c12) nonzero 2 parameters. SincethecaseVassumesN <d,wehave N2\u03c12 \u2264Nd\u03c12. Therefore,f intotalconsistsofO\u02dc(Nd\u03c12+N2\u03c12)=O\u02dc(Nd\u03c12)numberofnonzeroparameters. This provesthetheoremforthelastcase. NonzeroParameterCounts: ExistingUpperBounds. InSection1.1,theexistingupperboundis statedbycountingallparameters. Whencountingonlythenonzeroparameters,thecorresponding existingupperboundtakesadifferentform. Specifically,foranydatasetDwithinputdimensiond andsizeN,thereexistaneuralnetworkthatachievesrobustmemorizationonDwiththerobustness ratio\u03c1under\u2113 -norm,withthenumberofparametersP boundedasfollows: 2 \u221a \uf8f1 O\u02dc(N +d) if\u03c1\u2208(0,1/ d]. \uf8f2 \u221a \u221a P = O\u02dc(Nd2\u03c14+d) if\u03c1\u2208(1/ d,1/4d]. (85) \u221a \uf8f3O\u02dc(Nd) if\u03c1\u2208(1/4d,1). ThisisthecounterparttoEquation(2)thatconsidersallparametercounts. Asinthecaseoffull parametercount,thefirstandthethirdcaseinEquation(85)directlyfollowfromYuetal.[2024] andEgosietal.[2025]respectively. TheworkbyEgosietal.[2025]canbeimplicitlyimprovedto thesecondcaseunderthemoderate\u03c1condition,usingthesametranslationtechniqueprovidedin AppendixD.3. E.4 LemmasforNonzeroParameterCount Here,westateLemmasD.1andD.2\u2014thatcorrespondstoTheoremB.6ofYuetal.[2024]\u2014toits originalversionthatcontainsthenonzeroparametercountwith\u2113 -normintotheconsideration. 2 LemmaE.3(TheoremB.6, Yuetal.[2024]). ForanyD \u2208 D and\u03c1 \u2208 (0,1), define\u03b3 := d,N,C (1\u2212\u03c1)\u03f5 > 0andR > 1with\u2225x \u2225 \u2264 Rforalli \u2208 [N]. Then,thereexistsaneuralnetwork D i \u221e f with width O(d), depth O(N(log( d ) + logR)) that \u03c1-robustly memorizes D using at most \u03b32 O(Nd(log( d )+logR))nonzeroparameters. \u03b32 72",
  "introduction": "1 Introduction 1 1.1 WhatisKnownSoFar? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 SummaryofContribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2 Preliminaries 3 2.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.2 DatasetandRobustMemorization . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.3 ReLUNeuralNetwork . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.4 WhyOnly\u03c1=\u00b5/\u03f5 Matters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 D 3 NecessaryNumberofParametersforRobustMemorization 5 4 SufficientNumberofParametersforRobustMemorization 7 5 KeyProofIdeas 8 5.1 ProofSketchforProposition3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 5.2 ProofSketchforTheorem4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 6",
  "conclusion": "Conclusion 10 A ProofsforSection3 15 A.1 ExplicitProofofTheorem3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 A.2 NecessaryConditiononWidthforRobustMemorization . . . . . . . . . . . . . . 16 A.3 NecessaryConditiononParametersforRobustMemorization. . . . . . . . . . . . 17 A.4 LemmasforAppendixA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 B ProofsforSection4 22 B.1 SufficientConditionforRobustMemorizationwithSmallRobustnessRadius . . . 22 B.2 SufficientConditionforNear-PerfectRobustMemorizationwithModerateRobust- nessRadius . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 B.2.1 MemorizationofIntegerswithSublinearParametersinN . . . . . . . . . 30 B.2.2 PreciseControlofRobustMemorizationError . . . . . . . . . . . . . . . 33 B.3 SufficientConditionforRobustMemorizationwithLargeRobustnessRadius . . . 39 B.4 LemmasforLatticeMapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 B.5 DimensionReductionviaCarefulAnalysisoftheJohnson-LindenstraussLemma . 48 B.6 LemmasforBitComplexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 C Extensionsto\u2113 -norm 55 p C.1 ExtensionofNecessityConditionto\u2113 -norm . . . . . . . . . . . . . . . . . . . . 55 p C.1.1 LemmasforAppendixC.1 . . . . . . . . . . . . . . . . . . . . . . . . . . 59 C.2 ExtensionofSufficiencyConditionto\u2113 -norm . . . . . . . . . . . . . . . . . . . 60 p 13 --- Page 14 --- D ComparisontoExistingBounds 64 D.1 SummaryofParameterComplexityacross\u2113 -norms. . . . . . . . . . . . . . . . . 64 p D.2 ParameterComplexityoftheConstructionbyYuetal.[2024]. . . . . . . . . . . . 64 D.3 ParameterComplexityoftheConstructionbyEgosietal.[2025] . . . . . . . . . . 65 E NonzeroParameterCounts 66 E.1 NonzeroParameterCounts: Anillustration. . . . . . . . . . . . . . . . . . . . . . 66 E.2 NonzeroParameterCounts: LowerBounds . . . . . . . . . . . . . . . . . . . . . 66 E.3 NonzeroParameterCounts: UpperBounds . . . . . . . . . . . . . . . . . . . . . 67 E.4 LemmasforNonzeroParameterCount . . . . . . . . . . . . . . . . . . . . . . . . 72 14 --- Page 15 --- A ProofsforSection3 A.1 ExplicitProofofTheorem3.1 Theorem3.1. Let\u03c1\u2208(0,1). SupposeforanyD \u2208D ,thereexistsaneuralnetworkf \u2208F d,N,2 d,P thatcan\u03c1-robustlymemorizeD. Then,thenumberofparametersP mustsatisfy (cid:32) (cid:40) (cid:41) (cid:33) 1 \u221a \u221a P =\u2126 (\u03c12min{N,d}+1)d+min , d N . (cid:112) 1\u2212\u03c12 Proof. From Proposition 3.2, we obtain D \u2208 D such that any f : Rd \u2192 R that \u03c1-robustly d,N,2 memorizesDmusthavethefirsthiddenlayerwidthatleast\u03c12min{N \u22121,d}. Bytheassumption ofTheorem3.1,thereexistsf \u2208F that\u03c1-robustlymemorizesDwiththefirsthiddenlayerwidth d,P m\u2265\u03c12min{N \u22121,d}. Withthetriviallowerboundthatm\u22651,wehave 1 m\u2265max{\u03c12min{N \u22121,d},1}\u2265 (\u03c12min{N \u22121,d}+1). 2 SincewecountallparametersaccordingtoEquation(3),thenumberofparametersinthefirstlayer is(d+1)m. Therefore, 1 P \u2265(d+1)\u00b7m\u2265(d+1)\u00b7 (\u03c12min{N \u22121,d}+1)=\u2126(d(\u03c12min{N,d}+1)). 2 (cid:16) (cid:113) (cid:105) Inaddition,for\u03c1\u2208 0, 1\u2212 1 ,usingProposition3.3givesthelowerboundofparameters d (cid:32)(cid:115) (cid:33) N P =\u2126 . 1\u2212\u03c12 (cid:16) (cid:113) (cid:105) \u221a For\u03c1\u2208 0, 1\u2212 1 ,wehave \u221a1 \u2264 dsothatthefollowingrelationholds: d 1\u2212\u03c12 (cid:40) (cid:41) (cid:115) 1 \u221a \u221a N min , d \u00b7 N = . (cid:112) 1\u2212\u03c12 1\u2212\u03c12 (cid:16)(cid:113) (cid:17) \u221a (cid:113) For\u03c1\u2208 1\u2212 1,1 ,thelowerboundP =\u2126( Nd)obtainedbythecase\u03c1= 1\u2212 1 alsocan d \u221a d beapplied. Inthiscase, \u221a1 > dsothatthefollowingrelationholds: 1\u2212\u03c12 (cid:40) (cid:41) 1 \u221a \u221a \u221a min , d \u00b7 N = Nd. (cid:112) 1\u2212\u03c12 Hence,inboth\u03c1regimes, (cid:32) (cid:33) 1 \u221a \u221a P =\u2126 min{ , d} N , (cid:112) 1\u2212\u03c12 servesasthelowerboundonthenumberofparameters. BycombiningtheboundsfromProposition3.2andProposition3.3,weconclude: (cid:32) (cid:40) (cid:41)(cid:33) 1 \u221a \u221a P =\u2126 max (\u03c12min{N,d}+1)d,min{ , d} N (cid:112) 1\u2212\u03c12 (cid:32) (cid:33) 1 \u221a \u221a =\u2126 (\u03c12min{N,d}+1)d+min{ , d} N . (cid:112) 1\u2212\u03c12 15 --- Page 16 --- A.2 NecessaryConditiononWidthforRobustMemorization Proposition3.2. ThereexistsD \u2208D suchthat,forany\u03c1\u2208(0,1),anyneuralnetworkf :Rd \u2192 d,N,2 Rthat\u03c1-robustlymemorizesDmusthavethefirsthiddenlayerwidthatleast\u03c12min{N \u22121,d}. Proof. ToproveProposition3.2,weconsidertwocasesbasedontherelationshipbetweenN \u22121 andd. Inthefirstcase,whereN \u22121\u2264d,establishingthepropositionrequiresthatthefirsthidden layerhaswidthatleast\u03c12(N \u22121). Inthesecondcase,whereN \u22121>d,therequiredwidthisat least\u03c12d. Foreachcase,weconstructadatasetD \u2208D suchthatanynetworkthat\u03c1-robustly d,N,2 memorizesDmusthaveafirsthiddenlayerofwidthnosmallerthanthecorrespondingbound. Case I : N \u22121 \u2264 d. Let D = {(e ,2)} \u222a{(0,1)}. Then, D has separation constant j j\u2208[N\u22121] \u03f5 = 1/2. Letf beaneuralnetworkthat\u03c1-robustmemorizesD,anddenotethewidthofitsfirst D hiddenlayerasm. DenotebyW \u2208Rm\u00d7dtheweightmatrixofthefirsthiddenlayeroff. Assume forcontradictionthatm<\u03c12(N \u22121). Let \u00b5 = \u03c1\u03f5 denote the robustness radius. Then, the network f must distinguish every point in D B (e ,\u00b5)fromeverypointinB (0,\u00b5),forallj \u2208[N \u22121]. Therefore,foranyx\u2208B (e ,\u00b5)and 2 j 2 2 j x\u2032 \u2208B (0,\u00b5),wemusthave 2 Wx\u0338=Wx\u2032, orequivalently,x\u2212x\u2032 \u2208/ Null(W),whereNull(\u00b7)denotesthenullspaceofagivenmatrix. Note that B (e ,\u00b5)\u2212B (0,\u00b5):={x\u2212x\u2032 |x\u2208B (e ,\u00b5)andx\u2032 \u2208B (0,\u00b5)}=B (e ,2\u00b5). 2 j 2 2 j 2 2 j Hence,itisnecessarythatB (e ,2\u00b5)\u2229Null(W)=\u2205forallj \u2208[N \u22121],orequivalently, 2 j dist (e ,Null(W))\u22652\u00b5 forallj \u2208[N \u22121]. (5) 2 j Sincedim(Col(W\u22a4))\u2264m,whereCol(\u00b7)denotesthecolumnspaceofthegivenmatrix,itfollows thatdim(Null(W))\u2265d\u2212m. UsingLemmaA.2,wecanupperboundthedistancebetweentheset {e } \u2286Rdandanysubspaceofdimensiond\u2212m. j j\u2208[N\u22121] LetZ \u2286Null(W)beasubspacesuchthatdim(Z)=d\u2212m,andapplyLemmaA.2withsubstitutions d=d,t=N \u22121,k =d\u2212mandZ =Z. Theconditionsoflemma,namelyt\u2264dandk \u2265d\u2212t, aresatisfiedsinceN \u22121\u2264dandm<\u03c12(N \u22121)\u2264N \u22121. Therefore,weobtainthebound (cid:114) m min dist (e ,Z)\u2264 . j\u2208[N\u22121] 2 j N \u22121 BycombiningtheaboveinequalitywithEquation(5),weobtain (cid:114) (a) m 2\u00b5\u2264 min dist (e ,Null(W)) \u2264 min dist (e ,Z)\u2264 , (6) j\u2208[N\u22121] 2 j j\u2208[N\u22121] 2 j N \u22121 where (a) follows from that Z \u2286 Null(W). Since \u03f5 = 1/2, we have 2\u00b5 = 2\u03c1\u03f5 = \u03c1, so D D Equation(6)becomes (cid:114) m \u03c1\u2264 . N \u22121 Thisimpliesthatm \u2265 \u03c12(N \u22121),contradictingtheassumptionm < \u03c12(N \u22121). Therefore,the widthrequirementm\u2265\u03c12(N\u22121)isnecessary. ThisconcludesthestatementforthecaseN\u22121\u2264d. CaseII:N\u22121>d. Weconstructthefirstd+1datapointsinthesamemannerasinCaseI,using theconstructionforN =d+1. FortheremainingN \u2212d\u22121datapoints,wesetthemsufficiently distantfromthefirstd+1datapointstoensurethattheseparationconstantremains\u03f5 =1/2. D Inparticular,wesetx =2e , x =3e , \u00b7\u00b7\u00b7 , x =(N \u2212d)e andassigny =y = d+2 1 d+3 1 N 1 d+2 d+3 \u00b7\u00b7\u00b7=y =2. ComparedtothecaseN =d+1,thisconstructionpreserves\u03f5 whileaddingmore N D datapointstomemorize. Sincethefirstd+1datapointsareconstructedasinthecaseN =d+1, the same lower bound applies. Specifically, by the",
  "references": "References PeterLBartlett,NickHarvey,ChristopherLiaw,andAbbasMehrabian. Nearly-tightvc-dimension andpseudodimensionboundsforpiecewiselinearneuralnetworks. JournalofMachineLearning Research,20(63):1\u201317,2019. AlexanderBastounis,AndersCHansen,andVernerVlac\u02c7ic\u00b4. Themathematicsofadversarialattacks inai\u2013whydeeplearningisunstabledespitetheexistenceofstableneuralnetworks,2025. URL  EricBBaum. Onthecapabilitiesofmultilayerperceptrons. Journalofcomplexity,4(3):193\u2013215, 1988. S\u00e9bastienBubeck,RonenEldan,YinTatLee,andDanMikulincer. Networksizeandweightssize formemorizationwithtwo-layersneuralnetworks,2020. URL 02855. AYaChervonenkis.Ontheuniformconvergenceofrelativefrequenciesofeventstotheirprobabilities. InMeasuresofcomplexity: festschriftforalexeychervonenkis,pages11\u201330.Springer,2015. GavinWeiguangDing,KryYikChauLui,XiaomengJin,LuyuWang,andRuitongHuang. Onthe sensitivityofadversarialrobustnesstoinputdatadistributions,2019. URL abs/1902.08336. AmitsourEgosi,GiladYehudai,andOhadShamir. Logarithmicwidthsufficesforrobustmemoriza- tion,2025. URL RuiqiGao,TianleCai,HaochuanLi,Cho-JuiHsieh,LiweiWang,andJasonDLee. Convergence of adversarial training in overparametrized neural networks. Advances in Neural Information ProcessingSystems,32,2019. PaulWGoldbergandMarkRJerrum. Boundingthevapnik-chervonenkisdimensionofconcept classesparameterizedbyrealnumbers. MachineLearning,18(2-3):131\u2013148,1995. IanJ.Goodfellow,JonathonShlens,andChristianSzegedy. Explainingandharnessingadversarial examples,2015. URL SvenGowal,ChongliQin,JonathanUesato,TimothyMann,andPushmeetKohli. Uncoveringthe limits of adversarial training against norm-bounded adversarial examples, 2021. URL https: //arxiv.org/abs/2010.03593. BinghuiLi,JikaiJin,HanZhong,JohnHopcroft,andLiweiWang. Whyrobustgeneralizationindeep learningisdifficult: Perspectiveofexpressivepower. AdvancesinNeuralInformationProcessing Systems,35:4370\u20134384,2022. JiriMatousek. Lecturesondiscretegeometry,volume212. SpringerScience&BusinessMedia, 2013. Sejun Park, Jaeho Lee, Chulhee Yun, and Jinwoo Shin. Provable memorization via deep neural networksusingsub-linearparameters,2021. URL ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,DumitruErhan,IanGoodfellow, andRobFergus. Intriguingpropertiesofneuralnetworks, 2014. URL abs/1312.6199. 11 --- Page 12 --- Matus Telgarsky. Benefits of depth in neural networks. CoRR, abs/1602.04485, 2016. URL  GalVardi,GiladYehudai,andOhadShamir. OntheoptimalmemorizationpowerofReLUneural networks,2021. URL LijiaYu,Xiao-ShanGao,andLijunZhang. OPTIMALROBUSTMEMORIZATIONWITHRELU NEURALNETWORKS. InTheTwelfthInternationalConferenceonLearningRepresentations, 2024. URL ChulheeYun,SuvritSra,andAliJadbabaie. Smallrelunetworksarepowerfulmemorizers: atight analysisofmemorizationcapacity,2019. URL Chongzhi Zhang, Aishan Liu, Xianglong Liu, Yitao Xu, Hang Yu, Yuqing Ma, and Tianlin Li. Interpretingandimprovingadversarialrobustnessofdeepneuralnetworkswithneuronsensitivity. IEEETransactionsonImageProcessing,30:1291\u20131304,2021. ISSN1941-0042. doi: 10.1109/tip. 2020.3042083. URL 12 --- Page 13 --- Contents"
}