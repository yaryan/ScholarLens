--- Page 1 --- Swin Transformer: Hierarchical Vision Transformer using Shifted Windows ZeLiu†* YutongLin†* YueCao* HanHu*‡ YixuanWei† ZhengZhang StephenLin BainingGuo MicrosoftResearchAsia  Abstract This paper presents a new vision Transformer, called SwinTransformer,thatcapablyservesasageneral-purpose backbone for computer vision. Challenges in adapting Transformerfromlanguagetovisionarisefromdifferences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shiftedwindowingschemebringsgreaterefficiencybylim- iting self-attention computation to non-overlapping local Figure1.(a)TheproposedSwinTransformerbuildshierarchical windowswhilealsoallowingforcross-windowconnection. featuremapsbymergingimagepatches(showningray)indeeper This hierarchical architecture has the flexibility to model layersandhaslinearcomputationcomplexitytoinputimagesize due to computation of self-attention only within each local win- at various scales and has linear computational complexity dow(showninred). Itcanthusserveasageneral-purposeback- with respect to image size. These qualities of Swin Trans- bone for both image classification and dense recognition tasks. former make it compatible with a broad range of vision (b) In contrast, previous vision Transformers [20] produce fea- tasks, including image classification (87.3 top-1 accuracy ture maps of a single low resolution and have quadratic compu- onImageNet-1K)anddensepredictiontaskssuchasobject tationcomplexitytoinputimagesizeduetocomputationofself- detection (58.7 box AP and 51.1 mask AP on COCO test- attentionglobally. dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the- greaterscale[30,76],moreextensiveconnections[34],and artbyalargemarginof+2.7boxAPand+2.6maskAPon moresophisticatedformsofconvolution[70,18,84]. With COCO,and+3.2mIoUonADE20K,demonstratingthepo- CNNsservingasbackbonenetworksforavarietyofvision tential of Transformer-based models as vision backbones. tasks,thesearchitecturaladvanceshaveledtoperformance The hierarchical design and the shifted window approach improvementsthathavebroadlyliftedtheentirefield. also prove beneficial for all-MLP architectures. The code Ontheotherhand,theevolutionofnetworkarchitectures andmodelsarepubliclyavailableat innaturallanguageprocessing(NLP)hastakenadifferent com/microsoft/Swin-Transformer. path, where the prevalent architecture today is instead the Transformer [64]. Designed for sequence modeling and transduction tasks, the Transformer is notable for its use 1.Introduction of attention to model long-range dependencies in the data. Its tremendous success in the language domain has led re- Modeling in computer vision has long been dominated searchers to investigate its adaptation to computer vision, byconvolutionalneuralnetworks(CNNs). Beginningwith whereithasrecentlydemonstratedpromisingresultsoncer- AlexNet [39] and its revolutionary performance on the tain tasks, specifically image classification [20] and joint ImageNet image classification challenge, CNN architec- vision-languagemodeling[47]. tureshaveevolvedtobecomeincreasinglypowerfulthrough In this paper, we seek to expand the applicability of *Equalcontribution.†InternsatMSRA.‡Contactperson. Transformer such that it can serve as a general-purpose 1202 guA 71 ]VC.sc[ 2v03041.3012:viXra --- Page 2 --- backbone for computer vision, as it does for NLP and as CNNs do in vision. We observe that significant chal- lengesintransferringitshighperformanceinthelanguage domain to the visual domain can be explained by differ- ences between the two modalities. One of these differ- ences involves scale. Unlike the word tokens that serve as the basic elements of processing in language Trans- formers, visual elements can vary substantially in scale, a problem that receives attention in tasks such as object de- Figure2.Anillustrationoftheshiftedwindowapproachforcom- putingself-attentionintheproposedSwinTransformerarchitec- tection [42, 53, 54]. In existing Transformer-based mod- ture. In layer l (left), a regular window partitioning scheme is els [64, 20], tokens are all of a fixed scale, a property un- adopted, and self-attention is computed within each window. In suitable for these vision applications. Another difference thenextlayerl+1(right),thewindowpartitioningisshifted,re- is the much higher resolution of pixels in images com- sultinginnewwindows.Theself-attentioncomputationinthenew pared to words in passages of text. There exist many vi- windowscrossestheboundariesofthepreviouswindowsinlayer siontaskssuchassemanticsegmentationthatrequiredense l,providingconnectionsamongthem. prediction at the pixel level, and this would be intractable for Transformer on high-resolution images, as the compu- shifted window approach has much lower latency than the tational complexity of its self-attention is quadratic to im- sliding window method, yet is similar in modeling power agesize. Toovercometheseissues, weproposeageneral- (see Tables 5 and 6). The shifted window approach also purpose Transformer backbone, called Swin Transformer, provesbeneficialforall-MLParchitectures[61]. which constructs hierarchical feature maps and has linear TheproposedSwinTransformerachievesstrongperfor- computational complexity to image size. As illustrated in manceontherecognitiontasksofimageclassification,ob- Figure1(a),SwinTransformerconstructsahierarchicalrep- ject detection and semantic segmentation. It outperforms resentationbystartingfromsmall-sizedpatches(outlinedin theViT/DeiT[20,63]andResNe(X)tmodels[30,70]sig- gray)andgraduallymergingneighboringpatchesindeeper nificantly with similar latency on the three tasks. Its 58.7 Transformer layers. With these hierarchical feature maps, box AP and 51.1 mask AP on the COCO test-dev set sur- theSwinTransformermodelcanconvenientlyleveragead- pass the previous state-of-the-art results by +2.7 box AP vancedtechniquesfordensepredictionsuchasfeaturepyra- (Copy-paste[26]withoutexternaldata)and+2.6maskAP midnetworks(FPN)[42]orU-Net[51]. Thelinearcompu- (DetectoRS [46]). On ADE20K semantic segmentation, it tationalcomplexityisachievedbycomputingself-attention obtains 53.5 mIoU on the val set, an improvement of +3.2 locally within non-overlapping windows that partition an mIoUoverthepreviousstate-of-the-art(SETR[81]).Italso image (outlined in red). The number of patches in each achievesatop-1accuracyof87.3%onImageNet-1Kimage window is fixed, and thus the complexity becomes linear classification. to image size. These merits make Swin Transformer suit- It is our belief that a unified architecture across com- ableasageneral-purposebackboneforvariousvisiontasks, puter vision and natural language processing could benefit incontrasttopreviousTransformerbasedarchitectures[20] both fields, since it would facilitate joint modeling of vi- whichproducefeaturemapsofasingleresolutionandhave sual and textual signals and the modeling knowledge from quadraticcomplexity. both domains can be more deeply shared. We hope that A key design element of Swin Transformer is its shift Swin Transformer’s strong performance on various vision ofthewindowpartitionbetweenconsecutiveself-attention problemscandrivethisbeliefdeeperinthecommunityand layers, as illustrated in Figure 2. The shifted windows encourageunifiedmodelingofvisionandlanguagesignals. bridge the windows of the preceding layer, providing con- 2.RelatedWork nections among them that significantly enhance modeling power (see Table 4). This strategy is also efficient in re- CNN and variants CNNs serve as the standard network gardstoreal-worldlatency: allquerypatcheswithinawin- modelthroughoutcomputervision. WhiletheCNNhasex- dowsharethesamekeyset1, whichfacilitatesmemoryac- istedforseveraldecades[40],itwasnotuntiltheintroduc- cessinhardware. Incontrast,earlierslidingwindowbased tion of AlexNet [39] that the CNN took off and became self-attention approaches [33, 50] suffer from low latency mainstream. Since then, deeper and more effective con- on general hardware due to different key sets for different volutional neural architectures have been proposed to fur- query pixels2. Our experiments show that the proposed therpropelthedeeplearningwaveincomputervision,e.g., VGG [52], GoogleNet [57], ResNet [30], DenseNet [34], 1Thequeryandkeyareprojectionvectorsinaself-attentionlayer. 2While there are efficient methods to implement a sliding-window weights across a feature map, it is difficult for a sliding-window based basedconvolutionlayerongeneralhardware,thankstoitssharedkernel self-attentionlayertohaveefficientmemoryaccessinpractice. 2 --- Page 3 --- HRNet [65], and EfficientNet [58]. In addition to these resolution is high, due to its low-resolution feature maps architectural advances, there has also been much work on and the quadratic increase in complexity with image size. improving individual convolution layers, such as depth- There are a few works applying ViT models to the dense wiseconvolution[70]anddeformableconvolution[18,84]. vision tasks of object detection and semantic segmenta- While the CNN and its variants are still the primary back- tion by direct upsampling or deconvolution but with rela- bone architectures for computer vision applications, we tively lower performance [2, 81]. Concurrent to our work highlightthestrongpotentialofTransformer-likearchitec- are some that modify the ViT architecture [72, 15, 28] tures for unified modeling between vision and language. for better image classification. Empirically, we find our Ourworkachievesstrongperformanceonseveralbasicvi- Swin Transformer architecture to achieve the best speed- sual recognition tasks, and we hope it will contribute to a accuracy trade-off among these methods on image classi- modelingshift. fication,eventhoughourworkfocusesongeneral-purpose performance rather than specifically on classification. An- Self-attention based backbone architectures Also in- otherconcurrentwork[66]exploresasimilarlineofthink- spired by the success of self-attention layers and Trans- ing to build multi-resolution feature maps on Transform- former architectures in the NLP field, some works employ ers. Its complexity is still quadratic to image size, while self-attentionlayerstoreplacesomeorallofthespatialcon- ours is linear and also operates locally which has proven volutionlayersinthepopularResNet[33,50,80]. Inthese beneficial in modeling the high correlation in visual sig- works,theself-attentioniscomputedwithinalocalwindow nals [36, 25, 41]. Our approach is both efficient and ef- ofeachpixeltoexpediteoptimization[33],andtheyachieve fective, achieving state-of-the-art accuracy on both COCO slightlybetteraccuracy/FLOPstrade-offsthanthecounter- objectdetectionandADE20Ksemanticsegmentation. part ResNet architecture. However, their costly memory access causes their actual latency to be significantly larger 3.Method thanthatoftheconvolutionalnetworks[33]. Insteadofus- 3.1.OverallArchitecture ingslidingwindows,weproposetoshiftwindowsbetween consecutivelayers,whichallowsforamoreefficientimple- AnoverviewoftheSwinTransformerarchitectureispre- mentationingeneralhardware. sentedinFigure3,whichillustratesthetinyversion(Swin- T).ItfirstsplitsaninputRGBimageintonon-overlapping Self-attention/TransformerstocomplementCNNs An- patchesbyapatchsplittingmodule,likeViT.Eachpatchis otherlineofworkistoaugmentastandardCNNarchitec- treatedasa“token”anditsfeatureissetasaconcatenation ture with self-attention layers or Transformers. The self- oftherawpixelRGBvalues.Inourimplementation,weuse attention layers can complement backbones [67, 7, 3, 71, apatchsizeof4×4andthusthefeaturedimensionofeach 23,74,55]orheadnetworks[32,27]byprovidingtheca- patch is 4×4×3 = 48. A linear embedding layer is ap- pability to encode distant dependencies or heterogeneous pliedonthisraw-valuedfeaturetoprojectittoanarbitrary interactions. Morerecently, theencoder-decoderdesignin dimension(denotedasC). Transformer has been applied for the object detection and SeveralTransformerblockswithmodifiedself-attention instance segmentation tasks [8, 13, 85, 56]. Our work ex- computation(SwinTransformerblocks)areappliedonthese plores the adaptation of Transformers for basic visual fea- patchtokens. TheTransformerblocksmaintainthenumber tureextractionandiscomplementarytotheseworks. oftokens(H ×W),andtogetherwiththelinearembedding 4 4 arereferredtoas“Stage1”. Transformer based vision backbones Most related to Toproduceahierarchicalrepresentation, thenumberof our work is the Vision Transformer (ViT) [20] and its tokens is reduced by patch merging layers as the network follow-ups [63, 72, 15, 28, 66]. The pioneering work of getsdeeper. Thefirstpatchmerginglayerconcatenatesthe ViT directly applies a Transformer architecture on non- features of each group of 2×2 neighboring patches, and overlapping medium-sized image patches for image clas- applies a linear layer on the 4C-dimensional concatenated sification. It achieves an impressive speed-accuracy trade- features. This reduces the number of tokens by a multiple off on image classification compared to convolutional net- of2×2=4(2×downsamplingofresolution),andtheout- works. While ViT requires large-scale training datasets put dimension is set to 2C. Swin Transformer blocks are (i.e.,JFT-300M)toperformwell,DeiT[63]introducessev- appliedafterwardsforfeaturetransformation,withtheres- eral training strategies that allow ViT to also be effective olution kept at H × W. This first block of patch merging 8 8 usingthesmallerImageNet-1Kdataset. TheresultsofViT andfeaturetransformationisdenotedas“Stage2”.Thepro- on image classification are encouraging, but its architec- cedure is repeated twice, as “Stage 3” and “Stage 4”, with ture is unsuitable for use as a general-purpose backbone output resolutions of H × W and H × W, respectively. 16 16 32 32 network on dense vision tasks or when the input image These stages jointly produce a hierarchical representation, 3 --- Page 4 --- MLP MLP Stage 1 Stage 2 Stage 3 Stage 4 LN LN g on din ng ng ng Images Patch Partiti Linear Embed Tra BnS s lw ofo cin r kmer Patch Mergi Tra BnS s lw ofo cin r kmer Patch Mergi Tra BnS s lw ofo cin r kmer Patch Mergi Tra BnS s lw ofo cin r kmer W- LM NSA SW L-M NSA 2 2 6 2 (a) Architecture (b) Two Successive Swin Transformer Blocks Figure 3. (a) The architecture of a Swin Transformer (Swin-T); (b) two successive Swin Transformer Blocks (notation presented with Eq.(3)).W-MSAandSW-MSAaremulti-headselfattentionmoduleswithregularandshiftedwindowingconfigurations,respectively. with the same feature map resolutions as those of typical onanimageofh×wpatchesare3: convolutional networks, e.g., VGG [52] and ResNet [30]. As a result, the proposed architecture can conveniently re- Ω(MSA)=4hwC2+2(hw)2C, (1) place the backbone networks in existing methods for vari- Ω(W-MSA)=4hwC2+2M2hwC, (2) ousvisiontasks. wheretheformerisquadratictopatchnumberhw,andthe latterislinearwhenM isfixed(setto7bydefault). Global self-attention computation is generally unaffordable for a Swin Transformer block Swin Transformer is built by largehw,whilethewindowbasedself-attentionisscalable. replacing the standard multi-head self attention (MSA) module in a Transformer block by a module based on Shifted window partitioning in successive blocks The shiftedwindows(describedinSection3.2),withotherlay- window-based self-attention module lacks connections ers kept the same. As illustrated in Figure 3(b), a Swin acrosswindows,whichlimitsitsmodelingpower. Tointro- TransformerblockconsistsofashiftedwindowbasedMSA ducecross-windowconnectionswhilemaintainingtheeffi- module, followed by a 2-layer MLP with GELU non- cientcomputationofnon-overlappingwindows,wepropose linearity in between. A LayerNorm (LN) layer is applied ashiftedwindowpartitioningapproachwhichalternatesbe- before each MSA module and each MLP, and a residual tween two partitioning configurations in consecutive Swin connectionisappliedaftereachmodule. Transformerblocks. AsillustratedinFigure2,thefirstmoduleusesaregular 3.2.ShiftedWindowbasedSelf-Attention windowpartitioningstrategywhichstartsfromthetop-left pixel, and the 8×8 feature map is evenly partitioned into ThestandardTransformerarchitecture[64]anditsadap- 2×2windowsofsize4×4(M =4). Then,thenextmod- tationforimageclassification[20]bothconductglobalself- ule adopts a windowing configuration that is shifted from attention, where the relationships between a token and all that of the preceding layer, by displacing the windows by othertokensarecomputed.Theglobalcomputationleadsto ((cid:98)M(cid:99),(cid:98)M(cid:99))pixelsfromtheregularlypartitionedwindows. quadraticcomplexitywithrespecttothenumberoftokens, 2 2 Withtheshiftedwindowpartitioningapproach, consec- makingitunsuitableformanyvisionproblemsrequiringan utiveSwinTransformerblocksarecomputedas immensesetoftokensfordensepredictionortorepresenta high-resolutionimage. zˆl =W-MSA(cid:0) LN(cid:0) zl−1(cid:1)(cid:1) +zl−1, zl =MLP(cid:0) LN(cid:0) zˆl(cid:1)(cid:1) +zˆl, zˆl+1 =SW-MSA(cid:0) LN(cid:0) zl(cid:1)(cid:1) +zl, Self-attentioninnon-overlappedwindows Forefficient zl+1 =MLP(cid:0) LN(cid:0) zˆl+1(cid:1)(cid:1) +zˆl+1, (3) modeling, we propose to compute self-attention within lo- calwindows. Thewindowsarearrangedtoevenlypartition where zˆl and zl denote the output features of the (S)W- the image in a non-overlapping manner. Supposing each MSAmoduleandtheMLPmoduleforblockl,respectively; windowcontainsM ×M patches,thecomputationalcom- plexity of a global MSA module and a window based one 3WeomitSoftMaxcomputationindeterminingcomplexity. 4 --- Page 5 --- A CC A C We observe significant improvements over counterparts masked MSA withoutthisbiastermorthatuseabsolutepositionembed- B B B B ... ding, as shown in Table 4. Further adding absolute posi- masked window partition C A C AAAA tion embedding to the input as in [20] drops performance MSA cyclic shift reverse cyclic shift slightly,thusitisnotadoptedinourimplementation. Figure 4. Illustration of an efficient batch computation approach The learnt relative position bias in pre-training can be forself-attentioninshiftedwindowpartitioning. alsousedtoinitializeamodelforfine-tuningwithadiffer- entwindowsizethroughbi-cubicinterpolation[20,63]. W-MSA and SW-MSA denote window based multi-head 3.3.ArchitectureVariants self-attentionusingregularandshiftedwindowpartitioning We build our base model, called Swin-B, to have of configurations,respectively. model size and computation complexity similar to ViT- The shifted window partitioning approach introduces B/DeiT-B.WealsointroduceSwin-T,Swin-SandSwin-L, connections between neighboring non-overlapping win- whichareversionsofabout0.25×,0.5×and2×themodel dowsinthepreviouslayerandisfoundtobeeffectiveinim- sizeandcomputationalcomplexity, respectively. Notethat ageclassification,objectdetection,andsemanticsegmenta- the complexity of Swin-T and Swin-S are similar to those tion,asshowninTable4. of ResNet-50 (DeiT-S) and ResNet-101, respectively. The windowsizeissettoM = 7bydefault. Thequerydimen- Efficient batch computation for shifted configuration sion of each head is d = 32, and the expansion layer of Anissuewithshiftedwindowpartitioningisthatitwillre- each MLP is α = 4, for all experiments. The architecture sultinmorewindows, from(cid:100) h (cid:101)×(cid:100)w(cid:101)to((cid:100) h (cid:101)+1)× hyper-parametersofthesemodelvariantsare: M M M ((cid:100)w(cid:101)+1)intheshiftedconfiguration,andsomeofthewin- doM wswillbesmallerthanM ×M4. Anaivesolutionisto • Swin-T:C =96,layernumbers={2,2,6,2} pad the smaller windows to a size of M × M and mask • Swin-S:C =96,layernumbers={2,2,18,2} out the padded values when computing attention. When thenumberofwindowsinregularpartitioningissmall,e.g. • Swin-B:C =128,layernumbers={2,2,18,2} 2×2,theincreasedcomputationwiththisnaivesolutionis • Swin-L:C =192,layernumbers={2,2,18,2} considerable(2×2 → 3×3,whichis2.25timesgreater). Here, we propose a more efficient batch computation ap- whereC isthechannelnumberofthehiddenlayersinthe proachbycyclic-shiftingtowardthetop-leftdirection,asil- firststage. Themodelsize,theoreticalcomputationalcom- lustratedinFigure4.Afterthisshift,abatchedwindowmay plexity (FLOPs), and throughput of the model variants for be composed of several sub-windows that are not adjacent ImageNetimageclassificationarelistedinTable1. inthefeaturemap,soamaskingmechanismisemployedto limitself-attentioncomputationtowithineachsub-window. 4.Experiments With the cyclic-shift, the number of batched windows re- mainsthesameasthatofregularwindowpartitioning,and WeconductexperimentsonImageNet-1Kimageclassi- thus is also efficient. The low latency of this approach is fication [19], COCO object detection [43], and ADE20K showninTable5. semanticsegmentation[83]. Inthefollowing,wefirstcom- pare the proposed Swin Transformer architecture with the previous state-of-the-arts on the three tasks. Then, we ab- Relative position bias In computing self-attention, we latetheimportantdesignelementsofSwinTransformer. follow [49, 1, 32, 33] by including a relative position bias B ∈RM2×M2 toeachheadincomputingsimilarity: 4.1.ImageClassificationonImageNet-1K √ Attention(Q,K,V)=SoftMax(QKT/ d+B)V, (4) Settings Forimageclassification,webenchmarkthepro- posedSwinTransformeronImageNet-1K[19],whichcon- whereQ,K,V ∈ RM2×d arethequery,keyandvaluema- tains 1.28M training images and 50K validation images from 1,000 classes. The top-1 accuracy on a single crop trices;disthequery/keydimension,andM2 isthenumber isreported. Weconsidertwotrainingsettings: of patches in a window. Since the relative position along eachaxisliesintherange[−M+1,M−1],weparameter- • Regular ImageNet-1K training. This setting mostly izeasmaller-sizedbiasmatrixBˆ ∈ R(2M−1)×(2M−1),and follows [63]. We employ an AdamW [37] optimizer valuesinBaretakenfromBˆ. for 300 epochs using a cosine decay learning rate 4Tomakethewindowsize(M,M)divisiblebythefeaturemapsizeof scheduler and 20 epochs of linear warm-up. A batch (h,w),bottom-rightpaddingisemployedonthefeaturemapifneeded. size of 1024, an initial learning rate of 0.001, and a 5 --- Page 6 --- weight decay of 0.05 are used. We include most of (a)RegularImageNet-1Ktrainedmodels theaugmentationandregularizationstrategiesof[63] image throughputImageNet method #param.FLOPs intraining,exceptforrepeatedaugmentation[31]and size (image/s)top-1acc. RegNetY-4G[48] 2242 21M 4.0G 1156.7 80.0 EMA [45], which do not enhance performance. Note RegNetY-8G[48] 2242 39M 8.0G 591.6 81.7 thatthisiscontraryto[63]whererepeatedaugmenta- RegNetY-16G[48] 2242 84M 16.0G 334.7 82.9 tioniscrucialtostabilizethetrainingofViT. EffNet-B3[58] 3002 12M 1.8G 732.1 81.6 EffNet-B4[58] 3802 19M 4.2G 349.4 82.9 • Pre-training on ImageNet-22K and fine-tuning on EffNet-B5[58] 4562 30M 9.9G 169.1 83.6 ImageNet-1K. We also pre-train on the larger EffNet-B6[58] 5282 43M 19.0G 96.9 84.0 ImageNet-22K dataset, which contains 14.2 million EffNet-B7[58] 6002 66M 37.0G 55.1 84.3 imagesand22Kclasses. WeemployanAdamWopti- ViT-B/16[20] 3842 86M 55.4G 85.9 77.9 mizerfor90epochsusingalineardecaylearningrate ViT-L/16[20] 3842 307M 190.7G 27.3 76.5 schedulerwitha5-epochlinearwarm-up.Abatchsize DeiT-S[63] 2242 22M 4.6G 940.4 79.8 DeiT-B[63] 2242 86M 17.5G 292.3 81.8 of4096,aninitiallearningrateof0.001,andaweight DeiT-B[63] 3842 86M 55.4G 85.9 83.1 decay of 0.01 are used. In ImageNet-1K fine-tuning, Swin-T 2242 29M 4.5G 755.2 81.3 wetrainthemodelsfor30epochswithabatchsizeof Swin-S 2242 50M 8.7G 436.9 83.0 1024, a constant learning rate of 10−5, and a weight Swin-B 2242 88M 15.4G 278.1 83.5 decayof10−8. Swin-B 3842 88M 47.0G 84.7 84.5 (b)ImageNet-22Kpre-trainedmodels image throughputImageNet method #param.FLOPs Results with regular ImageNet-1K training Table 1(a) size (image/s)top-1acc. presents comparisons to other backbones, including both R-101x3[38] 3842 388M 204.6G - 84.4 Transformer-based and ConvNet-based, using regular R-152x4[38] 4802 937M 840.5G - 85.4 ImageNet-1Ktraining. ViT-B/16[20] 3842 86M 55.4G 85.9 84.0 ViT-L/16[20] 3842 307M 190.7G 27.3 85.2 Compared to the previous state-of-the-art Transformer- Swin-B 2242 88M 15.4G 278.1 85.2 based architecture, i.e. DeiT [63], Swin Transformers no- Swin-B 3842 88M 47.0G 84.7 86.4 ticeably surpass the counterpart DeiT architectures with Swin-L 3842 197M 103.9G 42.1 87.3 similar complexities: +1.5% for Swin-T (81.3%) over Table1.ComparisonofdifferentbackbonesonImageNet-1Kclas- DeiT-S (79.8%) using 2242 input, and +1.5%/1.4% for sification. Throughput is measured using the GitHub repository Swin-B (83.3%/84.5%) over DeiT-B (81.8%/83.1%) using of[68]andaV100GPU,following[63]. 2242/3842input,respectively. Compared with the state-of-the-art ConvNets, i.e. Reg- Net [48] and EfficientNet [58], the Swin Transformer 4.2.ObjectDetectiononCOCO achieves a slightly better speed-accuracy trade-off. Not- ing that while RegNet [48] and EfficientNet [58] are ob- Settings Object detection and instance segmentation ex- tained via a thorough architecture search, the proposed periments are conducted on COCO 2017, which contains SwinTransformerisadaptedfromthestandardTransformer 118Ktraining,5Kvalidationand20Ktest-devimages. An andhasstrongpotentialforfurtherimprovement. ablation study is performed using the validation set, and a system-level comparison is reported on test-dev. For the ablation study, we consider four typical object detection Results with ImageNet-22K pre-training We also pre- frameworks: Cascade Mask R-CNN [29, 6], ATSS [79], trainthelarger-capacitySwin-BandSwin-LonImageNet- RepPoints v2 [12], and Sparse RCNN [56] in mmdetec- 22K.Resultsfine-tunedonImageNet-1Kimageclassifica- tion [10]. For these four frameworks, we utilize the same tion are shown in Table 1(b). For Swin-B, the ImageNet- settings:multi-scaletraining[8,56](resizingtheinputsuch 22K pre-training brings 1.8%∼1.9% gains over training thattheshortersideisbetween480and800whilethelonger on ImageNet-1K from scratch. Compared with the previ- sideisatmost1333),AdamW[44]optimizer(initiallearn- ous best results for ImageNet-22K pre-training, our mod- ingrateof0.0001,weightdecayof0.05,andbatchsizeof els achieve significantly better speed-accuracy trade-offs: 16),and3xschedule(36epochs).Forsystem-levelcompar- Swin-Bobtains86.4%top-1accuracy,whichis2.4%higher ison, weadoptanimprovedHTC[9](denotedasHTC++) than that of ViT with similar inference throughput (84.7 with instaboost [22], stronger multi-scale training [7], 6x vs. 85.9 images/sec) and slightly lower FLOPs (47.0G vs. schedule (72 epochs), soft-NMS [5], and ImageNet-22K 55.4G).ThelargerSwin-Lmodelachieves87.3%top-1ac- pre-trainedmodelasinitialization. curacy,+0.9%betterthanthatoftheSwin-Bmodel. We compare our Swin Transformer to standard Con- 6 --- Page 7 --- (a)Variousframeworks ADE20K val test #param. FLOPs FPS Method BackboneAPboxAPboxAPbox #param.FLOPsFPS Method Backbone mIoU score 50 75 Cascade R-50 46.3 64.3 50.5 82M 739G 18.0 DANet[23] ResNet-101 45.2 - 69M 1119G 15.2 MaskR-CNN Swin-T 50.5 69.3 54.9 86M 745G 15.3 DLab.v3+[11] ResNet-101 44.1 - 63M 1021G 16.0 R-50 43.5 61.9 47.0 32M 205G 28.3 ACNet[24] ResNet-101 45.9 38.5 - ATSS Swin-T 47.2 66.5 51.3 36M 215G 22.3 DNL[71] ResNet-101 46.0 56.2 69M 1249G 14.8 R-50 46.5 64.6 50.3 42M 274G 13.6 OCRNet[73] ResNet-101 45.3 56.0 56M 923G 19.3 RepPointsV2 Swin-T 50.0 68.5 54.2 45M 283G 12.0 UperNet[69] ResNet-101 44.9 - 86M 1029G 20.1 Sparse R-50 44.5 63.4 48.2 106M 166G 21.0 OCRNet[73] HRNet-w48 45.7 - 71M 664G 12.5 R-CNN Swin-T 47.9 67.3 52.3 110M 172G 18.4 DLab.v3+[11] ResNeSt-101 46.9 55.1 66M 1051G 11.9 (b)Variousbackbonesw.CascadeMaskR-CNN DLab.v3+[11] ResNeSt-200 48.4 - 88M 1381G 8.1 APboxAPboxAPboxAPmaskAPmaskAPmaskparamFLOPsFPS SETR[81] T-Large‡ 50.3 61.7 308M - - DeiT-S† 48.0 675 .20 517 .75 41.4 645 .0 2 447 .5 3 80M 889G 10.4 UperNet DeiT-S† 44.0 - 52M 1099G 16.2 R50 46.3 64.3 50.5 40.1 61.7 43.4 82M 739G 18.0 UperNet Swin-T 46.1 - 60M 945G 18.5 Swin-T 50.5 69.3 54.9 43.7 66.6 47.1 86M 745G 15.3 UperNet Swin-S 49.3 - 81M 1038G 15.2 X101-32 48.1 66.5 52.4 41.6 63.9 45.2 101M 819G 12.8 UperNet Swin-B‡ 51.6 - 121M 1841G 8.7 Swin-S 51.8 70.4 56.3 44.7 67.9 48.5 107M 838G 12.0 UperNet Swin-L‡ 53.5 62.8 234M 3230G 6.2 X101-64 48.3 66.4 52.3 41.7 64.0 45.1 140M 972G 10.4 Table 3. Results of semantic segmentation on the ADE20K val Swin-B 51.9 70.9 56.5 45.0 68.4 48.7 145M 982G 11.6 andtestset. † indicatesadditionaldeconvolutionlayersareused toproducehierarchicalfeaturemaps.‡indicatesthatthemodelis (c)System-levelComparison pre-trainedonImageNet-22K. mini-val test-dev Method #param.FLOPs APboxAPmask APboxAPmask RepPointsV2*[12] - - 52.1 - - - under different model capacity using Cascade Mask R- GCNet*[7] 51.8 44.7 52.3 45.4 - 1041G CNN.SwinTransformerachievesahighdetectionaccuracy RelationNet++*[13] - - 52.7 - - - of 51.9 box AP and 45.0 mask AP, which are significant SpineNet-190[21] 52.6 - 52.8 - 164M 1885G gainsof+3.6boxAPand+3.3maskAPoverResNeXt101- ResNeSt-200*[78] 52.5 - 53.3 47.1 - - 64x4d, which has similar model size, FLOPs and latency. EfficientDet-D7[59] 54.4 - 55.1 - 77M 410G DetectoRS*[46] - - 55.7 48.5 - - Onahigherbaselineof52.3boxAPand46.0maskAPus- YOLOv4P7*[4] - - 55.8 - - - inganimprovedHTCframework,thegainsbySwinTrans- Copy-paste[26] 55.9 47.2 56.0 47.4 185M 1440G formerarealsohigh,at+4.1boxAPand+3.1maskAP(see X101-64(HTC++) 52.3 46.0 - - 155M 1033G Table2(c)). Regardinginferencespeed,whileResNe(X)tis Swin-B(HTC++) 56.4 49.1 - - 160M 1043G builtbyhighlyoptimizedCudnnfunctions,ourarchitecture Swin-L(HTC++) 57.1 49.5 57.7 50.2 284M 1470G isimplementedwithbuilt-inPyTorchfunctionsthatarenot Swin-L(HTC++)* 58.0 50.4 58.7 51.1 284M - all well-optimized. A thorough kernel optimization is be- Table2.ResultsonCOCOobjectdetectionandinstancesegmen- yondthescopeofthispaper. tation. †denotes that additional decovolution layers are used to producehierarchicalfeaturemaps.*indicatesmulti-scaletesting. Comparison to DeiT The performance of DeiT-S us- ing the Cascade Mask R-CNN framework is shown in Ta- vNets,i.e. ResNe(X)t,andpreviousTransformernetworks, ble 2(b). The results of Swin-T are +2.5 box AP and +2.3 e.g.DeiT.Thecomparisonsareconductedbychangingonly maskAPhigherthanDeiT-Swithsimilarmodelsize(86M the backbones with other settings unchanged. Note that vs.80M)andsignificantlyhigherinferencespeed(15.3FPS whileSwinTransformerandResNe(X)taredirectlyappli- vs. 10.4FPS).ThelowerinferencespeedofDeiTismainly cable to all the above frameworks because of their hierar- duetoitsquadraticcomplexitytoinputimagesize. chical feature maps, DeiT only produces a single resolu- tionoffeaturemapsandcannotbedirectlyapplied. Forfair Comparison to previous state-of-the-art Table 2(c) comparison,wefollow[81]toconstructhierarchicalfeature compares our best results with those of previous state-of- mapsforDeiTusingdeconvolutionlayers. the-art models. Our best model achieves 58.7 box AP and 51.1 mask AP on COCO test-dev, surpassing the previous ComparisontoResNe(X)t Table2(a)liststheresultsof bestresultsby+2.7boxAP(Copy-paste[26]withoutexter- Swin-TandResNet-50onthefourobjectdetectionframe- naldata)and+2.6maskAP(DetectoRS[46]). works.OurSwin-Tarchitecturebringsconsistent+3.4∼4.2 4.3.SemanticSegmentationonADE20K box AP gains over ResNet-50, with slightly larger model size,FLOPsandlatency. Settings ADE20K [83] is a widely-used semantic seg- Table 2(b) compares Swin Transformer and ResNe(X)t mentationdataset, coveringabroadrangeof150semantic 7 --- Page 8 --- ImageNet COCO ADE20k MSAinastage(ms) Arch.(FPS) method top-1 top-5 APbox APmask mIoU S1 S2 S3 S4 T S B w/oshifting 80.2 95.1 47.7 41.5 43.3 slidingwindow(naive) 122.5 38.3 12.1 7.6 183 109 77 shiftedwindows 81.3 95.6 50.5 43.7 46.1 slidingwindow(kernel) 7.6 4.7 2.7 1.8 488 283 187 nopos. 80.1 94.9 49.2 42.6 43.8 Performer[14] 4.8 2.8 1.8 1.5 638 370 241 abs.pos. 80.5 95.2 49.0 42.4 43.2 window(w/oshifting) 2.8 1.7 1.2 0.9 770 444 280 abs.+rel.pos. 81.3 95.6 50.2 43.4 44.0 shiftedwindow(padding) 3.3 2.3 1.9 2.2 670 371 236 rel.pos.w/oapp. 79.3 94.7 48.2 41.9 44.1 shiftedwindow(cyclic) 3.0 1.9 1.3 1.0 755 437 278 rel.pos. 81.3 95.6 50.5 43.7 46.1 Table5.Realspeedofdifferentself-attentioncomputationmeth- Table4.Ablationstudyontheshiftedwindowsapproachanddif- odsandimplementationsonaV100GPU. ferent position embedding methods on three benchmarks, using theSwin-Tarchitecture. w/oshifting: allself-attentionmodules adoptregularwindowpartitioning,withoutshifting;abs.pos.:ab- onCOCO,and+2.3/+2.9mIoUonADE20Kinrelationto solutepositionembeddingtermofViT;rel. pos.: thedefaultset- thosewithoutpositionencodingandwithabsoluteposition tingswithanadditionalrelativepositionbiasterm(seeEq.(4)); embedding,respectively,indicatingtheeffectivenessofthe app.:thefirstscaleddot-productterminEq.(4). relativepositionbias. Alsonotethatwhiletheinclusionof absolutepositionembeddingimprovesimageclassification accuracy (+0.4%), it harms object detection and semantic categories.Ithas25Kimagesintotal,with20Kfortraining, segmentation(-0.2box/maskAPonCOCOand-0.6mIoU 2K for validation, and another 3K for testing. We utilize onADE20K). UperNet[69]inmmseg[16]asourbaseframeworkforits While the recent ViT/DeiT models abandon translation highefficiency.MoredetailsarepresentedintheAppendix. invariance in image classification even though it has long beenshowntobecrucialforvisualmodeling, wefindthat Results Table 3 lists the mIoU, model size (#param), inductivebiasthatencouragescertaintranslationinvariance FLOPsandFPSfordifferentmethod/backbonepairs. From isstillpreferableforgeneral-purposevisualmodeling,par- theseresults,itcanbeseenthatSwin-Sis+5.3mIoUhigher ticularly for the dense prediction tasks of object detection (49.3vs. 44.0)thanDeiT-Swithsimilarcomputationcost. andsemanticsegmentation. It is also +4.4 mIoU higher than ResNet-101, and +2.4 mIoU higher than ResNeSt-101 [78]. Our Swin-L model Different self-attention methods The real speed of dif- withImageNet-22Kpre-trainingachieves53.5mIoUonthe ferentself-attentioncomputationmethodsandimplementa- val set, surpassing the previous best model by +3.2 mIoU tions are compared in Table 5. Our cyclic implementation (50.3mIoUbySETR[81]whichhasalargermodelsize). ismorehardwareefficientthannaivepadding, particularly for deeper stages. Overall, it brings a 13%, 18% and 18% 4.4.AblationStudy speed-uponSwin-T,Swin-SandSwin-B,respectively. In this section, we ablate important design elements in The self-attention modules built on the proposed theproposedSwinTransformer,usingImageNet-1Kimage shifted window approach are 40.8×/2.5×, 20.2×/2.5×, classification, CascadeMaskR-CNNonCOCOobjectde- 9.3×/2.1×,and7.6×/1.8×moreefficientthanthoseofslid- tection,andUperNetonADE20Ksemanticsegmentation. ing windows in naive/kernel implementations on four net- work stages, respectively. Overall, the Swin Transformer Shifted windows Ablations of the shifted window ap- architectures built on shifted windows are 4.1/1.5, 4.0/1.5, proach on the three tasks are reported in Table 4. Swin-T 3.6/1.5 times faster than variants built on sliding windows withtheshiftedwindowpartitioningoutperformsthecoun- forSwin-T,Swin-S,andSwin-B,respectively.Table6com- terpartbuiltonasinglewindowpartitioningateachstageby parestheiraccuracyonthethreetasks,showingthattheyare +1.1%top-1accuracyonImageNet-1K,+2.8boxAP/+2.2 similarlyaccurateinvisualmodeling. maskAP onCOCO,and +2.8mIoU onADE20K.The re- ComparedtoPerformer[14],whichisoneofthefastest sultsindicatetheeffectivenessofusingshiftedwindowsto Transformer architectures (see [60]), the proposed shifted buildconnectionsamongwindowsintheprecedinglayers. window based self-attention computation and the overall The latency overhead by shifted window is also small, as Swin Transformer architectures are slightly faster (see Ta- showninTable5. ble5),whileachieving+2.3%top-1accuracycomparedto PerformeronImageNet-1KusingSwin-T(seeTable6). Relativepositionbias Table4showscomparisonsofdif- 5.Conclusion ferent position embedding approaches. Swin-T with rela- tive position bias yields +1.2%/+0.8% top-1 accuracy on This paper presents Swin Transformer, a new vision ImageNet-1K, +1.3/+1.5 box AP and +1.1/+1.3 mask AP Transformer which produces a hierarchical feature repre- 8 --- Page 9 --- ImageNet COCO ADE20k When training from scratch with a 2242 input, we em- Backbone top-1top-5 APbox APmask mIoU ployanAdamW[37]optimizerfor300epochsusingaco- slidingwindow Swin-T 81.4 95.6 50.2 43.5 45.8 sinedecaylearningrateschedulerwith20epochsoflinear Performer[14] Swin-T 79.0 94.2 - - - warm-up. A batch size of 1024, an initial learning rate of shiftedwindow Swin-T 81.3 95.6 50.5 43.7 46.1 0.001, a weight decay of 0.05, and gradient clipping with Table 6. Accuracy of Swin Transformer using different methods a max norm of 1 are used. We include most of the aug- forself-attentioncomputationonthreebenchmarks. mentation and regularization strategies of [63] in training, including RandAugment [17], Mixup [77], Cutmix [75], sentationandhaslinearcomputationalcomplexitywithre- random erasing [82] and stochastic depth [35], but not re- spect to input image size. Swin Transformer achieves the peatedaugmentation[31]andExponentialMovingAverage state-of-the-artperformanceonCOCOobjectdetectionand (EMA)[45]whichdonotenhanceperformance. Notethat ADE20K semantic segmentation, significantly surpassing thisiscontraryto[63]whererepeatedaugmentationiscru- previous best methods. We hope that Swin Transformer’s cialtostabilizethetrainingofViT.Anincreasingdegreeof strongperformanceonvariousvisionproblemswillencour- stochasticdepthaugmentationisemployedforlargermod- ageunifiedmodelingofvisionandlanguagesignals. els, i.e. 0.2,0.3,0.5 for Swin-T, Swin-S, and Swin-B, re- AsakeyelementofSwinTransformer, theshiftedwin- spectively. dow based self-attention is shown to be effective and effi- For fine-tuning on input with larger resolution, we em- cient on vision problems, and we look forward to investi- ploy an adamW [37] optimizer for 30 epochs with a con- gatingitsuseinnaturallanguageprocessingaswell. stant learning rate of 10−5, weight decay of 10−8, and thesamedataaugmentationandregularizationsasthefirst Acknowledgement stageexceptforsettingthestochasticdepthratioto0.1. We thank many colleagues at Microsoft for their help, ImageNet-22K pre-training We also pre-train on the inparticular,LiDongandFuruWeiforusefuldiscussions; larger ImageNet-22K dataset, which contains 14.2 million BinXiao,LuYuanandLeiZhangforhelpondatasets. imagesand22Kclasses. Thetrainingisdoneintwostages. Forthefirststagewith2242 input, weemployanAdamW A1.DetailedArchitectures optimizer for 90 epochs using a linear decay learning rate scheduler with a 5-epoch linear warm-up. A batch size of ThedetailedarchitecturespecificationsareshowninTa- 4096, an initial learning rate of 0.001, and a weight decay ble7,whereaninputimagesizeof224×224isassumedfor of0.01areused. InthesecondstageofImageNet-1Kfine- allarchitectures. “Concatn×n”indicatesaconcatenation tuning with 2242/3842 input, we train the models for 30 of n×n neighboring features in a patch. This operation epochswithabatchsizeof1024,aconstantlearningrateof resultsinadownsamplingofthefeaturemapbyarateofn. 10−5,andaweightdecayof10−8. “96-d” denotes a linear layer with an output dimension of 96. “win. sz. 7×7” indicates a multi-head self-attention A2.2.ObjectdetectiononCOCO modulewithwindowsizeof7×7. For an ablation study, we consider four typical ob- A2.DetailedExperimentalSettings ject detection frameworks: Cascade Mask R-CNN [29, 6], ATSS [79], RepPoints v2 [12], and Sparse RCNN [56] in A2.1.ImageclassificationonImageNet-1K mmdetection [10]. For these four frameworks, we utilize thesamesettings: multi-scaletraining[8,56](resizingthe The image classification is performed by applying a input such that the shorter side is between 480 and 800 global average pooling layer on the output feature map of while the longer side is at most 1333), AdamW [44] opti- the last stage, followed by a linear classifier. We find this mizer(initiallearningrateof0.0001,weightdecayof0.05, strategytobeasaccurateasusinganadditionalclassto- andbatchsizeof16),and3xschedule(36epochswiththe ken as in ViT [20] and DeiT [63]. In evaluation, the top-1 learningratedecayedby10×atepochs27and33). accuracyusingasinglecropisreported. For system-level comparison, we adopt an improved HTC[9](denotedasHTC++)withinstaboost[22],stronger Regular ImageNet-1K training The training settings multi-scale training [7] (resizing the input such that the mostlyfollow[63]. Forallmodelvariants, weadoptade- shortersideisbetween400and1400whilethelongerside faultinputimageresolutionof2242. Forotherresolutions isatmost1600),6xschedule(72epochswiththelearning suchas3842,wefine-tunethemodelstrainedat2242 reso- rate decayed at epochs 63 and 69 by a factor of 0.1), soft- lution,insteadoftrainingfromscratch,toreduceGPUcon- NMS[5],andanextraglobalself-attentionlayerappended sumption. at the output of last stage and ImageNet-22K pre-trained 9 --- Page 10 --- downsp.rate Swin-T Swin-S Swin-B Swin-L (outputsize) concat4×4,96-d,LN concat4×4,96-d,LN concat4×4,128-d,LN concat4×4,192-d,LN 4× (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) stage1 win.sz.7×7, win.sz.7×7, win.sz.7×7, win.sz.7×7, (56×56) ×2 ×2 ×2 ×2 dim96,head3 dim96,head3 dim128,head4 dim192,head6 concat2×2,192-d,LN concat2×2,192-d,LN concat2×2,256-d,LN concat2×2,384-d,LN 8× (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) stage2 win.sz.7×7, win.sz.7×7, win.sz.7×7, win.sz.7×7, (28×28) ×2 ×2 ×2 ×2 dim192,head6 dim192,head6 dim256,head8 dim384,head12 concat2×2,384-d,LN concat2×2,384-d,LN concat2×2,512-d,LN concat2×2,768-d,LN 16× (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) stage3 win.sz.7×7, win.sz.7×7, win.sz.7×7, win.sz.7×7, (14×14) ×6 ×18 ×18 ×18 dim384,head12 dim384,head12 dim512,head16 dim768,head24 concat2×2,768-d,LN concat2×2,768-d,LN concat2×2,1024-d,LN concat2×2,1536-d,LN 32× (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) stage4 win.sz.7×7, win.sz.7×7, win.sz.7×7, win.sz.7×7, (7×7) ×2 ×2 ×2 ×2 dim768,head24 dim768,head24 dim1024,head32 dim1536,head48 Table7.Detailedarchitecturespecifications. modelasinitialization. Weadoptstochasticdepthwithra- Swin-T Swin-S Swin-B tioof0.2forallSwinTransformermodels. input top-1 throughput top-1 throughput top-1 throughput size acc (image/s) acc (image/s) acc (image/s) A2.3.SemanticsegmentationonADE20K 2242 81.3 755.2 83.0 436.9 83.3 278.1 2562 81.6 580.9 83.4 336.7 83.7 208.1 ADE20K [83] is a widely-used semantic segmentation 3202 82.1 342.0 83.7 198.2 84.0 132.0 dataset,coveringabroadrangeof150semanticcategories. 3842 82.2 219.5 83.9 127.6 84.5 84.7 Ithas25Kimagesintotal,with20Kfortraining,2Kforval- Table 8. Swin Transformers with different input image size on idation,andanother3Kfortesting.WeutilizeUperNet[69] ImageNet-1Kclassification. inmmsegmentation[16]asourbaseframeworkforitshigh efficiency. Backbone OptimizerAPboxAPb 5o 0xAPb 7o 5x APmaskAPm 50askAPm 75ask Intraining,weemploytheAdamW[44]optimizerwith R50 SGD 45.0 62.9 48.8 38.5 59.9 41.4 aninitiallearningrateof6×10−5,aweightdecayof0.01, AdamW 46.3 64.3 50.5 40.1 61.7 43.4 SGD 47.8 65.9 51.9 40.4 62.9 43.5 a scheduler that uses linear learning rate decay, and a lin- X101-32x4d AdamW 48.1 66.5 52.4 41.6 63.9 45.2 ear warmup of 1,500 iterations. Models are trained on 8 SGD 48.8 66.9 53.0 41.4 63.9 44.7 GPUswith2imagesperGPUfor160Kiterations. Foraug- X101-64x4d AdamW 48.3 66.4 52.3 41.7 64.0 45.1 mentations,weadoptthedefaultsettinginmmsegmentation Table 9. Comparison of the SGD and AdamW optimizers for of random horizontal flipping, random re-scaling within ResNe(X)t backbones on COCO object detection using the Cas- ratio range [0.5, 2.0] and random photometric distortion. cadeMaskR-CNNframework. Stochastic depth with ratio of 0.2 is applied for all Swin Transformer models. Swin-T, Swin-S are trained on the A3.2.DifferentOptimizersforResNe(X)tonCOCO standard setting as the previous approaches with an input of512×512. Swin-BandSwin-Lwith‡indicatethatthese Table 9 compares the AdamW and SGD optimizers of two models are pre-trained on ImageNet-22K, and trained theResNe(X)tbackbonesonCOCOobjectdetection. The withtheinputof640×640. Cascade Mask R-CNN framework is used in this compar- Ininference,amulti-scaletestusingresolutionsthatare ison. While SGD is used as a default optimizer for Cas- [0.5, 0.75, 1.0, 1.25, 1.5, 1.75]× of that in training is em- cade Mask R-CNN framework, we generally observe im- ployed. When reporting test scores, both the training im- provedaccuracybyreplacingitwithanAdamWoptimizer, agesandvalidationimagesareusedfortraining,following particularly for smaller backbones. We thus use AdamW commonpractice[71]. for ResNe(X)t backbones when compared to the proposed SwinTransformerarchitectures. A3.MoreExperiments A3.3.SwinMLP-Mixer A3.1.Imageclassificationwithdifferentinputsize We apply the proposed hierarchical design and the Table8liststheperformanceofSwinTransformerswith shifted window approach to the MLP-Mixer architec- different input image sizes from 2242 to 3842. In general, tures [61], referred to as Swin-Mixer. Table 10 shows the a larger input resolution leads to better top-1 accuracy but performanceofSwin-MixercomparedtotheoriginalMLP- withslowerinferencespeed. Mixer architectures MLP-Mixer [61] and a follow-up ap- 10 --- Page 11 --- image throughputImageNet endobjectdetectionwithtransformers.InEuropeanConfer- method #param.FLOPs size (image/s)top-1acc. enceonComputerVision,pages213–229.Springer,2020.3, MLP-Mixer-B/16[61] 2242 59M 12.7G - 76.4 6,9 ResMLP-S24[62] 2242 30M 6.0G 715 79.4 [9] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaox- ResMLP-B24[62] 2242 116M 23.0G 231 81.0 iao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Swin-T/D24 2562 28M 5.9G 563 81.6 Shi,WanliOuyang,etal. Hybridtaskcascadeforinstance (Transformer) segmentation. InProceedingsoftheIEEE/CVFConference Swin-Mixer-T/D24 2562 20M 4.0G 807 79.4 on Computer Vision and Pattern Recognition, pages 4974– Swin-Mixer-T/D12 2562 21M 4.0G 792 79.6 4983,2019. 6,9 Swin-Mixer-T/D6 2562 23M 4.0G 766 79.7 [10] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Swin-Mixer-B/D24 2242 61M 10.4G 409 80.3 Xiong,XiaoxiaoLi,ShuyangSun,WansenFeng,ZiweiLiu, (noshift) JiaruiXu,etal. Mmdetection: Openmmlabdetectiontool- Swin-Mixer-B/D24 2242 61M 10.4G 409 81.3 boxandbenchmark.arXivpreprintarXiv:1906.07155,2019. Table10.PerformanceofSwinMLP-MixeronImageNet-1Kclas- 6,9 sification. Dindictesthenumberofchannelsperhead. Through- [11] Liang-ChiehChen,YukunZhu,GeorgePapandreou,Florian putismeasuredusingtheGitHubrepositoryof[68]andaV100 Schroff, and Hartwig Adam. Encoder-decoder with atrous GPU,following[63]. separableconvolutionforsemanticimagesegmentation. In ProceedingsoftheEuropeanconferenceoncomputervision (ECCV),pages801–818,2018. 7 proach, ResMLP [61]. Swin-Mixer performs significantly [12] YihongChen,ZhengZhang,YueCao,LiweiWang,Stephen better than MLP-Mixer (81.3% vs. 76.4%) using slightly Lin,andHanHu. Reppointsv2: Verificationmeetsregres- smallercomputationbudget(10.4Gvs. 12.7G).Italsohas sionforobjectdetection. InNeurIPS,2020. 6,7,9 betterspeedaccuracytrade-offcomparedtoResMLP[62]. [13] Cheng Chi, Fangyun Wei, and Han Hu. Relationnet++: Theseresultsindicatetheproposedhierarchicaldesignand Bridgingvisualrepresentationsforobjectdetectionviatrans- theshiftedwindowapproacharegeneralizable. formerdecoder. InNeurIPS,2020. 3,7 [14] Krzysztof Marcin Choromanski, Valerii Likhosherstov, References David Dohan, Xingyou Song, Andreea Gane, Tamas Sar- los,PeterHawkins,JaredQuincyDavis,AfrozMohiuddin, [1] HangboBao,LiDong,FuruWei,WenhuiWang,NanYang, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, XiaodongLiu,YuWang,JianfengGao,SonghaoPiao,Ming and Adrian Weller. Rethinking attention with performers. Zhou,etal. Unilmv2: Pseudo-maskedlanguagemodelsfor In International Conference on Learning Representations, unifiedlanguagemodelpre-training. InInternationalCon- 2021. 8,9 ferenceonMachineLearning,pages642–652.PMLR,2020. 5 [15] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and [2] JoshBeal, EricKim, EricTzeng, DongHukPark, Andrew HuaxiaXia. Dowereallyneedexplicitpositionencodings Zhai,andDmitryKislyuk. Towardtransformer-basedobject forvisiontransformers? arXivpreprintarXiv:2102.10882, detection. arXivpreprintarXiv:2012.09958,2020. 3 2021. 3 [3] IrwanBello,BarretZoph,AshishVaswani,JonathonShlens, [16] MMSegmentation Contributors. MMSegmentation: and Quoc V. Le. Attention augmented convolutional net- Openmmlab semantic segmentation toolbox and bench- works,2020. 3 mark.  [4] Alexey Bochkovskiy, Chien-Yao Wang, and Hong- mmsegmentation,2020. 8,10 Yuan Mark Liao. Yolov4: Optimal speed and accuracy of [17] EkinDCubuk,BarretZoph,JonathonShlens,andQuocV objectdetection. arXivpreprintarXiv:2004.10934,2020. 7 Le. Randaugment: Practical automated data augmenta- [5] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and tion with a reduced search space. In Proceedings of the LarryS.Davis. Soft-nms–improvingobjectdetectionwith IEEE/CVF Conference on Computer Vision and Pattern onelineofcode. InProceedingsoftheIEEEInternational RecognitionWorkshops,pages702–703,2020. 9 ConferenceonComputerVision(ICCV),Oct2017. 6,9 [18] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong [6] ZhaoweiCaiandNunoVasconcelos. Cascader-cnn: Delv- Zhang,HanHu,andYichenWei. Deformableconvolutional ingintohighqualityobjectdetection. InProceedingsofthe networks. InProceedingsoftheIEEEInternationalConfer- IEEEConferenceonComputerVisionandPatternRecogni- enceonComputerVision,pages764–773,2017. 1,3 tion,pages6154–6162,2018. 6,9 [19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, [7] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han andLiFei-Fei. Imagenet: Alarge-scalehierarchicalimage Hu.Gcnet:Non-localnetworksmeetsqueeze-excitationnet- database. In2009IEEEconferenceoncomputervisionand worksandbeyond. InProceedingsoftheIEEE/CVFInter- patternrecognition,pages248–255.Ieee,2009. 5 nationalConferenceonComputerVision(ICCV)Workshops, [20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Oct2019. 3,6,7,9 Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, [8] NicolasCarion,FranciscoMassa,GabrielSynnaeve,Nicolas MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl- Usunier,AlexanderKirillov,andSergeyZagoruyko.End-to- vainGelly,JakobUszkoreit,andNeilHoulsby. Animageis 11 --- Page 12 --- worth16x16words: Transformersforimagerecognitionat theIEEE/CVFInternationalConferenceonComputerVision scale. InInternationalConferenceonLearningRepresenta- (ICCV),pages3464–3473,October2019. 2,3,5 tions,2021. 1,2,3,4,5,6,9 [34] GaoHuang,ZhuangLiu,LaurensVanDerMaaten,andKil- [21] Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Golnaz Ghiasi, ian Q Weinberger. Densely connected convolutional net- Mingxing Tan, Yin Cui, Quoc V Le, and Xiaodan Song. works. InProceedingsoftheIEEEconferenceoncomputer Spinenet: Learning scale-permuted backbone for recogni- visionandpatternrecognition,pages4700–4708,2017.1,2 tionandlocalization. InProceedingsoftheIEEE/CVFCon- [35] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil- ferenceonComputerVisionandPatternRecognition,pages ianQWeinberger. Deepnetworkswithstochasticdepth. In 11592–11601,2020. 7 European conference on computer vision, pages 646–661. [22] Hao-Shu Fang, Jianhua Sun, Runzhong Wang, Minghao Springer,2016. 9 Gou, Yong-Lu Li, and Cewu Lu. Instaboost: Boosting [36] David H Hubel and Torsten N Wiesel. Receptive fields, instance segmentation via probability map guided copy- binocularinteractionandfunctionalarchitectureinthecat’s pasting.InProceedingsoftheIEEE/CVFInternationalCon- visual cortex. The Journal of physiology, 160(1):106–154, ferenceonComputerVision,pages682–691,2019. 6,9 1962. 3 [23] JunFu, JingLiu, HaijieTian, YongLi, YongjunBao, Zhi- [37] Diederik P Kingma and Jimmy Ba. Adam: A method for wei Fang, and Hanqing Lu. Dual attention network for stochastic optimization. arXiv preprint arXiv:1412.6980, scenesegmentation. InProceedingsoftheIEEEConference 2014. 5,9 on Computer Vision and Pattern Recognition, pages 3146– [38] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan 3154,2019. 3,7 Puigcerver, JessicaYung, SylvainGelly, andNeilHoulsby. Big transfer (bit): General visual representation learning. [24] JunFu,JingLiu,YuhangWang,YongLi,YongjunBao,Jin- arXivpreprintarXiv:1912.11370,6(2):8,2019. 6 hui Tang, and Hanqing Lu. Adaptive context network for scene parsing. In Proceedings of the IEEE/CVF Interna- [39] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. tional Conference on Computer Vision, pages 6748–6757, Imagenet classification with deep convolutional neural net- 2019. 7 works. In Advances in neural information processing sys- tems,pages1097–1105,2012. 1,2 [25] Kunihiko Fukushima. Cognitron: A self-organizing multi- [40] YannLeCun,Le´onBottou,YoshuaBengio,PatrickHaffner, layeredneuralnetwork. Biologicalcybernetics,20(3):121– et al. Gradient-based learning applied to document recog- 136,1975. 3 nition. ProceedingsoftheIEEE,86(11):2278–2324, 1998. [26] GolnazGhiasi,YinCui,AravindSrinivas,RuiQian,Tsung- 2 YiLin,EkinDCubuk,QuocVLe,andBarretZoph.Simple [41] YannLeCun,PatrickHaffner,Le´onBottou,andYoshuaBen- copy-pasteisastrongdataaugmentationmethodforinstance gio. Object recognition with gradient-based learning. In segmentation. arXivpreprintarXiv:2012.07177,2020. 2,7 Shape,contourandgroupingincomputervision,pages319– [27] JiayuanGu, HanHu, LiweiWang, YichenWei, andJifeng 345.Springer,1999. 3 Dai. Learningregionfeaturesforobjectdetection. InPro- [42] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, ceedings of the European Conference on Computer Vision Bharath Hariharan, and Serge Belongie. Feature pyramid (ECCV),2018. 3 networks for object detection. In The IEEE Conference [28] KaiHan,AnXiao,EnhuaWu,JianyuanGuo,ChunjingXu, onComputerVisionandPatternRecognition(CVPR),July andYunheWang.Transformerintransformer.arXivpreprint 2017. 2 arXiv:2103.00112,2021. 3 [43] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays, [29] KaimingHe,GeorgiaGkioxari,PiotrDolla´r,andRossGir- PietroPerona,DevaRamanan,PiotrDolla´r,andCLawrence shick.Maskr-cnn.InProceedingsoftheIEEEinternational Zitnick. Microsoft coco: Common objects in context. In conferenceoncomputervision,pages2961–2969,2017. 6, European conference on computer vision, pages 740–755. 9 Springer,2014. 5 [30] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. [44] Ilya Loshchilov and Frank Hutter. Decoupled weight de- Deep residual learning for image recognition. In Proceed- cayregularization. InInternationalConferenceonLearning ingsoftheIEEEconferenceoncomputervisionandpattern Representations,2019. 6,9,10 recognition,pages770–778,2016. 1,2,4 [45] Boris T Polyak and Anatoli B Juditsky. Acceleration of [31] EladHoffer,TalBen-Nun,ItayHubara,NivGiladi,Torsten stochastic approximation by averaging. SIAM journal on Hoefler,andDanielSoudry.Augmentyourbatch:Improving controlandoptimization,30(4):838–855,1992. 6,9 generalizationthroughinstancerepetition.InProceedingsof [46] SiyuanQiao,Liang-ChiehChen,andAlanYuille.Detectors: theIEEE/CVFConferenceonComputerVisionandPattern Detectingobjectswithrecursivefeaturepyramidandswitch- Recognition,pages8129–8138,2020. 6,9 able atrous convolution. arXiv preprint arXiv:2006.02334, [32] HanHu,JiayuanGu,ZhengZhang,JifengDai,andYichen 2020. 2,7 Wei. Relation networks for object detection. In Proceed- [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya ingsoftheIEEEConferenceonComputerVisionandPattern Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Recognition,pages3588–3597,2018. 3,5 Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen [33] HanHu,ZhengZhang,ZhendaXie,andStephenLin. Local Krueger, and Ilya Sutskever. Learning transferable visual relationnetworksforimagerecognition. InProceedingsof modelsfromnaturallanguagesupervision,2021. 1 12 --- Page 13 --- [48] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, markforefficienttransformers. InInternationalConference Kaiming He, and Piotr Dolla´r. Designing network design onLearningRepresentations,2021. 8 spaces. In Proceedings of the IEEE/CVF Conference on [61] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lu- Computer Vision and Pattern Recognition, pages 10428– casBeyer,XiaohuaZhai,ThomasUnterthiner,JessicaYung, 10436,2020. 6 Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario [49] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee, Lucic,andAlexeyDosovitskiy. Mlp-mixer: Anall-mlpar- SharanNarang, MichaelMatena, Yanqi Zhou, WeiLi, and chitectureforvision,2021. 2,10,11 PeterJ.Liu. Exploringthelimitsoftransferlearningwitha [62] HugoTouvron,PiotrBojanowski,MathildeCaron,Matthieu unifiedtext-to-texttransformer. JournalofMachineLearn- Cord, Alaaeldin El-Nouby, Edouard Grave, Gautier Izac- ingResearch,21(140):1–67,2020. 5 ard,ArmandJoulin,GabrielSynnaeve,JakobVerbeek,and [50] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Herve´Je´gou.Resmlp:Feedforwardnetworksforimageclas- Bello,AnselmLevskaya,andJonShlens. Stand-aloneself- sificationwithdata-efficienttraining,2021. 11 attentioninvisionmodels. InAdvancesinNeuralInforma- [63] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco tionProcessingSystems,volume32.CurranAssociates,Inc., Massa,AlexandreSablayrolles,andHerve´ Je´gou. Training 2019. 2,3 data-efficient image transformers & distillation through at- [51] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U- tention. arXivpreprintarXiv:2012.12877,2020. 2,3,5,6, net: Convolutionalnetworksforbiomedicalimagesegmen- 9,11 tation. InInternationalConferenceonMedicalimagecom- [64] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko- puting and computer-assisted intervention, pages 234–241. reit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia Springer,2015. 2 Polosukhin.Attentionisallyouneed.InAdvancesinNeural InformationProcessingSystems,pages5998–6008,2017.1, [52] K. Simonyan and A. Zisserman. Very deep convolutional 2,4 networksforlarge-scaleimagerecognition. InInternational [65] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, ConferenceonLearningRepresentations,May2015. 2,4 ChaoruiDeng,YangZhao,DongLiu,YadongMu,Mingkui [53] Bharat Singh and Larry S Davis. An analysis of scale in- Tan,XinggangWang,etal. Deephigh-resolutionrepresen- variance in object detection snip. In Proceedings of the tationlearningforvisualrecognition. IEEEtransactionson IEEE conference on computer vision and pattern recogni- patternanalysisandmachineintelligence,2020. 3 tion,pages3578–3587,2018. 2 [66] WenhaiWang,EnzeXie,XiangLi,Deng-PingFan,Kaitao [54] Bharat Singh, Mahyar Najibi, and Larry S Davis. Sniper: Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Efficientmulti-scaletraining. InAdvancesinNeuralInfor- Pyramid vision transformer: A versatile backbone for mationProcessingSystems, volume31.CurranAssociates, dense prediction without convolutions. arXiv preprint Inc.,2018. 2 arXiv:2102.12122,2021. 3 [55] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon [67] XiaolongWang,RossGirshick,AbhinavGupta,andKaim- Shlens, Pieter Abbeel, and Ashish Vaswani. Bottle- ing He. Non-local neural networks. In IEEE Conference neck transformers for visual recognition. arXiv preprint on Computer Vision and Pattern Recognition, CVPR 2018, arXiv:2101.11605,2021. 3 2018. 3 [56] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chen- [68] Ross Wightman. Pytorch image mod- feng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan els.  Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end pytorch-image-models,2019. 6,11 object detection with learnable proposals. arXiv preprint [69] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and arXiv:2011.12450,2020. 3,6,9 JianSun. Unifiedperceptualparsingforsceneunderstand- [57] ChristianSzegedy,WeiLiu,YangqingJia,PierreSermanet, ing. In Proceedings of the European Conference on Com- Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent puterVision(ECCV),pages418–434,2018. 7,8,10 Vanhoucke, and Andrew Rabinovich. Going deeper with [70] SainingXie,RossGirshick,PiotrDolla´r,ZhuowenTu,and convolutions. In Proceedings of the IEEE conference on KaimingHe. Aggregatedresidualtransformationsfordeep computer vision and pattern recognition, pages 1–9, 2015. neural networks. In Proceedings of the IEEE Conference 2 on Computer Vision and Pattern Recognition, pages 1492– [58] MingxingTanandQuocLe. Efficientnet:Rethinkingmodel 1500,2017. 1,2,3 scalingforconvolutionalneuralnetworks. InInternational [71] MinghaoYin,ZhuliangYao,YueCao,XiuLi,ZhengZhang, ConferenceonMachineLearning,pages6105–6114.PMLR, Stephen Lin, and Han Hu. Disentangled non-local neural 2019. 3,6 networks. In Proceedings of the European conference on [59] MingxingTan,RuomingPang,andQuocVLe.Efficientdet: computervision(ECCV),2020. 3,7,10 Scalable and efficient object detection. In Proceedings of [72] LiYuan,YunpengChen,TaoWang,WeihaoYu,YujunShi, the IEEE/CVF conference on computer vision and pattern FrancisEHTay, JiashiFeng, andShuichengYan. Tokens- recognition,pages10781–10790,2020. 7 to-token vit: Training vision transformers from scratch on [60] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, imagenet. arXivpreprintarXiv:2101.11986,2021. 3 DaraBahri,PhilipPham,JinfengRao,LiuYang,Sebastian [73] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object- Ruder, and Donald Metzler. Long range arena : A bench- contextual representations for semantic segmentation. In 13 --- Page 14 --- 16thEuropeanConferenceComputerVision(ECCV2020), August2020. 7 [74] YuhuiYuanandJingdongWang. Ocnet:Objectcontextnet- work for scene parsing. arXiv preprint arXiv:1809.00916, 2018. 3 [75] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun,JunsukChoe,andYoungjoonYoo. Cutmix: Regular- izationstrategytotrainstrongclassifierswithlocalizablefea- tures. InProceedingsoftheIEEE/CVFInternationalCon- ferenceonComputerVision,pages6023–6032,2019. 9 [76] SergeyZagoruykoandNikosKomodakis.Wideresidualnet- works. InBMVC,2016. 1 [77] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and DavidLopez-Paz. mixup: Beyondempiricalriskminimiza- tion. arXivpreprintarXiv:1710.09412,2017. 9 [78] HangZhang,ChongruoWu,ZhongyueZhang,YiZhu,Zhi Zhang, Haibin Lin, Yue Sun, Tong He, Jonas Mueller, R Manmatha, etal. Resnest: Split-attentionnetworks. arXiv preprintarXiv:2004.08955,2020. 7,8 [79] ShifengZhang, ChengChi, YongqiangYao, ZhenLei, and Stan Z Li. Bridging the gap between anchor-based and anchor-freedetectionviaadaptivetrainingsampleselection. In Proceedings of the IEEE/CVF Conference on Computer VisionandPatternRecognition,pages9759–9768,2020. 6, 9 [80] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Explor- ingself-attentionforimagerecognition. InProceedingsof theIEEE/CVFConferenceonComputerVisionandPattern Recognition,pages10076–10085,2020. 3 [81] SixiaoZheng, JiachenLu, HengshuangZhao, XiatianZhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang,PhilipHSTorr,etal. Rethinkingsemanticsegmen- tation from a sequence-to-sequence perspective with trans- formers. arXiv preprint arXiv:2012.15840, 2020. 2, 3, 7, 8 [82] ZhunZhong,LiangZheng,GuoliangKang,ShaoziLi,and YiYang.Randomerasingdataaugmentation.InProceedings oftheAAAIConferenceonArtificialIntelligence,volume34, pages13001–13008,2020. 9 [83] BoleiZhou, HangZhao, XavierPuig, TeteXiao, SanjaFi- dler,AdelaBarriuso,andAntonioTorralba.Semanticunder- standingofscenesthroughtheade20kdataset. International JournalonComputerVision,2018. 5,7,10 [84] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. De- formableconvnetsv2: Moredeformable, betterresults. In Proceedings of the IEEE Conference on Computer Vision andPatternRecognition,pages9308–9316,2019. 1,3 [85] XizhouZhu,WeijieSu,LeweiLu,BinLi,XiaogangWang, andJifengDai. Deformable{detr}: Deformabletransform- ersforend-to-endobjectdetection. InInternationalConfer- enceonLearningRepresentations,2021. 3 14