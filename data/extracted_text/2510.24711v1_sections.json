{
  "abstract": "ABSTRACT Mixture-of-Experts(MoE)hasemergedasapowerfulparadigmforscalingmodel capacity while preserving computational efficiency. Despite its notable success in large language models (LLMs), existing attempts to apply MoE to Diffusion Transformers (DiTs) have yielded limited gains. We attribute this gap to fun- damental differences between language and visual tokens. Language tokens are semanticallydensewithpronouncedinter-tokenvariation,whilevisualtokensex- hibit spatial redundancy and functional heterogeneity, hindering expert special- ization in vision MoE. To this end, we present ProMoE, an MoE framework featuring a two-step router with explicit routing guidance that promotes expert specialization. Specifically, this guidance encourages the router to first partition image tokens into conditional and unconditional sets via conditional routing ac- cordingtotheirfunctionalroles,andsecondrefinetheassignmentsofconditional imagetokensthroughprototypicalroutingwithlearnableprototypesbasedonse- mantic content. Moreover, the similarity-based expert allocation in latent space enabledbyprototypicalroutingoffersanaturalmechanismforincorporatingex- plicitsemanticguidance,andwevalidatethatsuchguidanceiscrucialforvision MoE. Building on this, we propose a routing contrastive loss that explicitly en- hances the prototypical routing process, promoting intra-expert coherence and inter-expert diversity. Extensive",
  "experiments": "experiments, except for ProMoE-B-DDPM, whichusesaweightof10basedonthisablationstudy. F USAGE OF LARGE LANGUAGE MODELS (LLMS) In accordance with the ICLR 2026 policy, we report our use of a large language model (LLM) in preparing this manuscript. The LLM\u2019s role was strictly confined to language polishing, such as correcting grammar, refining wording, and improving readability. All scientific contributions, including the ideation,",
  "introduction": "1 INTRODUCTION Diffusionmodels(Hoetal.,2020)havemadesubstantialadvancesforvisualsynthesis(Rombach etal.,2022b;Yangetal.,2024;Weietal.,2024a;Wanetal.,2025). Drivenbythegrowingdemand for higher fidelity and quality, research has focused on scaling up diffusion models (Esser et al., 2024b)andpropelledanarchitecturalshiftfromU-Net(Ronnebergeretal.,2015)backbonestothe now-prevalentDiffusionTransformers(DiTs)(Peebles&Xie,2023). Despitetheproveneffective- nessofDiT-basedmodels(Esseretal.,2024a),theirdenseactivationofallparametersirrespective oftaskorinputincurssubstantialcomputationaloverhead,therebyhinderingfurtherscalability. To scale toward larger and more capable models, the large language model (LLM) community has widely adopted the Mixture-of-Experts (MoE) (Jacobs et al., 1991; Shazeer et al., 2017) paradigm, which expands model capacity while maintaining computational efficiency. Conceptu- ally, an MoE layer dispatches input tokens to specialized \u201cexpert\u201d sub-networks via a router and returns a weighted sum of the selected experts\u2019 outputs. Despite MoE\u2019s profound success in lan- guagemodeling(Jiangetal.,2024;Daietal.,2024), recenteffortstointegrateitintoDiTmodels havenotyieldedthesignificantgainsobservedinLLMs. Specifically,DiT-MoE(Feietal.,2024), which employs token-to-expert routing, often underperforms dense counterparts despite activating thesamenumberofparameters. Incontrast,EC-DiT(Sunetal.,2024),whichassignseachexpert a fixed quota of tokens, delivers only marginal gains even with extended training. More recently, \u2217ProjectLeader \u2020CorrespondingAuthor 1 5202 tcO 82 ]VC.sc[ 1v11742.0152:viXra --- Page 2 --- LLM (Llama-3 8B) Visual DiT (DiT-XL/2) Expert Subspace Similarity (Lower implies Higher Diversity) Avg(inter\u2212class distance) Avg(inter\u2212class distance) = \ud835\udfcf\ud835\udfd7.\ud835\udfd0\ud835\udfd6\ud835\udfd1 = 0.748 Avg(intra\u2212class distance) Avg(intra\u2212class distance) (a) t-SNE visualization of language and visual tokens (b) Comparison of expert diversity across MoE layers Figure 1: (a) We randomly sample 1k intermediate-layer tokens from 110 ImageNet classes for 10-cluster k-means clustering (differentiated by color). With class names/labels as inputs, LLM tokensformcompact,well-separatedclusterswithhighsemanticdensity,whereasvisualtokensare diffuse.Thisdisparityisquantifiedbytheratioofinter-tointra-classdistance(19.283\u226b0.748).(b) Wemeasure inter-expertdiversity usingsingular valuedecomposition oneach MoElayer\u2019s expert weight matrices and computing the mean similarity of the subspaces spanned by their top-k left singularvectors(Huetal.,2021). Incorporatingroutingguidance(Ours)enhancesexpertdiversity. DiffMoE (Shi et al., 2025), which introduces a global token-distribution routing scheme, still re- portsrelativelylimitedimprovements. ThispronouncedgapbetweenMoE\u2019stransformativeimpact in LLMs and its modest returns in DiT models motivates a fundamental question: What are the underlyingfactorsthatimpedetheeffectivenessofMoEinDiTmodels? Toanswerthisquestion,weexaminehowlinguisticandvisualinputsdifferinmodelsandhighlight the following two distinctive properties of visual inputs. 1) High Spatial Redundancy. Unlike discretetexttokens,whicharesemanticallydensewithsalientinter-tokendifferences,visualtokens (i.e.,imagepatches)arecontinuous,spatiallycoupled,andsubstantiallyredundant(Fig.1(a)). The highcorrelationbetweenpatchesoftenleadsexpertstolearnhomogeneousfeatures. 2)Functional Heterogeneity. The practice of classifier-free guidance (Ho & Salimans, 2022) in diffusion mod- els inherently introduces two functionally distinct input types: conditional and unconditional. A naiveMoEtreatsthemuniformlywithundifferentiatedrouting,ignoringtheirdifferentroles. These propertiescollectivelyimpedeeffectiveexpertdiversityandspecialization(Fig.1(b)). Motivatedbytheseobservations, werevisitthefoundationalprincipleofMoEdesign: expertspe- cialization,inwhicheachexpertacquiresfocusedandnon-overlappingknowledge(Daietal.,2024; Caietal.,2025). Wedecomposethisobjectiveintotwocriteria: Intra-ExpertCoherence, which ensuresthatanexpertconsistentlyprocessessimilarpatterns, maintainingastablefunctionalrole; and Inter-Expert Diversity, which encourages different experts to specialize in distinct tasks to achieve functional differentiation. In language modeling, the semantic density and separability of discretetexttokensprovideapotentinductivebiasthatnaturallyfostersexpertspecialization, sat- isfying both criteria. In contrast, for visual inputs, the combination of intrinsic redundancy and extrinsicfunctionalheterogeneitymakesexpertspecializationnon-trivial. Therefore,inthispaper, wemovebeyondimplicitexpertallocation,andintroduceexplicitroutingguidancedesignstopro- motebothintra-expertcoherenceandinter-expertdiversity. Tothisend,wepresentProMoE,aMixture-of-Expertsframeworkfeaturingatwo-steprouterwith explicitroutingguidancetopromoteexpertspecialization. Specifically,thisguidanceprovidestwo distinctroutingsignals:thetoken\u2019sfunctionalroleanditssemanticcontent.Guidedbythesesignals, the router implements two steps: conditional routing and prototypical routing. First, conditional routingaddressesfunctionalheterogeneitybypartitioningvisualtokensintounconditionalandcon- ditional sets. Unconditional image tokens, derived from image patches under null conditioning (e.g., empty labels or texts), are processed by dedicated unconditional experts. In contrast, condi- tionalimagetokens,obtainedfrompatchesunderspecificconditioning,aredispatchedtostandard MoEexperts.Thishardroutingmechanismenforcesfunctionalsegregation,fosteringspecialization acrossunconditionalandstandardexperts. Second,prototypicalroutingfurtherassignsconditional imagetokensusingasetoflearnableprototypes,eachassociatedwithaspecificexpert,bycomput- ingcosinesimilaritybetweentokenembeddingsandtheprototypesinlatentspace. While prototypical routing is flexible and effective, it still relies on implicit learning from token semantics. Fortunately,itssimilarity-basedallocationinlatentspaceprovidesanaturalmechanism forinjectingexplicitsemanticroutingguidance.Wevalidatetheimportanceofsemanticguidancein systematicexperiments(Sec.4.2),wherebothexplicit(classification-based)andimplicit(clustering- based)guidanceyieldclearimprovements. Buildingonthis,weproposearoutingcontrastiveloss 2 --- Page 3 --- that explicitly enhances the prototypical routing process by assigning semantically similar tokens to the same expert while preserving distinct token distributions across experts. Compared with alternativeguidancestrategies,theproposedcontrastivelossrequiresnomanuallabelsandismore robust,promotingintra-expertcoherenceandinter-expertdiversityinvisionMoE. ExtensiveexperimentalresultsdemonstrateProMoE\u2019ssuperiorperformanceandeffectivescalability on both Flow Matching and DDPM paradigms. Notably, ProMoE achieves significant gains over densemodelsdespiteusingfeweractivatedparameters,andsurpassesstate-of-the-artmethodsthat have1.7\u00d7moretotalparametersthanours. Insummary,ourcontributionsarefourfold: 1)Byanalyzingdifferencesbetweenlanguageandvi- sualtokens,wepresentProMoE,anMoEframeworkwithexplicitroutingguidanceforDiTmodels. 2)Wedesignatwo-steprouter,whereconditionalroutingfirstpartitionsimagetokensbyfunctional roles,andprototypicalroutingthenrefinesassignmentsusinglearnableprototypesbasedonseman- tic content. 3) We propose a routing contrastive loss that enhances prototypical routing, explicitly enforcing intra-expert coherence and inter-expert diversity. 4) Extensive",
  "related_work": "RELATED WORK Diffusion Models. Diffusion models (Ho et al., 2020; Nichol & Dhariwal, 2021) have made re- markableprogressinvisualsynthesis. Earlywork(Rombachetal.,2022a;Podelletal.,2023;Wei etal.,2024b)primarilyuseU-Net(Ronnebergeretal.,2015)architecturestrainedwiththeDDPM objective (Ho et al., 2020; Song et al., 2020). Recent models (Chen et al., 2023; Ma et al., 2024; Hatamizadehetal.,2024;Chuetal.,2024;Esseretal.,2024a;Weietal.,2025)haveshiftedtoDiffu- sionTransformers(DiT)(Peebles&Xie,2023),offeringsuperiorscalabilityandgenerativequality, and are trained with the more effective Rectified Flow (RF) (Liu et al., 2022), a flow-matching formulation (Lipman et al., 2022) that constructs a straight-line path between data and noise dis- tributions. In this work, we adopt a standard DiT backbone and train with both DDPM and RF objectives,demonstratingtheeffectivenessofourapproachacrossdifferenttrainingparadigms. Mixture of Experts. Mixture-of-Experts (MoE) (Jacobs et al., 1991; Shazeer et al., 2017; Lep- ikhinetal.,2020)aredesignedtoexpandmodelcapacitywhileminimizingcomputationaloverhead by sparsely activating sub-networks for distinct inputs. Inspired by MoE successes in LLMs (Dai etal.,2024;Liuetal.,2024;Lietal.,2025;Muennighoffetal.,2024),recentworkhasintegrated MoE to scale diffusion models to improve generative quality (Riquelme et al., 2021). Early MoE applicationsinU-Net-baseddiffusionmodels(Leeetal.,2024;Balajietal.,2022;Fengetal.,2023; Xueetal.,2023;Parketal.,2023;2024;Zhaoetal.,2024)oftenassignexpertsbydiffusiontimestep ranges,showingstrongscalingpotential. However,adaptingMoEtoDiTarchitecture(Shenetal., 2025;Sehwagetal.,2025;Chengetal.,2025)facesseverallimitations.Token-choiceroutingmeth- ods(e.g.,DiT-MoE(Feietal.,2024))sufferpoorexpertspecializationduetoimbalancedtokenas- signments,whereasexpert-choicemethods(e.g.,EC-DiT(Sunetal.,2024))thatfixtokenquotasper expertyieldonlymarginalgains. Morerecently,DiffMoE(Shietal.,2025)andExpertRace(Yuan etal.,2025)explorebatch-levelglobaltokenselectionandmutualexpert\u2013tokenrouting,yetstillrely onimplicitexpertlearningandstrugglewithlimitedexpertspecializationduetotheredundancyand functionalheterogeneityofvisualtokens.Incontrast,weanalyzelanguage\u2013visiontokendifferences andintroduceexplicitroutingguidancetotheMoErouterbasedonthetoken\u2019sfunctionalroleandits semanticcontent. Wefurtherenhancetheroutingprocessthroughtheproposedroutingcontrastive loss,promotingintra-expertcoherenceandinter-expertdiversity. 3 PRELIMINARIES Diffusion Models. Diffusion models are generative models that learn data distributions by re- versing a forward noising process. The continuous-time forward process can be formulated as x = \u03b1 x +\u03c3 \u03f5,witht \u2208 U(0,1)and\u03f5 \u223c N(0,I). \u03b1 and\u03c3 aremonotonicallydecreasingand t t 0 t t t increasingfunctionsoft,respectively. Forthereverseprocess,adenoisingnetworkF istrainedto \u03b8 predictthetargetyateachtimestept,conditionedonc(e.g.,classlabelsortextprompts): (cid:104) (cid:105) L=E \u2225y\u2212F (x ,c,t)\u22252 , (1) x0,c,\u03f5,t \u03b8 t 2 3 --- Page 4 --- + ProMoE Block + X Routing Scores Scale ProMoE Unconditional Expert 1 Expert 2 Expert 3 Shared Expert Expert Block Scale,Shift LayerNorm Two-Step Router + Step2: Semantic Routing PrototypicalRouting Guidance Scale Unconditional LatentSpace Multi-Head ImageTokens Self-Attention Scale,Shift MLP LayerNorm Conditional Routing Contrastive Image Tokens Learning Step1: ConditionalRouting Conditioning InputTokens Input Diffusion Transformer Block Tokens Unconditional Image Tokens Conditional Image Tokens Learnable Prototypes For Routing Figure2:OverviewofProMoEarchitecture.Theinputtokensaresplitbyconditionalroutinginto unconditionalandconditionalsubsets. Unconditionalimagetokensareprocessedbyunconditional experts. Conditional image tokens are assigned by prototypical routing with learnable prototypes. Theroutingcontrastivelearningexplicitlyenhancessemanticguidanceinprototypicalrouting. wherethetrainingtargetycanbetheGaussiannoise\u03f5forDDPMmodels(Hoetal.,2020),orthe vectorfield(\u03f5\u2212x )forRectifiedFlowmodels(Liuetal.,2022). 0 Mixture of Experts. Mixture-of-Experts (MoE) is an architectural paradigm that scales model capacity while preserving computational efficiency by selectively activating a subset of \u201cexperts\u201d sub-networks. AstandardMoElayercomprisesN expertsandatrainablerouterR. Eachexpert E E is implemented as a feed-forward network (FFN). Given input X \u2208 RB\u00d7L\u00d7D, where B is the i batch size, L is the token length, D is the hidden dimension, the router R maps the input X to token\u2013expertaffinityscoresS\u2208RB\u00d7L\u00d7NE viaanactivationfunctionA: S=A(R(X)). (2) Ateachforwardpass,therouteractivatesthetop-Khighest-scoringexpertsanddispatchestheinput tothem.Thefinaloutputistheweightedsumoftheactivatedexperts\u2019outputswithagatingfunction: (cid:26) S, ifS\u2208TopK(S) (cid:88)NE G= , MoE(X)= G \u2217E (X), (3) 0, Otherwise i i i=1 whereG\u2208RB\u00d7L\u00d7NE isthefinalgatingtensor. TherearetwocommonroutingparadigmsinMoE: Token-Choice (TC) and Expert-Choice (EC). In TC, each token independently selects its top-K experts;inEC,eachexpertselectsafixednumberoftop-K tokens. 4 PROMOE In this section, we present ProMoE, an MoE framework for DiTs that integrates a two-step router withexplicitroutingguidance. TheoverallpipelineisdepictedinFig.2. Wefirstdetailthetwo-step routerinSec.4.1. WethenvalidatetheimportanceofsemanticroutingguidanceinvisualMoEsin Sec.4.2andfurtherproposeroutingcontrastivelearningtoenhancesemanticguidanceinSec.4.3. 4.1 TWO-STEPROUTER TheProMoErouteroperatesintwosteps: conditionalroutingbasedonthetoken\u2019sfunctionalrole, followedbyfine-grainedprototypicalroutingbasedontokensemantics. 4 --- Page 5 --- Conditional Routing. Unlike LLMs, diffusion models typically employ classifier-free guidance (CFG)(Ho&Salimans,2022)atinferencetoenhancesamplequality. Specifically,CFGsteersthe generationprocessbycombiningthemodel\u2019sconditionalandunconditionalnoisepredictions. This paradigmnaturallydefinestwofunctionallyheterogeneoustokens: 1)unconditionalimagetokens, derivedfromimagepatchesundernullconditioning(e.g.,emptylabelsortexts);and2)conditional imagetokens,obtainedfrompatchesunderspecificconditioning(e.g.,classlabelsortexts). Tohandledifferenttokentypes,thefirststepoftheProMoErouteremployshardroutingbasedon input conditioning. Specifically, unconditional image tokens are deterministically assigned to N u unconditionalexperts, eachimplementedasafeed-forwardnetwork(FFN),analogoustostandard experts. Conversely,conditionalimagetokensarepassedtothesecondstepforfine-grainedrouting amongstandardMoEexperts. Thisexplicitpartitioningencouragesexpertstolearnthefunctional disparitybetweentokentypes,facilitatingthespecializationofunconditionalandstandardexperts. PrototypicalRouting. ThesecondstepofourProMoErouteristodispatchconditionalimageto- kensforfine-grainedexpertallocation. Concretely,weintroduceanovelprototypicalroutingmech- anismwheretheroutingweightsareparameterizedbyasetoflearnableprototypesP \u2208 RNE\u00d7D, asillustratedinFig.2. Eachprototypep correspondstoanexpertE andistrainedtorepresentthe i i sharedcharacteristicsofaclusterofsemanticallysimilartokens. Compared with standard MoE token assignment, which computes pre-activation scores Z \u2208 RB\u00d7L\u00d7NE viaalinearlayer,weassigntokensusingcosinesimilarity,whichismoreeffectiveand naturallysuitedformeasuringsemanticsimilarityinlatentspacebetweentokensandprototypes: x p\u22a4 Z =[R(X)] =\u03b1 i j , (4) i,j i,j \u2225x \u2225\u2225p \u2225 i j wherex andp arethei-thtokeninXandthej-thprototypeinP. \u03b1isascalingfactor. i j Then, the activation function A transforms the pre-activation scores Z into token\u2013expert affinity scoresS. Insteadofsoftmax,whichiscomputationallyexpensiveandsensitivetosequencelength, weoptforasimplemonotonicfunctionthatpreservesrelativerankings. Weevaluatebothsigmoid and identity functions, finding that the identity A(Z) = Z performs best in practice, as shown in Tab. 7. We argue that the identity activation enables direct top-K selection and provides stable training,thusimprovingperformance.Consequently,weadoptidentityactivationasS=Z.Finally, eachconditionalimagetokenisroutedtothetop-K expertswithgatingscoresG,asinEq.(3). Forward Process. Besides unconditional and standard experts, we also incorporate N shared s expertsthatprocessalltokenstolearnsharedknowledge(Daietal.,2024;Chengetal.,2025). For eachtoken, theoutputofourMoEblockisdefinedasthesumofthesharedexperts\u2019outputanda selectiveoutputdeterminedbythetokentype(conditionalorunconditional): (cid:88)Ns \uf8f1 \uf8f4\uf8f4\uf8f4\uf8f4\uf8f2(cid:88)NE G j \u2217E j(x) ifx\u2208X c MoE(x)= ES(x)+ j=1 , (5) (cid:124)i=1 (cid:123)(cid:122)i (cid:125) \uf8f4\uf8f4\uf8f4\uf8f4\uf8f3(cid:88)Nu E kU(x) ifx\u2208X u Shared k=1 whereES,E ,andEUaretheshared,standard,andunconditionalexperts,respectively. X andX i j k c u areconditionalandunconditionalimagetokensets,respectively,andX \u222aX =X. c u To maintain a constant number of activated parameters, MoE models often employ fine-grained expertsegmentation(Daietal.,2024),wheretheinnerhiddendimensionofeachexpertisdivided bythenumberofactivatedexperts. Inourmostsettings,eachforwardpassofourmodelactivates exactly two experts: the single shared expert and one expert selected from the combined pool of standardandunconditionalexperts. Therefore, tomatchthecomputationalcostofadensemodel, wedividethehiddendimensionofeachexpert\u2019sintermediatelayerbyafactoroftwo. 4.2 SEMANTICROUTINGGUIDANCE Duetotheinherenthighspatialredundancyofvisualtokens,anaiveMoErouterfailstosufficiently distinguish tokens for effective routing, leading experts to learn homogeneous features. Conse- quently, additional semantic routing guidance is required to promote intra-expert coherence and 5 --- Page 6 --- inter-expertdiversity. Tovalidatethis,weconductexperimentsbyaugmentingtheMoErouterwith twoguidancetypes: 1)ExplicitRoutingGuidanceand2)ImplicitRoutingGuidance. Explicit Routing Guidance. We design a routing classification loss that uses class labels to ex- plicitly guide token assignment. Specifically, we manually partition the 1K ImageNet classes into N superclasses based on coarse labels in (Feng & Patras, 2023), and allocate one expert per su- c perclass. Sincelabelsaresample-level,weinstantiatetherouterasaclassifierC: weaverage-pool theinputXoverthetokenlengthdimensiontoobtainX\u00af,feedX\u00af intoC toproducesample\u2013expert affinityscoresS\u00af \u2208RB\u00d7Nc,andassigntheexpertwithhighestscore. Duringtraining,wesupervise theroutingprocesswithacross-entropylossL =CE(S\u00af,c\u00af),wherec\u00afisthesuperclasslabel. cls Implicit Routing Guidance. We replace the standard MoE router with k-means clustering, as- signingalltokensinaclustertoasingleexpert. Unliketheroutingclassificationlossthatprovides explicitsupervision,thisdesignoffersimplicitguidancebymeasuringtokensimilarity,encouraging semanticallysimilartokenstobeco-assigned.Concretely,weinitializeN clustercentroidsbyran- E domlysamplingtokens. Ateachforwardpass, wecomputeeachtoken\u2019sdistancestoallcentroids toobtaindistance-basedtoken\u2013expertaffinityscores. Eachtokenisthenassignedtoitsnearestcen- troidandthusroutedtothecorrespondingexpert. Duringtraining,centroidsareupdatediteratively byreplacingeachwiththemeanoftheircurrentlyassignedtokens.",
  "results": "results of DiT-XL/2 on different layers and diffusion timesteps. 19 --- Page 20 --- Figure12: MoresamplesgeneratedbyProMoE-XL-Flowafter2Miterationswithcfg=4.0. 20",
  "methods": "methodology, experimental design, and final conclusions, are entirely our own.TheLLMwasusedsolelyasawriting-enhancementtoolanddidnotcontributetothescientific aspectsofthework. Wehavereviewedthemanuscriptandtakefullresponsibilityforitscontent. 17 --- Page 18 --- Table13:Ablationstudyof\u03bb inProMoE-B-DDPMonImageNet(256\u00d7256)after500Ktrain- RCL ingsteps. cfg=1.0 cfg=1.5 \u03bb (500K) RCL FID50K\u2193 IS\u2191 FID50K\u2193 IS\u2191 1 40.48 36.77 18.34 80.07 2 40.37 37.46 18.01 81.88 5 40.33 37.08 18.03 81.1 10 40.37 37.84 17.90 82.65 Algorithm1ProMoELayer(Training) Input: X\u2208RB\u00d7L\u00d7D (inputsequence),c\u2208ZB (batchlabels) Variables: N (number of standard experts), K (number of activated standard experts), P \u2208 E RNE\u00d7D (Learnable prototypes for routing), E (List of standard expert FFNs), EU (Unconditional expertFFN),ES(SharedexpertFFN),\u03bb (coefofRoutingcontrastiveloss),\u03c4 (temperature) RCL 1: Initialize: O\u2190zeros like(X) \u25b7Initializefinaloutput 2: /***Step1. FunctionalRouting***/ 3: M u \u2190(c==emptyconditioning) \u25b7Getmaskofunconditionalimagetokens 4: M c \u2190\u00acM u \u25b7Getmaskofconditionalimagetokens 5: X u \u2190X[expand(M u)] \u25b7Getunconditionalimagetokens 6: X c \u2190X[expand(M c)] \u25b7Getconditionalimagetokens 7: /***Step2. UnconditionalImageTokensProcessing***/ 8: O U \u2190EU(X u) 9: O[M u]\u2190O U 10: ifany(M c)then 11: /***Step3. PrototypicalRouting***/ 12: X\u2032 c \u2190reshape(X c,(\u22121,D)) \u25b7Flattenconditionalimagetokens 13: n \u2190X\u2032.shape[0] \u25b7Getnumberofconditionalimagetokens c c 14: Z\u2208Rnc\u00d7NE \u2190L2 Normalize(X\u2032)\u00d7L2 Normalize(P)\u22a4 \u25b7Getpre-activationscores c 15: S\u2190Identity(Z) \u25b7Gettoken\u2013expertaffinityscores 16: G\u2208Rnc\u00d7K,indices\u2208Znc\u00d7K \u2190TopK(S,K) \u25b7Getgatingtensorandindices 17: /***Step4. ConditionalImageTokensProcessing***/ 18: O\u2032 \u2190zeros like(X\u2032) C c 19: fori\u21900toN E \u22121do 20: m i \u2190(indices==i).any(dim=1) \u25b7Maskoftokensroutedtoexperti 21: ifany(m i)then 22: G i \u2190sum(G[m i]\u00d7(indices[m i]==i),dim=1) \u25b7Finalgatingscores 23: O\u2032 C[m i]\u2190O\u2032 C[m i]+G i.unsqueeze(1)\u00d7E i(X\u2032 c[m i]) \u25b7Updatefinaloutput 24: endif 25: endfor 26: OC \u2190reshape(O\u2032 C,X c.shape) 27: O[M c]\u2190O[M c]+OC 28: /***Step5. RoutingContrastiveLearning***/ 29: aux loss\u2190\u03bb RCL\u00d7L RCL(X c,indices,P,\u03c4) 30: endif 31: /***Step6. SharedExpertProcessing***/ 32: O\u2190O+ES(X) 33: Return: O,aux loss 18 --- Page 19 --- \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) = 26.657 = 23.001 = 21.743 \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc4e\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc4e\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc4e\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) LayerNumber: 7 LayerNumber: 9 LayerNumber: 11 \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) = 20.418 = 18.087 = 14.202 \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc4e\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc4e\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc4e\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) LayerNumber: 13 LayerNumber: 15 LayerNumber: 17 Figure10: Moret-SNEvisualizationresultsofLlama-38Bondifferentlayers. \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) = 0.873 = 0.748 = 0.731 \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc4e\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc4e\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc4e\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) LayerNumber: 2 (Shallow Layer) LayerNumber: 12 (Middle Layer) LayerNumber: 24 (Deep Layer) DiffusionTimestep: 50(Last Step) DiffusionTimestep: 50(Last Step) DiffusionTimestep: 50(Last Step) \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) = 0.784 = 0.704 = 0.752 \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc4e\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc4e\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) \ud835\udc34\ud835\udc63\ud835\udc54(\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc4e\u2212\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52) LayerNumber: 12 LayerNumber: 12 LayerNumber: 12 DiffusionTimestep: 10 (Early) DiffusionTimestep: 25 (Middle) DiffusionTimestep: 40 (Late) Figure 11: More t-SNE visualization",
  "conclusion": "CONCLUSION Inthispaper,wepresentProMoE,aMixture-of-Expertsframeworkfeaturingatwo-steprouterwith explicit routing guidance to promote expert specialization. We analyze differences between lan- guageandvisiontokens: discretetexttokensaresemanticallydense,whereasvisualtokensexhibit high spatial redundancy and functional heterogeneity, hindering the effectiveness of MoE in DiT models. Toaddressthis,weintroduceroutingguidancebasedonthetoken\u2019sfunctionalroleandse- manticcontent,yieldingatwo-steproutercomprisingconditionalroutingandprototypicalrouting. 9 --- Page 10 --- Furthermore,weproposearoutingcontrastivelossthatenhancessemanticguidanceinprototypical routing, explicitly promoting intra-expert coherence and inter-expert diversity. Extensive experi- mentsdemonstratethatProMoEoutperformsdenseDiTandexistingMoESOTAs,evenwithfewer activatedortotalparameters,providingarobustsolutionforapplyingMoEtoDiTmodels. Limitations. WhilewefollowstandardevaluationprotocolsandreportFID50KandIS,thesemet- rics may not fully capture fine-grained perceptual quality or semantic faithfulness. Moreover, we validateProMoEonlyonimagegeneration;extendingittomultiplemodalitiesremainsanopenand meaningfuldirectionthatweleaveforfuturework. ETHICS STATEMENT Our method achieves substantial improvements on the ImageNet benchmark over dense DiT and state-of-the-artMoEmethods,providinganeffectivesolutionforscalingDiTwithMoE.Nonethe- less,itinheritscommonrisksofgenerativemodels,suchasthepotentialtocreatefakedata. Robust imageforgerydetectionmayhelpmitigatetheseconcerns. Inaddition,weadheretoethicalguide- linesinallexperiments. REPRODUCIBILITY STATEMENT We make the following efforts to ensure the reproducibility of ProMoE: (1) All",
  "references": "REFERENCES David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. Technical report,Stanford,2006. YogeshBalaji,SeungjunNah,XunHuang,ArashVahdat,JiamingSong,QinshengZhang,Karsten Kreis,MiikaAittala,TimoAila,SamuliLaine,etal. ediff-i:Text-to-imagediffusionmodelswith anensembleofexpertdenoisers. arXivpreprintarXiv:2211.01324,2022. Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. A survey on mixtureofexpertsinlargelanguagemodels. IEEETransactionsonKnowledgeandDataEngi- neering,2025. JunsongChen,JinchengYu,ChongjianGe,LeweiYao,EnzeXie,YueWu,ZhongdaoWang,James Kwok,PingLuo,HuchuanLu,etal. Pixart-\u03b1: Fasttrainingofdiffusiontransformerforphotore- alistictext-to-imagesynthesis. arXivpreprintarXiv:2310.00426,2023. Kun Cheng, Xiao He, Lei Yu, Zhijun Tu, Mingrui Zhu, Nannan Wang, Xinbo Gao, and Jie Hu. Diff-moe: Diffusion transformer with time-aware and space-adaptive experts. In Forty-second InternationalConferenceonMachineLearning,2025. XiangxiangChu,JianlinSu,BoZhang,andChunhuaShen. Visionllama: Aunifiedllamabackbone forvisiontasks. InEuropeanConferenceonComputerVision,pp.1\u201318.Springer,2024. DamaiDai,ChengqiDeng,ChenggangZhao,RXXu,HuazuoGao,DeliChen,JiashiLi,Wangding Zeng,XingkaiYu,YuWu,etal.Deepseekmoe:Towardsultimateexpertspecializationinmixture- of-expertslanguagemodels. arXivpreprintarXiv:2401.06066,2024. JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei. Imagenet: Alarge-scalehi- erarchicalimagedatabase. In2009IEEEconferenceoncomputervisionandpatternrecognition, pp.248\u2013255.Ieee,2009. PrafullaDhariwalandAlexanderNichol. Diffusionmodelsbeatgansonimagesynthesis. Advances inneuralinformationprocessingsystems,34:8780\u20138794,2021. 10 --- Page 11 --- AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,Aiesha Letman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal. Thellama3herdofmodels. arXive-prints,pp.arXiv\u20132407,2024. PatrickEsser, SumithKulal, AndreasBlattmann, RahimEntezari, JonasMu\u00a8ller, HarrySaini, Yam Levi,DominikLorenz,AxelSauer,FredericBoesel,etal. Scalingrectifiedflowtransformersfor high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024a. PatrickEsser, SumithKulal, AndreasBlattmann, RahimEntezari, JonasMu\u00a8ller, HarrySaini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers forhigh-resolutionimagesynthesis. InForty-firstinternationalconferenceonmachinelearning, 2024b. Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Scaling diffusion transformersto16billionparameters. arXivpreprintarXiv:2407.11633,2024. ChenFengandIoannisPatras. Maskcon: Maskedcontrastivelearningforcoarse-labelleddataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19913\u201319922,2023. Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxi- ang Liu, Weichong Yin, Shikun Feng, et al. Ernie-vilg 2.0: Improving text-to-image diffusion modelwithknowledge-enhancedmixture-of-denoising-experts. InProceedingsoftheIEEE/CVF ConferenceonComputerVisionandPatternRecognition,pp.10135\u201310145,2023. AliHatamizadeh,JiamingSong,GuilinLiu,JanKautz,andArashVahdat. Diffit: Diffusionvision transformers for image generation. In European Conference on Computer Vision, pp. 37\u201355. Springer,2024. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Ganstrainedbyatwotime-scaleupdateruleconvergetoalocalnashequilibrium. Advancesin neuralinformationprocessingsystems,30,2017. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598,2022. JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin neuralinformationprocessingsystems,33:6840\u20136851,2020. EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,2021. RobertAJacobs,MichaelIJordan,StevenJNowlan,andGeoffreyEHinton. Adaptivemixturesof localexperts. Neuralcomputation,3(1):79\u201387,1991. AlbertQJiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,BlancheSavary,ChrisBam- ford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtralofexperts. arXivpreprintarXiv:2401.04088,2024. Yunsung Lee, JinYoung Kim, Hyojun Go, Myeongho Jeong, Shinhyeok Oh, and Seungtaek Choi. Multi-architecture multi-expert diffusion models. In Proceedings of the AAAI Conference on ArtificialIntelligence,volume38,pp.13427\u201313436,2024. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, MaximKrikun,NoamShazeer,andZhifengChen.Gshard:Scalinggiantmodelswithconditional computationandautomaticsharding. arXivpreprintarXiv:2006.16668,2020. AonianLi,BangweiGong,BoYang,BojiShan,ChangLiu,ChengZhu,ChunhaoZhang,Congchao Guo,DaChen,DongLi,etal. Minimax-01: Scalingfoundationmodelswithlightningattention. arXivpreprintarXiv:2501.08313,2025. 11 --- Page 12 --- YaronLipman,RickyTQChen,HeliBen-Hamu,MaximilianNickel,andMattLe. Flowmatching forgenerativemodeling. arXivpreprintarXiv:2210.02747,2022. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, ChengqiDeng,ChenyuZhang,ChongRuan,etal. Deepseek-v3technicalreport. arXivpreprint arXiv:2412.19437,2024. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transferdatawithrectifiedflow. arXivpreprintarXiv:2209.03003,2022. NanyeMa,MarkGoldstein,MichaelSAlbergo,NicholasMBoffi,EricVanden-Eijnden,andSain- ing Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. InEuropeanConferenceonComputerVision,pp.23\u201340.Springer,2024. NiklasMuennighoff,LucaSoldaini,DirkGroeneveld,KyleLo,JacobMorrison,SewonMin,Wei- jia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, et al. Olmoe: Open mixture-of-experts languagemodels. arXivpreprintarXiv:2409.02060,2024. AlexanderQuinnNicholandPrafullaDhariwal. Improveddenoisingdiffusionprobabilisticmodels. InInternationalconferenceonmachinelearning,pp.8162\u20138171.PMLR,2021. Byeongjun Park, Sangmin Woo, Hyojun Go, Jin-Young Kim, and Changick Kim. Denoising task routingfordiffusionmodels. arXivpreprintarXiv:2310.07138,2023. Byeongjun Park, Hyojun Go, Jin-Young Kim, Sangmin Woo, Seokil Ham, and Changick Kim. Switch diffusion transformer: Synergizing denoising tasks with sparse mixture-of-experts. In EuropeanConferenceonComputerVision,pp.461\u2013477.Springer,2024. WilliamPeeblesandSainingXie. Scalablediffusionmodelswithtransformers. InProceedingsof theIEEE/CVFInternationalConferenceonComputerVision,pp.4195\u20134205,2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Mu\u00a8ller, Joe Penna,andRobinRombach. Sdxl: Improvinglatentdiffusionmodelsforhigh-resolutionimage synthesis. arXivpreprintarXiv:2307.01952,2023. Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andre\u00b4 SusanoPinto,DanielKeysers,andNeilHoulsby. Scalingvisionwithsparsemixtureofexperts. AdvancesinNeuralInformationProcessingSystems,34:8583\u20138595,2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo\u00a8rn Ommer. High- resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconfer- enceoncomputervisionandpatternrecognition,pp.10684\u201310695,2022a. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo\u00a8rn Ommer. High- resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFCon- ferenceonComputerVisionandPatternRecognition,pp.10684\u201310695,2022b. OlafRonneberger,PhilippFischer,andThomasBrox. U-net: Convolutionalnetworksforbiomedi- calimagesegmentation.InInternationalConferenceonMedicalimagecomputingandcomputer- assistedintervention,pp.234\u2013241.Springer,2015. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improvedtechniquesfortraininggans. Advancesinneuralinformationprocessingsystems, 29, 2016. VikashSehwag,XianghaoKong,JingtaoLi,MichaelSpranger,andLingjuanLyu. Stretchingeach dollar:Diffusiontrainingfromscratchonamicro-budget. InProceedingsoftheComputerVision andPatternRecognitionConference,pp.28596\u201328608,2025. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, andJeffDean. Outrageouslylargeneuralnetworks: Thesparsely-gatedmixture-of-expertslayer. arXivpreprintarXiv:1701.06538,2017. 12 --- Page 13 --- YingShen,ZhiyangXu,JiuhaiChen,ShizheDiao,JiaxinZhang,YuguangYao,JoyRimchala,Is- miniLourentzou,andLifuHuang.Latte-flow:Layerwisetimestep-expertflow-basedtransformer. arXivpreprintarXiv:2506.06952,2025. MingleiShi,ZiyangYuan,HaotianYang,XintaoWang,MingwuZheng,XinTao,WenliangZhao, Wenzhao Zheng, Jie Zhou, Jiwen Lu, et al. Diffmoe: Dynamic token selection for scalable diffusiontransformers. arXivpreprintarXiv:2503.14487,2025. YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,andBen Poole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. arXivpreprint arXiv:2011.13456,2020. HaotianSun,TaoLei,BowenZhang,YanghaoLi,HaoshuoHuang,RuomingPang,BoDai,andNan Du. Ec-dit: Scaling diffusion transformers with adaptive expert-choice routing. arXiv preprint arXiv:2410.02098,2024. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXivpreprintarXiv:2503.20314,2025. YujieWei,ShiweiZhang,ZhiwuQing,HangjieYuan,ZhihengLiu,YuLiu,YingyaZhang,Jingren Zhou, andHongmingShan. Dreamvideo: Composingyourdreamvideoswithcustomizedsub- ject and motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pp.6537\u20136549,2024a. YujieWei,ShiweiZhang,HangjieYuan,XiangWang,HaonanQiu,RuiZhao,YutongFeng,Feng Liu,ZhizhongHuang,JiaxinYe,etal. Dreamvideo-2: Zero-shotsubject-drivenvideocustomiza- tionwithprecisemotioncontrol. arXivpreprintarXiv:2410.13830,2024b. YujieWei, ShiweiZhang, HangjieYuan, BiaoGong, LongxiangTang, XiangWang, HaonanQiu, HengjiaLi,ShuaiTan,YingyaZhang,etal.Dreamrelation:Relation-centricvideocustomization. arXivpreprintarXiv:2503.07602,2025. EnzeXie,JunsongChen,JunyuChen,HanCai,HaotianTang,YujunLin,ZhekaiZhang,Muyang Li,LigengZhu,YaoLu,etal. Sana: Efficienthigh-resolutionimagesynthesiswithlineardiffu- siontransformers. arXivpreprintarXiv:2410.10629,2024. Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo. Raphael: Text-to-image generation via large mixture of diffusion paths. Advances in Neural InformationProcessingSystems,36:41693\u201341706,2023. ZhuoyiYang,JiayanTeng,WendiZheng,MingDing,ShiyuHuang,JiazhengXu,YuanmingYang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models withanexperttransformer. arXivpreprintarXiv:2408.06072,2024. Yike Yuan, Ziyu Wang, Zihao Huang, Defa Zhu, Xun Zhou, Jingyi Yu, and Qiyang Min. Expert race: Aflexibleroutingstrategyforscalingdiffusiontransformerwithmixtureofexperts. arXiv preprintarXiv:2503.16057,2025. Wangbo Zhao, Yizeng Han, Jiasheng Tang, Kai Wang, Yibing Song, Gao Huang, Fan Wang, and YangYou. Dynamicdiffusiontransformer. arXivpreprintarXiv:2410.03456,2024. 13 --- Page 14 --- APPENDIX A EXPERIMENTAL SETUP Baselines. We compare against open-source state-of-the-art DiT-based MoE"
}