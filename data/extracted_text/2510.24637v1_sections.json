{
  "abstract": "Abstract.html. NormanP.Jouppi, DoeHyunYoon, MatthewAshcraft, MarkGottscho, ThomasB.Jablin, GeorgeKurian, James Laudon,ShengLi,PeterMa,XiaoyuMa,ThomasNorrie,NishantPatil,SushmaPrasad,CliffYoung,Zongwei Zhou,andDavidPatterson. TenLessonsFromThreeGenerationsShapedGoogle\u2019sTPUv4i: IndustrialProduct. In 2021ACM/IEEE48thAnnualInternationalSymposiumonComputerArchitecture(ISCA),pages1\u201314,June2021. doi:10.1109/ISCA52012.2021.00010. ISSN:2575-713X. 20",
  "introduction": "1 Introduction SpikingNeuralNetworks(SNNs)areconsideredasthethethirdgenerationofArtificialNeuralNetworks(ANNs). SNNs focus on the encoding and the processing of the information using binary and asynchronous signals known asspikes. Thiscomputingparadigm,inspiredbythemessagingmechanismusedbybiologicalneurons,isthought tobeamongstthesourcesoftheenergyefficiencyofthebrain. However,thebinarynatureofspikesalsoleadsto considerableinformationloss,i.e. quantizationerrors,causingperformancedegradationcomparedtoANNsusing floating-pointoperations. SNNsquantizationerrorcanbereducedbyincreasingthenumberoftimesteps,thereforethe latencyoverthenetwork. However,withalongerconversiontime,morespikesaregenerated,thusincreasingtheenergy consumptionaswell. Severaltechniqueshavebeenproposedtominimizeboththequantizationerrorandthelatencyof SNNs.TheseapproachescanbeeitherappliedtotheANN-to-SNNconversionordirectlyduringtheSNNtrainingusing 5202 tcO 82 ]EN.sc[ 1v73642.0152:viXra --- Page 2 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks thesurrogategradient(SG)method. InLietal.[2022]andRathiandRoy[2021]theauthorsadoptanANN-to-SNN conversionschemewherethefiringthresholdofthespikingneuronsisoptimizedafterconversiontobettermatch thedistributionofthemembranepotential. InCastagnettietal.[2023a]aSNNistrainedusingSGandtheAdaptive Integrate-and-Fire(ATIF)neuron. ThisATIFneuronhasbeenproposedasanalternativetotheoriginalIntegrateand Fire(IF)neuron,thefiringthreshold(V )beingalearnableparameterratherthananempiricalhyper-parameter. InGuo th etal.[2022a],theauthorsalsouseSGtotraintheSNN,butintroduceadistributionlosstoshiftthemembranepotential distributionintotheconversionrangeofthespikingneurons. Withtheseapproaches,requiringonlyfewtimesteps, itispossibletogetSNNswithalmostnoaccuracylosswhencomparedtotheequivalentANNs. Asanexample,a SNNtrainedon4timestepscanachieveanaccuracyonly1%belowtheequivalentnon-quantized,i.e. FloatingPoint (FP),ANNsonCIFAR-10Lietal.[2022]. Tofurtherdecreasethelatency,recentapproachesproposetogobeyond binaryspikesandintroducemulti-levelspikingneurons. Thismechanismexpandstheoutputofspikingneuronsfroma singlebittomultiplebits,thusincreasingtheinformationthatcanbetransmittedateachtimestep. InXiaoetal.[2024] forinstance,ithasbeenshownthatusinga4-levelspikingneuronandonetimestep,thesamelevelofaccuracyofa welloptimizedbinarySNNstrainedon4timestepsLietal.[2022]canbeachieved. Mostofthepreviousworksonly focusontheSNNlatency. However,ithasbeenshownLemaireetal.[2023],Dampfhofferetal.[2023]thatbesidesthe latency,anotherimportantparameterthathastobeoptimizedtoimprovetheenergyefficiencyisthesparsityofthe network,inotherwordsthenumberofspikes,eitherbinaryormulti-level,generatedduringtheinference. Inthiswork weproposetoenhancethesparsityofSNNfromtwopointsofview. Firstattheneuronallevel,whereweintroducea multi-levelspikingneuronmodelthatcanseamlesslydealwithbothtimeandthespikevaluetoreducethequantization error. Thenatthearchitecturallevel,whereanovelspikingResNetarchitectureisproposed. Throughacarefulanalysis ofthespikepropagationintheresiduallayersofthearchitecturewehighlightaneffectthatwecallspikeavalanche. Eventscomingfromthedirectandresidualpathssumupandcreatepeaksofactivitythatcanpotentiallydecrease thesparsityoftheSNN.Finallywecomparebinaryandmulti-levelSNNsfromtheenergy-efficiencypointofview. Our analysis based on the metric proposed in Lemaire et al. [2023], is intended to be independent from low-level implementationchoices. Moreover,ourenergyestimationtakesintoaccountnotonlythesynapticoperationsbutalso allthememoryaccessesthattakeplaceinanevent-drivenexecutionscenarioonaneuromorphichardwareaccelerator. Wethereforeprovideenergyestimationresultsthatareclosertowhatcouldbereasonablyobtainedonarealhardware implementation. Thisapproachisessentialtoavoidoverestimatingtheenergygains. Themaincontributionsofourworkarelistedbelow: \u2022 Weproposeamulti-levelmodelofanIFspikingneuroncompatiblewithSNNdirecttrainingusingSG. \u2022 WetrainSNNsondifferentimageclassificationproblemsandcharacterizethecorrespondingspikingactivity. Weprovidestate-of-the-artaccuracyresultsonCIFAR-10/100imagedatasets,usingonly1timestepwhile reducingtheenergyconsumptionbyafactorof3comparedtoanequivalentANN. \u2022 On neuromorphic data we provide a new state of the art latency/accuracy",
  "results": "results, with a marginally better accuracy for SEW-ResNetwhenN > 2. However, wecanobservethatSEW-ResNetgeneratesamuchhigherlevelofspiking 16 --- Page 17 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks Figure11: (A)Top-1accuracyand(B)spikingactivityofSparse-ResNet18andSEW-ResNet18ontheCIFAR-10 dataset. ForalltheconfigurationsthenumberoftimestepsT =1. Thespikingactivityiscomputedbyaveragingover alltheimagesoftheCIFAR-10testset. activity for all the configurations except when N = 1. As an example, when N = 2 SEW-ResNet fires 242662 spikes/imagewhileSparse-ResNetgenerates218343spikes/image,whichcorrespondstoa10%reductionintheoverall activitywhileprovidingverysimilaraccuracy. Thespikingactivitygapbetweenbothnetworkarchitectureswidensas Nincreases. FortheconfigurationN =4,Sparse-ResNetgenerates25%lessspikes/imagecomparedtoSEW-ResNet withonly0.25%accuracydrop. ThespikingactivityreductionprovidedbySparse-ResNetrisesto30%whenN =8 withanaccuracygaplessthan1%. AsdiscussedinSec. 4.3andshowninFig. 10,thedifferenceinspikingactivityis mainlyduetothespikeavalancheeffectcausedbythefirsttwoshortcutconnectionsintheResNet18network. This effectwouldbeexacerbatedindeepernetworks,thatiswhenmoreconsecutivelayersuseskipconnectionssuchas intheResNet34architectureHeetal.[2016],whenuptofiveconsecutiveresidualblocksuseshortcutconnections. AnotherexampleistheMobileNetV2architecturesSandleretal.[2019],whichiscomposedof17consecutivelayersof invertedresidualblocks. Ashortcutconnectstheinputwiththeoutputofeachblock. Forthespikingversionofthat particulararchitecture,thespikeavalancheeffectwouldbesignificantlyhigherifthespikesareaggregatedattheendof eachresidualconnectionsasshowninFig. 4. Inthatcasetheuseofbarrierneurons,asproposedinthispaper,could leadtoasignificantreductioninthetotalspikingactivityandthusanenergyefficiencyimprovement. 7 ConclusionandFutureworks Inthispaper,weproposedtoimprovethesparsityandenergyefficiencyofSNNsbothatneuronalandatnetworklevel. Byleveragingmulti-levelspikingneuronswehaveshownthattheenergyefficiencyofSNNscabbeimproved. By reducingthequantizationnoiseweareabletoprovidestateoftheartresultsusingonly1timestep,thereforeatthe lowestpossiblelatency. Ourapproachcanbealsoappliedtoneuromorphicdata,forwhichstateoftheartaccuracywere obtainedwhilereducingthelatencybyafactorof10. Atthearchitecturallevel,weidentifiedthespikeavalancheeffect thatimpactsmostofspikingresidualarchitectures. Sparse-ResNethasbeenproposedasasolutiontotheavalanche effect. Thisresidualarchitectureisindeedabletoprovidestateoftheartaccuracyresultsonimageclassificationwhile reducingbymorethan20%thenetworkactivity. Weidentifiedseveraldeepresidualarchitectures,widelyusedinimage processing,thatcanbeimpactedbythespikeavalancheeffect. Webelievethatitwouldbeinterestingtoextendour approachtothesearchitecturesandevaluatetheenergyefficiencygains,bothwithrealisticenergymodelsandusing realneuromorphichardware.",
  "methods": "methods, both with binary and multi-level neurons, are significantly outperformedastheyrequirealatency10to20timeshighertogetasimilarperformancelevel. MoreresultsonsparsityandenergyconsumptionwillbeprovidedinSec. 6.2,toassesswetherthereductioninlatency alsotranslatesinenergyefficiencyimprovement. 11 --- Page 12 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks 6 Ablationstudies 6.1 Energyefficiencycomparisonbetweenmulti-levelandbinaryspikingneuronsonImageclassification Someexperimentswereconductedtocomparetheenergyefficiencyofbinaryandmulti-levelSNNsontheCIFAR-10 dataset. Todoso,differentconfigurationsoftheVGG16architecturewerestudied,bothintermsoflatencyT andspikes valueN. InthefirstconfigurationwefixN =1andtraintheVGG16networkwithdifferentnumberoftimestepsin therangeT \u2208[1,10]. Inthesecondconfiguration,wesetthenumberoftimesteptoT =1andthevalueofthespikes intherangeN \u2208[1,10]. Thissetupallowscomparingconfigurationswithequalnumberofquantizationintervals. As anexampletheconfigurations[T = 4,N = 1]and[T = 1,N = 4]bothprovidethesamenumberofquantization intervals. However,inthefirstcasethequantizationprocesstakesplacethroughtimebyprocessingmultiplebinary spikes. Inthesecondcasethediscretizationprocessisperformedinonetimestepthroughmulti-valuedspikes. Foreach configuration,SNNsaretrainedusingthesetupdescribedinSec. 5.1. Figure6: Top-1accuracyofVGG16ontheCIFAR-10dataset. Thehorizontaldottedlinerepresentstheaccuracyof theANNFPversionofthenetwork. TheaccuracyofthedifferentSNNsconfigurationsaswellastheANNbaselineisshowninFig. 6. Asexpected,the accuracygapbetweentheANNandtheSNNsreduceswhenincreasingthenumberofquantizationintervals. Moreover, as it can be observed, there is a functional equivalence between both SNN configurations. For a given number of quantizationintervals,theaccuracyconvergestoveryclosevalues. TheseresultsthusconfirmtheanalysisofSec. 4.2, wherewehaveshownthatthesamequantizationfunctioncanbeobtainedwithdifferent[N,T]configurations. ThetotalspikingactivityaswellasthetotalenergyconsumptionareshowninFig. 7(A)andFig. 7(B)respectively. Eventhoughthedifferentbinaryandmulti-levelconfigurationsarefunctionallyequivalent,wecanobservethattheyare considerablydifferentbothintermsofspikingactivityandenergyconsumption. Asanexample,the[T =4,N =1] configuration generates 130\u00b7103 binary spikes for each inference, while the same functional configuration with multi-levelneurons,[T = 1,N = 4],onlyproduces57\u00b7103 valuedspikes. Thatis43%lessspikesthathavetobe processedforeachinputimage. Moreover,itcanbeobservedthatforthebinarySNNsthetotalactivitygrowsalmost linearlywiththetimestepswhilethereisa(sub)logarithmicincreaseforthemulti-levelconfigurations. Thespiking activitydirectlyimpactsthetotalenergyconsumptionshownFig. 7(B).ItcanbeobservedthatevenifthebinarySNNs aremoreenergyefficientthantheANNatlow-timesteps,e.g. thebinarySNNsprovides35%energyreductionwhen [T =4,N =1],thegapquicklyreducesasTincreases. AtT =8forinstance,thebinarySNNsalreadyconsumes 10%moreenergythantheANN.Ontheotherhand,themulti-levelVGG16maintainsahighlevelofenergyefficiency ateachoperatingpoint. Asanexample,when[T =1,N =4]thereisa66%energyreductioncomparedtotheANN. Atthe[T =1,N =8]operatingpointtheenergyreductionisstill60%comparedtotheANN. Finally,thecompleteenergyconsumptionbreakdownisshowninTab. 3. Hereweprovidethedetailedestimationfor theANNandtheconfigurationswith4and8quantizationintervalsforboththebinaryandthemulti-levelSNNs. As itcanbeseen,thetotalenergyconsumption,especiallyfortheSNNs,isstronglydominatedbythecostofmemory accesses. Eachspikegeneratedbyaneuronleadstothreememoryaccesses(tworeadoperationsforretrievingthe weights and the current value of the membrane potential and a write operation) and a synaptic operation (ACC or N \u00d7ACCforthebinaryandmulti-valuedcaserespectively). Theenergycostforonememoryaccessisonaverage 10to100timeshigherthanasynapticoperationJouppietal.[2021]. Indeed,wecanobservedifferencesoftwoto 12 --- Page 13 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks Figure7: SpikingactivityandtotalenergyconsumptionofSNNsandANNfortheCIFAR-10dataset. Weprovide (A)thetotalspikingactivityofVGG16fordifferentconfigurationsofmulti-levelSNNs{T = 1, N \u2208 [1,10]}and binary SNNs {T \u2208 [1,10], N = 1}. (B) The total energy consumption of the different SNN configurations. The horizontaldottedlinerepresentsthetotalenergyconsumptionoftheANN. Table3: Energyconsumptionbreakdownforbinaryandmulti-levelVGG16SNNsandANNonCIFAR-10. Allenergy valuesareexpressedin[nJ]. E correspondstothetotalenergyconsumptionofthemulti-levelSNN,whileE N binary referstothebinarySNN.Thesymbol[\u2193]meanslowerisbetter. ANN SNN binary multi-level binary multi-level [T =4,N =1] [T =1,N =4] [T =8,N =1] [T =1,N =8] Memory Potentials - 6.0\u00b7106 3.1\u00b7106 10.0\u00b7106 3.6\u00b7106 Weights 5.58\u00b7106 2.4\u00b7106 1.3\u00b7106 4.1\u00b7106 1.54\u00b7106 Bias 6.9\u00b7102 27.8\u00b7102 6.9\u00b7102 55.7\u00b7102 6.9\u00b7102 In/Out 6.1\u00b7106 3.5\u00b7103 1.54\u00b7103 5.7\u00b7103 1.8\u00b7103 Total 11.6\u00b7106 8.4\u00b7106 4.38\u00b7106 14.2\u00b7106 5.19\u00b7106 Synaptic 1.29\u00b7106 13.3\u00b7103 27.9\u00b7103 22.4\u00b7103 65.4\u00b7103 Operations Addressing 4.9\u00b7102 9.0\u00b7102 5.5\u00b7102 14.8\u00b7102 8.7\u00b7102 Total 12.9\u00b7106 8.5\u00b7106 4.41\u00b7106 14.28\u00b7106 5.26\u00b7106 E /E [\u2193] 0.51 0.37 N binary E /E [\u2193] 0.65 0.34 1.10 0.4 SNN ANN threeordersofmagnitudebetweentheenergyconsumedbythememoryaccessesandthesynapticoperations. Wecan alsoobservethat,asexpected,theenergyconsumedbythesynapticoperationsisgreaterforthemulti-valuedspike. However,astheamountofgeneratedspikesislowerthanbinaryspikes,thisincreaseislargelycounterbalancedbythe decreaseofenergyrequiredforaccessingthememory. Theseresultsclearlyconfirmthatthetotalamountofspikes, showninFig. 7(1)isthemostimportantmetrictoconsiderforreducingtheenergyconsumptionofSNNs. Neitherthe averageactivitypertimestepnorthelatencyalonearesufficienttoproperlyevaluatetheoverallenergyconsumption. In thatsenseourresultsshowthat,usingmulti-valuedspikes,theenergyefficiencyofSNNscanbegreatlyimprovedby2 to3timesdependingonthenumberofquantizationintervals. 6.2 Energyefficiencycomparisonbetweenmulti-levelandbinaryspikingneuronsonNeuromorphicdata classification InthissectionwestudytwoconfigurationsoftheVGG16architecture,withmulti-levelandbinaryneurons,onthe CIFAR-10-DVSneuromorphicclassificationdataset. Specificallywecomparethemulti-levelVGG16networkgiven in Tab. 2 with a binary VGG16 trained on T = 10 timesteps. Both configurations thus provide 10 quantization 13 --- Page 14 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks Table4: Energyconsumptionbreakdownforbinaryandmulti-levelVGG16SNNsandANNonCIFAR-10-DVS.All energyvaluesareexpressedin[nJ]. E correspondstothetotalenergyconsumptionofthemulti-levelSNN,while N E referstothebinarySNN.Thesymbol[\u2193]meanslowerisbetter. binary ANN SNN binary multi-level [T =10,N =1] [T =1,N =10] Memory Potentials - 233.3\u00b7106 90.6\u00b7106 Weights 52\u00b7106 32.2\u00b7106 12.8\u00b7106 Bias 6.9\u00b7102 69.7\u00b7102 6.9\u00b7102 In/Out 186.3\u00b7106 40.5\u00b7103 15.8\u00b7103 Total 238.3\u00b7106 265.5\u00b7106 103.4\u00b7106 Synaptic 12.1\u00b7106 170.8\u00b7103 664.6\u00b7103 Operations Addressing 6.2\u00b7103 10.3\u00b7103 7.7\u00b7103 Total 250.4\u00b7106 265.7\u00b7106 104.1\u00b7106 E /E [\u2193] 0.39 N binary E /E [\u2193] 1.06 0.41 SNN ANN intervalsaswellassimilaraccuracylevels: 76.97%when{T =1, N =10}and75.5%forthebinaryconfiguration {T =10, N =1}. TheenergybreakdownforbothSNNsaswellastheANNisgiveninTab. 4. First it can be observed that the total energy consumption for both the SNNs and the ANN is almost twenty time higherthantheCIFAR-10resultsshowninTab. 3. ThehigherresolutionofCIFAR-10-DVS(128\u00d7 128)compared toCIFAR-10(32\u00d7 32)leadstoanincreaseofthenumberofmemoryaccessesandoperationstoprocesstheinput framesaswellasthefeaturemapsofhiddenlayers. However,resultsshowninTab. 4confirmtheenergyefficiency improvementsofmulti-levelSNNs,evenonneuromorphicdata. Asitcanbeobserved,themulti-levelVGG16reduces by60%theenergyconsumptioncomparedtotheANN.Theenergygainscomefromthereductionofenergyrelatedto memorytransfers. Themulti-levelSNNconsumesintotal103.4\u00b7106nJforthememoryaccesses,while238.3\u00b7106nJ arerequiredfortheANN.Inthemulti-levelSNN,mostoftheenergyisconsumedforaccessingtheneuronpotentials (90.6\u00b7106nJ)whileonly12.8\u00b7106nJarerelatedtothememoryaccessesassociatedwiththesynapticweights. On theotherhand,theANNconsumesfourtimesmoreenergyforaccessingthesynapticweights(52\u00b7106nJ)andtwice moreforaccessingthefeaturemaps,i.e. In/Out186.3\u00b7106nJ.Finally,thereisadifferenceofalmostthreeorderof magnitudeintheenergyconsumptionofthesynapticoperationsbetweentheANNandtheSNNs. However,forthe ANNtheenergyrelatedtothesynapticoperationsaccountsforlessthan5%ofthetotalenergybudget. Ourresultsare closetothosealreadyobservedinDampfhofferetal.[2023],thatistheenergycostofsynapticoperationsisverysmall comparedtotheoneoftransferringdatafromthememory. Therefore,theenergygainsobtainedbyreducingthecostof thesynapticoperationsaremarginalwhenconsideringthetotalenergyconsumption. Finally,thebinarySNNdoesnot exhibitenergygainscomparedtotheANNwhenusinglatenciesthatprovidestateoftheartaccuracyresultsonthe CIFAR-10-DVSdataset,i.e. 10ormoretimesteps. WhenT =10theSNNfires1.5\u00b7106spikeswhileonly591\u00b7103 spikesareemittedwhenN =1. AsshowninTab. 4,itrepresentsa60%reductioninthetotalspikingactivitywhichtranslatesinanequivalentenergy gainbetweenmulti-levelandbinarySNNs. Theseresultsagainconfirmthat,evenforneuromorphicdatasets,theenergy consumptioncanbeloweredbyreducingthetotalamountofspikesgeneratedbytheSNN. 6.3 GradientpropagationinspikingResNetarchitectures InthissectionwestudythegradientpropagationinthespikingresidualarchitecturesSEW-ResNetandtheSparse- ResNet that has been introduced in Sec. 4.4. To assess the improvement provided by the STE we also consider a variantofSparse-ResNetwhereallthespikingneurons,includingthebarrierneurons,usethesamesurrogatederivative functionsshowninFig. 5. Foreachblockofthethreearchitectures,SEW-REsNet18,Sparse-ResNet18withSTEand Sparse-ResNet18withoutSTEwemeasurethegradientnormonthedirectpath, \u2202L,theresidualpath \u2202L andafter \u2202Al \u2202Rl thesummationpoint \u2202L. Thegradientisaveragedover104 minibatchesoftheCIFAR-10dataset. Wealsorepeat \u2202Ol the simulation by training the networks 10 times starting from different initial conditions. The gradient mean and standarddeviationshowninFig. 8areprovidedforeachresidualblockofthenetwork. Theexperimentalresults 14 --- Page 15 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks Figure8: UsingSTEhelpsovercomethevanishinggradient. ThegradientsoftheresidualblocksinSEW-ResNet18, Sparse-ResNet18withSTEandSparse-ResNet18withoutSTEontheCIFAR-10dataset. Theblock0isthefirstblock aftertheinput. confirm the analysis given in Sec. 4.4. As it can be observed, the gradients norm on the direct and residual paths ofSEW-ResNetandSparse-ResNetwithSTEareclosertoeachotherandconsistentlyhigherthanthegradientsof Sparse-ResNetwithoutSTE.Indeed,whenSTEisnotusedtheupdatesofthegradientonSparse-ResNetareweaker, thusslowingdownthetraininganddecreasingtheperformance. ThisbehaviorcanbeobservedinFig. 9whereweshow thevalidationloss,computedattheendofeachtrainingepochforthethreenetworksontheCIFAR-10dataset. The lossofSparse-ResNetwithoutSTEdeclinesmoreslowlyandfinallyreachesahighervaluecomparedtoSEW-ResNet andSparse-ResNetwithSTE. 6.4 Spikespropagationinresiduallayers Inthissectionweprovidesomeexperimentalresultstoassesstheeffectivenessofthebarrierneuronsinlimitingthe spikepropagationintheresiduallayers.Todoso,wecomparetheactivityofSEW-ResNetandSparse-ResNetwithSTE measuredontheCIFAR-10dataset. Bothnetworksaretrainedusingthe[T =1,N =4]configuration,thatprovides almostthesameaccuracy,asshowninTab. 1. Wemeasurethenumberofgeneratedspikesinthedirectandtheresidual pathsforbothnetworks. Thespikesthataregeneratedatthesummationpointarecomputedasthesumofthenumberof spikescomingfromthedirectpathandthespikescomingfromtheresidualpathforSEW-ResNet. ForSparse-ResNet, wemeasureinsteadthenumberofspikesafterthebarrierneuron.Ourmeasurecorresponds,inbothcases,totheamount ofspikesthataretransferredfromonelayertothenextinthecontextofanevent-basedcommunication/processing scheme. TheexperimentalresultsareshowninFig. 10,wherewecanobservethatSEW-ResNetconsistentlygenerates morespikesinthefirstlayersofthenetworkcomparedtoSparse-ResNet. Thethenumberofgeneratedspikesstabilizes fromthemiddletothelastlayers,wherebothnetworksgeneratealmostthesameactivity. Thespikeavalancheeffect, 15 --- Page 16 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks Figure9: ComparisonofthevalidationlossofSEW-REsNet18,Sparse-ResNet18withSTEandSparse-ResNet18 withoutSTEonCIFAR-10. that we qualitatively described in Sec. 4.3, can be clearly observed in Fig. 10. The number of spikes generated bySEW-ResNetatthefirst(sum )andespeciallyatthesecond(sum )summationpointsareconsiderablyhigher 0 1 comparedtoSparse-ResNet. SEW-ResNetproduces45665spikesatthesummationpointsum whileonly37150spikesarefiredbySparse-ResNet. 0 Theavalancheeffectisevenmorevisibleatsum wherethereare67848spikesforSEW-ResNetandonly35819for 1 Sparse-ResNet. Thatis47%lessactivityinoneofthelayerthatgeneratesmostofthespikesinthewholenetwork. TheseresultsconfirmthatthebarrierneuronsofSparse-ResNetareabletopreventthespikeavalancheeffect,thus reducingthenetworkactivityaswediscussinmoredetailsinthenextsection. Figure10: Layer-wiseactivityofSEW-ResNet18andSparse-ResNet18. Top: numberofspikesgeneratedinthe directpath(block )andafterthesummationpoint(sum ). Bottom: numberofspikesgeneratedintheresidualpath i i (res ). ThevaluesareaveragedoveralltheimagesoftheCIFAR-10testset. i 6.5 AnalysisofsparsityinspikingResNetarchitectures Inthisfinalsectionweprovideexperimentalresultsonthespikingactivityformulti-levelSEW-ResNetandSparse- ResNet. TheaccuracyaswellasthetotalspikingactivityfordifferentconfigurationsonCIFAR-10,i.e. {T =1, N \u2208 [1,8]},areshowninFig. 11. As it can be observed, both networks provide very similar accuracy",
  "references": "References Chen Li, Lei Ma, and Steve Furber. Quantization Framework for Fast Spiking Neural Networks. Frontiers in Neuroscience,16,2022. ISSN1662-453X. URL 2022.918793. NitinRathiandKaushikRoy. DIET-SNN:ALow-LatencySpikingNeuralNetworkWithDirectInputEncodingand LeakageandThresholdOptimization. IEEETransactionsonNeuralNetworksandLearningSystems,pages1\u20139, 17 --- Page 18 --- Allinonetimestep: EnhancingSparsityandEnergyefficiencyinMulti-levelSpikingNeuralNetworks 2021. ISSN2162-2388. doi:10.1109/TNNLS.2021.3111897. ConferenceName: IEEETransactionsonNeural NetworksandLearningSystems. Andrea Castagnetti, Alain Pegatoquet, and Beno\u00eet Miramond. Trainable quantization for Speedy Spiking Neural Networks. Frontiers in Neuroscience, 17, 2023a. ISSN 1662-453X. URL  articles/10.3389/fnins.2023.1154241. YufeiGuo,XinyiTong,YuanpeiChen,LiwenZhang,XiaodeLiu,ZheMa,andXuhuiHuang. RecDis-SNN:Rectifying MembranePotentialDistributionforDirectlyTrainingSpikingNeuralNetworks. In2022IEEE/CVFConferenceon ComputerVisionandPatternRecognition(CVPR),pages326\u2013335,NewOrleans,LA,USA,June2022a.IEEE. ISBN 978-1-66546-946-3. doi:10.1109/CVPR52688.2022.00042. URL 9880053/. YongjunXiao,XianlongTian,YongqiDing,PeiHe,MengmengJing,andLinZuo. Multi-BitMechanism: ANovel InformationTransmissionParadigmforSpikingNeuralNetworks,July2024. URL 05739. arXiv:2407.05739[cs]version: 1. EdgarLemaire,Lo\u00efcCordone,AndreaCastagnetti,Pierre-EmmanuelNovac,JonathanCourtois,andBeno\u00eetMiramond. AnAnalyticalEstimationofSpikingNeuralNetworksEnergyEfficiency. InMohammadTanveer,SonaliAgarwal, SeiichiOzawa,AsifEkbal,andAdamJatowt,editors,NeuralInformationProcessing,pages574\u2013587,Cham,2023. SpringerInternationalPublishing. ISBN978-3-031-30105-6. doi:10.1007/978-3-031-30105-6_48. ManonDampfhoffer,ThomasMesquida,AlexandreValentian,andLorenaAnghel. AreSNNsReallyMoreEnergy- EfficientThanANNs?anIn-DepthHardware-AwareStudy.IEEETransactionsonEmergingTopicsinComputational Intelligence, 7(3):731\u2013741, June 2023. ISSN 2471-285X. doi:10.1109/TETCI.2022.3214509. URL https: //ieeexplore.ieee.org/document/9927729/?arnumber=9927729. Conference Name: IEEE Transactions onEmergingTopicsinComputationalIntelligence. Wei Fang, Zhaofei Yu, Yanqi Chen, Tiejun Huang, Timoth\u00e9e Masquelier, and Yonghong Tian. Deep Residual LearninginSpikingNeuralNetworks. InAdvancesinNeuralInformationProcessingSystems,volume34,pages 21056\u201321069.CurranAssociates,Inc.,2021a. URL afe434653a898da20044041262b3ac74-"
}