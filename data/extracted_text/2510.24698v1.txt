--- Page 1 --- 2025-10-29 PARALLELMUSE: Agentic Parallel Thinking for Deep Information Seeking BaixuanLi†((cid:0)),DingchuZhang†,JialongWu†,WenbiaoYin((cid:0)),ZhengweiTao,YidaZhao, LiwenZhang,HaiyangShen,RunnanFang,PengjunXie,JingrenZhou,YongJiang((cid:0)) TongyiLab ,AlibabaGroup   Abstract Parallelthinkingexpandsexplorationbreadth,complementingthedeepexplo- rationofinformation-seeking(IS)agentstofurtherenhanceproblem-solving capability. However,conventionalparallelthinkingfacestwokeychallengesin thissetting: inefficiencyfromrepeatedlyrollingoutfromscratch,anddifficulty inintegratinglong-horizonreasoningtrajectoriesduringanswergeneration,as limitedcontextcapacitypreventsfullconsiderationofthereasoningprocess. Toaddresstheseissues,weproposePARALLELMUSE,atwo-stageparadigm designedfordeepISagents. Thefirststage,Functionality-SpecifiedPartial Rollout,partitionsgeneratedsequencesintofunctionalregionsandperforms uncertainty-guidedpathreuseandbranchingtoenhanceexplorationefficiency. Thesecondstage,CompressedReasoningAggregation,exploitsreasoningre- dundancytolosslesslycompressinformationrelevanttoanswerderivationand synthesizeacoherentfinalanswer. Experimentsacrossmultipleopen-source agents and benchmarks demonstrate up to 62% performance improvement witha10–30%reductioninexploratorytokenconsumption. 1 Introduction Deepinformation-seeking (IS)agents1 (OpenAI,2025b; Team,2025a;b) canactively uncoverhard-to- accessinformation,extendinglargelanguagemodelsbeyondstatictrainingdataandempoweringthem toreasonoverreal-worldknowledge. Thiscapabilityemergesfromacontinualloopofenvironmental2 interactionandinternalreasoning,throughwhichtheagentincrementallybuildsreasoningdepthwithin asingleexecutiontoeffectivelysolvecomplexproblems(Wuetal.,2025c;a;Lietal.,2025c;Taoetal.,2025; Lietal.,2025b).Inthissetting,parallelthinkingprovidesanaturalformoftest-timescaling:byexpanding thenumberofparallelexplorationpaths,itbroadenstheagent’ssearchwhilemaintainingreasoning depthalongeachpath,therebyenhancingoverallperformancewithoutalteringmodelparameters. As commonly recognized, parallel thinking can be viewed as a two-stage process (Li et al., 2025a), involvinganinitialstageofexploratorysamplingandasubsequentstagededicatedtoanswergeneration across sampled candidates. In this work, we extend this paradigm to the setting of deep IS agents, †Equalcontribution. (cid:0)   1Theagentsdiscussedinthisworkarefunction-callingagentsthatadheretothestandardReAct(Yaoetal.,2023) paradigm,operatingthroughaniterativethink→toolcallloop. 2Thisworkfocusesondeepinformation-seekingagents,wheretheterm“environment”specificallyreferstothe webenvironmentorinformationsourceswithwhichtheagentinteracts. 1 5202 tcO 82 ]LC.sc[ 1v89642.0152:viXra --- Page 2 --- referredtoasPARALLELMUSE. Weanalyzehowthecharacteristicsofeachstagemanifestunderagentic conditionsandproposesystematicoptimizationstrategiesderivedfromthesepilotobservations. First, in the exploratorysamplingstage, conventional rollout strategies in parallel thinking typically restartfromscratchateachiteration,resamplingtheentireexplorationspace(AI,2025;Fuetal.,2025; Zengetal.,2025). Duringcertainreasoningphases,however,explorationdiversityisinherentlylow, makingrepeatedrolloutsinefficientandtoken-expensive. Priorworkintroducespartialrolloutmethods thatestimateexplorationpotentialviauncertaintyandselectivelybranchwhereuncertaintyishigh(Hou etal.,2025;Dongetal.,2025;Lietal.,2025e),buttheseapproachesassumefunctionalhomogeneityacross tokens,implyingthatalltokenscontributeequallytoexplorationandexhibitsimilaruncertainty. Thisassumptionholdsinpurelyreasoning-orientedtaskssuchasmathematicsorcodingbutfailsin agenticISsettings,wherethemodelmustgeneratebothreasoningandtool-callactions. Thesebehaviors naturallyformdistinctfunctionalregionswithdifferentuncertaintypatterns.Motivatedbythisobservation, weproposetheFunctionality-SpecifiedPartialRolloutmethodasthefirststageofthePARALLELMUSE framework. Themethodsegmentsthegeneratedsequenceintofunctionalregions,estimatesuncertainty independentlywithineach,andselectivelyexpandsrolloutsforreasoningstepswithhigherexploration potential. Thisenablesbehavior-levelestimationofexplorationpotential,allowingtargetedexploration acrossdifferentfunctionalbehaviors,andimprovingoverallefficiencyinagentictasks. Second,intheanswergenerationstage,parallelthinkingproducesmultiplereasoningcandidatesfrom whichasingleanswermustbederived,typicallythroughanswerselection(Wangetal.,2022;Fuetal., 2025)oransweraggregation(Jiangetal.,2023;Liangetal.,2024;Zhangetal.,2025b;Qiaoetal.,2025). Incomplexagentictaskswithvastsamplingspaces,thecorrectanswermaynotdominatenumerically, asitoftenconstitutesonlyasmallfractionofallpossiblesampledoutcomes. Moreover,thecontinual incorporation of external, non–model-generated information shifts the output distribution, further hindering confidence calibration (Jang et al., 2024). As a result, majority voting (Wang et al., 2022) and confidence-based selection (Fu et al., 2025) often fail. For answer aggregation, focusing only on finalanswersneglectsintermediatereasoning,whileincorporatingentirereasoningtracesisinfeasible forlong-horizonagentsduetocontextlimits. Recentwork(Qiaoetal.,2025)seeksacompromiseby aggregating only the last few reasoning steps, but this discards earlier content, which often reflects planningandproblemdecompositionandisessentialforevaluatingthecoherenceofthefinalanswer. To address this challenge, we first conceptualize the IS task as a process of discovering key entities and building connections among them (Li et al., 2025c; Tao et al., 2025; Li et al., 2025b). Based on our preliminary observations, only a small portion of the entities explored by an agent contribute meaningfully to the final answer, revealing substantial redundancy and strong potential for lossless compressioninthegeneratedinteraction–reasoningtrajectories. Buildingonthisinsight,wepropose theCompressedReasoningAggregationmethodasthesecondstageofthePARALLELMUSEparadigm. Themethodfirstcondensesallreasoningcandidatesintoconcise,structuredreportsthatpreserveonly informationrelevanttoanswerderivation,andthenaggregatesthesecompressedreportstoproduce thefinalanswer. Thisapproachenhancesprocessingefficiencyandmitigatesthebiasofmajority-based selection,enablingmorereliableandcoherentanswergeneration. Theproposed PARALLELMUSE isevaluatedonfouropen-sourcedeepISagents,includingGPT-OSS- 20B(OpenAI,2025a),GPT-OSS-120B,DeepSeek-V3.1-Terminus(Liuetal.,2024),andTongyi-DeepResearch- 30B-A3B(Team,2025b),acrossfourchallengingbenchmarksthatjointlyassessdeepsearchandreasoning abilities: BrowseComp(Weietal.,2025),BrowseComp-zh(Zhouetal.,2025),GAIA(Mialonetal.,2023), andHumanity’sLastExam(HLE)(Phanetal.,2025). ExtensiveexperimentsshowthatPARALLELMUSE achieves up to 62% improvement while requiring only 70–90% of the exploratory token cost of con- ventional parallel thinking. Beyond the empirical gains, our analysis provides key insights into the mechanismsofdeepISagents,offeringguidanceforfutureresearchinagenticreasoning. 2 --- Page 3 --- 2 Pilot Observation Webeginwithapreliminaryanalysisofthecharacteristicsofdeepinformation-seeking(IS)agentsand theirassociatedtasks, providinginsightsfromtwoperspectives: (i)exploratorysampling(trajectory rollout)and(ii)theresultinginteraction–reasoningtrajectories. 2.1 Distinct Uncertainty Patterns Across Functional Reasoning Steps In deep IS tasks, agents must not only reason over internal knowledge but also explore unknown informationthroughtooluseandenvironmentalinteraction. Whilepurereasoningmodelsusetokens exclusively for internal reasoning, deep IS agents additionally allocate tokens for tool invocation to retrieveexternalinformation,reflectingdistinctfunctionalrolesintokenutilization. Formally, each step consists of a reasoning segment, a tool invocation, and its corresponding tool response. WedenotethesetoftokensgeneratedbythemodelatsteptasT = {x ,x ,...,x },with t t,1 t,2 t,m x denotingthei-thtoken. Thesetispartitionedintotwosubsets: Tr,representingreasoningtokens, t,i t andTe,representingexplorationtokens. Thispartitionholdsforeachstep,implyingTr∪Te = T . By t t t t extension,aggregatingthesesetsacrosstheentiretrajectoryyieldsglobalsets(T,Tr,Te). Incontrast,a purereasoningtaskwouldhaveanemptyexplorationset,Te = ∅,satisfyingT = Tr. Furthermore,weobservethattheuncertaintyassociatedwithtokensintheTr andTe subsetsexhibits distincttemporaldynamicsduringtheagenticinteraction-reasoningprocess. Toquantitativelycapture thisbehavior,weusetheperplexity(PPL)ofeachreasoningstep,whichisdefinedastheaveragePPLof tokenswithinstept,asaproxyforthedeepISagent’sself-uncertainty. (cid:32) |T| (cid:33) PPL(f,t) =exp − |T1 tf| i∑ =t 0logp (x t,i | x<t,i) , x t,i ∈ |T tf|, f ∈ {r,e}, (1) where f representsthefunctionalregionoftheentiretrajectory,whichispartitionedintoareasoning regionrandanexplorationregione. 0.15 0.10 0.05 0.00 0 5 10 ytisneD Reasoning Region Exploration Region GPT-OSS-20B DeepSeek-V3.1-T Tongyi-DR 0.15 0.20 0.10 0.15 0.10 0.05 0.05 0.00 0.00 0 5 10 0 5 10 Figure1: KDE-smootheddistributionofstepswithtop-4uncertaintyontheBrowseCompsubset(trun- catedtoearlierstepsaslateronesaretypicallymorecertain). DeepSeek-V3.1-TdenotesDeepSeek-V3.1- Terminus,andTongyi-DRdenotesTongyi-DeepResearch-30B-A3B. We analyze PPL(r,t) and PPL(e,t) across steps to characterize the distinct uncertainty dynamics of reasoningandexplorationwithintheagenticreasoning-interactionprocess. AsshowninFigure1,across multipledeepISmodels,weexaminethedistributionofthetop-4uncertaintystepsobservedduringtask execution. Theresultsrevealaconsistentpattern: explorationuncertaintyreachesitshighestlevelsatthe earlieststages,whennoexternalinformationhasyetbeengathered,whilereasoninguncertaintypeaks slightlylaterastheagentbeginsintegratingretrievedinformationintoitsinternalreasoningprocess. Specifically,explorationuncertaintytendstopeakatthebeginningofthetask,whentheagenthasnot yetgatheredexternalinformationandmustexploretheenvironmentunderminimalpriorknowledge. 3 --- Page 4 --- Reasoninguncertainty,incontrast,reachesitspeakslightlylater—stillintheearlystage—whentheagent beginstoprocessandintegratethenewlyretrievedinformationintoitsinternalreasoningchain. Asthe taskproceeds,bothformsofuncertaintygraduallydeclineastheagentaccumulatesknowledgeandits reasoningprocessbecomesmoregrounded,resultinginincreasinglyconfidentdecisionsandactions. ThisobservationfurtherinformsthedesignoftheFunctionality-SpecifiedPartialRolloutmethodin PARALLELMUSE,whichenhancesagenticparallelthinkingbyenablingmoreefficientexploration. 2.2 From Exploration Redundancy to Losslessly Compressible Trajectory Followingrecentstudies(Lietal.,2025c;Taoetal.,2025;Lietal.,2025b),deepIStaskscanbeformulated as a process of entity discovery and relation construction. Formally, given an initial query or objective q, the agent incrementally builds a set of discovered entities V = {v ,v ,...,v } through iterative 1 2 N interactionswithexternalinformationsources. Ateachstept,theagentperformsexplorationtoretrieve candidateentitiesV˜ andreasoningtodeterminetheirrelevanceandrelationalconnections. Theevolving t informationstateoftheagentcanthusbeexpressedas: G = (V ,R ), (2) t t t where V denotes the set of effective entities (i.e., entities considered valid for answer derivation) and t R ⊆ V ×V representstherelationsconstructedamongthem. Thegoalofthetaskistoiterativelyrefine t t t G untilitcontainsthekeyentitiesandconnectionsnecessaryforderivingthefinalanswer. Oncethe t entireagenticreasoningprocessterminates,thefinalgraphG ⊇ I (whereI denotesall final answer answer informationessentialforanswerderivation),servingasthecorerepresentationofthereasoningtrajectory. BasedonthisformulationofdeepIStasks,weapproximatetheredundancyofatask’scompletereasoning trajectorybymeasuringtheproportionofeffectiveentities—thosethatcontributedirectlyorindirectlyto answerderivation—withinallentitiesdiscoveredduringexecution. Formally,letV denotethesetof total allentitiesexploredbytheagentduringatask,andletV ⊆ V representthesubsetofentitiesthat eff total aredirectlyorindirectlyusefulforderivingthefinalanswer. TheredundancyratioΓ isdefinedas: red |V | Γ =1− eff . (3) red |V | total AhigherΓ indicatesgreaterredundancyintheagent’sinteraction–reasoningtrajectory,whichcan red beinterpretedasastrongerpotentialforlosslesscompressionofthereasoningprocess. Inthiscontext, lossless compression refers to reducing redundant entities and reasoning steps while preserving all 100 80 60 40 20 0 GPT-OSS-120BDeepSeek-V3.1-T Tongyi-DR muN ytitnE reasoninginformationnecessaryforthe completederivationofthefinalanswer Avg. All Entity Avg. Effective Entity (i.e.,extractingandrepresentingG final), BrowseComp GAIA 103.4 and thus Γ can be regarded as an 98.9 43.1 red 40 approximateindicatorofthedegreeof 78.3 34.1 losslesscompressibility. 30 25.5 Accordingly,wecomputethereasoning 20 trajectoryredundancyofseveralmain- streamdeepISagentsduringrealtask -9 63 .. 84% -9 44 .. 70% -9 81 .. 27% 10 -8 61 .. 42% -7 59 .. 32% -8 72 .. 64% execution. AsillustratedinFigure2,all 0 GPT-OSS-120BDeepSeek-V3.1-T Tongyi-DR modelsexhibitconsistentlyhighredun- Figure2: Averageentitycountpertaskandpermodel,where dancy,indicatingthatthereasoningtra- entitiesareextractedbyGPT-4.1basedonthecompletereason- jectoriesindeepIStasksarehighlyloss- ingtrajectoryandground-truthanswer. lessly compressible. This observation supportsthedesignoftheCompressed ReasoningAggregationmethodinPARALLELMUSE,whichaimstointegrateasmucheffectivereasoning informationaspossibleintofinalaggregationwithminimalinformationloss. 4 --- Page 5 --- Functionality-Specified Partial Rollout Compressed Reasoning Aggregation ...  Initial Rollout ... ...  Get Branch 2.0 2.5 ... 2.4 1.9 ... ... ... ... ... ...  Partial Rollout ... ...  Compression  Aggregation User Prompt Think (Reasoning) Tool Call (Exploration) Tool Response Answer Compressed Reasoning Report Figure3:WorkflowofPARALLELMUSE,including(Left)theFunctionality-SpecifiedPartialRollout,where theGetBranchshowstheselectionoftop-kstepsbasedon(exploration)tool-calluncertainty(justasan exampleofbranchingcriterion),and(Right)theCompressedReasoningAggregation. 3 ParallelMuse TheproposedPARALLELMUSEisatwo-stageagenticparallelthinkingparadigmcomprisingtwocom- plementary components: (i) Functionality-Specified Partial Rollout and (ii) Compressed Reasoning Aggregation. AsshowninFigure3,thesecorrespondtothefirststageofexploratorysamplingandthe secondstageofanswergenerationintheoverallparallelthinkingprocess,respectively. 3.1 Functionality-Specified Partial Rollout Functionality-SpecifiedBranchingStepIdentification. Agentmodelsinherentlypartitiongenerated tokensintofunctionalregions,typicallyreasoningandexploration,signaledbyspecialtokenssuchas <think>and<tool_call>. Weleveragethesemarkerstoidentifydistinctfunctionalsegmentswithinthe generationprocess. Toenablemoretargetedpartialrollout,itisessentialtoidentifyreasoningstepswith higherexplorationpotential. Wemeasurethispotentialthroughthemodel’sgenerationuncertaintyat eachstep,ashigheruncertaintyindicatesgreaterdiversityinpossiblecontinuationsandthusabroader exploration space. Accordingly, we compute the step-level perplexity (PPL) within each functional region,asdefinedinequation1,toquantifytheagent’sgenerationuncertainty. Thisprocessisconductedinanofflinemannertoensureoptimalbranchselection. AsshowninFigure 3(Left),wefirstgenerate Minitialtrajectoriesfromscratch,computestep-levelPPLforreasoningand exploration regions along these trajectories for each step, and select the top-k steps with the highest uncertaintyinthechosenfunctionalregion f ∈ {r,e}(definedinequation1)asbranchingpointsfor subsequentpartialrollouts. Itisnotedthat M,k,and f aretunablehyper-parameters. AsynchronousPartialRollout. Fromtheselectedhigh-uncertaintysteps, N−M additionalpartial rolloutsarelaunchedasynchronouslytoexpandexploration,whereNistheoverallsamplingbudget. Eachbranchdirectlyreusestheprecedingcontextratherthanregeneratingitfromscratch,continuing from cached hidden states in the key–value (KV) cache (Li et al., 2024; Wu et al., 2025b). This reuse eliminatesredundantforwardpasses,yieldingsubstantialsavingsinbothtokenandcomputecost. Weimplementanasynchronousrolloutenginetoparallelizebranchgenerationwhilepreservingeach branch’scausaldecodingconsistency,enablingmultiplebranchestoexpandconcurrentlyandefficiently. 5 --- Page 6 --- Theaccelerationcomesfromtwosources:(i)prefixreuseviaKVcachingand(ii)asynchronousparallelization. Letbranchjreuseaprefixoftokenlength p andgenerateasuffixoftokenlengths . Withcolddecoding j j (noKVreuse), thecostis Ccold = c ·(p +s ); withKVreuse, thecostis Chot = c·s . Here, c > 0 j cold j j j j denotestheper-tokencomputecostundercacheddecoding(withKVreuse),andc ≥ cdenotesthe cold per-tokencostwhenregeneratingfromscratch(withoutKVreuse). ReuseFactor≡ ∑ jCc jold = c cold (cid:32) 1+ ∑ jp j(cid:33) . (4) ∑ jCh jot c ∑ js j AsynchronousschedulingparallelizeshotdecodingacrossPactivebranches. Ifα ∈ [0,1]denotesthe parallelizableratio,thethroughputgainobeystheAmdahl-typebound(Amdahl,1967): 1 ParaFactor(P) ≤ . (5) (1−α)+α/P Combining equation 4 and equation 5 yields the overall speedup: Speedup ≲ ReuseFactor× total ParaFactor(P). In the practical regime with efficient KV caching (c ≈ c ), high parallelizability cold (α ≈1),andPwithinhardwareconcurrency,thissimplifiesto (cid:32) (cid:33) ∑ p j j Speedup ≈ 1+ P. (6) total ∑ s j j Thisdesignjointlyexploitsdeterministicprefixreuseandasynchronousparallelizationtoachievenear- linearspeedupinexplorationefficiencywithrelativelylowertokencost. 3.2 Compressed Reasoning Aggregation StructuredReport-StyleCompression. BuildingontheobservationsinSection2.2,wenotethatthe completereasoningtrajectoriesobtainedafterthefirst-stageexploratorysamplingindeepinformation- seeking (IS) tasks exhibit high redundancy, implying strong lossless compressibility with respect to answerderivation. Toeffectivelyintegratericherintermediatereasoninginformationduringtheanswer aggregationstagewhilemaintainingcomputationalefficiencyandavoidingcontextoverflow,wefirst compresseachcandidatereasoningtrajectoryproducedintheexploratorystage. As shown in Figure 3 (Right), for each reasoning trajectory generated in solving deep IS tasks, the compressionobjectiveistoproduceastructuredreportthatpreserveskeyelementsessentialtoanswer derivation. Thereportrecords: (i) SolutionPlanning:describeshowthemainproblemisdecomposedintosubproblems,including theirdependencystructureandexecutionorder. (ii) Solution Methods: specifies the tools invoked to solve each subproblem, the corresponding parametersused,andanysubanswersthatcontributedirectlyorindirectlytothefinalsolution. (iii) Final Reasoning: illustrates how the identified subproblems and associated subanswers are integratedtoderivethefinalanswer. Irrelevantexploratorycontent,includingredundanttoolresponsesandineffectivereasoningortoolcalls, isremoved. Thisprocesseffectivelyreconstructstheagent’sinternalinformationstategraphG (defined inequation2),whichcomprehensivelycapturesallinformationrelevanttoanswerderivation. Reasoning-GuidedAnswerAggregation. AfterobtainingNcompressedreportsfromtheexploratory sampling stage, we can jointly consider all N globally compressed reasoning candidates within the limited context window, rather than focusing only on their final answers or partial reasoning traces. This enables a more comprehensive evaluation of reasoning coherence and supports a more reliable determinationoftheoptimalanswer. Inthisaggregationstage,weexplicitlypreventthemodelfrom 6 --- Page 7 --- relyingsolelyonanswerconsistencyasacorrectnesssignal,mitigatingthebiastowardmajorityanswers andensuringthatreasoningcoherenceremainstheprimarycriterion. Furthermore,werestrictthemodel fromtriviallyconcatenatingorenumeratingdifferentanswerstopreserveaggregationvalidity. It is also important to note that each report already contains sufficient tool-calling provenance and attributioninformationforanswerderivation. Therefore,duringtheaggregationphase,themodeldoes notperformadditionaltoolinvocationsforsecondaryverificationbutinsteadconductsreasoningpurely overtheinformationencodedintheNreports. Empiricalresultslaterdemonstratethatthisapproachis botheffectiveandcomputationallyefficient. 4 Experiments Wefocusonevaluatingtheeffectivenessandefficiencyofapplying PARALLELMUSE toexistingdeep information-seeking(IS)agents. Comprehensiveexperimentsareconductedtoexaminetheimpactofits twostagesbothindividuallyandjointly,assessinghoweachcontributestooveralltaskperformance. 4.1 Setup Benchmarks. WeevaluatePARALLELMUSEonfourchallengingdeepISbenchmarks:BrowseComp(Wei etal.,2025),BrowseComp-zh(Zhouetal.,2025),GAIA(Mialonetal.,2023),andHumanity’sLastExam (HLE)(Phanetal.,2025). Thesebenchmarksjointlyassessbothdeepsearchandreasoningcapabilities, withBrowseCompandBrowseComp-zhplacinggreateremphasisondeepsearch,HLEfocusingmore onreasoning,andGAIAprovidingabalancedevaluationacrossbothdimensions. For efficient text-only evaluation, we use sampled subsets from large-scale datasets: 200 randomly selectedtasksfromBrowseComp,157search-focusedtext-onlytasksfromHLE,and103text-onlytasks fromGAIA(Lietal.,2025d),whileusingthefull289-tasksetforBrowseComp-zh. Tools. WeadoptthestandardtoolconfigurationcommonlyusedindeepISagents(Wuetal.,2025a;Li etal.,2025c;Taoetal.,2025;Lietal.,2025b;Qiaoetal.,2025),whichincludestwocoretoolsforinteracting withthewebenvironmentandretrievingexternalinformation: • Search: PerformsbatchedGooglequeriesandreturnsthetop-10rankedresultsforeach. • Visit: FetcheswebpagesfrommultipleURLsandextractsinformationrelevanttothegivengoal. AgentModels. Weselectfouropen-sourceagentmodelswithdiverseparameterscalesandadvanced tool-usecapabilitiesfordeepIStasks: GPT-OSS-20B(OpenAI,2025a),GPT-OSS-120B,DeepSeek-V3.1- Terminus(DeepSeek-V3.1-T,671B)(Liuetal.,2024),andTongyi-DeepResearch-30B-A3B(Tongyi-DR-30B- A3B)(Team,2025b). Allagentmodelsareinvokedundertheofficialfunction-callingprotocol. Unless otherwisespecified,weusethesameagentmodeltoperformbothstagesofthePARALLELMUSE. Baselines. Inadditiontothestandardinferencebaselinewithoutanyparallelthinking(NoScaling), we compare PARALLELMUSE against several mainstream parallel thinking baselines. These include: (i) Self-Consistency(MajorityVote) (Wang et al., 2022), which se- lects the most frequent answer across multiple trajectories as Table1: Defaultsettingsofourpro- the final output; (ii) Max#ToolCall (Zeng et al., 2025), which posedPARALLELMUSE. heuristically chooses the answer derived from the trajectory with the largest number of environment interactions; and (iii) Hyper-Parameters Values DeepConf(WeightedVote)(Fuetal.,2025),whichweightsanswers SamplingBudgetN 8 by the model’s confidence over each trajectory and selects the #InitialRolloutM 1 answerwiththehighestfinalscore. BranchingPPLTop-K 2 #BranchingTimesperStep 3 EvaluationMetricsandHyper-Parameters. Allevaluationsare performedundertheLLM-as-a-Judgeparadigm(Guetal.,2024), 7 --- Page 8 --- using the official evaluation prompts and judging models specified by each benchmark’s released configuration. FortheNoScalingmethod,wereporttheaveragepassrateoverNindependentrollouts, whileforparallelthinkingmethods,whichyieldasinglefinalanswerfrom N rollouts,wereportthe passrateofthatfinaloutput. ForourproposedPARALLELMUSE,unlessotherwisespecified,thedefault hyper-parametersettingsarelistedinTable1. Toensurefaircomparisonandreproducibility,allagent model–specifichyper-parametersarealignedwiththeirofficialoptimalconfigurationsfortoolusage. 4.2 Overall Performance Table2: Overallperformance. Scoresmarkedwith‡representfull-benchmarkresults,whereasunmarked scorescorrespondtoourbenchmarksettings. BothPARALLELMUSEandotherparallelthinkingbaselines are evaluated under the default configurations as described in Section 4.1. The specific strategy for selectingfunctionalregionsinPARALLELMUSE’spartialrolloutisdiscussedinSection4.3. Model/Framework Method BrowseComp BrowseComp-zh GAIA HLE Closed-SourceDeepInformation-SeekingAgents Claude-4-Sonnet NoScaling 12.2‡ 29.1 68.3 20.3‡ OpenAI-o3 NoScaling 49.7‡ 58.1 70.5 26.6‡ KimiResearcher NoScaling – – – 26.9‡ OpenAIDeepResearch NoScaling 51.5‡ 42.9 67.4 26.6‡ ChatGPTAgent NoScaling 68.9‡ – – 41.6‡ Open-SourceDeepInformation-SeekingAgents NoScaling 30.9 28.6 63.4 24.2 MajorityVote 44.0 38.8 69.9 24.2 GPT-OSS-20B Max#ToolCall 17.0 19.0 58.3 26.1 WeightedVote 41.0 37.0 68.9 31.2 PARALLELMUSE 49.0 44.3 72.8 32.5 NoScaling 34.9|33.8‡ 36.0 74.3 36.3 MajorityVote 48.5 46.7 77.7 43.3 GPT-OSS-120B Max#ToolCall 17.5 26.3 68.9 36.9 WeightedVote 48.0 45.7 82.5 45.2 PARALLELMUSE 56.5 54.3 85.4 45.9 NoScaling 23.2 36.1 61.0 25.0|21.7‡ MajorityVote 30.0 45.0 70.9 26.1 DeepSeek-V3.1-T Max#ToolCall 17.5 28.0 57.3 27.4 WeightedVote 29.5 45.0 70.9 28.0 PARALLELMUSE 39.0 50.2 74.8 37.6 NoScaling 51.0|43.4‡ 45.3 73.6 38.5|32.9‡ MajorityVote 60.0 56.8 77.7 40.1 Tongyi-DR-30B-A3B Max#ToolCall 41.0 36.3 75.7 38.2 WeightedVote 62.0 53.6 78.6 42.7 PARALLELMUSE 65.0 57.1 79.6 52.2 Wereporttheperformanceofclosed-sourcedeepISagentsacrossallbenchmarksandcomparethemwith open-sourceagentsequippedwithourproposedPARALLELMUSEandseveralrepresentativeparallel thinkingbaselines. AsshowninTable2,PARALLELMUSEconsistentlyachievesthehighestperformance gainsoverallbaselinesacrosseveryagentmodelandbenchmark. Notably,whenappliedtoTongyi-DR- 30B-A3B,itattainsperformancecomparabletoorsurpassingthatofmostclosed-sourceagents. 8 --- Page 9 --- ItisimportanttonotethatwefurtherobservethattheWeightedVote,whichreliesonself-estimatedconfi- dence,underperformsMajorityVoteacrossallmodelsexceptTongyi-DR-30B-A3B.Thiscanbeattributed to confidence miscalibration in agentic settings: as agents repeatedly integrate external, non–model- generated content (e.g., tool responses), their internal probability distributions shift, degrading the reliabilityofconfidencescores(Jangetal.,2024;Chhikara,2025). TheexceptionistheHLEbenchmark, whichemphasizesreasoningwithlimitedexternalinteraction,andTongyi-DR-30B-A3B,whichbenefits fromcontinualpre-training(Suetal.,2025)thatimprovescalibrationoverSearchandVisittoolresponses. Incontrast,ourproposedPARALLELMUSEavoidsconfidence-basedanswerselectionaltogether,mitigat- ingthissourceofbiasandyieldingconsistentandsubstantialimprovementsacrossallagentmodels. 4.3 Analysis of Partial Rollout over Distinct Functional Regions Table 3: Performance comparison between full from-scratch rollouts and partial rollouts guided by functional-regionuncertainty. DetailedconfigurationsarelistedinTable1. AgentModel FunctionalRegion BrowseComp BrowseComp-zh GAIA HLE FromScratch: NoRegion 34.9 36.0 74.2 36.3 GPT-OSS-120B Partial: Reasoning 37.9 43.1 76.1 36.3 Partial: Exploration 39.9 41.6 77.9 37.5 Partial: Mixed 38.1 42.9 76.7 37.1 FromScratch: NoRegion 23.2 36.1 61.0 25.0 DeepSeek-V3.1-T Partial: Reasoning 26.5 39.8 60.2 26.4 Partial: Exploration 23.8 37.9 60.6 25.3 Partial: Mixed 23.4 38.1 60.3 25.1 To analyze the effect of identifying branching steps based on different functional regions in partial rollout,wereporttheaveragepassrateafter8rollouts,asshowninTable3. TheFromScratchsetting denotesfullrolloutswithoutreusingcontext,wherefunctionalregionselectionisnotapplicable. Forour proposedPARALLELMUSE3,weevaluatethreefunctionality-specifiedpartialrolloutstrategies: using uncertaintyfromtheReasoningregiononly,usinguncertaintyfromtheExplorationregiononly,anda Mixedconfigurationwherehalfofthetop-2branchingstepsareselectedfromeachregion. TheMixed settingactsasacompromise,integratinguncertaintycuesfrombothregions. The results show that the effectiveness of branching based on different functional-region uncertain- tiesvariesacrossmodels,reflectingtheirinherentbehavioralandcapabilitydifferences. Forinstance, GPT-OSS-120Bbenefitslessfromreasoning-basedbranching,asitsstrongadaptivereasoningmecha- nismalreadyyieldsconsistentlyhigh-qualityreasoningwithlimitedexplorationpotential. Incontrast, DeepSeek-V3.1-Temploysfunctioncallingoutsideofthethinkingmode,resultinginweakerinternal reasoningcapacityandthushighersamplingpotentialinreasoningsteps. Theseinsightsheuristically informourchoiceoffunctional-regionuncertaintyforpartialrolloutinPARALLELMUSE. We also observe that partial rollout consistently outperforms full from-scratch rollout in most cases. Thisimprovementarisesfrommoretargetedexploration. IndeepIStasks,whereinteractionwiththe webenvironmentinducesanextremelylargeexplorationspace,unguidedrolloutsstruggletoidentify effectivesearchpathsandoftenfallintolocaloptima. Incontrast, uncertainty-guidedpartialrollout functionsanalogouslytoMonte-CarloTreeSearch(MCTS)(Browneetal.,2012): whileMCTSreuses high-rewardtrajectories,PARALLELMUSEreuseslow-uncertainty(low-potential)pathsandselectively 3Weomitresultsofpartialrolloutwithoutfunctional-regiondistinction(i.e.,treatingalltokensashomogeneous), asourpreliminaryexperimentsshowthatthissettingperformscomparablytofullfrom-scratchrolloutsandprovides noobservablegainsfrombranchingathigh-uncertaintysteps. 9 --- Page 10 --- expands exploration at high-uncertainty steps. This strategy allocates the limited sampling budget towardregionswithgreaterexpectedexplorationgain,enhancingbothefficiencyandeffectiveness. 4.4 Performance Gains from Compressed Reasoning Aggregation 80 70 60 50 40 BrowseComp BrowseComp-zh GAIA HLE erocS In Section 4.3, we examined the perfor- mancegainsarisingfromthefirst-stagepar- No Scaling 80.6 tialrolloutoftheproposedPARALLELMUSE. Majority Vote 77.778.6 Inthissection,weisolateandanalyzethe Weighted Vote 73.6 Ours effectivenessofitssecond-stageanswerag- gregationmethod,independentlyassessing 63.5 62.0 itscontributiontooverallperformance. 60.0 56.8 57.1 53.6 As shown in Figure 4, even without the 52.2 51.0 sampling(exploration)gainsfromthefirst- 45.3 stage partial rollout, the proposed Com- 42.7 40.1 pressed Reasoning Aggregation (the second- 38.5 stageofPARALLELMUSE)aloneyieldsthe mostsignificantimprovement.Notably,this Figure 4: Performance gains from different answer gen- approachperformsnear-losslesscompres- eration methods, with sampling fixed to 8 from-scratch sionovereachagenticreasoningtrajectory rolloutstoisolatesampling(exploration)effects. to efficiently integrate reasoning informa- tionwithoutinvokingadditionaltoolcalls for secondary verification. By maximizing the exploitation of existing sampled information during aggregation,itachievesabalancedimprovementinbothefficiencyandsolutionquality. 4.5 Efficiency Gains through Context Reuse and Trajectory Compression Figure 5: Efficiency gains using PARALLELMUSE. (i) (Left) Token reduction through context reuse in ourpartialrolloutmethod. Wetakethetokenconsumptionpertrajectoryofthefrom-scratchrolloutas thebaseline. Thegreenbarsrepresentthetokencostafterapplyingpartialrollout(thenumbersabove indicatetheratiorelativetothebaseline),whiletheremainingbluebarsshowtheproportionoftokens saved. (ii)(Right)Comparisonofcontexttokenusagebeforeandaftertrajectorycompression. WeconductadetailedanalysisoftheefficiencygainsachievedbyourproposedPARALLELMUSE,which primarilyarisefromtwocomplementarysources: (i) Token reduction via context reuse in Functionality-Specified Partial Rollout. As shown in Figure 5 (Left), our method (Partial Rollout) achieves up to 28% token savings by effectively reusingcontextinsteadofregeneratingitfromscratch(RolloutfromScratch). Theefficiencygain increaseswithsamplingscale,indicatingbetterscalability. (ii) ContextefficiencyviatrajectorycompressioninCompressedReasoningAggregation. Asshown 10 --- Page 11 --- inFigure5(Right),compressingtheagenticreasoningtrajectoryreducescontexttokenusageby upto99%relativetothefulltrajectory,achievinganalmostcompletecompression. Thisenables multi-trajectoryreasoningaggregationwithincontextlimitsandimprovesprocessingefficiency. Insummary,theproposedPARALLELMUSEisnotaconventionaltest-timescalingstrategythatimproves performancebyaggressivelysacrificingefficiency. Instead,byleveragingatask-informeddesign,itscales computationwhereitmattersmost,allocatingadditionaltokensandreasoningcapacityasneededto high-utilityregionswhileeliminatingmostredundantoravoidablecomputation. 4.6 Impact of Model Capability on Compressed Reasoning Aggregation In the proposed PARALLELMUSE, the compres- sionprocessinCompressedReasoningAggregation Table4:Performancegainsfromusingstrongermod- canbeviewedasextractingandreconstructingthe els for Compressed Reasoning Aggregation. Rollout agent’sinternalinformationstategraphG(defined configurationdetailedinTable1. inequation2)fromthefullagenticreasoningtra- jectory. Thisgraph,describedbythecompressed RolloutModel AggregationModel BrowseComp report,encapsulatesallinformationnecessaryfor GPT-OSS-20B 49.0 answerderivation. Hence,thequalityofcompres- GPT-OSS-20B GPT-OSS-120B↑ 50.5 GPT-5↑↑ 55.5 siondependsonthefidelityofthisextractionand Tongyi-DR-30B-A3B 65.0 reconstruction,whichdirectlyaffectssubsequent Tongyi-DR-30B-A3B GPT-5↑ 66.0 aggregationperformance. Toexaminewhetherastrongermodelcanperform higher-qualitycompressionandyieldbetteraggregation,weevaluatethesettingwheretheCompressed ReasoningAggregationstageisexecutedbymodelsstrongerthanthoseusedforthefirst-stagepartial rollout. AsshowninTable4,whenthefirst-stagesamplingisconductedwithGPT-OSS-20B,replacing itwithastrongerGPT-OSS-120Bfortheaggregationstageleadstoaclearperformanceimprovement. FurthersubstitutionwithGPT-5bringscontinuousgains,andasimilartrendisobservedontheTongyi- DR-30B-A3Bmodel,confirmingthatthecompressedreporteffectivelyrepresentstheagent’sinternal informationstategraphandthathigher-qualitygraphreconstructionenhancesoverallperformance. This resultalsosuggestsapracticalinsightformulti-agentdesign(Hanetal.,2024;Lietal.,2024): combining modelsofdifferentstrengthscanbalanceefficiencyandperformance. 5 Related Work 5.1 Deep Information-Seeking Agents Deepinformation-seeking(IS)agentsareautonomoussystemsdesignedtoengageinmulti-stepinter- actionwithexternalinformationenvironments(suchastheweb)andintegrateretrieveddatathrough reasoninginordertoaddresscomplexknowledge-intensivetasks. Thedevelopmentofsuchagentshasbenefitedfrombothproprietaryandopen-sourceinitiatives. Onthe proprietaryside,systemsfrommajorresearchorganizationshavesetbenchmarksfordeepexploration and reasoning capabilities (OpenAI, 2025b; Team, 2025a; AI, 2025; Perplexity, 2025; Anthropic, 2025; DeepMind,2025),thoughtheirarchitecturesandtrainingprotocolsremainclosed. Ontheopen-source front,communityeffortshaveadvancedtransparencyandreproducibilityindeepISagentdesign(Zhang etal.,2025a;Wuetal.,2025c;a;Lietal.,2025c;b;Taoetal.,2025;Gengetal.,2025;Sunetal.,2025;Gao etal.,2025;Liuetal.,2025;Luetal.,2025;Team,2025b),drivingcontinuousprogressinthisdomain. Inthiswork,wefurtherexploretheuniquecharacteristicsofdeepISagentsandproposePARALLELMUSE toexploitthesepropertiesmoreeffectively,therebyenhancingbothcapabilityandefficiency. 11 --- Page 12 --- 5.2 Parallel Thinking for Test-Time Scaling Parallel thinking (Wang et al., 2025) serves as an effective test-time scaling strategy for reasoning, particularlyinagenticsettingsthatrequiredeepandcomplexinteractions.Itgeneratesmultiplereasoning trajectoriestocapturediversereasoningbehaviorsandjointlydeterminesthefinalanswer. Conceptually,parallelthinkingfollowsatwo-stageparadigm(Lietal.,2025a): exploratorysamplingand answergeneration. Thefirststageexploresdiversereasoningpathsthroughindependentsampling(Wei etal.,2022;Zengetal.,2025),structuredrollouts,orintermediatepartialrollouts(AI,2025;Dongetal., 2025; Hou et al., 2025; Li et al., 2025e) where branches share context but remain flexible. In agentic reasoningwithvastexplorationspaces,independentandpartialrolloutsaregenerallymoreeffective andefficientthanstructuredones. Thesecondstagefocusesonsynthesizingresultsviaeitheranswer selection(Wangetal.,2022;Fuetal.,2025),whichisefficientbutoftenbiased,oransweraggregation(Jiang etal.,2023;Liangetal.,2024;Zhangetal.,2025b;Qiaoetal.,2025),whichismorestablebutchallenged bytheneedtoidentifywhichintermediatereasoningcontributesmosttothefinaloutcome. Mostexistingparallelthinkingmethodsinherittheassumptionsofpurereasoningtasks. Buildingon a detailed analysis of agentic reasoning, especially in deep IS tasks, we propose PARALLELMUSE, a paradigmthatfullyleveragesthesepropertiestomoreeffectivelyunlockthepotentialofdeepISagents. 6 Conclusion This work investigates the challenges of applying parallel thinking to deep information-seeking (IS) agents. Conventionalparallelthinkingstrategiesoftenwastecomputationthroughredundantrollouts andstruggletointegratelong-horizonreasoningduetolimitedcontextcapacity. Buildingonanin-depth analysisofthecharacteristicsofdeepIStasks,weincorporatetheseinsightsintomethoddesignand proposePARALLELMUSE,atwo-stageparadigmthatenhancesbothexplorationefficiencyandreasoning aggregation. Experimentalresultsacrossmultipleopen-sourceagentsandbenchmarksdemonstratethat PARALLELMUSEachievessubstantialperformanceimprovementswhilegreatlyreducingexploratory tokenconsumption,highlightingitseffectivenessforefficientandscalabledeepISreasoning. 7 Limitations and Future Work In this work, we focus primarily on question-answering–oriented deep IS tasks, where the toolset is limitedtoSearchandVisit. WhilethisconfigurationisoptimalfordeepIStasks,moregeneralagentic tasksofteninvolveabroaderrangeoftools(Fangetal.,2025),leadingtosubstantiallylargerexploration spaces. Designingeffectiveparallelthinkingstrategiesundersuchcomplextoolconfigurationstoextend applicabilitytogeneralagenticsettingsremainsanopendirectionforfutureresearch. 12 --- Page 13 --- References SkyworkAI. Skywork-deepresearch.  GeneMAmdahl.Validityofthesingleprocessorapproachtoachievinglargescalecomputingcapabilities. InProceedingsoftheApril18-20,1967,springjointcomputerconference,pp.483–485,1967. Anthropic. Introducingclaude4,2025. URL Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp Rohlfshagen,StephenTavener,DiegoPerez,SpyridonSamothrakis,andSimonColton. Asurveyof montecarlotreesearchmethods. IEEETransactionsonComputationalIntelligenceandAIingames,4(1): 1–43,2012. PrateekChhikara. Mindtheconfidencegap: Overconfidence,calibration,anddistractoreffectsinlarge languagemodels. arXivpreprintarXiv:2502.11028,2025. GoogleDeepMind. Gemini2.5,2025. URL ni-model-thinking-updates-march-2025/. GuantingDong, HangyuMao, KaiMa, LichengBao, YifeiChen, ZhongyuanWang, ZhongxiaChen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprintarXiv:2507.19849,2025. RunnanFang,ShihaoCai,BaixuanLi,JialongWu,GuangyuLi,WenbiaoYin,XinyuWang,XiaobinWang, LiangcaiSu,ZhenZhang,etal. Towardsgeneralagenticintelligenceviaenvironmentscaling. arXiv preprintarXiv:2509.13311,2025. YichaoFu,XueweiWang,YuandongTian,andJiaweiZhao. Deepthinkwithconfidence. arXivpreprint arXiv:2508.15260,2025. Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, and Yi Wu. Beyond ten turns: Unlocking long-horizon agentic search with large-scale asynchronous rl. arXiv preprintarXiv:2508.07976,2025. XinyuGeng,PengXia,ZhenZhang,XinyuWang,QiuchenWang,RuixueDing,ChenxiWang,Jialong Wu,YidaZhao,KuanLi,etal. Webwatcher: Breakingnewfrontierofvision-languagedeepresearch agent. arXivpreprintarXiv:2508.05748,2025. JiaweiGu,XuhuiJiang,ZhichaoShi,HexiangTan,XuehaoZhai,ChengjinXu,WeiLi,YinghanShen, ShengjieMa,HonghaoLiu,etal. Asurveyonllm-as-a-judge. arXivpreprintarXiv:2411.15594,2024. ShanshanHan,QifanZhang,YuhangYao,WeizhaoJin,andZhaozhuoXu. Llmmulti-agentsystems: Challengesandopenproblems. arXivpreprintarXiv:2402.03578,2024. ZhenyuHou,ZiniuHu,YujiangLi,RuiLu,JieTang,andYuxiaoDong.Treerl:Llmreinforcementlearning withon-policytreesearch. arXivpreprintarXiv:2506.11902,2025. ChaeyunJang,HyungiLee,SeanieLee,andJuhoLee. Calibrateddecision-makingthroughllm-assisted retrieval. arXivpreprintarXiv:2411.08891,2024. DongfuJiang,XiangRen,andBillYuchenLin. Llm-blender: Ensemblinglargelanguagemodelswith pairwiserankingandgenerativefusion. InProceedingsofthe61stAnnualMeetingoftheAssociationfor ComputationalLinguistics(Volume1: LongPapers),pp.14165–14178,2023. BaixuanLi,YunlongFan,TianyiMa,MiaoGao,ChuanqiShi,andZhiqiangGao. Raspberry: Retrieval- augmentedmontecarlotreeself-playwithreasoningconsistencyformulti-hopquestionanswering. In FindingsoftheAssociationforComputationalLinguistics: ACL2025,pp.11258–11276,2025a. 13 --- Page 14 --- HaoyangLi,YimingLi,AnxinTian,TianhaoTang,ZhanchaoXu,XuejiaChen,NicoleHu,WeiDong, QingLi,andLeiChen. Asurveyonlargelanguagemodelaccelerationbasedonkvcachemanagement. arXivpreprintarXiv:2412.19442,2024. KuanLi,ZhongwangZhang,HuifengYin,RuiYe,YidaZhao,LiwenZhang,LituOu,DingchuZhang, XixiWu,JialongWu,XinyuWang,ZileQiao,ZhenZhang,YongJiang,PengjunXie,FeiHuang,and JingrenZhou. Websailor-v2: Bridgingthechasmtoproprietaryagentsviasyntheticdataandscalable reinforcementlearning,2025b. URL KuanLi,ZhongwangZhang,HuifengYin,LiwenZhang,LituOu,JialongWu,WenbiaoYin,BaixuanLi, ZhengweiTao,XinyuWang,WeizhouShen,JunkaiZhang,DingchuZhang,XixiWu,YongJiang,Ming Yan,PengjunXie,FeiHuang,andJingrenZhou. Websailor: Navigatingsuper-humanreasoningfor webagent,2025c. URL Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability. CoRR,abs/2504.21776,2025d. doi: 10.48550/ARXIV.2504.21776. URL rXiv.2504.21776. YizhiLi,QingshuiGu,ZhoufutuWen,ZiniuLi,TianshunXing,ShuyueGuo,TianyuZheng,XinZhou, XingweiQu,WangchunshuZhou,etal. Treepo: Bridgingthegapofpolicyoptimizationandefficacy andinferenceefficiencywithheuristictree-basedmodeling. arXivpreprintarXiv:2508.17445,2025e. TianLiang,ZhiweiHe,WenxiangJiao,XingWang,YanWang,RuiWang,YujiuYang,ShumingShi,and ZhaopengTu. Encouragingdivergentthinkinginlargelanguagemodelsthroughmulti-agentdebate. InProceedingsofthe2024ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.17889–17904, 2024. AixinLiu,BeiFeng,BingXue,BingxuanWang,BochaoWu,ChengdaLu,ChenggangZhao,Chengqi Deng,ChenyuZhang,ChongRuan,etal.DeepSeek-V3technicalreport.arXivpreprintarXiv:2412.19437, 2024. JuntengLiu,YunjiLi,ChiZhang,JingyangLi,AiliChen,KeJi,WeiyuCheng,ZijiaWu,ChengyuDu, QidiXu,etal. Webexplorer: Exploreandevolvefortraininglong-horizonwebagents. arXivpreprint arXiv:2509.06501,2025. RuiLu,ZhenyuHou,ZihanWang,HanchenZhang,XiaoLiu,YujiangLi,ShiFeng,JieTang,andYuxiao Dong. Deepdive: Advancingdeepsearchagentswithknowledgegraphsandmulti-turnrl. arXiv preprintarXiv:2509.10446,2025. Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmarkforgeneralaiassistants. InTheTwelfthInternationalConferenceonLearningRepresentations, 2023. OpenAI. gpt-oss-120b&gpt-oss-20bmodelcard,2025a. URL OpenAI. Deepresearchsystemcard,2025b. URL ard.pdf. Perplexity. Perplexitydeepresearch,2025. URL LongPhan,AliceGatti,ZiwenHan,NathanielLi,JosephinaHu,HughZhang,ChenBoCalvinZhang, MohamedShaaban,JohnLing,SeanShi,etal. Humanity’slastexam. arXivpreprintarXiv:2501.14249, 2025. 14 --- Page 15 --- Zile Qiao, Shen Huang, Jialong Wu, Kuan Li, Wenbiao Yin, Xinyu Wang, Liwen Zhang, Baixuan Li, ZhengweiTao,WeizhouShen,XixiWu,YongJiang,PengjunXie,FeiHuang,JunZhang,andJingren Zhou. WebResearcher: Unleashingunboundedreasoningcapabilityinlong-horizonagents,2025. Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen, Chenxi Wang, Maojia Song, Xinyu Wang, Kuan Li, Jialong Wu, Xuanzhong Chen, et al. Scaling agents via continual pre-training. arXiv preprint arXiv:2509.13310,2025. ShuangSun,HuatongSong,YuhaoWang,RuiyangRen,JinhaoJiang,JunjieZhang,FeiBai,JiaDeng, WayneXinZhao,ZhengLiu,etal. Simpledeepsearcher: Deepinformationseekingviaweb-powered reasoningtrajectorysynthesis. arXivpreprintarXiv:2505.16834,2025. Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang,XinyuWang,YongJiang,PengjunXie,FeiHuang,andJingrenZhou. WebShaper: Agentically datasynthesizingviainformation-seekingformalization,2025. KimiTeam. Kimiresearchertechreport,2025a. URL r/. Tongyi DeepResearch Team. Tongyi deepresearch: A new era of open-source ai researchers. https: //github.com/Alibaba-NLP/DeepResearch,2025b. XuezhiWang,JasonWei,DaleSchuurmans,QuocLe,EdHuaihsinChi,andDennyZhou.Self-consistency improveschainofthoughtreasoninginlanguagemodels. ArXiv,abs/2203.11171,2022. ZiqiWang,BoyeNiu,ZipengGao,ZhiZheng,TongXu,LinghuiMeng,ZhongliLi,JingLiu,YilongChen, ChenZhu,etal. Asurveyonparallelreasoning. arXivpreprintarXiv:2510.12164,2025. JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,DennyZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural informationprocessingsystems,35:24824–24837,2022. JasonWei,ZhiqingSun,SpencerPapay,ScottMcKinney,JeffreyHan,IsaFulford,HyungWonChung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: A simple yet challenging benchmarkforbrowsingagents. arXivpreprintarXiv:2504.12516,2025. JialongWu,BaixuanLi,RunnanFang,WenbiaoYin,LiwenZhang,ZhengweiTao,DingchuZhang,Zekun Xi,GangFu,YongJiang,PengjunXie,FeiHuang,andJingrenZhou. Webdancer: Towardsautonomous informationseekingagency,2025a. URL JialongWu,ZhenglinWang,LinhaiZhang,YilongLai,YulanHe,andDeyuZhou. SCOPE:Optimizing key-valuecachecompressioninlong-contextgeneration. InWanxiangChe,JoyceNabende,Ekaterina Shutova,andMohammadTaherPilehvar(eds.),Proceedingsofthe63rdAnnualMeetingoftheAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 10775–10790, Vienna, Austria, July 2025b. AssociationforComputationalLinguistics. ISBN979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.52 9. URL JialongWu,WenbiaoYin,YongJiang,ZhenglinWang,ZekunXi,RunnanFang,LinhaiZhang,YulanHe, DeyuZhou,PengjunXie,andFeiHuang. Webwalker: Benchmarkingllmsinwebtraversal,2025c. URL ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikNarasimhan,andYuanCao. Re- act: Synergizing reasoning and acting in language models. In International Conference on Learning Representations(ICLR),2023. WeihaoZeng,KeqingHe,ChuqiaoKuang,XiaoguangLi,andJunxianHe. Pushingtest-timescaling limitsofdeepsearchwithasymmetricverification. arXivpreprintarXiv:2510.06135,2025. 15 --- Page 16 --- DingchuZhang,YidaZhao,JialongWu,BaixuanLi,WenbiaoYin,LiwenZhang,YongJiang,YufengLi, KeweiTu,PengjunXie,andFeiHuang. Evolvesearch: Aniterativeself-evolvingsearchagent,2025a. URL QiyuanZhang,FuyuanLyu,ZexuSun,LeiWang,WeixuZhang,ZhihanGuo,YufeiWang,IrwinKing, Xue Liu, and Chen Ma. What, how, where, and how well? a survey on test-time scaling in large languagemodels. CoRR,2025b. PeilinZhou,BruceLeon,XiangYing,CanZhang,YifanShao,QichenYe,DadingChong,ZhilingJin, ChenxuanXie,MengCao,etal.Browsecomp-zh:Benchmarkingwebbrowsingabilityoflargelanguage modelsinchinese. arXivpreprintarXiv:2504.19314,2025. 16