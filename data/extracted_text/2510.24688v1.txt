--- Page 1 --- 1 MIC-BEV: Multi-Infrastructure Camera Bird‚Äôs-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection Yun Zhang, Zhaoliang Zheng, Johnson Liu, Zhiyu Huang‚àó, Zewei Zhou, Zonglin Meng, Tianhui Cai, and Jiaqi Ma Abstract‚ÄîInfrastructure-basedperceptionplaysacrucialrole in intelligent transportation systems, offering global situational awarenessandenablingcooperativeautonomy.However,existing camera-based detection models often underperform in such sce- nariosduetochallengessuchasmulti-viewinfrastructuresetup, diverse camera configurations, degraded visual inputs, and vari- ous road layouts. We introduce MIC-BEV, a Transformer-based bird‚Äôs-eye-view (BEV) perception framework for infrastructure- based multi-camera 3D object detection. MIC-BEV flexibly sup- portsavariablenumberofcameraswithheterogeneousintrinsic and extrinsic parameters and demonstrates strong robustness under sensor degradation. The proposed graph-enhanced fusion module in MIC-BEV integrates multi-view image features into Cam1 Cam2 the BEV space by exploiting geometric relationships between c)Camera-BEVGrid Geometric Relation cameras and BEV cells alongside latent visual cues. To support t fora rin inin frg asa tn rud cte uv ra el -u ba at sio en d, ow be jecin tt dro etd eu cc tie onM ,2 feI a, ta urs iy nn gth de ivti ec rsd ea cta as met - Cam3 ùúë ! Cam2 ùëß$ùúë$ ùúë‚Äô Cam2 ùõ•ùë•",$ ùë¶ ùë• e Er xa tec no sn ivfi egu er xa pt ei ro in ms, enr to sad onla by oo tu hts M, a 2n Id anen dvi tr ho enm ree an l-t wal orc lo dnd di at tio an ses t. ùëß!=ùëß%&‚Äô ùõ•ùë•",$ Cam ùëß1 ùë¶ ùõø",$ ùõ•ùë¶",$ Cam1 ùëÖ- RoScenes demonstrate that MIC-BEV achieves state-of-the-art ùëë",$ ùõ•ùë¶",$ ùëß‚Äô ùë• ùõ•ùë•",! ùëë),‚Äô ùõø),‚Äô p cher af lo ler nm ga inn gce ci on n3 dD itioo nb sje ,c it nd ce lute dc it ni gon e. xI tt ra el mso ere wm eaa ti hn es rro ab nu dst su en nd soer r ùõ•ùë¶",! ùõ•ùë•",! ùëë),‚Äô BEVGrid Caùõ• mùë¶" 3,! ùõø"ùëë ,!",!ùõ•ùë¶),‚Äô ùõ•ùë•),‚Äô degradation. These results highlight the potential of MIC-BEV Extracted geometric relations ‚Üí Edge features for GNN ùëÖ! for real-world deployment. The dataset and source code are d)Geometric-Relation-Aware Fusion available at:  Image Feature Cam2 Index Terms‚ÄîBEV perception, Infrastructure-based sensing, Cam Node 3D object detection, Graph-enhanced fusion Feature hn h1 h2 h3 g),‚Äô g),/ Cam1 BEV Node I. INTRODUCTION Feature hp IN inF teR llA igS eT ntR tU raC nT spU oR rtE at- ib oa ns se yd stp ee mrc se ,p pt ri oo vn idis ina gk ce ry itie cn alab sule pr pf oo rr t Weights ùúî!,# ùúî),‚Äôùúî), ùúî. ),/ Cam3 0 Weight 1 for traffic monitoring, situational awareness, and cooperative autonomy in urban environments [1]‚Äì[3]. Sensors deployed at intersections, crosswalks, and merging zones offer a strategic advantage for observing traffic participants from elevated viewpoints, providing broader and more stable observations. This spatial advantage facilitates long-term monitoring and enhances the ability to detect dynamic objects [4]. Although LiDAR sensors have been widely adopted for infrastructure- basedobjectdetectionduetotheiraccurate3Dmeasurements, they remain costly, maintenance-intensive, and sensitive to mounting and calibration errors [5]‚Äì[7]. In contrast, cameras are significantly more affordable, scalable, and easier to de- ploy. They also provide rich semantic information that en- hancessceneunderstandinginlarge-scalesensingapplications, All authors are with the University of California, Los Angeles (UCLA), CA90095,USA.Email:{yun666,zhz03,jwu7,zhiyuh,zeweizhou,meng925,  ‚àóCorrespondingauthor:ZhiyuHuang. Edge-Enhanced GNN Assign weight to views a)DiverseRoadGeometry&CameraConfiguration Cam2 Cam1 Cam3 2-cam: L-shapedroad 3-cam: 4-way intersection 4-cam:5-way intersection 4-cam:4-way intersection b)Multi-Infrastructure Camera Views ùëù BEV cell ùëù ùëù Cam3 3DView BEVView ùëë",$ ùëë",! ùõ•ùë¶),‚Äô ùõ•ùë•),‚Äô Cam1 Cam2 Cam3 Flatten Edgegp,n g),. hp Fig. 1: Overview of the proposed MIC-BEV framework for multi-camera infrastructure perception. (a) Example scenariosofinfrastructuresensingwithdiverseroadgeometry and camera configuration. (b) Multiple cameras capture the scenefromdifferentviewpointsandprojectobjectsontoade- finedBEVgrid.(c)MIC-BEVencodesthegeometricrelations betweeneachcameraandBEVcell,suchasdistances,angles, and height differences, to construct edge features for a graph neural network (GNN). (d) The GNN in MIC-BEV performs geometricrelation-awarefusion,assigningimportanceweights to each camera for adaptive multi-view feature aggregation. making them an attractive alternative for infrastructure-based perception [8]. While single-camera infrastructure perception systems have been extensively studied [9]‚Äì[11], their spatial coverage and robustness are inherently limited, particularly under occlusion 5202 tcO 82 ]VC.sc[ 1v88642.0152:viXra --- Page 2 --- 2 or in complex scenes. Multi-camera infrastructure sensing that MIC-BEV achieves strong and consistent perfor- addresses these limitations by integrating information from mance, and validating its adaptability to heterogeneous multiple viewpoints to achieve more comprehensive and re- infrastructure layouts and robustness. silient scene understanding [12]. Nonetheless, it introduces several critical challenges. 1) Spatially distributed sensors. II. RELATEDWORK Cameras deployed across large spatial distances often have A. Camera-based BEV Perception overlapping fields of view with significant perspective dif- BEV representations have become a dominant paradigm ferences and occlusions. These multi-view conditions make in camera-based 3D perception, offering a unified spatial spatial alignment and feature fusion across views challenging. abstraction across multi-view inputs. Early works such as 2) Variability in camera configurations. Unlike vehicle- OFT[14]andCADDN[15]projectmonocularimagefeatures mounted sensors that follow consistent mounting patterns, into BEV space to enable 3D object detection. Lift-Splat- infrastructure cameras are deployed with diverse quantities, Shoot [16] advances this by lifting 2D image features into spatiallayouts,orientations,fieldsofview(FoV),anddegrees 3Dfrustumsusingpredicteddepthdistributionsandprojecting of overlap. Each intersection has a distinct design, requiring them into BEV space through vertical accumulation, while models to adapt to a wide range of installation configurations. BEVDet [17] improves efficiency for multi-view settings. 3) Sensor reliability and robustness. Infrastructure cameras BEVDepth [18] further enhances depth modeling through may degrade or fail over time without immediate detection or LiDAR-guided supervision, and GeoBEV [19] improves the repair.Therefore,perceptionmodelsmustmaintainrobustness geometric fidelity of BEV representations via radial-Cartesian againstmissing,corrupted,orlow-qualityvisualinputsduring sampling and centroid-aware depth supervision. real-world deployment. Transformer-basedmethodsfurtheradvanceBEVdetection. Toaddressthesechallenges,weproposeMIC-BEV(Multi- DETR3D [20] extends DETR [21] to 3D detection by sam- Infrastructure Camera Bird‚Äôs-Eye-View Transformer), an ef- pling image features at learned 3D reference points through fective 3D object detection model for infrastructure-based deformable attention, removing the need for explicit depth multi-camera perception using a Bird‚Äôs-Eye-View (BEV) rep- estimation. PETR [22] introduces 3D positional encodings to resentation.MIC-BEVintroducesarelation-enhancedspatial lift 2D features into a pseudo-3D space, while PETRv2 [23] cross-attention mechanism that fuses multi-view features by addsdepthsupervisionandimprovedalignmentformulti-view jointly considering camera-specific embeddings and their ge- fusion. BEVFormer [24], proposed shortly after PETR, estab- ometric relations with each BEV cell through a graph neural lishes a strong foundation for BEV-based perception by intro- network (GNN) [13], as illustrated in Fig. 1. This enables ducing learnable BEV queries and spatiotemporal deformable adaptiveweightingofinformationfromheterogeneouscamera attention for dense and temporally consistent feature fusion. viewpointsandimprovesmulti-viewfeatureaggregation.MIC- Subsequentmethodsexplorecomplementarydirectionsforim- BEVfurtherincorporatesbothmap-levelandobject-levelBEV proving temporal reasoning and efficiency. StreamPETR [25] segmentation to enhance spatial understanding. In addition, extends PETR with a memory-based temporal stream for MIC-BEV adapts to diverse camera and road layouts and em- videoinference.Incontrast,BEVDet4D[26],SoloFusion[27], ploys camera masking strategies such as random dropout and and BEVNext [28] follow the dense BEV-based paradigm, Gaussian blur during training to improve robustness against enhancing depth modeling, long-term temporal fusion, and occlusion, sensor degradation, and camera failure. multi-view feature efficiency, respectively. Toovercomethescarcityofreal-worlddatasetsthatcapture While these methods achieve impressive results in ego- diverse infrastructure configurations, weather conditions, and centric,vehicle-mountedscenarios,theytypicallyassumefixed camera layouts, we introduce M2I, a large-scale synthetic and calibrated camera layouts surrounding the ego vehi- dataset for Multi-camera, Multi-layout Infrastructure percep- cle. In contrast, infrastructure-based systems deploy spatially tion.M2Icoversawiderangeofinfrastructureswithvariations distributed cameras with diverse orientations, heights, and in camera quantity, spatial layout, heading angle, FoV, and fields of view, which vary significantly across intersections, degrees of overlap, as well as challenging conditions such as road geometries, and installation sites. Such heterogeneity adverse weather and varying lighting. This dataset provides a introduces substantial challenges for geometric alignment and comprehensive benchmark for model training and evaluation. multi-viewfeaturefusion,motivatingthedevelopmentofBEV The main contributions of this paper are summarized as: perceptionframeworksspecificallydesignedforinfrastructure- 1) We propose MIC-BEV, a BEV-based 3D object detec- based environments. tion model for multi-camera infrastructure perception that fuses heterogeneous multi-view observations using B. Infrastructure-based 3D Perception and Datasets spatial cross-attention enhanced with graph-based rela- tion modeling and fusion. Infrastructure-based perception systems often rely on Li- 2) WeintroduceM2I,alarge-scalesyntheticdatasetfeatur- DAR [29]‚Äì[31] or LiDAR-camera fusion for 3D object detec- ingdiverseandrealisticmulti-camerasettingsandscene tion [32]‚Äì[35]. However, due to the high deployment cost of conditions,enablingcomprehensiveevaluationofmodel LiDAR and its lack of semantic information, camera-only ap- generalization and robustness. proachesaregaininggrowinginterest[11],[36],[37].Earlyef- 3) We conduct comprehensive experiments on the M2I fortsfocusedonmonocular3Ddetectionusingdatasetssuchas dataset and real-world RoScenes dataset, demonstrating Rope3D [38] and DAIR-V2X [39]. Methods like BEVHeight --- Page 3 --- 3 [40], BEVHeight++ [41], and CoBEV [42] enhance spatial angle. To support spatial understanding, we formulate a BEV understandingbyleveragingdepth-heightcues.Morerecently, semantic segmentation task with two complementary levels: MonoUNI[43]introducesnormalizeddepthfeaturestoreduce map-level and object-level. The model predicts a semantic relianceonexplicitheightcues,achievingbettergeneralization map MÀÜ = [MÀÜ map,MÀÜ object], where MÀÜ map ‚àà RNmap√óHbev√óWbev from infrastructure to vehicle perspectives. captures static map semantics (e.g., road, crosswalk), and Recent simulation-based datasets such as AgentVerse [44] MÀÜ object ‚àà RNobject√óHbev√óWbev captures dynamic objects (e.g., and cooperative datasets such as V2XVerse [45] extend this vehicle, pedestrian). Each BEV cell predicts a class-wise direction by modeling heterogeneous infrastructures and vehi- probability distribution for both static and dynamic elements, cle‚Äìinfrastructurecoordinationunderdiverseweatherandtraf- allowing the model to reason jointly about the environment fic conditions. At the same time, multi-camera configurations and the spatial distribution of objects. have emerged as a practical solution for expanding spatial coverage and improving robustness. Existing datasets such B. Overall Architecture as V2X-Real [46] and RCooper [47] explore multi-camera MIC-BEV is designed for infrastructure-mounted cameras perception in single-intersection and corridor environments, with diverse road layouts and spatial configurations, where while RoScenes [48] targets highway scenes. Calibration- viewpoints are highly heterogeneous and fields of view vary free frameworks [49] further alleviate dependence on precise across scenes. As shown in Fig. 2, the model comprises extrinsic calibration, and occupancy-based approaches such three main components: 1) an image backbone for multi-view as MC-BEVRO [50] highlight the potential of infrastructure- feature extraction, 2) a Transformer encoder that aggregates mounted cameras for large-scale traffic monitoring. More image features into a unified BEV representation through recently,RoBEVandRopeBEV[51]establishstrongbaselines temporalmodelingandrelation-enhancedspatialattention,and byfusingmulti-viewfeaturesusingfeature-guidedqueriesand 3) task-specific decoding heads for 3D object detection and rotation-aware embeddings, respectively. BEV segmentation. However,existingmulti-camerafusionstrategiesarelargely Unlike BEVFormer [24], which is designed for vehicle- implicit and lack interpretability at the per-view level. In mounted cameras with fixed and calibrated views, MIC- addition, the spatial and environmental diversity of current BEV is specifically tailored for infrastructure-based percep- infrastructure-baseddatasetsisnarrow,typicallyrestrictedtoa tion, where camera poses, heights, and orientations differ singlescenewithfixedinfrastructurearrangementsandlimited across scenes. To address this variability, MIC-BEV incorpo- weather or lighting variation. These constraints hinder model rates a relation-enhanced attention mechanism that models generalization across heterogeneous infrastructures and real- geometry-awarerelationsbetweeneachcameraandBEVcell, worlddeploymentconditions.Toaddressthesechallenges,we such as the relative distance and viewing angle, through a introducetheM2Idataset,whichencompassesawiderangeof GNN. This design enables the model to adaptively weight intersection types, camera configurations, and environmental features from multiple cameras according to their geometric conditions. We propose MIC-BEV, a relation-aware-fusion relevance to each BEV cell. BEV framework that leverages a GNN to capture geometric dependencies among cameras and dynamically infer per-view C. Variable Multi-Camera Inputs fusionweights,enablinginterpretableandadaptivemulti-view fusion across diverse infrastructure scenarios. Infrastructure sensing systems often operate with varying numbers of cameras and heterogeneous fields of view, de- III. METHODOLOGY pending on intersection layouts or deployment constraints. To ensureadaptability,MIC-BEVisdesignedtohandleadynamic A. Problem Definition setofinputcameras.Whenthenumberofavailablecamerasis The objective of this work is to develop a multi-camera fewer than the maximum supported (N ), we pad the input max 3Dobjectdetectionmodelforinfrastructure-mountedsensors, with dummy images (zero-valued tensors) and assign identity augmented with a BEV segmentation task to enhance spatial matrices as their calibration parameters. These placeholders reasoning. Given a set of synchronized multi-view RGB im- are effectively ignored in downstream spatial attention and ages, the model Det(¬∑) jointly predicts a set of 3D bounding graph computations by ensuring that their 3D projections boxes BÀÜ and a BEV segmentation MÀÜ: producenon-positivedepths,therebyexcludingthemfromthe BÀÜ,MÀÜ =Det (cid:0) {(I ,E ,K )}N (cid:1) , (1) set of valid contributing views V hit (see Section III-D). œï n n n n=1 To improve robustness, we introduce a view-masking aug- where I is the RGB image from the n-th camera, E and mentation during training to simulate sensor degradation or n n K arethecorresponding extrinsicandintrinsicmatrices,and temporary camera failures. For each training sample, with n œï denotes the learnable parameters of the model. The number probability p , one camera view is randomly selected and m of cameras N varies across different scenes. either replaced with a zero-valued image or blurred using a The primary task is 3D object detection, which involves Gaussian kernel; with probability 1‚àíp , all views remain m predicting a set of bounding boxes BÀÜ in a shared BEV unchanged. This strategy encourages MIC-BEV to adaptively coordinate frame. Each box BÀÜ is parameterized as BÀÜ = rely on the available camera views, improving resilience to i i (x,y,z,l,w,h,œà), where (x,y,z) denotes the object‚Äôs posi- missingordegradedinputswhilemaintainingfullperformance tion, (l,w,h) its bounding box dimensions, and œà its yaw when all cameras are operational. --- Page 4 --- 4 Feature Extraction Graph Construction Segmentation Results Seg Head Det Head ùëî!,! BEVFeature Bt BEV Cam Multi-ViewFeatures √ó 6 No Qd pes ‚Ä¶ ùëî!,# No Cd nes Backbone 4Cam Weight Prediction C1 C2 C3 C4 Four-wayIntersection hp hn ùëî$,% Q1 œâ1,1 œâ1,2 œâ1,3 œâ1,4 Multi-Camera Input Q2 œâ2,1 œâ2,2 œâ2,3 œâ2,4 GAT Q3 œâ3,1 œâ‚Ä¶3,2 œâ3,3 œâ3,4 Softmax (row-wise) Qpœâp,1 œâp,2 œâ ddp d, 3d œâp,4 MountainRoad 4Cam HitViewsVhit œâ1,1=0.67 œâ1,2=0.33 3Cam HighwayRamp 2Cam 3Cam Variable Number ofCamera View BEV Queries Q reyaLremrofsnarT Detection Results ùëî!,& ùëî!,‚Äô Add & Norm Feed Forward Weight Matrix Add & Norm RandomMask GaussianBlur Relation-Enhanced SpatialCross-Attention 2Cam Add & Norm Feature Fusion Temporal Self-Attention History BEV Bt-1 Variable Multi-Camera Inputs MIC-BEV Transformer Relation-Enhanced SpatialCross-Attention Object Detection and BEV Segmentation Fig.2:OverviewoftheMIC-BEVarchitecture.Themodeltakesmulti-viewimagesfromavariablenumberofinfrastructure- mountedcamerasasinputandextractsfeaturesthroughasharedbackbone.Acameramaskingmoduleappliesrandomdropout or Gaussian noise to simulate degraded views. The extracted features are fused into a BEV representation via Transformer layers with temporal self-attention and our proposed Relation-Enhanced Spatial Cross-Attention. GAT networks are used to dynamically assign view-dependent weights based on camera node features and geometric relations between the camera and its visible BEV cells. The resulting BEV features are used for both object detection and BEV semantic segmentation tasks. D. MIC-BEV Transformer pointsr =(x,y,z )usingapredefinedsetofanchorheights p,j j {z }Nref. These pillars help capture semantic features across The process of encoding multi-camera features into BEV j j=1 different heights. Each 3D reference point r is projected spaceinMIC-BEVissummarizedinAlgorithm1.TheTrans- p,j onto the n-th camera view as 2D coordinates u(n). Only formermodelisdesignedtohandleheterogeneouscameracon- p,j camera views where the projected points fall within valid figurations while preserving spatial and temporal consistency. imageboundsareincludedinthehit-viewsetV ‚äÜ1,...,N. 1) Encoder and BEV Queries: We employ a ResNet- hit For each hit view n ‚àà V , we apply deformable attention 101 [52] backbone coupled with a Feature Pyramid Network hit (FPN) [53] to extract multi-scale features from each camera (DeformAttn) [54] around the projected locations {u( pn ,j)}N j=re 1f of 3D reference points associated with BEV query Q . This image. The BEV representation is defined as a 2D grid p anchored to the ground plane and centered at the scene. We produces a per-view feature f p(n) ‚ààRC. initialize a learnable tensor Q ‚àà RHbev√óWbev√óC to represent Graph Construction.Toadaptivelyweighthecontribution the grid, where H and W denote the grid resolution, and of each camera view for every BEV cell, we formulate the bev bev C is the feature dimension. Each cell Q ‚àà RC serves as a fusion process as a bipartite graph attention problem. The p latent query corresponding to a grid location p=(x,y) in the conventionaluniformaveragingofmulti-viewfeaturesignores BEVspace.TheseBEVqueriesinteractwithmulti-viewimage how informative or reliable each view is, especially under features via spatial cross-attention and are iteratively refined occlusions or perspective bias. We therefore learn the fusion to capture spatial cues encoded by the mounted cameras. weights œâ p,n using a geometry- and content-aware Graph 2) Temporal Self-Attention: To capture object moving Attention Network (GAT) [55]. dynamics, temporal self-attention (TSA) allows current BEV We construct a bipartite graph G = (V cam,V bev,E), where queries Q to attend to the previous BEV map B t‚àí1. In each camera node C n ‚àà V cam represents a pooled image static infrastructure setups, this is simplified without ego- feature map from camera n, and each BEV grid cell node motion compensation. Incorporating temporal context helps Q p ‚àà V bev is represented by a BEV query located at p. The recoveroccludedorintermittentlyvisibleobjectsandstabilizes node features are defined as: predictions over consecutive frames. h =Q ‚ààRC for BEV nodes, (2) p p 3) Relation-Enhanced Spatial Cross-Attention: Given a set of multi-view camera feature maps {F(n)}N n=1, the pro- h = 1 (cid:88)K f ‚ààRC for camera nodes, (3) posed Relation-Enhanced Spatial Cross-Attention (ReSCA) n K n,k aggregates them into a unified BEV representation F ‚àà k=1 RC√óHbev√óWbev. For each BEV query Q p located at p in the where K =H√óW is the number of tokens from the camera BEV grid, we generate a vertical stack of N 3D reference feature map Fn ‚àà RC√óH√óW, with H and W denoting the ref --- Page 5 --- 5 Algorithm 1: Forward Pass of MIC-BEV Encoding By jointly normalizing geometric features, we ensure that Input: {B }T : past T frames BEV history queue; the network is invariant to map scale, BEV resolution, and {F(n)t }‚àí Nk camk := m1 ulti-view image features; elevation difference, enabling generalization across scenes {K ,En= }1 N : intrinsics/extrinsics; with different layouts or camera setups. n n n=1 L: number of Transformer layers; N ref: number of anchor heights Algorithm 2: Camera-to-BEV Fusion Weights via GAT Output: B out: current-frame BEV features Input: Q‚ààRB√óNq√óC: BEV queries; 1. Generate reference points {F(n)‚ààRB√óC√óH√óW}Ncam: camera features; n=1 r3D ‚Üê GETREFPOINTS3D(H bev,W bev,N ref) ; M‚àà{0,1}B√óNq√óNcam: BEV visibility mask; // Shape: [N ,H ,W ,3] (P,œà,œÜ): camera positions, yaw, pitch; f : GATv2 ref bev bev Œ∏ 2. Project 3D anchors to camera views (once) Output: W‚ààRB√óNq√óNcam: fusion weights {(u n,m n)}N n=1 ‚Üê POINTSAMPLING(r3D,{K n,E n}) 1. Node features: Mean-pool image tokens to get ; // m n masks invalid points camera embeddings C [n]= 1 (cid:80) F(n) ; use 3. Initialize temporal references (static cameras) b HW h,w b,:,h,w Q as BEV node features. b if T >0 then 2. Edges: From the visibility mask B prev ‚Üê CONCAT({B t‚àík}T k=1) E b ={(n‚Üíq)|M b,q,n =1} (connect visible else cameras to BEV cells). B prev ‚Üê ZEROSLIKE(Q) 3. Edge attributes: For each edge (b,q,n) compute 4. Transformer layers geometry vector B ‚ÜêQ ; // Initialize with learnable 0 BEV queries g b,q,n =[‚àÜ Rxx,‚àÜ Ryy, zmzn ax,‚àö R‚à•d x2‚à• +2 R y2,cosŒ¥,sinŒ¥,cosœÜ,sinœÜ]. for ‚Ñì=1 to L do 4. GAT propagation: // Temporal Self-Attention (TSA) B ‚Üê TSA(B ,B ); S b =f Œ∏(Q b,C b,E b,g b)‚ààRNq√óNcam. ‚Ñì ‚Ñì‚àí1 prev 5. Normalization: Mask invalid edges (S =‚àí‚àû if B ‚Ñì ‚Üê NORM(B ‚Ñì); b,q,n M =0), Then apply softmax across cameras: // Relation-Enhanced Spatial b,q,n W =softmax (S ). Cross-Attention (ReSCA) b,q,: n b,q,: return W. B ‚Ñì ‚Üê RESCA(B ‚Ñì,{F(n)},r3D,{u n},{m n}) B ‚Ñì ‚Üê NORM(B ‚Ñì); // Feed-Forward Network (FFN) Weight Prediction. We employ a GAT network f to Œ∏ B ‚Ñì ‚Üê FFN(B ‚Ñì); model the interaction between each BEV node, camera node, B ‚Ñì ‚Üê NORM(B ‚Ñì); and their geometric relation, as detailed in Algorithm 2. return B t =B L Specifically, f Œ∏ is instantiated as a GATv2 [56] with three layers, four attention heads per layer, and 128 hidden units. Nodefeaturesh (BEV)andh (camera)arefirstprojected p n heightandwidthofthefeaturemap,respectively.f denotes to 128-dimensional embeddings using a shared 1√ó1 convolu- n,k the k-th token feature from camera n. tion.EachGATv2layerincorporatesELUactivation,dropout, Edges E are directed from cameras to visible BEV nodes, andresidualconnectionstoensurestabletrainingandeffective E = {(n,p) | Q is visible from camera C }. Each edge message passing across the graph. p n (n ‚Üí p) is annotated with a geometric relation descriptor Thenetworkoutputsascalarlogits p,n foreachrelationpair g ‚ààR8, consisting of: (n ‚Üí p), representing the importance weight of the camera p,n node n for the BEV cell p: (cid:104) g p,n = ‚àÜ Rxp x,n, ‚àÜ Ryp y,n, zmzn ax, ‚àö||d Rp x2,n +| R|2 y2, (cid:105) (4) s p,n =f Œ∏(h p, h n, g p,n). (5) cosŒ¥ , sinŒ¥ , cosœÜ , sinœÜ , p,n p,n n n The logits are reshaped to (B,N ,N ) and normalized where d =(‚àÜx ,‚àÜy )=(x ‚àíx ,y ‚àíy ) is the 2D q cam p,n p,n p,n p n p n across cameras using a visibility-aware softmax. For non- planar offset between the BEV grid and the camera center. visible cameras, we set s ‚Üê ‚àí‚àû to exclude them. The R and R are normalization constants corresponding to the p,n x y final fusion weights œâ are then computed as: sensing range in x and y directions, used to scale spatial p,n offsetstoaconsistentrangewithin[‚àí1,1].Similarly,z isthe n exp(s ) camera‚Äôs height, normalized by the maximum camera height œâ = p,n . (6) z max.Œ¥ p,n istheheadingdifferencebetweenthecamera‚Äôsyaw p,n (cid:80) n‚Ä≤‚ààVhitexp(s p,n‚Ä≤) and the angle from camera n to the BEV cell at location p, and œÜ is the pitch angle of camera n. To ensure rotational Thisformulationenablesadaptiveweightingacrossmultiple n continuity and avoid discontinuities near ¬±œÄ, we use heading cameraviews,whereeachBEVcelldynamicallyattendstothe withitssineandcosinecomponents,i.e.,cosŒ¥ andsinŒ¥ . most informative and geometrically relevant viewpoints. p,n p,n --- Page 6 --- 6 Feature Fusion. The final BEV feature is computed by restricted camera configurations, limited scene diversity, and fusing all visible views with learned weights œâ : insufficient environmental variation. p,n (cid:88) (cid:88) Limited camera configuration. Existing infrastructure ReSCA(Q )= œâ ¬∑f(n), œâ =1, p p,n p p,n datasets exhibit limited variability in camera setups, often n‚ààVhit n relying on single static viewpoints or fixed multi-camera ar- f(n) =(cid:88)Nref DeformAttn(Q , u(n), F(n)). (7) rangements. Monocular camera datasets such as Rope3D [38] p p p,j t and DAIR-V2X-I [39] use a single stationary camera, leading j=1 to narrow spatial coverage and no possibility for cross-view This geometry- and content-aware fusion strategy enables fusion. Multi-camera datasets, including RCooper [47], V2X- the model to selectively emphasize the most informative Real [60], and V2X-PnP [58], place cameras at a single and geometrically favorable camera views, while suppressing intersection with fixed geometry, providing no variation in occluded or degraded inputs. As a result, the fused BEV setupacrossdifferentenvironments.Inbothcases,cameralay- representationbecomesmorerobust,interpretable,andreliable outs, heights, and orientations remain homogeneous, limiting across a wide range of camera configurations. modeladaptabilitytodiverseroadgeometriesanddeployment conditions in the real world. Limited scene diversity. Beyond camera setup, existing E. Object Detection and BEV Segmentation datasets capture only a narrow range of spatial and con- The BEV Transformer layers output a shared BEV feature textual environments. Intersection-focused datasets such as map B t ‚àà RC√óHbev√óWbev, which serves as a unified repre- RCooper[47],V2X-Real[60],andV2X-PnP[58]arerecorded sentation for both object detection and BEV segmentation. at a single, fixed intersection, where the same geometry, lay- Twotask-specificconvolutionaldecodersoperateinparallelon out,andsurroundingsappearthroughoutthedataset.Similarly, this shared feature map. Each decoder comprises four stacked highway-focused datasets like RoScenes [48] are limited to 3 √ó 3 convolutional blocks with Group Normalization and one highway with linear road topology and low interaction ReLUactivations,followedbya1√ó1classificationlayer.The density, lacking complex cross-traffic or vulnerable road users decoders operate at full BEV resolution without upsampling. suchaspedestriansandcyclists.Asaresult,thesebenchmarks 1) Detection Head: For object detection, we adopt a offerminimalvariationinspatiallayoutorcontextualdiversity, DETR-style decoder [21] with N object queries. Each query q constrainingthedevelopmentofperceptionmodelscapableof outputs a class probability vector yÀÜ ‚àà Rnobj+1 including a generalizing across diverse real-world road environments. background class, and bounding box attributes ÀÜb. Limited dynamic conditions. Most existing infrastructure The detection loss L follows a DETR-style bipartite det datasetsarecollectedprimarilyundercleardaytimeconditions, matching formulation: providinglimitedvariationinilluminationorweather.Adverse L =Œª L +Œª L , (8) environments such as heavy rain and dense fog are typically det cls cls reg reg excluded, while dusk and night scenes appear only sparsely. where L cls is a focal loss with weight Œª cls = 2.0 and L reg is Consequently, models trained on these datasets often struggle an L1 loss with weight Œª reg =0.25. to handle realistic illumination changes, sensor noise, and 2) Segmentation Head: For segmentation, the model pre- dynamic temporal variations that critically affect perception dicts static map logits MÀÜ map ‚àà Rnmap√óH√óW and dynamic robustness in practice. object logits MÀÜ object ‚àà R(nobj+1)√óH√óW. The corresponding ground-truth masks M ,M are derived from rasterized map object B. M2I Dataset HD maps and instance annotations. In practice, we apply bilinear interpolation to align the decoder outputs with the To address these limitations, we introduce the M2I (Multi- label map dimensions when necessary. The segmentation loss camera Multi-layout Infrastructure) Perception Dataset, a is defined as: large-scale benchmark explicitly designed for 3D perception across heterogeneous roadside environments. Table I summa- L =CE(MÀÜ ,M )+CE(MÀÜ ,M ), (9) seg map map object object rizes a comparison of M2I with prior infrastructure-based and where CE(¬∑,¬∑) denotes standard softmax cross-entropy. V2X perception datasets. The model is trained with a combined multi-task loss: M2I contains 278K frames, 927K images, and 11.5 million annotated bounding boxes, each paired with synchronized L=L +ŒªL , (10) det seg RGB cameras, LiDAR, 3D bounding boxes, and semantic where Œª=2 is the task balance weight. BEV maps. It includes 1,103 scenario clips covering a wide spectrum of road geometries. Each clip spans 200-300 frames (10 Hz) with an average of 40 road users, covering multiple IV. DATASETS object classes such as cars, trucks, pedestrians, and cyclists. A. Limitations of Existing Infrastructure Sensing Datasets M2I supports tasks including 3D object detection, semantic Existing datasets only partially reflect the complexity of segmentation, multi-object tracking, and trajectory prediction. real-world infrastructure perception, hindering the generaliza- The dataset is divided into 772 training, 110 validation, and tion of learned models across diverse deployment scenarios. 221testscenariosusinga7:1:2ratioattheclipleveltoensure The main limitations can be grouped into three categories: temporal consistency and reproducibility. --- Page 7 --- 7 TABLE I: Comparison of infrastructure perception datasets. Weather and map availability are key attributes, while variation captures diversity across layouts, locations, and fields of view. Prior datasets often have fixed camera setups and narrow scene types, whereas M2I offers broader diversity and richer supervision. Dataset Type Year Frames Boxes # Cams Extreme Weather Map Vary Layouts Vary Locations Vary FoVs V2X-Sim-I[3] Sim 2022 60K 26.6K 4 ‚úì V2XSet-I[57] Sim 2022 44K 233K 4 ‚úì DAIR-V2X-I[39] Real 2022 10K 493K 1 ‚úì Rope3D[38] Real 2022 50K 1.5M 1 ‚úì V2X-Real-I[46] Real 2023 171K 1.2M 4 V2X-Seq-I[46] Real 2023 39K 464K 2 ‚úì V2X-PnP-I[58] Real 2024 208K 1.45M 4 ‚úì TUMTraf [59] Real 2024 800 30k 4 ‚úì RCooper[47] Real 2024 50K 242K 2‚Äì4 ‚úì RoScenes[48] Real 2024 215K 21.1M 8 ‚úì ‚úì ‚úì M2I (Ours) Sim 2025 278K 11.6M 1‚Äì4 ‚úì ‚úì ‚úì ‚úì ‚úì a)ExampleRoadGeometryandCamera Placement Cam1 Cam1 Cam1 Cam1 Cam1 Cam2 Cam2 Cam2 Cam2 Cam3 Cam2 Cam3 Cam1 Cam3 1-cam: Highway ramp 2-cam: L-shapedroad 2-cam: 4-way intersection 3-cam: 3-way intersection 3-cam: Gasstation 3-cam: Highwayramp Cam1 Cam1 Cam1 Cam1 Cam1 Cam3 Cam2 Cam3 Cam1 Cam3 Cam2 Cam2 Cam2 Cam3 Cam4 Cam3 Cam2 Cam2 Cam4 Cam3 Cam4 Cam4 Cam4 3-cam: T- intersection 4-cam: 4-way intersection 4-cam:Roundabout 4-cam:4-way intersection4-cam:4-way intersection4-cam:5-way intersection b)Example Weather and Lighting c)Example Blur Rain Dusk Fog Night LightBlur HeavyBlur Fig. 3: Scene diversity in the M2I dataset. (a) Examples of camera placement configurations across various intersection types, ranging from single-camera highway ramps to multi-camera urban intersections. (b) Diverse weather and illumination conditions, including rain, fog, dusk, and night scenes, highlighting domain variability. (c) Different levels of image blur representing sensor degradation from light to heavy. These variations illustrate the broad coverage and realism of M2I for infrastructure perception. Scene Diversity. Collecting large-scale datasets across var- sections,T-junctions,roundabouts,highwayramps,single-lane ied sites is both financially and logistically challenging, lim- ruralroads,windingmountainpaths,andbridges.Evenwithin iting the availability of geometrically diverse real-world data. the same geometric category, locations differ in intersection Constructed using the CARLA simulator [61], M2I spans 41 size, lane configuration, and contextual elements such as locations across 7 towns, each characterized by distinct urban pedestrian crossings and traffic signals. Examples can be seen structures and layout styles. These towns differ in spatial inFig.3(a).EachlocationispairedwithasemanticBEVmap, scale, architectural form, and traffic context, ranging from allowing models to leverage structural and contextual priors compact residential districts to mountainous highways and for perception. This multi-level diversity captures both large- dense downtown grids. scale layout variability and fine-grained structural distinctions representative of real-world infrastructure environments. Across these towns, M2I includes eight categories of road geometries, such as three-way, four-way, and five-way inter- Camera Configuration Diversity. M2I captures realistic --- Page 8 --- 8 a) Scene Composition a) Scene Composition c) Camera Composition b) Agent Composition c) Camera Composition Fig. 4: M2I Dataset Composition. (a) Scene composition showing splits by weather, lighting, road geometry, and camera count.(b)Agentcompositionillustratingclassproportions,velocityprofiles,andfootprintareasacrossobjecttypes.(c)Camera composition showing histograms of camera height, pitch, and yaw across all scenes. These statistics demonstrate the diversity and balance of the M2I dataset for multi-camera perception. variation in infrastructure-mounted camera setups modeled shown in Fig. 3 (c). Traffic density also varies across and aftercommontrafficsurveillancesystems.Eachscenecontains within environments: urban scenes feature heavier vehicle 1-4 cameras selected from 12 real-world configurations that and pedestrian flow, while rural and highway settings are includetypicalmountingpositionsonlightpoles,trafficlights, comparatively sparse. Each environment includes three traffic and building facades, covering a broad range of heights, levels (low, medium, high) to represent different interaction orientations, and fields of view. complexities and occlusion conditions. As illustrated in Fig. 4 (c), camera heights range from 3 m This combination of environmental variation, sensor degra- to over 10 m, while pitch angles range from -5¬∞ to -35¬∞. The dation, and traffic diversity establishes M2I as a comprehen- yawdistributionspansnearlythefull360¬∞,indicatingcoverage sivebenchmarkforevaluatingperceptionmodelsunderdiverse of multiple viewing directions rather than fixed orientations. and dynamically changing real-world conditions. M2I further introduces intra-geometry variation to emulate ObjectCompositionDiversity.AsshowninFig.4(b),M2I realistic deployment differences observed in practice. For covers four major object categories: cars, trucks, cyclists, and instance,infour-wayintersections,camerascanbedistributed pedestrians, distributed across a wide range of environments across all four corners [59], aligned along two opposite cor- from highways to urban intersections. Each vehicle type ners [60], or concentrated near the center [47]. This variation includes multiple subcategories, such as sedans, vans, and leads to heterogeneous occlusion patterns, asymmetric view- trailers, reflecting the heterogeneity of traffic participants. points, and diverse visibility ranges, effectively capturing the The object proportions in M2I mirror those in large-scale variability of real-world infrastructure sensors. datasets such as nuScenes [62] and Waymo [63], ensuring EnvironmentalDiversity.Beyondgeometricandstructural realistic scene composition. Object scale and motion patterns variation, M2I encompasses a wide spectrum of environmen- vary substantially across locations: larger and faster-moving tal and illumination conditions. The dataset includes diverse vehicles dominate highways, while smaller and slower agents weather scenarios such as heavy rain, dense fog, and light such as pedestrians and cyclists are more common in urban drizzle, as well as varying times of day, including dawn, scenes.Thisdiversityinobjecttype,scale,anddynamicssup- dusk, and night. Examples of these conditions are illustrated ports a comprehensive evaluation of perception models under in Fig. 3 (b). Such variations introduce visual challenges such varied spatial layouts, motion patterns, and traffic densities. as reduced visibility, specular glare, and contrast degradation, which significantly affect object appearance. C. RoScenes Dataset To further enhance realism, M2I simulates sensor degra- We further evaluate on the established RoScenes dation, including blur caused by adverse weather or aging dataset [48], the largest real-world multi-view roadside sensors, as well as temporary sensor dropout and occlusion, perception benchmark to date. RoScenes provides an --- Page 9 --- 9 TABLE IV: Dataset-specific hyperparameters important complement to M2I by enabling validation under real-world sensor noise, calibration drift, and data imbalance Hyperparameter M2I RoScenes conditions that are difficult to simulate accurately. PerceptionRange [‚àí51.2,51.2]2 [‚àí400,400]√ó[‚àí100,100] RoScenes covers an 800-meter highway segment equipped Post-CenterRange [‚àí61.2,61.2]2 [‚àí410,410]√ó[‚àí110,110] with 8 synchronized infrastructure-mounted cameras posi- VoxelSize(m) 0.2√ó0.2 0.2√ó0.2 tioned on poles and gantries along the roadside. The dataset GridSize 512√ó512 4000√ó1000 BEVStride 4 4 contains over 21 million 3D bounding box annotations, cap- BEVSize 200√ó200 1000√ó250 turing diverse traffic flow patterns across normal and heavy Nq 200 900 traffic. Each scene is annotated with four object categories, nmap 7 ‚Äì including car, van, bus, and truck. The clips are recorded n obj 4 4 MaxCameraCount 4 8 underbothdaytimeandnighttimeconditionsinclearweather. AlthoughenvironmentaldiversityislimitedcomparedtoM2I, RoScenes provides realistic visual artifacts such as motion (ResNet101-DCN) as the image encoder. Training is con- blur, exposure variation, and partial occlusions caused by ducted for 10 epochs using the AdamW optimizer with an camera distance and mounting angles. initiallearningrateof2√ó10‚àí4 andacosineannealingsched- Forvalidation,theauthorscuratedclipsrepresentingthetop ule. Experiments are performed on 8 NVIDIA L40S GPUs 10% most occluded and bottom 10% least occluded scenes with a batch size of 1 per GPU and mixed-precision (FP16) as hard and easy subsets, respectively. This stratified design training to reduce memory usage. MIC-BEV hyperparameters allows for controlled evaluation of model performance under are specified in Table II, and model training hyperparameters varying levels of visual complexity. are summarized in Table III. By combining M2I‚Äôs controlled multi-layout diversity with Dataset-specific model variations are specified in Table IV. the RoScenes dataset‚Äôs large-scale real-world recordings, we For the M2I dataset, input images are resized to 800√ó600 obtain a comprehensive evaluation framework that tests both pixels, and the BEV grid is defined as 200 √ó 200, cover- generalization and robustness across simulated and real road- ing a [‚àí51.2,m,51.2,m] region along both the X and Y side domains. axes. For the RoScenes dataset, input images are kept at 1920 √ó 1080 resolution, with a BEV grid of 1000 √ó 250 spanning [‚àí400,m,400,m] in X and [‚àí100,m,100,m] in Y. V. EXPERIMENTS TheBEVoriginisalignedwitheithertheintersectioncenteror A. Experimental Setup the reference camera position, depending on the scene layout. SinceRoSceneslackssemanticmapsandLiDARsupervision, TABLE II: MIC-BEV Hyperparameters BEV segmentation is disabled for this dataset, and only 3D object detection is evaluated. Hyperparameter Value UsedIn ToassessrobustnesstosensordegradationinM2I,weapply Hbev,Wbev dataset-specific BEVGrid a controlled view-masking augmentation during testing using Nref 8 3DReferenceGrid/SCA C (channeldim) 256 AllModules the RandomMaskMultiView module. For each sample, one Nq (objectqueries) dataset-specific DETRDecoder cameraviewisrandomlycorruptedbyeitherzeroingtheimage L(Transformerlayers) 6 BEVTransformer or applying a Gaussian blur (kernel size 11, œÉ ‚àà [3.0,10.0]). T (temporalcontextframes) 2 TSAModule nmap dataset-specific BEVSegmentationHead The corruption is applied only when multiple real views are nobj dataset-specific BEVSegmentationHead available. To ensure reproducibility, all random operations pm 0.25 CameraViewMasking Œª(segmentationlossweight) 2.0 LossFunction are deterministically seeded using a hash of the sample Dropout(GATv2) 0.1 Camera-BEVGAT identifier, maintaining consistent corruption across runs. This GATHiddenDim 128 Camera-BEVGAT setupenablessystematicevaluationofmodelrobustnessunder GATLayers 3 Camera-BEVGAT GATHeads 4 Camera-BEVGAT degraded or missing views. SegDecoderDepth 4ConvLayers BEVSegmentationHead Evaluation Metrics. We adopt standard 3D detection met- rics from the nuScenes benchmark to evaluate MIC-BEV, including mean Average Precision (mAP) and nuScenes De- TABLE III: Training hyperparameters tection Score (NDS) [62]. To account for object distance and detectionfidelity,wereportclass-wiseaverageprecisionacross Hyperparameter Value a range of distance thresholds. The overall mAP metric is LearningRate 2√ó10‚àí4 computed as: WeightDecay 0.01 BatchSize(perGPU) 1 1 (cid:88)(cid:88) Epochs 10 mAP= |C|¬∑|D| AP c,d, (11) Warm-upSteps 500 c‚ààCd‚ààD LRSchedule CosineAnnealing MinimumLRRatio 1√ó10‚àí3 where C is the set of object classes, D is the set of distance GradientClipping max norm=1,norm type=2 thresholds, and AP is the average precision for class c at c,d distance threshold d. Implementation Details. All models are built upon NDSisacompositescorethatintegratesmAPwithfiveTrue a ResNet-101 backbone with deformable convolutions Positive metrics: mean Average Translation Error (mATE), --- Page 10 --- 10 TABLE V: Comparison of 3D object detection performance on the M2I test set under three evaluation conditions: Normal, Robust (sensor dropout or degradation), and Extreme Weather (heavy rain or fog). Normal Robust ExtremeWeather Method mAP‚Üë NDS‚Üë mATE‚Üì mASE‚Üì mAP‚Üë NDS‚Üë mATE‚Üì mASE‚Üì mAP‚Üë NDS‚Üë mATE‚Üì mASE‚Üì Lift-Splat-Shoot 0.446 0.437 0.742 0.489 0.336 0.371 0.781 0.510 0.367 0.334 0.764 0.631 PETR 0.596 0.595 0.301 0.107 0.415 0.453 0.595 0.156 0.440 0.468 0.689 0.193 BEVFormer 0.637 0.678 0.235 0.072 0.513 0.593 0.288 0.084 0.445 0.452 0.535 0.131 PETRv2 0.651 0.689 0.213 0.093 0.505 0.582 0.295 0.156 0.584 0.588 0.530 0.127 StreamPETR 0.677 0.702 0.211 0.067 0.512 0.579 0.293 0.110 0.591 0.611 0.489 0.102 BEVNeXt 0.681 0.704 0.206 0.069 0.527 0.591 0.291 0.112 0.603 0.617 0.447 0.103 GeoBEV 0.690 0.664 0.203 0.063 0.533 0.576 0.251 0.100 0.623 0.621 0.392 0.110 UVTR 0.698 0.661 0.201 0.062 0.558 0.603 0.220 0.071 0.631 0.639 0.283 0.079 DETR3D 0.701 0.677 0.289 0.056 0.540 0.580 0.338 0.063 0.677 0.661 0.320 0.069 MIC-BEV(Ours) 0.767 0.771 0.179 0.061 0.647 0.678 0.215 0.065 0.709 0.701 0.241 0.077 meanAverageScaleError(mASE),meanAverageOrientation These results highlight the efficacy of MIC-BEV‚Äôs camera- Error (mAOE), mean Average Velocity Error (mAVE), and grid relation-enhanced attention, which enables effective meanAverageAttributeError(mAAE).Itprovidesabalanced multi-view feature fusion in the BEV space. The M2I bench- evaluation of detection accuracy and localization fidelity: mark further validates MIC-BEV‚Äôs robustness to diverse cam- (cid:34) (cid:35) era placements and degraded sensor configurations, while 1 (cid:88) demonstrating its ability to detect small and distant objects. NDS= 5¬∑mAP+ (1‚àímin(1,mTP)) , (12) 10 Qualitative Evaluation. Examples of detection and BEV mTP segmentation of MIC-BEV are provided in Fig. 5. As shown where mTP‚àà{mATE,mASE,mAOE,mAVE,mAAE}. in Fig. 6, compared with existing baselines, MIC-BEV re- Baselines.Wecompareourmethodagainstwell-established liably identifies objects missed by other methods, corrects detectionmodels,includingLift-Splat-Shoot(LSS)[16],BEV- misaligned bounding boxes, and suppresses false positives. Former [24], DETR3D [20], PETR [22], PETRv2 [23], The BEV segmentation head provides spatial priors that help GeoBEV [19], BEVNeXt [28], StreamPETR [25], and UVTR separate foreground instances from background clutter, im- [64]. We do not include RoBEV or RopeBEV in the M2I proving localization precision. Additionally, camera masking benchmark due to the lack of available open-source code. during training simulates partial sensor failures, encouraging These models vary in feature lifting strategies and network the model to utilize complementary viewpoints and thereby architectures but share the same input, providing a fair and enhancingitsrobustnessunderocclusionanddegradedinputs. competitivebenchmarkforinfrastructure-based3Dperception. To ensure compatibility with variable camera inputs, we ap- TABLE VI: Per-class object detection results on the M2I ply our padding mechanism to all baselines. The RoScenes testing set (Normal), using mAP as the primary metric. benchmark used in our evaluation is adapted from the setup introduced in RopeBEV [51]. Method mAP‚Üë Pedestrian Truck Car Cyclist Avg. Lift-Splat-Shoot 0.444 0.397 0.599 0.344 0.446 B. Results on M2I PETR 0.762 0.573 0.611 0.436 0.596 Quantitative Evaluation. Table V summarizes the 3D ob- BEVFormer 0.810 0.623 0.667 0.449 0.637 PETRv2 0.815 0.646 0.679 0.463 0.651 jectdetectionresultsontheM2Itestsetunderthreeevaluation StreamPETR 0.857 0.668 0.687 0.497 0.677 settings: Normal, Robust, and Extreme Weather. MIC-BEV BEVNeXt 0.866 0.670 0.692 0.499 0.681 achievesstate-of-the-artperformanceacrossallsettings.Under GeoBEV 0.794 0.719 0.717 0.529 0.690 UVTR 0.768 0.721 0.728 0.577 0.698 theNormalcondition,MIC-BEVattainsanmAPof0.767and DETR3D 0.815 0.689 0.714 0.589 0.701 anNDSof0.771,surpassingthestrongestbaseline,DETR3D, MIC-BEV 0.860 0.777 0.806 0.626 0.767 by more than 9.4% in mAP. The performance gap becomes more pronounced as input Per-classdetectionperformance.TableVIreportstheper- quality degrades. In the Robust setting, where one camera is class mean average precision (mAP) on the M2I test set blurred or missing, MIC-BEV maintains competitive results under the Normal setting. MIC-BEV achieves the highest with 0.647 mAP and 0.678 NDS, corresponding to only a accuracy across all object categories. For pedestrians, MIC- 15.6% performance drop in mAP from the Normal case. In BEV achieves 0.860 mAP, surpassing DETR3D and PETRv2 contrast, baseline models such as DETR3D and BEVFormer (both 0.815), demonstrating a strong capability in detecting exhibit degradation of up to 23%. Under Extreme Weather small and dynamic objects. For trucks, MIC-BEV achieves conditions, which include heavy rain and dense fog, MIC- 0.777 mAP, outperforming UVTR (0.721) and reflecting ro- BEVcontinuestoperformthebestwith0.709mAPand0.701 bustness to large and geometrically diverse vehicles. For NDS. Across all settings, MIC-BEV consistently outperforms cars, MIC-BEV attains 0.806 mAP, surpassing UVTR (0.728) the strongest competing methods in mAP, achieving state-of- and DETR3D (0.714), showing consistent precision in dense the-art detection accuracy. scenes. Although cyclist detection remains challenging, MIC- --- Page 11 --- 11 Background Truck Crosswalk Car Sidewalk Cyclist Driving Pedestrian Shoulder Parking 341 2 GT Prediction Heading Cam1 Cam2 Cam4 a)Foggy urban four-way intersection witah) Ecxaemnplter Saenlairzioe 1d infrastructure design 4 3 1 2 Cam1 Cam2 Cam4 b)Extreme-weather suburban four-way intersebc) tEixoamnpl ew Seintahrio c2orner infrastructure design 3 4 1 2 Cam1 Cam3 Cam4 c)Clear-day urban four-way intersectionc ) wExaimthpl ec Seonarrnioe 3r infrastructure design 3 1 2 Cam1 Cam2 Cam3 d)Clear-day highway ramp with specd)i Eaxlaimzpele dSe nianriof 4rastructure design 4 3 1 2 Cam1 Cam3 Cam4 e)Dusk five-way intersection with ec) Eoxarmnpeler S einnarfior 5astructure design 341 2 Cam2 Cam3 Cam4 f) Night four-way intersection with cefn) Etrxaamlpilze Seedna riion 6frastructure design Fig.5:Qualitative3DdetectionresultsontheM2Idataset.MIC-BEVaccuratelydetectsmultipleobjecttypesacrossdiverse urban scenes with varying traffic densities and weather or lighting conditions. --- Page 12 --- 12 remroFVEB D3RTED VEB-CIM Day Night ExtremeWeather T Cr au rck G P HrT ee ad dic inti gon Cam2 Undetected Misdetected Missed Cyclist Pedestrian Pedestrian Pedestrian Pedestrian Missed Car FalsePositive Undetected Cam1 Cam4 Pedestrian 2nd bP loe cd k ev die bw yin tr uc ca km1 Cam2 Cam1 Cam4 Cam1 FalsePositive Cam2 DS eh teif cte tiod n U Pn ed de et se tc rit ae nd OrS ieh nif tt ae td ion Undetected Cam1 Pedestrian 2nd Ped viewin cam1 Cam2 Cam4 Cam4 blockedbytruck Cam1 Cam1 FalsePositive Background Crosswalk Cam2 Sidewalk SD hri ov uin lg der 1 2 1 2 1 2 4 4 3 3 4 3 Cam1 Cam4 P be lod c v ki ee dw bin y tc ra um ck1 Cam2 Cam1 Cam4 Cam1 Fig.6:QualitativecomparisonofMIC-BEVonM2IDatasetwithbaselinemodels.ComparedtoBEVFormerandDETR3D, MIC-BEV produces more accurate and consistent detections across day, night, and extreme-weather scenes by leveraging relation-aware multi-view fusion. In Intersection 2, a pedestrian partially occluded by a truck is missed by the baselines but successfully detected by MIC-BEV. TABLE VII: Comparison of 3D object detection performance on the RoScenes validation set under Easy and Hard levels. Easy Hard Average Method mAP‚Üë NDS‚Üë mATE‚Üì mASE‚Üì mAOE‚Üì mAP‚Üë NDS‚Üë mATE‚Üì mASE‚Üì mAOE‚Üì NDS‚Üë SOLOFusion 0.129 0.308 0.878 0.144 0.517 0.066 0.202 0.844 0.144 1.000 0.255 BEVDet4D 0.200 0.428 0.896 0.094 0.041 0.139 0.393 0.922 0.099 0.038 0.411 BEVDet 0.299 0.506 0.742 0.079 0.042 0.184 0.445 0.754 0.087 0.043 0.476 StreamPETR 0.513 0.619 0.690 0.102 0.032 0.284 0.496 0.739 0.107 0.031 0.558 PETRv2 0.587 0.674 0.590 0.090 0.032 0.414 0.580 0.633 0.100 0.029 0.627 BEVFormer 0.609 0.693 0.560 0.078 0.030 0.433 0.597 0.600 0.090 0.029 0.645 DETR3D 0.644 0.722 0.501 0.067 0.031 0.471 0.633 0.508 0.080 0.028 0.678 RoBEV 0.684 0.753 0.442 0.058 0.031 0.524 0.672 0.438 0.077 0.027 0.713 RopeBEV 0.721 0.786 0.435 0.056 0.030 0.545 0.685 0.416 0.078 0.027 0.736 MIC-BEV(Ours) 0.742 0.796 0.220 0.058 0.028 0.561 0.719 0.255 0.077 0.027 0.757 BEV still leads with 0.626 mAP, outperforming DETR3D complex traffic scenarios, which is an essential capability for (0.589) and UVTR (0.577). Overall, MIC-BEV achieves an infrastructure-based perception. average mAP of 0.767, substantially higher than all baselines, Fig. 7 provides a qualitative comparison of MIC-BEV‚Äôs confirming its ability to generalize across varied layouts and detection results on the RoScenes validation set, visualized conditions. across eight synchronized roadside cameras and the corre- sponding fused BEV view covering an 800-meter detection C. Results on RoScenes range.Examples(a)-(b)showdaytimescenesontwodifferent Table VII presents 3D object detection results on the highway segments, while (c)-(d) depict twilight and nighttime RoScenes benchmark. MIC-BEV achieves the highest overall conditions. performance across both evaluation splits. On the Easy set, Across all lighting and weather conditions, MIC-BEV pro- MIC-BEV attains 0.742 mAP and 0.796 NDS, while on duces dense, spatially consistent detections that align well the more challenging Hard set, it achieves 0.561 mAP and across overlapping camera views. The model successfully 0.719NDS,whereissuessuchassevereocclusion,long-range detectsvehiclesatlongrange,maintainstightboundingboxes targets, and limited viewpoint overlap are more prevalent. even under partial occlusion, and avoids double-counting ob- Compared to RopeBEV, MIC-BEV improves mAP by 3% jectsappearinginmultiplecameras.Inthelow-lightandnight- on both the Easy split and Hard split, while significantly time examples, MIC-BEV remains robust to glare, reflection, reducing mATE on the Hard scenes by 38.7%. Notably, MIC- andheadlightbloom,continuingtolocalizevehiclesaccurately BEV shows strong detection capability across the large-scale where baselines often fail or produce fragmented boxes. The detection area (800-meter range across eight camera views), fusedBEVmapsexhibitcoherentstructure,withclearlysepa- reliably detecting distant and densely distributed vehicles in ratedlanesandminimalfalsepositivesinbackgroundregions. --- Page 13 --- 13 Truck Car Van Bus Cam8 CCaamm77 Cam3 Cam4 8 7 3 4 2 1 6 5 Cam6 Cam5 Cam2 Cam1 a) Daytime scene on Road A b) Daytime scene on Road B c) Twilight (low-light) scene on Road A d) Night scene on Road B Fig. 7: Qualitative results of MIC-BEV on the RoScenes validation set. Each example shows outputs from 8 roadside cameras and the corresponding fused BEV view over an 800-meter range. (a)-(b) depict daytime scenes, while (c)-(d) show twilight and nighttime conditions. MIC-BEV shows strong performance under diverse lighting and camera configurations. --- Page 14 --- 14 Cam1 Cam2 1 1 Cam2 2 2 1 5 5 5 5 4 2 Cam1 1 4 4 4 3 3 2 Cam1 Cam2 Cam1 Perspective View BEVWeightMap BEVWeightMap Perspective View Cam3 Cam4 1 1 Cam3 2 2 3 3 4 5 5 2 1 2 4 4 4 5 5 3 3 Cam4 Cam3 Cam4 Perspective View BEVWeightMap BEVWeightMap Perspective View Fig. 8: Camera-wise importance weights for BEV grid cells at the intersection center. Each row corresponds to a pair of perspective views and BEV weight maps for Cam1 to Cam4. The BEV maps visualize the spatial contribution of each camera, where warmer colors denote higher weights. Numbered objects (‚ë†-‚ë§) represent key targets selected for detailed analysis. Objects annotated with colored circles (‚ë†, ‚ë°, ‚ë¢, ‚ë£, ‚ë§) are visible in the corresponding perspective view, while the same numbered markers without color in other cameras indicate that the objects are occluded or outside the field of view. These visual results reinforce the quantitative findings, boundary in Cam2 and Cam3, where partial occlusion occurs. demonstratingthatMIC-BEV‚Äôscamera-gridrelation-enhanced ThemodelappropriatelyprioritizesCam4,wheretheobjectis attention effectively aggregates cross-view information and well-centered and clearly observed. maintains geometric consistency under challenging visibility, InthecaseofObject1,whichappearsinCam1,Cam2,and large spatial extent, and heterogeneous camera placement. Cam4. However, all views capture it at a distance or near the periphery. Hence, the GNN assigns roughly uniform weights. Object 3 is visible only in Cam3 and Cam4, both offering D. Camera Importance Weight Analysis direct,minimallyoccludedviews.MIC-BEVassignsbalanced Unlikevehicle-mountedcameras,infrastructurecamerasof- and high weights to both cameras. Finally, Object 5, seen ten capture scenes from elevated and wide viewpoints, which across all cameras, receives the highest weight from Cam2, introduces significant perspective distortions. Objects near which observes the object near the center with the clearest the image periphery may appear stretched, foreshortened, or geometric fidelity. geometrically inconsistent with their true scale. Such distor- ThesepatternsindicatethatMIC-BEV‚ÄôsGNNdoesnotsim- tions directly impact the reliability of spatial cues extracted ply learn a static weight distribution. Rather, it implements an from each camera view. MIC-BEV‚Äôs relation-aware fusion interpretable, geometry-aware process, suppressing distorted mechanism effectively adapts the contribution of each camera or occluded observations while emphasizing geometrically based on viewpoint geometry and object clarity, offering reliable and semantically informative views. interpretable insights into how spatial reliability is learned. Fig. 8 illustrates the learned camera-wise importance E. Runtime performance weights assigned by the GNN in MIC-BEV across four cameras monitoring an intersection. The model consistently WeevaluateinferenceefficiencyonanNVIDIAL40SGPU, assigns higher weights to cameras where objects are clearly comparing MIC-BEV with representative baseline models. As visible, centered in the field of view, and minimally distorted. summarized in Table VIII, MIC-BEV achieves an inference Forexample,Object4isvisibleinallfourcamerasbutappears speed of 4.65 FPS, which is comparable to BEVFormer (4.76 heavily elongated in Cam1 due to an oblique viewing angle. FPS) and suitable for applications requiring high accuracy. Correspondingly, MIC-BEV assigns a lower weight to Cam1 Although lighter models such as LSS (17.52 FPS) and PETR and emphasizes Cam2 and Cam4, which observe the object (13.33 FPS) achieve higher frame rates, they incur substan- from less distorted, more front-facing perspectives. Similarly, tial performance losses, with mAP values below 0.60. This Object 2 is captured in all views but lies near the image comparison highlights MIC-BEV‚Äôs ability to balance detec- --- Page 15 --- 15 tionaccuracyandcomputationalefficiency,achievingsuperior However, excessively high masking rates (e.g., p ‚â•0.5) be- m accuracy while maintaining practical runtime. gintodegradeperformanceontheM2I-Normalset,suggesting that overly frequent view removal limits the model‚Äôs ability TABLE VIII: Comparison of inference speed (frames per to fully exploit multi-view redundancy. Overall, these results second) and mAP on the M2I-Normal testing set. demonstrate that introducing moderate stochastic masking duringtrainingprovidesafavorablebalancebetweenstandard Model mAP(M2I-Normal) FPS accuracy and robustness. LSS 0.446 17.52 PETR 0.596 13.33 BEVFormer 0.637 4.76 TABLEX:Effectoftrainingcameramaskingprobability(p m) PETRv2 0.651 9.17 on performance over the M2I normal and robust testing sets. StreamPETR 0.677 11.11 BEVNeXt 0.681 10.20 mAP‚Üë GeoBEV 0.690 6.25 MaskingRatepm UVTR 0.698 9.09 M2I-Normal M2I-Robust DETR3D 0.701 11.76 0.00 0.764 0.598 MIC-BEV 0.767 4.65 0.25 0.767 0.647 0.50 0.757 0.650 0.75 0.742 0.652 F. Ablation Studies 3) Influence of Relation Encoding: We conduct an ab- 1) ComponentAblation: Weperformacomponentablation lation study to evaluate the contribution of latent camera on the M2I test set to evaluate the contribution of each key features, distance-based relations, and angle-based relations moduleinMIC-BEV,includingcameramasking,theauxiliary withintheGNN-basedfusionmodule.Startingfromabaseline BEV segmentation head, and the Relation-Enhanced Spatial that excludes all three components, we observe a mAP of Cross-Attention (ReSCA) module. As shown in Table IX, 0.691andNDSof0.727.Introducingonlygeometricrelations removing any component leads to a consistent performance (distance and angle) without camera features yields a notable drop under both the M2I-Normal and M2I-Robust settings. improvement (mAP: 0.729, NDS: 0.745), highlighting the Enabling random camera masking improves robustness to importance of spatial structure among infrastructure-mounted sensor failures by encouraging the model to rely on com- cameras. Using only camera features provides comparable plementary camera views. The auxiliary BEV segmentation gains (mAP: 0.725, NDS: 0.740), indicating that latent image head further enhances spatial reasoning, providing geometric featuresarebeneficialbutslightlylesseffectivethangeometric priors that guide detection. Finally, by studying the effect of relations. Combining camera features with distance relations the ReSCA module, we replace it with a regular spatial cross- furtherboostsperformanceto0.761mAPand0.765NDS.The attention layer without graph-based weighting, and observe a full model, which incorporates all three components, achieves clear reduction in both mAP and NDS. This confirms that thebestresults(mAP:0.767,NDS:0.771),demonstratingthat the GNN weighting mechanism in ReSCA effectively learns latentcamerafeaturesandgeometricrelationsarecomplemen- view-dependent importance for multi-camera fusion, leading tary and essential for multi-view feature fusion. to stronger overall performance. TABLE IX: Ablation study of MIC-BEV components on TABLE XI: Influence of incorporating camera features, dis- the M2I-Normal testing set. Cam. Masking: random camera tance,andanglerelationsintheGNN-basedmulti-viewfusion masking; BEV Seg.: the auxiliary BEV segmentation head; module. ReSCA: Relation-enhanced spatial cross-attention module M2I-Normal Cam.Feature Distances Angles Cam. BEV M2I-Normal M2I-Robust mAP‚Üë NDS‚Üë ReSCA Masking Seg. mAP‚Üë NDS‚Üë mAP‚Üë NDS‚Üë ‚úó ‚úó ‚úó 0.691 0.727 ‚úó ‚úó ‚úó 0.637 0.678 0.513 0.593 ‚úó ‚úì ‚úì 0.729 0.745 ‚úì ‚úó ‚úó 0.649 0.689 0.574 0.631 ‚úì ‚úó ‚úó 0.725 0.740 ‚úì ‚úì ‚úó 0.691 0.727 0.597 0.647 ‚úì ‚úì ‚úó 0.761 0.765 ‚úì ‚úì ‚úì 0.767 0.771 0.647 0.678 ‚úì ‚úì ‚úì 0.767 0.771 2) Influence of Masking Rate: We conduct an ablation 4) Robustness across Random Seeds: To evaluate con- studytoanalyzetheeffectoftheview-maskingprobabilityp sistency under random perturbations, we assess MIC-BEV‚Äôs m during training on the M2I-Normal and Robust testing set. As stability across different test-time degradations. The robust shown in Table X, applying moderate masking enhances ro- test set is constructed by randomly applying image blur or bustness against sensor degradation, while maintaining strong maskingonecameraviewforeachsample.WecompareMIC- performance under normal conditions. Specifically, when p BEV against BEVFormer and DETR3D over three random m increases from 0.0 to 0.25, the model‚Äôs mAP for the M2I- seeds, reporting the mean and standard deviation of mAP Robust set improves by 8%, indicating that exposure to in Table XII. MIC-BEV achieves both the highest average simulated sensor failures effectively encourages the model to accuracy and the lowest variance, indicating greater stability rely on complementary visual cues from the remaining views. and reliability under random sensor degradation. --- Page 16 --- 16 TABLE XII: Detection performance across random seeds for [4] Z.Bai,G.Wu,X.Qi,Y.Liu,K.Oguchi,andM.J.Barth,‚ÄúInfrastructure- the M2I robust testing set. basedobjectdetectionandtrackingforcooperativedrivingautomation: A survey,‚Äù in 2022 IEEE Intelligent Vehicles Symposium (IV). IEEE, M2I-Robust 2022,pp.1366‚Äì1373. Method [5] M. Lo¬®tscher, N. Baumann, E. Ghignone, A. Ronco, and M. Magno, mAP‚Üë NDS‚Üë ‚ÄúAssessingtherobustnessoflidar,radaranddepthcamerasagainstill- BEVFormer 0.513¬±0.009 0.593¬±0.008 reflecting surfaces in autonomous vehicles: An experimental study,‚Äù in DETR3D 0.540¬±0.007 0.580¬±0.008 2023 IEEE 9th World Forum on Internet of Things (WF-IoT). IEEE, MIC-BEV 0.647¬±0.006 0.678¬±0.007 2023,pp.1‚Äì6. [6] W.Jiang,H.Xiang,X.Cai,R.Xu,J.Ma,Y.Li,G.H.Lee,andS.Liu, ‚ÄúOptimizingtheplacementofroadsidelidarsforautonomousdriving,‚Äù inProceedingsoftheIEEE/CVFInternationalConferenceonComputer VI. CONCLUSIONS Vision,2023,pp.18381‚Äì18390. [7] T.-H.Kim,G.-H.Jo,H.-S.Yun,K.-S.Yun,andT.-H.Park,‚ÄúPlacement We propose MIC-BEV, a Transformer-based framework method of multiple lidars for roadside infrastructure in urban environ- ments,‚ÄùSensors,vol.23,no.21,p.8808,2023. formulti-camerainfrastructure-basedperception,togetherwith [8] L. Kloeker, G. Joeken, and L. Eckstein, ‚ÄúEconomic analysis of smart M2I, a comprehensive synthetic dataset and benchmark en- roadsideinfrastructuresensorsforconnectedandautomatedmobility,‚Äùin compassing diverse road layouts, camera placements, illumi- 2023IEEE26thInternationalConferenceonIntelligentTransportation Systems(ITSC). IEEE,2023,pp.2331‚Äì2336. nation conditions, and adverse weather. MIC-BEV introduces [9] W.Wang,Y.Lu,G.Zheng,S.Zhan,X.Ye,Z.Tan,J.Wang,G.Wang, a camera-BEV spatial relation-aware attention mechanism andX.Li,‚ÄúBevspread:Spreadvoxelpoolingforbird‚Äôs-eye-viewrepre- that explicitly models geometric relations between each cam- sentationinvision-basedroadside3dobjectdetection,‚ÄùinProceedingsof theIEEE/CVFConferenceonComputerVisionandPatternRecognition, era and BEV cell through a graph neural network, enabling 2024,pp.14718‚Äì14727. adaptive multi-view fusion under heterogeneous configura- [10] L. Yang, X. Zhang, J. Yu, J. Li, T. Zhao, L. Wang, Y. Huang, tions. In addition, a dual-level BEV segmentation head jointly C. Zhang, H. Wang, and Y. Li, ‚ÄúMonogae: Roadside monocular 3d objectdetectionwithground-awareembeddings,‚ÄùIEEETransactionson learns map-level and object-level priors to enhance spatial Intelligent Transportation Systems, vol. 25, no. 11, pp. 17587‚Äì17601, reasoning, while camera masking strategies such as random 2024. dropout and Gaussian blur improve robustness against sensor [11] Z. Li, Z. Chen, A. Li, L. Fang, Q. Jiang, X. Liu, and J. Jiang, ‚ÄúUnsuperviseddomainadaptationformonocular3dobjectdetectionvia degradation, occlusion, and partial failures. self-training,‚Äù in European conference on computer vision. Springer, Extensive experiments demonstrate that MIC-BEV consis- 2022,pp.245‚Äì262. tently achieves state-of-the-art performance across both syn- [12] R.Xu,Z.Tu,H.Xiang,W.Shao,B.Zhou,andJ.Ma,‚ÄúCobevt:Cooper- ativebird‚Äôseyeviewsemanticsegmentationwithsparsetransformers,‚Äù thetic (M2I) and real-world (RoScenes) datasets. On M2I, arXivpreprintarXiv:2207.02202,2022. the model shows strong detection accuracy across normal, [13] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu, ‚ÄúA robust, and extreme weather settings, with a significant im- comprehensive survey on graph neural networks,‚Äù IEEE Transactions provement in mAP compared to leading baseline methods. on Neural Networks and Learning Systems, vol. 32, no. 1, pp. 4‚Äì24, 2021. OnRoScenes,MIC-BEVgeneralizeseffectivelytolarge-scale [14] T. Roddick, A. Kendall, and R. Cipolla, ‚ÄúOrthographic feature highway scenes and complex urban intersections, maintaining transform for monocular 3d object detection,‚Äù 2018. [Online]. superior detection stability under limited overlap and chal- Available: [15] L. Zhang, Y. Liu, X. Wang, Y. He, G. Li, Y. Zhang, C. Liu, Z. Jiang, lenging lighting. Qualitative analyses further reveal that the and Y. Liu, ‚ÄúCaddn: A content-aware downsampling-based detection relation-enhanced attention dynamically emphasizes informa- methodforsmallobjectsinremotesensingimages,‚ÄùIEEETransactions tiveviewpoints,suppressesredundantordistortedfeatures,and onGeoscienceandRemoteSensing,vol.63,pp.1‚Äì17,2025. [16] J. Philion and S. Fidler, ‚ÄúLift, splat, shoot: Encoding images from improves the detection of small or distant targets. arbitrary camera rigs by implicitly unprojecting to 3d,‚Äù in European Beyondstaticobjectdetection,futureresearchwillfocuson conferenceoncomputervision. Springer,2020,pp.194‚Äì210. extending MIC-BEV toward multi-object tracking and trajec- [17] J. Huang, G. Huang, Z. Zhu, Y. Ye, and D. Du, ‚ÄúBevdet: High- performancemulti-camera3dobjectdetectioninbird-eye-view,‚ÄùarXiv tory forecasting to capture dynamic interactions among road preprintarXiv:2112.11790,2021. users. Another direction is to explore real-time deployment [18] Y. Li, Z. Ge, G. Yu, J. Yang, Z. Wang, Y. Shi, J. Sun, and Z. Li, through lightweight backbones and knowledge distillation for ‚ÄúBevdepth: Acquisition of reliable depth for multi-view 3d object de- tection,‚ÄùinProceedingsoftheAAAIconferenceonartificialintelligence, edge devices. Finally, we plan to expand the M2I benchmark vol.37,2023,pp.1477‚Äì1485. with additional real-world data to bridge the simulation-to- [19] J.Zhang,Y.Zhang,Y.Qi,Z.Fu,Q.Liu,andY.Wang,‚ÄúGeobev:Learn- realitygapandenablecomprehensiveevaluationacrossdiverse inggeometricbevrepresentationformulti-view3dobjectdetection,‚Äùin ProceedingsoftheAAAIConferenceonArtificialIntelligence,vol.39, urban contexts. no.9,2025,pp.9960‚Äì9968. [20] Y.Wang,V.C.Guizilini,T.Zhang,Y.Wang,H.Zhao,andJ.Solomon, ‚ÄúDetr3d: 3d object detection from multi-view images via 3d-to-2d REFERENCES queries,‚ÄùinConferenceonrobotlearning. PMLR,2022,pp.180‚Äì191. [21] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and [1] H.Yu,W.Yang,J.Zhong,Z.Yang,S.Fan,P.Luo,andZ.Nie,‚ÄúEnd- S. Zagoruyko, ‚ÄúEnd-to-end object detection with transformers,‚Äù in to-endautonomousdrivingthroughv2xcooperation,‚ÄùAAAI,2025. European conference on computer vision. Springer, 2020, pp. 213‚Äì [2] M. Tang, D. Yu, P. Li, C. Song, P. Zhao, W. Xiao, and N. Chen, 229. ‚ÄúAmulti-sceneroadsidelidarbenchmarktowardsdigitaltwinsofroad [22] Y.Liu,T.Wang,X.Zhang,andJ.Sun,‚ÄúPetr:Positionembeddingtrans- intersections,‚Äù ISPRS Annals of the Photogrammetry, Remote Sensing formationformulti-view3dobjectdetection,‚ÄùinEuropeanconference andSpatialInformationSciences,vol.10,pp.341‚Äì348,2024. oncomputervision. Springer,2022,pp.531‚Äì548. [3] Y.Li,D.Ma,Z.An,Z.Wang,Y.Zhong,S.Chen,andC.Feng,‚ÄúV2x- [23] Y.Liu,J.Yan,F.Jia,S.Li,A.Gao,T.Wang,andX.Zhang,‚ÄúPetrv2: sim: Multi-agent collaborative perception dataset and benchmark for A unified framework for 3d perception from multi-camera images,‚Äù in autonomous driving,‚Äù IEEE Robotics and Automation Letters, vol. 7, Proceedings of the IEEE/CVF international conference on computer no.4,pp.10914‚Äì10921,2022. vision,2023,pp.3262‚Äì3272. --- Page 17 --- 17 [24] Z. Li, W. Wang, H. Li, E. Xie, C. Sima, T. Lu, Q. Yu10568349, and [44] W. Chen, Y. Su, J. Zuo, C. Yang, C. Yuan, C. Qian, C.-M. Chan, J.Dai,‚ÄúBevformer:learningbird‚Äôs-eye-viewrepresentationfromlidar- Y. Qin, Y. Lu, R. Xie et al., ‚ÄúAgentverse: Facilitating multi-agent col- cameraviaspatiotemporaltransformers,‚ÄùIEEETransactionsonPattern laborationandexploringemergentbehaviorsinagents,‚ÄùarXivpreprint AnalysisandMachineIntelligence,2024. arXiv:2308.10848,vol.2,no.4,p.6,2023. [25] S. Wang, Y. Liu, T. Wang, Y. Li, and X. Zhang, ‚ÄúExploring object- [45] G.Liu,Y.Hu,C.Xu,W.Mao,J.Ge,Z.Huang,Y.Lu,Y.Xu,J.Xia, centrictemporalmodelingforefficientmulti-view3dobjectdetection,‚Äù Y.Wangetal.,‚ÄúTowardscollaborativeautonomousdriving:Simulation inProceedingsoftheIEEE/CVFinternationalconferenceoncomputer platformandend-to-endsystem,‚ÄùIEEETransactionsonPatternAnalysis vision,2023,pp.3621‚Äì3631. andMachineIntelligence,2025. [26] J. Huang and G. Huang, ‚ÄúBevdet4d: Exploit temporal cues in [46] H. Xiang, Z. Zheng, X. Xia, R. Xu, L. Gao, Z. Zhou, X. Han, multi-camera 3d object detection,‚Äù 2022. [Online]. Available: https: X. Ji, M. Li, Z. Meng et al., ‚ÄúV2x-real: a largs-scale dataset for //arxiv.org/abs/2203.17054 vehicle-to-everything cooperative perception,‚Äù in European Conference [27] J. Park, C. Xu, S. Yang, K. Keutzer, K. Kitani, M. Tomizuka, and onComputerVision. Springer,2024,pp.455‚Äì470. W. Zhan, ‚ÄúTime will tell: New outlooks and a baseline for temporal [47] R. Hao, S. Fan, Y. Dai, Z. Zhang, C. Li, Y. Wang, H. Yu, W. Yang, multi-view3dobjectdetection,‚ÄùarXivpreprintarXiv:2210.02443,2022. J. Yuan, and Z. Nie, ‚ÄúRcooper: A real-world large-scale dataset for [28] Z.Li,S.Lan,J.M.Alvarez,andZ.Wu,‚ÄúBevnext:Revivingdensebev roadsidecooperativeperception,‚ÄùinProceedingsoftheIEEE/CVFcon- frameworks for 3d object detection,‚Äù in Proceedings of the IEEE/CVF ferenceoncomputervisionandpatternrecognition,2024,pp.22347‚Äì conference on computer vision and pattern recognition, 2024, pp. 22357. 20113‚Äì20123. [48] X.Zhu,H.Sheng,S.Cai,B.Deng,S.Yang,Q.Liang,K.Chen,L.Gao, [29] Z. Yang, J. Mao, W. Yang, Y. Ai, Y. Kong, H. Yu, and W. Zhang, J. Song, and J. Ye, ‚ÄúRoscenes: A large-scale multi-view 3d dataset ‚ÄúLidar-based end-to-end temporal perception for vehicle-infrastructure forroadsideperception,‚ÄùinEuropeanConferenceonComputerVision. cooperation,‚Äù IEEE Internet of Things Journal, vol. 12, no. 13, pp. Springer,2024,pp.331‚Äì347. 22862‚Äì22874,2025. [49] S. Fan, Z. Wang, X. Huo, Y. Wang, and J. Liu, ‚ÄúCalibration-free [30] W. Zimmer, J. Wu, X. Zhou, and A. C. Knoll, ‚ÄúReal-time and robust bev representation for infrastructure perception,‚Äù in 2023 IEEE/RSJ 3d object detection with roadside lidars,‚Äù in Proceedings of the 12th International Conference on Intelligent Robots and Systems (IROS). InternationalScientificConferenceonMobilityandTransport:Mobility IEEE,2023,pp.9008‚Äì9013. InnovationsforGrowingMegacities. Springer,2023,pp.199‚Äì219. [50] A.Vaghela,D.Lu,A.A.Verma,B.Chakravarthi,H.Wei,andY.Yang, ‚ÄúMc-bevro: Multi-camera bird eye view road occupancy detection for [31] H. Yu, Y. Tang, E. Xie, J. Mao, J. Yuan, P. Luo, and Z. Nie, trafficmonitoring,‚ÄùarXivpreprintarXiv:2502.11287,2025. ‚ÄúVehicle-infrastructurecooperative3dobjectdetectionviafeatureflow [51] J.Jia,G.Yi,andY.Shi,‚ÄúRopebev:Amulti-cameraroadsideperception prediction,‚Äù2023.[Online].Available: networkinbird‚Äôs-eye-view,‚ÄùarXivpreprintarXiv:2409.11706,2024. [32] W. Zimmer, J. Birkner, M. Brucker, H. T. Nguyen, S. Petrovski, [52] K.He,X.Zhang,S.Ren,andJ.Sun,‚ÄúDeepresiduallearningforimage B.Wang,andA.C.Knoll,‚ÄúInfradet3d:Multi-modal3dobjectdetection recognition,‚ÄùinProceedingsoftheIEEEconferenceoncomputervision basedonroadsideinfrastructurecameraandlidarsensors,‚Äùin2023IEEE andpatternrecognition,2016,pp.770‚Äì778. IntelligentVehiclesSymposium(IV). IEEE,2023,pp.1‚Äì8. [53] T.-Y.Lin,P.Dolla¬¥r,R.Girshick,K.He,B.Hariharan,andS.Belongie, [33] X. Bai, Z. Hu, X. Zhu, Q. Huang, Y. Chen, H. Fu, and C.-L. Tai, ‚ÄúFeaturepyramidnetworksforobjectdetection,‚ÄùinProceedingsofthe ‚ÄúTransfusion: Robust lidar-camera fusion for 3d object detection with IEEEconferenceoncomputervisionandpatternrecognition,2017,pp. transformers,‚ÄùinProceedingsoftheIEEE/CVFconferenceoncomputer 2117‚Äì2125. visionandpatternrecognition,2022,pp.1090‚Äì1099. [54] X.Zhu,W.Su,L.Lu,B.Li,X.Wang,andJ.Dai,‚ÄúDeformableDETR: [34] Z. Liu, H. Tang, A. Amini, X. Yang, H. Mao, D. Rus, and S. Han, Deformabletransformersforend-to-endobjectdetection,‚ÄùarXivpreprint ‚ÄúBevfusion:Multi-taskmulti-sensorfusionwithunifiedbird‚Äôs-eyeview arXiv:2010.04159,2020. representation,‚ÄùarXivpreprintarXiv:2205.13542,2022. [55] P.VelicÀákovic¬¥,G.Cucurull,A.Casanova,A.Romero,P.Lio,andY.Ben- [35] Z. Meng, Y. Zhang, Z. Zheng, Z. Zhao, and J. Ma, ‚ÄúAgentalign: gio,‚ÄúGraphattentionnetworks,‚ÄùarXivpreprintarXiv:1710.10903,2017. Misalignment-adapted multi-agent perception for resilient inter-agent [56] S. Brody, U. Alon, and E. Yahav, ‚ÄúHow attentive are graph attention sensorcorrelations,‚ÄùarXivpreprintarXiv:2412.06142,2024. networks?‚ÄùarXivpreprintarXiv:2105.14491,2021. [36] H.Hu,Z.Liu,S.Chitlangia,A.Agnihotri,andD.Zhao,‚ÄúInvestigating [57] R. Xu, H. Xiang, Z. Tu, X. Xia, M.-H. Yang, and J. Ma, ‚ÄúV2x-vit: theimpactofmulti-lidarplacementonobjectdetectionforautonomous Vehicle-to-everything cooperative perception with vision transformer,‚Äù driving,‚ÄùinProceedingsoftheIEEE/CVFconferenceoncomputervision inEuropeanconferenceoncomputervision. Springer,2022,pp.107‚Äì andpatternrecognition,2022,pp.2550‚Äì2559. 124. [37] Z. Zheng, Y. Zhang, Z. Meng, J. Liu, X. Xia, and J. Ma, ‚ÄúInspe: [58] Z. Zhou, H. Xiang, Z. Zheng, S. Z. Zhao, M. Lei, Y. Zhang, T. Cai, Rapid evaluation of heterogeneous multi-modal infrastructure sensor X. Liu, J. Liu, M. Bajji et al., ‚ÄúV2xpnp: Vehicle-to-everything spatio- placement,‚Äù2025. temporal fusion for multi-agent perception and prediction,‚Äù arXiv [38] X. Ye, M. Shu, H. Li, Y. Shi, Y. Li, G. Wang, X. Tan, and E. Ding, preprintarXiv:2412.01812,2024. ‚ÄúRope3d: The roadside perception dataset for autonomous driving and [59] W.Zimmer,G.A.Wardana,S.Sritharan,X.Zhou,R.Song,andA.C. monocular 3d object detection task,‚Äù in Proceedings of the IEEE/CVF Knoll,‚ÄúTumtrafv2xcooperativeperceptiondataset,‚ÄùinProceedingsof Conference on Computer Vision and Pattern Recognition, 2022, pp. theIEEE/CVFconferenceoncomputervisionandpatternrecognition, 21341‚Äì21350. 2024,pp.22668‚Äì22677. [39] H. Yu, Y. Luo, M. Shu, Y. Huo, Z. Yang, Y. Shi, Z. Guo, H. Li, [60] H. Xiang, Z. Zheng, X. Xia, S. Z. Zhao, L. Gao, Z. Zhou, T. Cai, X. Hu, J. Yuan et al., ‚ÄúDair-v2x: A large-scale dataset for vehicle- Y.Zhang,andJ.Ma,‚ÄúV2x-realo:Anopenonlineframeworkanddataset infrastructure cooperative 3d object detection,‚Äù in Proceedings of the forcooperativeperceptioninreality,‚ÄùECCV,2024. IEEE/CVFconferenceoncomputervisionandpatternrecognition,2022, [61] A.Dosovitskiy,G.Ros,F.Codevilla,A.Lopez,andV.Koltun,‚ÄúCarla: pp.21361‚Äì21370. An open urban driving simulator,‚Äù in Conference on robot learning. [40] L.Yang,K.Yu,T.Tang,J.Li,K.Yuan,L.Wang,X.Zhang,andP.Chen, PMLR,2017,pp.1‚Äì16. ‚ÄúBevheight: A robust framework for vision-based roadside 3d object [62] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, detection,‚Äù in Proceedings of the IEEE/CVF Conference on Computer A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, ‚Äúnuscenes: A VisionandPatternRecognition,2023,pp.21611‚Äì21620. multimodal dataset for autonomous driving,‚Äù in Proceedings of the [41] L.Yang,T.Tang,J.Li,K.Yuan,K.Wu,P.Chen,L.Wang,Y.Huang, IEEE/CVFconferenceoncomputervisionandpatternrecognition,2020, L. Li, X. Zhang et al., ‚ÄúBevheight++: Toward robust visual centric 3d pp.11621‚Äì11631. objectdetection,‚ÄùIEEETransactionsonPatternAnalysisandMachine [63] P.Sun,H.Kretzschmar,X.Dotiwalla,A.Chouard,V.Patnaik,P.Tsui, Intelligence,2025. J. Guo, Y. Zhou, Y. Chai, B. Caine et al., ‚ÄúScalability in perception [42] H. Shi, C. Pang, J. Zhang, K. Yang, Y. Wu, H. Ni, Y. Lin, R. Stiefel- for autonomous driving: Waymo open dataset,‚Äù in Proceedings of the hagen, and K. Wang, ‚ÄúCobev: Elevating roadside 3d object detection IEEE/CVFconferenceoncomputervisionandpatternrecognition,2020, with depth and height complementarity,‚Äù IEEE Transactions on Image pp.2446‚Äì2454. Processing,2024. [64] Y. Li, Y. Chen, X. Qi, Z. Li, J. Sun, and J. Jia, ‚ÄúUnifying voxel- [43] J. Jinrang, Z. Li, and Y. Shi, ‚ÄúMonouni: A unified vehicle and basedrepresentationwithtransformerfor3dobjectdetection,‚ÄùAdvances infrastructure-sidemonocular3dobjectdetectionnetworkwithsufficient inNeuralInformationProcessingSystems,vol.35,pp.18442‚Äì18455, depth clues,‚Äù Advances in Neural Information Processing Systems, 2022. vol.36,pp.11703‚Äì11715,2023.