{
  "abstract": "Abstract.html. [45] T. Zhou, L. Wang, R. Chen, W. Wang, and Y. Liu. Accelerating Reinforcement Learning for Autonomous Driving Using Task-Agnostic and Ego- Centric Motion Skills. In 2023 IEEE/RSJ In- ternational Conference on Intelligent Robots and Systems (IROS), pages 11289\u201311296, Oct. 2023. doi:10.1109/IROS55552.2023.10341449. 16 --- Page 17 --- Appendices Algorithm 3 Hierarchical RL with Combined Options Initialize critic models \u03b8 ,\u03b8\u2032 q,i q,i A Algorithms B \u2190\u2205 for each episode do Below we provide the concrete implementations of the for each environment step t do generic Algorithm 1 for hierarchical reinforcement learning if t=0 or s \u2208T \u2229T then \u25b7 \u03f5-greedy \u03b2 withsingleoptions(Algorithm2),combinedoptions(Algo- with probt abilito yv,t \u03f5\u22121 do od,t\u22121 m rithm 3) and hybrid options (Algorithm 4). (o ,o )\u223cU[Oav(s )\u00d7Oav(s )] v,t d,t v t d t else Algorithm 2 Hierarchical RL with Options (o ,o )= argmax q(s ,(o ,o );\u03b8 ) v,t d,t t v d q,1 Initialize critic models \u03b8 ,\u03b8\u2032 ov\u2208Ovav(s t),od\u2208Odav(s t) q,i q,i end B \u2190\u2205 else if s \u2208T then for each episode do t ov,t\u22121 o =o for each environment step t do d,t d,t\u22121 with probability \u03f5 do if wt i= th0 po rr os bt ab\u2208 ilT ito yt\u2212 \u03f51 dth oen \u25b7 Activat \u25b7e \u03f5n -ge rw eeo dp yti \u03b2on o v,t \u223cU[O vav(s t)] m else o \u223cU[Oav(s )] t t o =argmax q(s ,(o ,o );\u03b8 ) else v,t ov\u2208Ovav(s t) t v d,t q,1 end o =argmax q(s ,o;\u03b8 ) endt o\u2208Oav(s t) t q,1 else if s t \u2208T od,t\u22121 then o =o else v,t v,t\u22121 with probability \u03f5 do o t =o t\u22121 o \u223cU[Oav(s )] end if d,t d t else a =\u03c0 (s) s tt +1 \u223co \u03c4t (\u00b7|s t,a t) eno dd,t =argmax od\u2208Odav(s t)q(s t,(o v,t,o d);\u03b8 q,1) r =r(s ,a ,s ) t t t t+1 else B \u2190B\u222a{(s ,o ,r ,s )} t t t t+1 (o ,o )=(o ,o ) end for v,t d,t v,t\u22121 d,t\u22121 end if for each gradient step do (cid:20) 1 0(cid:21) (cid:20) 0 0(cid:21) Sample batch B from B a t = 0 0 \u03c0 ov,t(s)+ 0 1 \u03c0 od,t(s) for all (s ,o ,r ,s )\u2208B do t t t t+1 s \u223c\u03c4(\u00b7|s ,a ) t+1 t t if s \u2208T then \u25b7 Determine next active option ot+1 =aro gt max q(s ,o\u2032;\u03b8 ) r t =r(s t,a t,s t+1) t+1 o\u2032\u2208Oav(s t+1) t+1 q,1 B \u2190B\u222a{(s t,(o v,t,o d,t),r t,s t+1)} else end for o =o for each gradient step do t+1 t end if Sample batch B from B y t =r t+\u03b3min i\u2208{1,2}q(s t+1,o t+1;\u03b8 q\u2032 ,i) for all (s t,(o v,t,o d,t),r t,s t+1)\u2208B do end for if s \u2208T \u2229T then \u25b7 \u03c0 L(cid:98)q(\u03b8 q,i;B)= |B1 |(cid:80) B(cid:0) q(s t,o t;\u03b8 q,i)\u2212y t(cid:1)2 (ot v+ ,t1 +1,o dov ,t,t +1)=od,t m \u03b8 \u03b8q m\u2032,i \u2190\u2190 \u03c4\u03b8 \u03b8q m,i\u2212 +\u03b7 (q 1\u2207 \u2212\u03b8q \u03c4,i )L \u03b8(cid:98) m\u2032q(\u03b8 q,i;B) o\u2032 v\u2208Ovav(sa t+rg 1)m ,o\u2032 da \u2208x Odav(s t+1)q(s t+1,(o\u2032 v,o\u2032 d);\u03b8 q,1) end for else if s \u2208T then t+1 ov,t end for o =o d,t+1 d,t o =argmax q(s ,(o\u2032,o );\u03b8 ) v,t+1 o\u2032 v\u2208Ovav(s t+1) t+1 v d,t+1 q,1 else if s \u2208T then t+1 od,t o =o v,t+1 v,t o =argmax q(s ,(o ,o\u2032);\u03b8 ) d,t+1 o\u2032 d\u2208Odav(s t+1) t+1 v,t+1 d q,1 else (o ,o )=(o ,o ) v,t+1 d,t+1 v,t d,t end if y =r +\u03b3min q(s ,(o ,o );\u03b8\u2032 ) t t i\u2208{1,2} t+1 v,t+1 d,t+1 q,i end for L(cid:98)q(\u03b8 q,i;B)= |B1 |(cid:80) B(cid:0) q(s t,(o v,t,o d,t);\u03b8 q,i)\u2212y t(cid:1)2 \u03b8 q,i \u2190\u03b8 q,i\u2212\u03b7 q\u2207 \u03b8q,iL(cid:98)q(\u03b8 q,i;B) \u03b8\u2032 \u2190\u03c4\u03b8 +(1\u2212\u03c4)\u03b8\u2032 m m m end for end for 17 --- Page 18 --- Algorithm 4 Hierarchical RL with Hybrid Options Initialize actor and critic models \u03b8 ,\u03b8\u2032,\u03b8 ,\u03b8\u2032 \u00b5 \u00b5 q,i q,i B \u2190\u2205 for each episode do for each environment step t do \u2206v\u02dc \u223cN1 (\u00b5(s ;\u03b8 );\u03c32) \u25b7 Truncated Gaussian t \u22121 t \u00b5 \u03f5 \u03b2 m,c if t=0 or s \u2208T then t od,t\u22121 with probability \u03f5 do \u25b7 \u03f5-greedy \u03b2 m,d o \u223cU[Oav(s )] d,t d t else o =argmax q(s ,(\u2206v\u02dc,o );\u03b8 ) d,t od\u2208Odav(s t) t t d q,1 end else o =o d,t d,t\u22121 end if \u2206v =\u03c3 (\u2206v\u02dc;s ) \u25b7 State-dependent bounds [10] t pwl t t (cid:2) (cid:3) \u2206d = 0 1 \u03c0 (s ) t (cid:2) (cid:3)od,t t a = \u2206v ;\u2206d t t t s \u223c\u03c4(\u00b7|s ,a ) t+1 t t r =r(s ,a ,s ) t t t t+1 B \u2190B\u222a{(s ,(\u2206v\u02dc,o ),r ,s )} t t d,t t t+1 end for for each gradient step do Sample batch B from B for all (s ,(\u2206v\u02dc,o ),r ,s )\u2208B do t t d,t t t+1 \u2206v\u02dc =\u00b5(s ;\u03b8\u2032) \u25b7 \u03c0\u2032 t+1 t+1 \u00b5 m,c \u03f5 \u223cN(0,\u03c32)| \u25b7 Target policy c c [\u2212c,c] smoothing [12] \u2206v\u02dc \u2190\u2206v\u02dc +\u03f5 t+1 t+1 c if s \u2208T then \u25b7 \u03c0 t+1 od,t m,d o = argmax q(s ,(\u2206v\u02dc ,o\u2032);\u03b8 ) d,t+1 t+1 t+1 d q,1 o\u2032 d\u2208Odav(s t+1) else o =o d,t+1 d,t end if y =r +\u03b3min q(s ,(\u2206v\u02dc ,o );\u03b8\u2032 ) t t i\u2208{1,2} t+1 t+1 d,t+1 q,i end for L(cid:98)q(\u03b8 q,i;B)= |B1 |(cid:80) B(cid:0) q(s t,(\u2206v\u02dc t,o d,t);\u03b8 q,i)\u2212y t(cid:1)2 L(cid:98)\u00b5(\u03b8 \u00b5;B)=\u2212 |B1 |(cid:80) (cid:80) q(s t,(\u00b5(s t;\u03b8 \u00b5),o d);\u03b8 q,1) B od\u2208Odav(s t) R(cid:98)s(\u03b8 \u00b5;B)= |B1 |(cid:80) B(cid:0) \u00b5(s t+1;\u03b8 \u00b5)\u2212\u2206v\u02dc t(cid:1)2 \u25b7 Smoothness regularization [9] \u03b8 q,i \u2190\u03b8 q,i\u2212\u03b7 q\u2207 \u03b8q,iL(cid:98)q(\u03b8 q,i;B) (cid:0) (cid:1) \u03b8 \u00b5 \u2190\u03b8 \u00b5\u2212\u03b7 \u00b5\u2207 \u03b8\u00b5 L(cid:98)\u00b5(\u03b8 \u00b5;B)+\u03bb sR(cid:98)s(\u03b8 \u00b5;B) \u03b8\u2032 \u2190\u03c4\u03b8 +(1\u2212\u03c4)\u03b8\u2032 Figure 7: Evaluation of the selected policies in various traffic m m m end for conditions and comparison with a conservative IDM/MOBIL end for policy. Solid lines represent the mean value over 10 episodes, the shaded areas denote the minimum and maximum values. 18 --- Page 19 --- Figure 8: Option activity for the selected policies over options during E = 10 evaluation episodes. The percentages denote the fraction of time spent with a certain option active. B Experimental",
  "introduction": "Introduction, volume 258 of A Bradford Book. jectory Planning for Autonomous Vehicles Using MIT Press, 2nd edition, Nov. 2018. ISBN 978-0- Hierarchical Reinforcement Learning. In 2021 262-35270-3.URL IEEE International Intelligent Transportation Sys- uWV0DwAAQBAJ. tems Conference (ITSC), pages 601\u2013606, Sept. 2021. [36] R. S. Sutton, D. Precup, and S. Singh. Between doi:10.1109/ITSC48978.2021.9564634. MDPsandsemi-MDPs: Aframeworkfortemporalab- [27] G.Neumann,W.Maass,andJ.Peters. Learningcom- straction in reinforcement learning. Artificial Intelli- plex motions by sequencing simpler motion templates. gence, 112(1):181\u2013211, Aug. 1999. ISSN 0004-3702. In Proceedings of the 26th Annual International Con- doi:10.1016/S0004-3702(99)00052-1. ference on Machine Learning, ICML 2009, ICML \u201909, [37] D. Tanneberg, K. Ploeger, E. Rueckert, and J. Pe- pages753\u2013760,NewYork,NY,USA,June2009.Asso- ters. SKID RAW: Skill Discovery From Raw Tra- ciation for Computing Machinery. ISBN 978-1-60558- jectories. IEEE Robotics and Automation Let- 516-1. doi:10.1145/1553374.1553471. ters, 6(3):4696\u20134703, July 2021. ISSN 2377-3766. doi:10.1109/LRA.2021.3068891. [28] M. Neunert, A. Abdolmaleki, M. Wulfmeier, T. Lampe, T. Springenberg, R. Hafner, F. Ro- [38] C. Tessler, S. Givony, T. Zahavy, D. Mankowitz, mano, J. Buchli, N. Heess, and M. Riedmiller. and S. Mannor. A Deep Hierarchical Approach to Continuous-Discrete Reinforcement Learning for Lifelong Learning in Minecraft. In Proceedings of Hybrid Control in Robotics. In Proceedings the 31st AAAI Conference on Artificial Intelligence, of the Conference on Robot Learning, volume AAAI 2017, volume 31, pages 1553\u20131561, Feb. 2017. 100, pages 735\u2013751. PMLR, May 2020. URL doi:10.1609/aaai.v31i1.10744.  [39] S. Thrun and A. Schwartz. Finding Structure in [29] S. Pateria, B. Subagdja, A.-h. Tan, and C. Quek. Hi- Reinforcement Learning. In Advances in Neural erarchical Reinforcement Learning: A Comprehensive Information Processing Systems 7, NIPS 1994, Survey.ACMComput.Surv.,54(5):109:1\u2013109:35,June volume 7, pages 385\u2013392. MIT Press, 1994. URL 2021. ISSN 0360-0300. doi:10.1145/3453160.  7ce3284b743aefde80ffd9aec500e085-",
  "methods": "methods use ated to Leuven.AI - KU Leuven institute for AI, B-3000, Leu- skills with a fixed time horizon and perform Semi-MDP ven,Belgium. Theresearchleadingtotheseresultshasreceived (option-to-option) updates. In contrast, our options are funding from the European Research Council under the Euro- predefined and have variable length, while the master poli- pean Union\u2019s Horizon 2020 research and innovation program / cies are trained using intra-option updates. This is cru- ERC Advanced Grant E-DUALITY (787960). This paper re- cial to support combined or hybrid options, and allows to flects only the authors\u2019 views and the Union is not liable for seamlessly combine slow lane change manoeuvres with fast any use that may be made of the contained information. The acceleration manoeuvres. resources and services used in this work were provided by the VSC (Flemish Supercomputer Center), funded by the Research 7",
  "experiments": "experiments. comparedtootherapproachesinmorecongestedtrafficsit- uations. Finally, due to the use of constrained options for Table 2: Overview of the used hyperparameters. lateral control, the hierarchical setups achieve better align- ment within the lane with almost no deviation from lane Hyperparameters Values center. Maximumepisodelength T 5000 Figure8showstheactivityofoptionsduringevaluationof (truncation) Totaltrainingepisodes 300 the selected (best performing) policies. These plots can be Warmuptimesteps 6400 usedtoanalyzethelearneddrivingbehaviourinsomemore Replaybuffersize |B| 1000000 detail, and thus increase the interpretability of the learned Batchsize |B| 64 modelsascomparedtothecontinuousbaselinemodels. For Discountfactor \u03b3 0.99 example, we can observe that the policy over combined op- Learningrates(strides) \u03b7q 5\u00b710\u22124(1) \u03b7\u00b5 5\u00b710\u22124(2) tions effectively uses the flexibility of combining longitudi- Targetnetworks nal and lateral manoeuvres, as it actively chooses to alter averagingconstant \u03c4 1\u00b710\u22123(2) the vehicle\u2019s velocity during lane changes \u223c 50% of the (stride) Networkarchitecture\u2013 (critic) 64\u00d732 time. Performing such an analysis or determining which hiddendimensions (actor) 32\u00d716\u00d78 manoeuvres are being performed, is much harder for poli- cies over continuous actions. In fact, only the immediate",
  "references": "reference signals selected by a policy over continuous ac- tions are known at any moment in time. In contrast, for the policies over options the exact manoeuvre selected by the master policy is known at every moment in time (and can be determined for any possible traffic situation on the road). Remark that for lateral control, the maintain and emer- gencyoptionsbothprovidethesameactiontomaintainthe lateral position (unless this would be unsafe). It seems the selected master policies have learned a slight preference for the (lateral) emergency option, which is unintended. This can be easily resolved by adding a small penalty to the re- 19",
  "results": "Results for",
  "related_work": "Related Work large datasets of interaction data [31, 37]. In contrast, in this work we specifically predefine a small set of options, tailored to the considered autonomous driving tasks, and Hierarchical and Hybrid Reinforcement apply them in various hierarchical control setups. From Learning thisperspective,ourmethodologyissimilartotheworksof Variousreinforcementlearningmethodstodealwithhierar- Neumannetal.[27]andDalaletal.[7],whichlearnamaster chicalandhybridactionspaceshavebeenproposed,mainly policyforselectingtheparametersofpredefinedparameter- differing in the specific structure of the action space or the ized skills. Although we apply more recent deep RL tech- consideredapplication. Neunertetal.[28]provideageneral niquesandapplyintra-optionlearning,whereastheseworks algorithm for finding optimal hybrid policies with mixed andothers[19,38]typicallyperformSemi-MDP(option-to- continuous and discrete action spaces. In other works, the option) updates. focus lies often on hybrid action spaces with a more par- Barretoetal.[5]andPengetal.[30]introducealgorithms ticular structure, such as parameterized action spaces in that allow the agent to execute a combination of learned 12 --- Page 13 --- options. Both approaches are more generic than our pro- change manoeuvres; hence, such an approach should add posed method for hierarchical control with combined op- appropriate deliberation costs on option interruption. An- tions. More precisely, these",
  "conclusion": "Conclusion Foundation - Flanders (FWO) and the Flemish Government. This paper introduced several options tailored to the au-"
}